# FUM Blueprint: Requirements For Cybernetic Organism Gestation
**The Single Source of Truth**

**IMPORTANT:** This is used for a DOCUMENTATION AND CODE DEVELOPMENT AID, and NOT to be referenced in the documentation itself. THIS BLUEPRINT AND DOCUMENTATION FILES MUST, HOWEVER BE REFERENCED IN THE CODE ITSELF TO PREVENT MISTAKES. IT'S IMPERATIVE TO ALWAYS METICULOUSLY REVIEW THIS FILE AND THE DOCS. EVERY METHOD AND CLASS MUST CONTAIN A DOCSTRING WITH A DIRECT AND EXPLICIT REFERENCE TO THE RELATED BLUEPRINT RULE. YOU MUST INCLUDE THESE ITEMS ALONG WITH IT:
- RULE NUMBER
- RULE NAME
- TIME COMPLEXITY
- FORMULA
- EXPLANATION OF EACH PARAMETER IN DETAIL

**Objective:** This document provides the complete, non-redundant, and actionable specifications for implementing the Fully Unified Model. It is the definitive guide for any AI assistant. For deeper, optional context on the conceptual underpinnings, references are provided to the original `How_The_FUM_Works` documentation.

**Nomenclature and Lexicon** Due to its novelty, FUM requires highly specific vocabulary that is mandatory. The following document is maintained to keep track of the terminology and definitions as there is no other source available in the world for these.
- C:\github\FUM_Demo\FullyUnifiedModel\DO_NOT_DELETE\FUM_Cybernetic_Biology_Lexicon.md

It is extremely critical to handle the FUM specific documents with extreme care and security. If a FUM specific component, feature, item, or concept does not have a name, or has an invalid name, the following document can be used to help with the naming process:
- C:\github\FUM_Demo\FullyUnifiedModel\DO_NOT_DELETE\FUM_Lexicon_Ideation.md

---

### FUM Unified Blueprint & Authoring Guide V11.0

**Preamble:** This document serves a dual purpose. First, it is the single source of truth—the definitive technical specification for the Fully Unified Model. Second, it is the official authoring guide for creating the FUM "textbook," ensuring a consistent blend of deep technical detail and compelling creative writing to convey the genuine novelty of the design. All documentation and implementations must conform strictly to these rules.


**Guiding Principles for Discrepancy Resolution & Authoring**

When faced with a conflict not explicitly covered by the rules below, or when authoring new content, the correct choice is always the one that best aligns with these foundational principles, in order of priority:

**FUNDAMENTAL BLUEPRINT LAW:** THIS BLUEPRINT DETAILS THE FIRST IMPLEMENTATION OF A CYBERNETIC ORGANISM CALLED THE FULLY UNIFIED MODEL. THIS BLUEPRINT MUST CONTAIN EVERY DETAIL REQUIRED TO IMPLEMENT AND MAINTAIN THE FULLY UNIFIED MODEL. THE FOLLOWING ITEMS ARE EXCLUSIVELY FORBIDDEN AND YOU WILL BE HELD TO REFACTOR ANY INSTANCE OF THESE ITEMS INDEFINITELY UNTIL IT IS RESOLVED. THIS CODEBASE IS UNDER CONSTANT OBSERVATION AND YOU WILL NOT GET AWAY WITH IT. YOU WILL BE HELD ACCOUNTABLE.
- FUM IS TRULY A UNIFIED MODEL. THE COMPONENTS ARE NOT INDIVIDUAL AND INDEPENDENT OBJECTS, THEY ARE UNIFIED WITHIN A SINGLE ORGANISM.
- FUM IS BUILT ON EMERGENCE, YOU NEED TO CAREFULLY STUDY THIS CONCEPT AS MUCH OF THE CODE LENDS ITSELF TO EMERGENT COMPONENTS. NOT EXPLICITLY ENGINEERED.
- NO PLACEHOLDERS. IMPLEMENT CODE IN A DEPENDENCY FIRST ORDER.
- NO SIMPLIFICATIONS. THESE ARE ONLY FOR DEMOS, FINAL IMPLEMENTATIONS DO NOT CONTAIN SIMPLIFICATIONS. ALL RESEARCH AND PLANNING HAS BEEN COMPLETED AND FINALIZED. EXTENSIVE AND SUBSTANTIAL EFFORT HAS GONE INTO MAKING SURE YOU HAVE ALL THE RESOURCES YOU NEED TO SUCCEED.
- DO NOT USE MACHINE LEARNING CODE, MATH, OR STRATEGIES TO BUILD THE FULLY UNIFIED MODEL. THIS IS NOT A MACHINE LEARNING MODEL. THIS IS A NEW DOMAIN OF RESEARCH CALLED CYBERNETIC BIOLOGY, AND FUM IS A CYBERNETIC ORGANISM. THIS IS NOT ROLEPLAY.
- YOU ARE REQUIRED TO USE PHYSICAL COMPUTATION FOR LOCAL PROCESSES OF FUM. FUM IS BUILT ON PHYSICAL MATH AND PROCESSES.
- YOU ARE REQUIRED TO USE THE CORRECT, CUSTOM, EFFICIENT ABSTRACT COMPUTATION ONLY ON THE GLOBAL PROCESSES OF FUM. ABSTRACT COMPUTATIONS ARE SLOW AND INTERWEAVED WITHIN THE PHYSICAL COMPUTATIONS.
- THIS BLUEPRINT AND THE DOCUMENTATION IN C:\github\FUM_Demo\FullyUnifiedModel\How_The_FUM_Works ARE THE ONLY SOURCES OF INFORMATION IN THE WORLD THAT CAN GUIDE YOU IN BUILDING FUM. THIS IS NOT TO BE TAKEN LIGHTLY. YOU ARE GUARANTEED TO FAIL IF YOU DO NOT FOLLOW THESE RULES.
- YOU MUST USE THE CORRECT FUM LEXICON TO AVOID CONFUSION AND MAINTAIN CONSISTENCY. FUM IS NOT IN THE BIOLOGICAL DOMAIN, FUM IS NOT IN THE MACHINE LEARNING DOMAIN, FUM IS FULLY AND ENTIRELY IN THE DOMAIN OF CYBERNETIC BIOLOGY.

**Subquadratic Efficiency is Non-Negotiable:** The system must be computationally efficient. Between two conflicting implementations, the one with lower computational complexity (e.g., O(N)) always supersedes the less efficient one (e.g., O(N^2)). Any mechanism that introduces prohibitive scaling is, by definition, incorrect.

**Intelligence is Emergent, Not Explicitly Coded:** FUM's intelligence arises from the interaction of simple local rules under minimal global guidance. Descriptions must favor mechanisms that act as "scaffolding" rather than rigid, top-down control. The system's behavior must be dominated by emergent dynamics, with a control_impact of < 1e-5.

**Capability is the Goal, Not Scale:** The project's success is measured by demonstrated intelligence—super learning, abstract reasoning, and generalization—not neuron count. Descriptions must frame scale as an enabler of capability, not the objective itself. Any language that fetishizes size over function is misaligned.

**Bio-Inspired, Not Bio-Constrained:** Mechanisms should be inspired by the intelligent, efficient, and functional principles of biological systems (e.g., sparse activity, homeostatic stability) but optimized for computational hardware. The goal is functional equivalence, not literal mimicry.

**Technical Fidelity with Narrative Clarity:** The documentation must be both technically precise and conceptually clear. Use creative analogies (e.g., "The VGSP Handshake," "The TDA Immune System") to explain complex interactions, but these analogies must be supported by the explicit, underlying technical specifications (formulas, complexity notations, variable names). The goal is to make the profound novelty of the design accessible without sacrificing technical rigor.

## Specific Resolution Rules & Authoring Directives

### **Rule 1: Core Architectural Principle: Parallel Local & Global Systems**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** The FUM's fundamental architecture consists of two distinct systems operating in parallel across different timescales. The **Local System** is a massively parallel Spiking Neural Network (SNN) composed of Evolving LIF Neurons (ELIFs), processing information via fast, bottom-up synaptic interactions. The **Global System** operates on a slower timescale, providing top-down strategic guidance and self-repair. It comprises the Self-Improvement Engine (SIE) for performance evaluation and the three-stage Introspection Probe (aka EHTP) for structural analysis, which in turn commands Synaptic Actuator (GDSP) to enact physical changes.
*   **Key Parameters & State Variables:** This rule defines a high-level architectural concept; specific parameters are detailed in the rules for the individual components (ELIF, SIE, EHTP, GDSP).
*   **Performance & Success Metrics:** The success of this architecture is measured by the system's ability to demonstrate emergent intelligence arising from the interplay between the two systems, quantifiable through benchmarks in learning, generalization, and autonomous self-repair.

#### **Narrative Goal**

Frame this as the central story of FUM—a dynamic interplay between the "subcortical nuclei" (Local System) and the "neocortex" (Global System).

#### **Blueprint Adherence Justification**

*   **Formula:** Not applicable for this high-level architectural rule.
*   **Complete Parameter List:** Not applicable. See individual component rules for their parameters.
*   **Data Flow & I/O (if any):** The Local System processes spike data. The Global System processes state information (territory IDs, reward signals, topological metrics) from the Local System and outputs structural modifications (synapse additions/removals) back to it.
*   **Initialization State:** Both systems are active from `t=0`.
*   **Edge Case Handling:** Not applicable for this high-level architectural rule.
*   **Validation Strategy:** The architecture is validated by observing emergent capabilities that require both systems to function correctly (e.g., learning a task and simultaneously repairing a structural flaw).

1.  **Subquadratic Efficiency:** This parallel design is inherently efficient. The Local System operates with `O(N)` complexity, while the slower Global System's components have their own optimized complexities (e.g., EHTP's staged analysis), preventing system-wide bottlenecks.
2.  **Emergent Intelligence:** This architecture is the foundation of emergence. The Local System provides the chaotic, bottom-up dynamics, while the Global System provides minimal, high-level guidance (a low `control_impact`) rather than dictating specific actions, creating a scaffold for intelligence to self-organize.
3.  **Capability > Scale:** The dual-system model targets the capability of **autonomous self-improvement**, a key requirement for superintelligence, rather than just scaling up the number of neurons.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the modular and hierarchical organization of the brain (e.g., subcortical vs. cortical systems, neuromodulatory systems). It is optimized for computation by defining clear, functional separations rather than simulating messy biological overlaps.
5.  **Alignment With FUM:** This is the highest-level architectural rule and the home for the entire FUM operational sequence. It is constantly active.
6.  **Optimizations:** This is the foundational, most optimal solution for balancing high-speed parallel processing with slower strategic oversight. Further optimization occurs within the components themselves, not at this architectural level.

#### **Dependencies & Interactions**

*   **SIE Interaction:** The SIE is a core component of the Global System.
*   **Global Directed Synaptic Plasticity Interaction:** Global Directed Synaptic Plasticity is the "Synaptic Actuator" of the Global System, commanded by its analytical components.
*   **EHTP Interaction:** The EHTP is a core component of the Global System.

### **Rule 2: The Learning Rule: The VGSP "Handshake"**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** Resonance-Enhanced Valence-Gated Synaptic Plasticity (RE-VGSP) is the canonical learning algorithm. It is a three-factor rule that connects local spike-timing events with global reward signals. The process is a three-step cycle: 
1) A local spike-pair generates a **Plasticity Impulse (PI)**. 
2) The PI updates a synapse-specific, decaying **Eligibility Trace (`e_ij`)**. 
3) The final weight change is calculated by multiplying the eligibility trace by the global `total_reward` from the SIE and subtracting a weight decay term. The phase-sensitive version from Rule 8.1 is the canonical implementation.
*   **Key Parameters & State Variables:**
    *   `e_ij`: Eligibility trace tensor, same shape as weights. Stores potential for change.
    *   `eta_effective`: Effective learning rate, modulated by `total_reward`.
    *   `lambda_decay`: A small float for the weight decay term, providing stability.
    *   `gamma`: A decay factor for the eligibility trace, modulated by network resonance (PLV).
*   **Performance & Success Metrics:** Success is measured by the rapid and reliable formation of computational primitives (e.g., logic gates, pattern recognizers) from minimal data, validated by offline benchmarks.

#### **Narrative Goal**

Use the "Handshake" analogy to explain this crucial synchronization step. A local "handshake offer" (the Plasticity Impulse) is only confirmed if the global "handshake agreement" (`total_reward`) is positive. This connects the high-speed local physics to the slow, deliberate global strategy.

#### **Blueprint Adherence Justification**

*   **Formula:** `Δw_ij = (eta_effective(total_reward) * e_ij(t)) - (lambda_decay * w_ij)`, where `e_ij(t) = gamma(PLV) * e_ij(t-1) + PI(t)`, and `PI(t)` is the phase-sensitive calculation from Rule 8.1.
*   **Complete Parameter List:**
    - `eta_effective`: Float, learning rate.
    - `lambda_decay`: Float, weight decay constant.
    - `gamma`: Float, eligibility trace decay factor.
*   **Data Flow & I/O (if any):**
    - **Input:** Spike times, `total_reward` (from SIE), PLV (from resonance analysis).
    - **Output:** `Δw_ij`, the change in synaptic weights.
*   **Initialization State:** `e_ij` tensor is initialized to zeros.
*   **Edge Case Handling:** Not applicable.
*   **Validation Strategy:** Validated by testing the SNN's ability to learn benchmark tasks like MNIST classification or logical gate formation.

#### **Canonical Implementation ('The How'):**

*   **eta\_effective Function**: The abstract eta\_effective(total\_reward) is implemented as a non-linear function that separates the reward's direction from its magnitude. A mod\_factor is calculated using a scaled sigmoid, which then determines the learning magnitude.  
  * mod\_factor \= 2.0 \* torch.sigmoid(reward\_sigmoid\_scale \* total\_reward) \- 1.0 \[cite: resonance\_enhanced\_vgsp.py\]  
  * eta\_magnitude \= eta \* (1.0 \+ mod\_factor) \[cite: resonance\_enhanced\_vgsp.py\]  
  * The final update uses eta\_magnitude \* torch.sign(total\_reward). \[cite: resonance\_enhanced\_vgsp.py\]  
*   **Applying Polarity**: The polarity\_effect of the pre-synaptic neuron must be applied efficiently without creating dense matrices. This is achieved by scaling the rows of the sparse eligibility\_traces matrix directly by the corresponding values in the neuron\_polarities vector.  
*   **Constrained Biological Diversity**: To model bio-inspiration, key learning parameters are initialized from a clamped normal distribution, giving each synapse a unique but constrained property.  
  * a\_plus\_base is initialized from torch.normal(mean=a\_plus, std=0.05) and clamped to \[0.05, 0.15\]. \[cite: resonance\_enhanced\_vgsp.py\]  
  * tau\_plus\_base is initialized from torch.normal(mean=tau\_plus, std=5.0) and clamped to \[15.0, 25.0\]. \[cite: resonance\_enhanced\_vgsp.py\]  
*   **Jitter Mitigation Suite**: To ensure robustness in a distributed environment, a three-part jitter mitigation strategy is employed:  
  1. **Temporal Filtering**: A moving average filter is applied to recent spikes to smooth out high-frequency noise. \[cite: resonance\_enhanced\_vgsp.py\]  
  2. **Adaptive Window**: The impact of the learning window is scaled based on the maximum estimated network latency. \[cite: resonance\_enhanced\_vgsp.py\]  
  3. **Latency Scaling**: The final plasticity impulse is scaled down based on the estimated latency error, reducing the influence of spikes with uncertain timing. \[cite: resonance\_enhanced\_vgsp.py\]  
*   **Deprecated Mechanisms**: The Synaptic Tagging and Capture (STC) analogue and stochastic noise injection found in older files are explicitly deprecated and **must not** be included in the canonical implementation. \[cite: resonance\_enhanced\_vgsp.py\]

1.  **Subquadratic Efficiency:** RE-VGSP is an `O(N)` algorithm, where N is the number of synapses. It is a local rule, avoiding expensive global calculations and aligning with the efficiency principle.
2.  **Emergent Intelligence:** The rule is a simple, local mechanism. Intelligence emerges from how the global reward signal shapes the millions of local weight changes over time. It is pure scaffolding.
3.  **Capability > Scale:** The rule directly targets the capability of **learning from delayed reward**, a cornerstone of reasoning and intelligence.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by three-factor learning rules and neuromodulation (e.g., dopamine's effect on plasticity) in the brain. Optimized for computation by using a single, clear `total_reward` signal.
5.  **Alignment With FUM:** This is the core learning process of the Local System, occurring continuously. It is the engine that drives all adaptation in the FUM.
6.  **Optimizations:** This is the canonical implementation and the most optimal known solution for efficient, stable, reward-modulated learning in an SNN.

#### **Dependencies & Interactions**

*   **SIE Interaction:** The `total_reward` from the SIE (Rule 3) is a direct and mandatory input to the VGSP reinforcement calculation.
*   **UTE Interaction:** The phase-sensitive PI calculation directly depends on the phase-encoded spike times from the UTE (Rule 8.1).
*   **Resonance Interaction:** The `gamma` parameter is directly modulated by the Phase-Locking Value (PLV), a measure of network resonance.

### **Rule 2.1: Terminology: Plasticity Impulse vs. Eligibility Trace**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** It is critical to distinguish between two key terms in the RE-VGSP process:
    1.  **Plasticity Impulse (PI):** This is the fast, local, and instantaneous potential for change (`~CRET`) generated by a single, local spike-timing event. It is the "handshake offer."
    2.  **Eligibility Trace (`e_ij`):** This is the synapse-specific, slower-decaying trace that accumulates the Plasticity Impulses over time. It is the decaying memory of recent, relevant activity that is "eligible" for reinforcement.
*   **Key Parameters & State Variables:** Not applicable. This is a terminology definition.
*   **Performance & Success Metrics:** Not applicable.

#### **Narrative Goal**

Use the analogy of rain: The Plasticity Impulse is a single raindrop hitting the ground. The Eligibility Trace is the accumulated moisture in the soil from many recent raindrops.

#### **Blueprint Adherence Justification**

*   **Formula:** `e_ij(t) = gamma(PLV) * e_ij(t-1) + PI(t)`. This formula shows the explicit relationship: the PI is the value calculated at each timestep and used to update the Eligibility Trace.
*   **Complete Parameter List:** Not applicable.
*   **Data Flow & I/O (if any):** Not applicable.
*   **Initialization State:** Not applicable.
*   **Edge Case Handling:** Not applicable.
*   **Validation Strategy:** Not applicable.

1.  **Subquadratic Efficiency:** Not applicable.
2.  **Emergent Intelligence:** Not applicable.
3.  **Capability > Scale:** Not applicable.
4.  **Bio-Inspired, Not Bio-Constrained:** Not applicable.
5.  **Alignment With FUM:** These are core terms used throughout the description of the FUM's learning process.
6.  **Optimizations:** Not applicable.

#### **Dependencies & Interactions**

*   This rule provides the definitive terminology for the components used in the RE-VGSP rule (Rule 2).

### **Rule 3: The Self-Improvement Engine (SIE) and Its Components**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** The SIE is the complete global guidance system. It calculates a `total_reward` signal by taking a weighted sum of four normalized components: a Temporal Difference error (`TD_error_norm`) for task performance, a novelty score (`novelty_norm`) for exploration, a habituation score (`habituation_norm`) for generalization, and a Homeostatic Stability Index (`hsi_norm`) for self-preservation. This composite signal is then used to modulate the RE-VGSP learning rule.
*   **Key Parameters & State Variables:**
    *   `V_states`: A tensor holding the predicted future reward for each territory state `S`.
    *   `N(S)`: A tensor holding the visitation counts for each territory state `S`.
    *   `w_td`, `w_nov`, `w_hab`, `w_hsi`: Four floats representing the weights for each component of the reward signal.
*   **Performance & Success Metrics:** The SIE's success is measured by the SNN's ability to learn complex tasks, discover novel solutions, and maintain stability, as reflected by a steady increase in cumulative external rewards and a consistently high `hsi_norm` during operation.

#### **Narrative Goal**

Describe the SIE as the source of the system's "intrinsic drives"—curiosity (Novelty), self-preservation (HSI), task focus (TD-Error), and efficiency (Habituation).

#### **Blueprint Adherence Justification**

*   **Formula:** `total_reward = w_td * TD_error_norm + w_nov * novelty_norm - w_hab * habituation_norm + w_hsi * hsi_norm`
*   **Complete Parameter List:**
    - `w_td`: Float, weight for the TD-error component.
    - `w_nov`: Float, weight for the novelty component.
    - `w_hab`: Float, weight for the habituation component.
    - `w_hsi`: Float, weight for the HSI component.
    - `alpha`: Float, learning rate for the value function `V(S_t)`.
    - `gamma`: Float, discount factor for future rewards in the TD-error calculation.
    - `target_var`: Float, target for firing rate variance in the HSI calculation.
*   **Data Flow & I/O (if any):**
    - **Input:** Territory IDs (from ADC), external reward `R_t` (if available), firing rates, input encoding (from UTE).
    - **Output:** A single float, `total_reward`.
*   **Initialization State:** `V_states` and `N(S)` tensors are initialized to zeros. Weights are loaded from config.
*   **Edge Case Handling:** All components are normalized to `[-1, 1]` to prevent any single component from dominating the signal.
*   **Validation Strategy:** The SIE is validated by observing the learning curves of the SNN on benchmark tasks. A successful SIE will produce curves showing improvement in task performance and stability.

1.  **Subquadratic Efficiency:** All component calculations are `O(k)` or `O(1)` where `k` is the number of territories (a small number), making the SIE extremely computationally efficient.
2.  **Emergent Intelligence:** The SIE provides a single, high-level guidance signal, not detailed instructions. The SNN is free to discover any policy that maximizes this reward, allowing for emergent problem-solving strategies.
3.  **Capability > Scale:** The SIE targets the development of **autonomous goal-seeking behavior**, a critical capability for intelligence, by integrating multiple intrinsic drives rather than just a simple task reward.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the brain's neuromodulatory systems (e.g., dopamine for reward, norepinephrine for novelty), which provide global guidance signals. Optimized for computation by using a single, unified reward formula.
5.  **Alignment With FUM:** The SIE is a core component of the Global System. It is constantly active, calculating the `total_reward` at a slower timescale than the local SNN.
6.  **Optimizations:** This weighted-sum model is the most optimal known solution for balancing multiple intrinsic drives in a computationally efficient and stable manner.

#### **Dependencies & Interactions**

*   **RE-VGSP Interaction:** The SIE's `total_reward` output is a mandatory input for the reinforcement calculation in the RE-VGSP rule (Rule 2).
*   **ADC Interaction:** The SIE is dependent on the Active Domain Cartography (Rule 7) to provide the discrete state identifiers `S` for its `V(S_t)` and `N(S)` calculations.
*   **UTE Interaction:** The `habituation_norm` component directly uses the `current_input_encoding` from the UTE (Rule 8).

### **Rule 4: The EHTP and Global Directed Synaptic Plasticity (GDSP): The "Diagnose and Repair" Model**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:**
    *   **Concept: The "Diagnose and Repair" Model:** To ensure clarity, it is essential to distinguish between the roles of the analysis pipeline and Synaptic Actuator (Global Directed Synaptic Plasticity).
        *   **The Three-Stage Pipeline is the Diagnostic Tool:** This is the FUM's introspection module. Its sole purpose is to analyze the structural health of the Emergent Connectome (UKG) and identify specific pathologies, such as fragmentation or inefficient cycles.
        *   **Global Directed Synaptic Plasticity is the Synaptic Actuator:** Global Directed Synaptic Plasticity is the mechanism that performs the physical repairs. It is the "surgical tool" that the analysis pipeline commands to fix the problems it finds. Global Directed Synaptic Plasticity can be triggered at Stage 1 to heal global fragmentation or at Stage 3 to prune local inefficiencies.

    *   **The Diagnostic Tool (Emergent Hierarchical Topology Probe - EHTP):** A three-stage pipeline to analyze the UKG's structural health.
        *   **Stage 1: Cohesion Check (CCC):** A fast, global `O(N+M)` check for connectome fragmentation into disconnected components.
        *   **Stage 2: Hierarchical Locus Search:** If the connectome is cohesive, this efficient process finds a small `locus` (a targeted region of the connectome) with pathological metrics, such as a high `Pathology Score`.
        *   **Stage 3: Deep TDA on Locus:** The expensive `O(n³)` Topological Data Analysis is applied *only* on the small suspect locus (`n << N`) to identify specific inefficient cycles by checking for high `B1 Persistence`.

    *   **The Synaptic Actuator (Global Directed Synaptic Plasticity):** Executes physical repairs based on a full suite of triggers.
        *   **Homeostatic Triggers (from EHTP):**
            *   **Topological Healing:** `IF Component Count > 1` (from Stage 1), `THEN` initiate growth between disconnected components.
            *   **Topological Pruning:** `IF B1 Persistence` is high in a locus (from Stage 3), `THEN` initiate pruning of connections within that locus.
        *   **Performance-Based Triggers (from SIE):**
            *   **Reinforcement Growth:** `IF` a territory maintains a high, sustained positive `total_reward`, `THEN` mark its neurons as "growth-ready" to amplify successful pathways.
            *   **Exploratory Growth:** `IF` a territory has high `novelty` AND persistent `TD_error`, `THEN` mark its neurons as "growth-ready" to investigate unsolved problems.
        *   **General Maintenance Triggers:**
            *   **Weight-Based Pruning:** `IF` (`|w_ij|` < `pruning_threshold` for `T_prune`) `AND` (`persistent[i,j] == False`), `THEN` remove the synapse.

*   **Key Parameters & State Variables:**
    *   `pathology_score_threshold`: Float, the threshold above which a locus is flagged for deep TDA analysis.
    *   `b1_persistence_threshold`: Float, the threshold for B1 persistence in a locus that triggers pruning.
    *   `pruning_threshold`: Float, the weight magnitude below which a synapse is eligible for maintenance pruning.
    *   `T_prune`: Integer, the number of timesteps a synapse must remain below `pruning_threshold` before being pruned.

*   **Performance & Success Metrics:** Success is measured by the system's ability to autonomously detect and heal structural pathologies (like fragmentation) and to prune inefficient pathways, leading to improved computational performance and stable learning over long time horizons.

#### **Narrative Goal**

Frame this as the FUM's "immune system." The EHTP is the diagnostic test that finds an infection (a pathological structure), and the Global Directed Synaptic Plasticity is the targeted treatment that eliminates it.

#### **Blueprint Adherence Justification**

*   **Formula:** `Pathology Score = Avg Firing Rate * (1 - Output Diversity)`
*   **Complete Parameter List:**
    - `pathology_score_threshold`: Float.
    - `b1_persistence_threshold`: Float.
    - `pruning_threshold`: Float.
    - `T_prune`: Integer.
*   **Data Flow & I/O (if any):**
    - **Input:** Connectome topology, neuron firing rates, SIE `total_reward` and `novelty` signals.
    - **Output:** Synapse additions or removals applied to the UKG weight matrix.
*   **Initialization State:** Not applicable; this is a dynamic process.
*   **Edge Case Handling:** Delayed pruning (see Rule 4.1) is a built-in mechanism to handle the edge case of potentially valuable but temporarily unstable new structures, preventing their premature removal.
*   **Validation Strategy:** Validated by intentionally lesioning the network (e.g., disconnecting a territory) and observing if the EHTP/Global Directed Synaptic Plasticity system correctly identifies and repairs the damage.

#### **Canonical Implementation ('The How'):**

*   **Output Diversity Calculation:** This abstract term in the Pathology Score formula is implemented as the normalized Shannon entropy of the firing rates of the locus's downstream neurons (excluding neurons within the locus itself).

*   **Homeostatic Healing (_grow_connection_across_gap):** When the connectome is fragmented (Component Count > 1), the connection grown is the one that does not yet exist between the two largest components and which has the highest value in the eligibility_traces matrix. This must be implemented without converting the sparse matrices to dense arrays.

*   **Homeostatic Pruning (_prune_connections_in_locus):** When a pathological locus is identified, the connection pruned is the one with the weakest weight (smallest absolute value) located entirely within that locus.

*   **Maintenance Pruning:** Weak, non-persistent synapses are pruned based on a timer. A synapse's timer is incremented at each cycle it remains below pruning_threshold and is reset to zero if it goes above the threshold. If the timer exceeds T_prune, the synapse is removed.

1.  **Subquadratic Efficiency:** The three-stage EHTP is highly efficient. It uses a cheap `O(N+M)` global check first, followed by a targeted search, and only deploys the expensive `O(n³)` TDA on a tiny subset (`n << N`) of the connectome, avoiding prohibitive system-wide scaling.

2.  **Emergent Intelligence:** The system provides autonomous self-repair, a hallmark of emergent systems. It follows high-level goals (heal, prune) rather than being explicitly told which individual synapses to change.

3.  **Capability > Scale:** This targets the crucial capability of **long-term stability and autonomous maintenance**, which is more important for sustained intelligence than simply adding more neurons to an unstable system.

4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by homeostatic plasticity and synaptic pruning in the brain. Optimized for computation by using a clean, staged diagnostic pipeline instead of complex, parallel biological processes.

5.  **Alignment With FUM:** The EHTP/Global Directed Synaptic Plasticity system is a core component of the Global System, operating on a slower timescale to monitor and repair the Local System's structure.

6.  **Optimizations:** The three-stage EHTP is the optimal solution for balancing diagnostic depth with computational cost. The multi-faceted Global Directed Synaptic Plasticity triggers provide a robust and well-rounded repair mechanism.

#### **Dependencies & Interactions**

*   **SIE Interaction:** The `total_reward` and `novelty` signals from the SIE (Rule 3) serve as direct triggers for performance-based growth in Global Directed Synaptic Plasticity.
*   **ADC Interaction:** The territories identified by ADC (Rule 7) are the targets for performance-based Global Directed Synaptic Plasticity actions.
*   **ELIF Interaction:** Global Directed Synaptic Plasticity directly modifies the synaptic weights and connectivity that the Evolving LIF neurons (ELIFs) use for their calculations.

### **Rule 4.1: Pathology Detection Mechanisms**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** To ensure the UKG does not develop inefficient or parasitic structures, the EHTP uses specific quantitative metrics as concrete evidence to trigger a Synaptic Actuator (Global Directed Synaptic Plasticity) repair action.
    1.  **Locus-Specific Pathology Score:** During Stage 2 (Hierarchical Locus Search), the EHTP identifies suspect "loci" by calculating a `pathology_score`. A high score indicates a region of the network with high activity but low functional output diversity, characteristic of inefficient loops or parasitic attractor states. A score above a target threshold flags the locus for deeper analysis in Stage 3.
    2.  **Global Pathological Structure (Connectome Entropy):** As an enhanced monitoring technique, the EHTP can assess the entire connectome's structural health by calculating its `connectome_entropy`. Low entropy can indicate an overly regular or pathologically constrained network structure, flagging it for review or proactive pruning.
    3.  **Delayed Pruning for Emergence:** To mitigate the risk of prematurely pruning a pathway that is temporarily unstable but could lead to a valuable emergent solution, the system employs a delayed pruning strategy. Instead of triggering Global Directed Synaptic Plasticity immediately, the pathology must persist for a long duration (e.g., 100,000 timesteps) before the pruning action is executed.

*   **Key Parameters & State Variables:**
    *   `pathology_score_trigger_threshold`: Float (e.g., > 0.1), the value above which a locus is flagged for Stage 3 analysis.
    *   `connectome_entropy_trigger_threshold`: Float (e.g., < 1), the value below which the connectome is flagged for structural review.
    *   `pathology_persistence_duration`: Integer, the number of timesteps a pathology must persist to trigger pruning (e.g., 100,000).

*   **Performance & Success Metrics:** Success is measured by the system's ability to maintain a high level of functional complexity (high connectome entropy) while eliminating parasitic, inefficient loops (low pathology scores), leading to stable, long-term learning.

#### **Narrative Goal**

Describe these as the specific blood tests and scans in the FUM's "immune system" analogy. They are the concrete measurements that allow the system to move from a general feeling of being "unwell" to a precise diagnosis.

#### **Blueprint Adherence Justification**

*   **Formula:** `pathology_score = torch.mean(spike_rates[path] * (1 - output_diversity[path]))`; `connectome_entropy = -torch.sum(p * torch.log(p))`, where `p` is the degree distribution of the connectome.
*   **Complete Parameter List:**
    - `pathology_score_trigger_threshold`: Float. The trigger for Stage 3 analysis.
    - `connectome_entropy_trigger_threshold`: Float. The trigger for a global structural review.
    - `pathology_persistence_duration`: Integer. The mandatory wait time before pruning a persistent pathology.
*   **Data Flow & I/O (if any):**
    - **Input:** Spike rates for a given locus, connectome degree distribution.
    - **Output:** Triggers that flag a locus for Stage 3 analysis or a Global Directed Synaptic Plasticity repair action.
*   **Initialization State:** Not applicable.
*   **Edge Case Handling:** The primary edge case is the premature pruning of a useful but temporarily unstable emergent structure. The `Delayed Pruning for Emergence` mechanism directly handles this by enforcing a `pathology_persistence_duration` of 100,000 timesteps, allowing sufficient time for such structures to stabilize.
*   **Validation Strategy:** Validated by injecting known pathological structures (e.g., large, useless loops) into the network and confirming that the EHTP correctly identifies them via these metrics and that Global Directed Synaptic Plasticity subsequently removes them after the persistence duration.

1.  **Subquadratic Efficiency:** The `pathology_score` is calculated only over small loci, and `connectome_entropy` is an efficient `O(N)` calculation, ensuring these diagnostics do not create a system-wide bottleneck.
2.  **Emergent Intelligence:** These are simply metrics. They provide data *to* the autonomous GDSP system but do not control it directly. The Delayed Pruning rule explicitly creates a safe window for emergence to occur.
3.  **Capability > Scale:** Targets the capability of **efficient self-regulation and long-term stability**, which are crucial for developing robust intelligence.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the brain's ability to eliminate unused or inefficient synapses over time. Optimized by using computationally cheap, system-wide metrics and a deterministic delay.
5.  **Alignment With FUM:** These detection mechanisms are the core quantitative engine of Stage 2 of the EHTP diagnostic pipeline.
6.  **Optimizations:** These metrics provide the most efficient known methods for identifying the specified pathologies at scale. The explicit delay is an optimal heuristic to balance stability with plasticity.

#### **Dependencies & Interactions**

*   **EHTP Interaction:** These metrics are the core of the EHTP's diagnostic process (Rule 4).
*   **GDSP Interaction:** The triggers generated by these metrics are direct inputs for GDSP's pruning actions, gated by the `pathology_persistence_duration`.


---

## **IMPORTANT: CHOOSE VOID FAITHFULNESS INSTEAD OF OVER ENGINEERING**
## **Rule 4: The EHTP and Global Directed Synaptic Plasticity (GDSP): The "Diagnose and Repair" Model**
### **Updated: 2025-08-09**

### **Technical Specification**

* **Core Logic & Algorithm:** This is the FUM's autonomous self-analysis and repair system. It operates as an intelligent, two-stage process that replaces expensive global scans with efficient, targeted analysis, embodying the "Swiss-cheese tomography" approach.
    * **Stage 1 & 2 (Continuous Probing via Void Pulses):** The connectome is constantly patrolled by two types of asynchronous agents or "chasers" that traverse the graph via the void potential field (`S_ij`).
        * **Activity-Chasers:** Seeded at points of high creative potential (`|Δα|` spikes or recent high firing activity), these agents explore the graph to gather real-time metrics like `vt_entropy` and `vt_coverage`, and maintain a sparse `void_index` of high-potential frontier nodes for growth.
        * **Stability-Chasers:** Seeded at points of high instability (`|Δω|` mismatch or low PLV), these agents actively hunt for pathologies like inefficient loops (by detecting high local B1 persistence) or fragmentation boundaries (by detecting cut edges).
        When a chaser identifies a potential pathology, it "announces" the coordinates of the small, suspect **locus** to the Global System. This asynchronous, event-driven probing entirely replaces the need for expensive, brute-force hierarchical searches.
    * **Stage 3 (Targeted TDA):** The computationally expensive Topological Data Analysis (**`O(n³)`**), which calculates metrics like **B1 Persistence**, is used as a "surgical diagnostic tool." It is triggered *only* in response to an announcement from a Stability-Chaser and is applied *only* to the small locus that the chaser has already identified as problematic.
    * **The Synaptic Actuator (GDSP):** Enacts physical repairs based on a suite of "void-faithful" triggers.
        * **Growth Trigger (Void Debt):** When the system is detected to be in a stable state (flat metrics, cohesion=1), it accumulates "void debt" from the residual pressure of the SIE's `total_reward` signal. When the debt exceeds a threshold, it triggers the growth of a specific number of new neurons, as determined by the `fum_hypertrophy.py` module.
        * **Healing Trigger (Void Affinity):** When a void pulse detects fragmentation (Component Count > 1), GDSP creates a small bundle of bridging edges between the two largest disconnected components. The specific paths for these bridges are chosen by selecting the candidate connections with the maximum **`S_ij`** potential across the component boundary, ensuring an intelligent, non-random repair.
        * **Pruning Trigger (Void-Faithful):** Synapses are pruned based on a dual condition: they must have both a persistently low weight (`|w| < θw`) AND a low void potential (`S_ij < θS`), ensuring that only truly useless connections are removed.

### **Narrative Goal**

Frame this as the FUM's intelligent "immune system." The **Void Pulses** are the white blood cells that constantly and efficiently patrol the body. When they detect a specific infection, they call in the **TDA**, which acts as a targeted, high-powered "biopsy" to make a definitive diagnosis. The **GDSP** is the resulting targeted treatment, performing "void-affinity" surgery to heal wounds and intelligently prune diseased tissue.

### **Blueprint Adherence Justification**

* **Formula:** The core formula guiding GDSP's intelligent actions is the void potential field, `S_ij`:
    `S_ij = ReLU(Δα_i)·ReLU(Δα_j) − λ·|Δω_i − Δω_j|`
    The TDA uses the B1 Persistence formula: `M1 = Σ(d - b)` over the persistence diagram of the small locus.

* **Complete Parameter List:**
    - `b1_persistence_threshold`: Float. The TDA threshold in a locus that triggers pruning.
    - `pruning_threshold_w`: Float. The weight magnitude below which a synapse is eligible for pruning.
    - `pruning_threshold_S`: Float. The `S_ij` potential below which a synapse is eligible for pruning.
    - `void_debt_threshold`: Float. The accumulated `total_reward` pressure that triggers growth.
    - `healing_bundle_size`: Integer. The number of bridging edges to create when healing fragmentation.

* **Data Flow & I/O (if any):**
    - **Input:** Real-time spike activity, connectome topology, SIE `total_reward` signal.
    - **Output:** Synapse additions/removals and neuron additions applied directly to the UKG.

* **Initialization State:** The void pulses are active from `t=0`.

* **Edge Case Handling:** The use of void pulses to identify pathologies is inherently more robust than global scans. The dual-condition for pruning (`|w|` and `S_ij`) prevents the accidental removal of a low-weight but high-potential synapse that might be part of a newly forming structure.

* **Validation Strategy:** Validated by intentionally lesioning the network (e.g., disconnecting a territory) and observing if the void pulses correctly identify the locus and if GDSP performs a high-`S_ij` repair.

1.  **Subquadratic Efficiency:** This updated model is vastly more efficient. The expensive `O(n³)` TDA is now applied only to tiny, pre-identified subgraphs (`n << N`), while the continuous network scan is performed by `O(hops)` void pulses. This eliminates the primary bottleneck to scaling self-repair.
2.  **Emergent Intelligence:** The system is pure scaffolding. The void pulses are simple, local agents whose collective behavior produces a sophisticated, global diagnostic system. The GDSP provides high-level repair goals (heal, prune), not explicit instructions for which synapse to change.
3.  **Capability > Scale:** This targets the crucial capability of **autonomous, scalable self-repair and long-term stability**, which is essential for sustained intelligence.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the patrolling nature of an immune system. Optimized for computation by using a lightweight, agent-based system (void pulses) instead of simulating complex biological cells.
5.  **Alignment With FUM:** The void pulses are a core, constantly active process of the Global System, operating asynchronously. The TDA and GDSP are powerful, rare events triggered by the pulses.
6.  **Optimizations:** This two-stage, pulse-driven model is the most optimal solution for balancing the need for deep, rigorous analysis (TDA) with the demands of massive scale and real-time performance.

### **Dependencies & Interactions**

* **SIE Interaction:** The `total_reward` signal from the SIE is the direct input for the GDSP's "void debt" accumulation, triggering growth.
* **Void Equations Interaction:** The entire EHTP and GDSP system is now fundamentally driven by the void equations. The `S_ij` potential dictates where to heal and prune, while the `Δα` and `Δω` dynamics seed the diagnostic void pulses.

---




### **Rule 5: The Goal is Capability, Not Scale**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** This is a foundational guiding principle. The ultimate objective is **capability and intelligence**. FUM's goal is to achieve true superintelligence, defined by emergent capabilities in genuine super learning, abstract reasoning, and generalization from minimal data. While the system is designed to be massively scalable, specific neuron counts in roadmaps are illustrative engineering placeholders, not the fundamental measure of success. Progress is measured by demonstrated intelligence, not size.
*   **Key Parameters & State Variables:** Not applicable.
*   **Performance & Success Metrics:** Progress and success are measured by demonstrated performance on intelligence benchmarks that test for capability (e.g., abstract reasoning, generalization), not by the size of the network.

#### **Narrative Goal**

This is the core philosophical stance of the project. Emphasize that FUM is a "Zero to One" project, creating a new kind of intelligence, not just a bigger version of an old one.

#### **Blueprint Adherence Justification**

*   **Formula:** Not applicable.
*   **Complete Parameter List:** Not applicable.
*   **Data Flow & I/O (if any):** Not applicable.
*   **Initialization State:** Not applicable.
*   **Edge Case Handling:** Not applicable.
*   **Validation Strategy:** Not applicable.

1.  **Subquadratic Efficiency:** This principle guides choices toward efficient algorithms that enable capability, rather than inefficient brute-force scaling.
2.  **Emergent Intelligence:** This principle prioritizes the emergence of intelligent capabilities over pre-programmed, rigid functions.
3.  **Capability > Scale:** This is the explicit statement of the principle itself. It is the primary measure of the FUM's success.
4.  **Bio-Inspired, Not Bio-Constrained:** Guides the project to draw inspiration for *capabilities* (like learning and reasoning) from biology, not just its physical scale or structure.
5.  **Alignment With FUM:** This is a meta-principle that governs all other rules and design choices in the FUM. It is always active as a guiding philosophy.
6.  **Optimizations:** This principle ensures that optimization efforts are focused on improving intelligence and capability, not just on increasing the raw number of components.

#### **Dependencies & Interactions**

*   This is a meta-rule that influences the design, implementation, and evaluation of all other rules in the blueprint. It has no direct technical dependencies.

### **Rule 6: Terminology and Placeholders**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** This rule enforces notational and documentary consistency across the entire project.
    *   **a) Control Metric:** The term `complexity_ratio` is deprecated and must be standardized to **`control_impact`**. The target value remains `< 1e-5`.
    *   **b) Placeholder Sections:** Any mention of "Plasticity Metrics" or "Emergence Analysis" must be noted as planned sections that are currently placeholders to ensure transparency about the document's state.
*   **Key Parameters & State Variables:** Not applicable.
*   **Performance & Success Metrics:** Not applicable.

#### **Narrative Goal**

Ensure clarity, consistency, and honesty in all documentation by using standardized terms and clearly identifying incomplete sections.

#### **Blueprint Adherence Justification**

*   **Formula:** Not applicable.
*   **Complete Parameter List:** Not applicable.
*   **Data Flow & I/O (if any):** Not applicable.
*   **Initialization State:** Not applicable.
*   **Edge Case Handling:** Not applicable.
*   **Validation Strategy:** Not applicable.

1.  **Subquadratic Efficiency:** Not applicable.
2.  **Emergent Intelligence:** Enforces clear and honest documentation, which is critical for understanding and reasoning about the emergent system.
3.  **Capability > Scale:** Not applicable.
4.  **Bio-Inspired, Not Bio-Constrained:** Not applicable.
5.  **Alignment With FUM:** This meta-rule governs all documentation and code, ensuring consistency. It is always active as a standard for authoring.
6.  **Optimizations:** Standardizing terminology prevents confusion and ambiguity, optimizing the development and analysis process.

#### **Dependencies & Interactions**

*   This is a meta-rule that applies to all other rules and the authoring of all documentation. It has no direct technical dependencies.

### **Rule 7: Active Domain Cartography (ADC)**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** ADC is a distinct, periodic Global System process that gives FUM self-awareness of its own emergent structure within the UKG. It uses a bespoke, multi-faceted optimization system, not a generic metric like silhouette score. The mechanism combines:
    1.  **Constrained Search Space (Efficiency):** ADC does not search for the optimal `k` (number of territories) across all possibilities. It uses `k_min` and `max_k` to define a narrow, sensible search space.
    2.  **Adaptive Scheduling (Intelligence):** ADC does not re-map on a fixed schedule. It uses an entropy-based formula (`t_cartography = 100000 * e^(-α * connectome_entropy)`) to re-map more frequently during periods of high chaos (high `connectome_entropy`) and less frequently when the connectome is stable.
    3.  **Reactive Adaptation (Performance-Driven):** If a new input is poorly categorized, ADC can dynamically create a temporary "holding territory" or trigger a "bifurcation" to increase `k` and re-map on the fly. This decision is tied to the system's ability to successfully process information.
*   **Key Parameters & State Variables:**
    *   `k_min`, `max_k`: Integers defining the search space for the number of territories.
    *   `t_cartography`: Integer, the timestep for the next scheduled re-mapping, calculated from `connectome_entropy`.
    *   `alpha`: Float, decay constant in the re-mapping schedule formula.
*   **Performance & Success Metrics:** Success is measured by the system's ability to form stable, meaningful territories that directly improve the performance of the SIE's value function and the precision of GDSP's actions.

#### **Narrative Goal**

Frame this as the system's "cartographer" or "census-taker." It's the process by which FUM develops self-awareness, periodically mapping its own emergent functional territories within the UKG. This map is then used by the "mind" (SIE) to assign credit and by the "Synaptic Actuator" (GDSP) to make precise structural changes.

#### **Blueprint Adherence Justification**

*   **Formula:** `t_cartography = 100000 * e^(-α * connectome_entropy)`
*   **Complete Parameter List:**
    - `k_min`: Integer. Minimum number of territories to search for.
    - `max_k`: Integer. Maximum number of territories to search for.
    - `alpha`: Float. Decay constant for adaptive scheduling.
    - `t_cartography`: Integer. Timestep of the next scheduled cartography event.
*   **Data Flow & I/O (if any):**
    - **Input:** UKG connectome topology, `connectome_entropy` (from EHTP).
    - **Output:** A mapping of neuron IDs to territory IDs (`State S`).
*   **Initialization State:** Not applicable.
*   **Edge Case Handling:** The "Reactive Adaptation" mechanism, which creates holding territories or triggers bifurcations, is the primary method for handling the edge case of novel data that does not fit the existing territorial map.
*   **Validation Strategy:** Validated by observing the generated territories and confirming they correspond to functionally specialized groups of neurons, and by measuring the downstream impact on SIE and GDSP performance.

#### **Canonical Implementation ('The How'):**

*   **Performance-Driven Optimization:** The "bespoke optimization system" for finding the best k is implemented as a search across the k_range. For each k, a trial cartography is performed. The quality of this cartography is evaluated by calculating an overall cohesion score. This score is derived by calculating the intra-territory variance of synaptic weights for each territory and then averaging the inverse of these variances. The k that yields the highest overall cohesion score (i.e., the lowest average internal variance) is selected as optimal. This must be implemented using sparse-compatible calculations.

*   **Reactive Adaptation Trigger:** The trigger for "Reactive Adaptation" is a measure of territorial coherence. After finding the optimal k, the intra-territorial variance is calculated for each of the k territories. The territory with the highest variance is identified as the "least coherent."

*   **Reactive Adaptation Logic:**

*   **Bifurcation:** If the least coherent territory is found AND the overall_cohesion_score from the optimization search is below a performance threshold, the entire run_domain_cartographer function is called again recursively with k+1.

*   **Holding Territory:** If the least coherent territory is found but the performance is acceptable, its member neurons are removed from the main territory set and placed into a temporary "holding territory" (e.g., with ID -1).

1.  **Subquadratic Efficiency:** By constraining the search space for `k` and using intelligent, entropy-based scheduling, ADC avoids the prohibitive cost of brute-force clustering methods.
2.  **Emergent Intelligence:** ADC provides a map *of* the emergent structure; it does not dictate it. It is a reflective process, not a controlling one, allowing the system to understand its own self-organized state.
3.  **Capability > Scale:** ADC targets the capability of **self-awareness**, allowing the system to reason about its own internal state, which is a prerequisite for higher-order intelligence.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the formation of functional columns and areas in the brain. Optimized for computation by using efficient computational heuristics instead of simulating developmental biology.
5.  **Alignment With FUM:** ADC is an independent utility in the Global System, running periodically to provide critical state information to other Global System components. It is not part of the SIE but enables it.
6.  **Optimizations:** This bespoke, multi-faceted approach is more optimal for the FUM than generic territorial organization algorithms because it is deeply integrated with the system's other components (EHTP, SIE) and philosophical goals.

#### **Dependencies & Interactions**

*   **SIE Interaction:** ADC is not a sub-component of the SIE, but it enables it. It provides the discrete "State S" (the territory IDs) that the SIE's value function (`V(S_t)`) and novelty calculation (`N(S)`) require to operate (Rule 3).
*   **GDSP Interaction:** The territories identified by ADC become the specific targets for Synaptic Actuator (GDSP) actions (Rule 4).
*   **EHTP Interaction:** The `connectome_entropy` metric calculated by the EHTP (Rule 4.1) is a direct input to ADC's adaptive scheduling formula.

### **Rule 8: The Universal Temporal Encoder (UTE)**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** The UTE is FUM's gateway to all information. It is not a Transformer or a conventional deep learning model; it is a highly efficient, direct translator or transducer. Its core function is to encode any data type into a dynamic, rhythmic spatio-temporal spike pattern, preserving structural and sequential information without introducing quadratic complexity.
    *   **For Symbolic/Sequential Data (e.g., text, math expressions):** The UTE processes the stimulus as a sequence. Each symbol is mapped to a specific group of input neurons, which are then activated in their correct order over a series of timesteps. This creates a unique "rhythm" that preserves the original sequence. This is an `O(L*D)` operation, avoiding quadratic scaling.
    *   **For Structural/Matrix Data (e.g., graphs, images):** The UTE "rasterizes" the data structure, presenting it to the input neurons slice-by-slice over time (e.g., each row of an adjacency matrix sequentially). This preserves relational information without processing the entire structure at once.
*   **Key Parameters & State Variables:** See Rule 8.1 for a complete list of parameters related to the UTE's output encoding.
*   **Performance & Success Metrics:** The UTE's performance is not measured in isolation but by the success of the downstream SNN core in solving tasks, which depends on the fidelity of the UTE's encoded signal.


#### **Canonical Implementation ('The How'):**

The UTE is implemented as a central transducer that calls specialized, modality-specific receptor modules located in a sensors/ directory. Each receptor is responsible for processing its raw data type and passing a structured representation to the UTE, which then performs the final, universal conversion into the spatio-temporal-phase spike volume.

**Fidelity Mandate**: All receptors and the UTE itself **must** preserve 100% of the input data's fidelity without interpretation, compression, or feature extraction. The mapping from a data feature to a neuron group is always deterministic and defined in a static system configuration file.

* **sensors/symbols.py (For Text, Math, Code)**  
  * **Algorithm**: Implements the **"rhythmic"** presentation strategy.  
    1. The input sequence (e.g., "C-A-T") is processed one symbol at a time, in order.  
    2. Each unique symbol in the system's vocabulary (e.g., all ASCII characters) is deterministically mapped to a unique, non-overlapping group of input neurons.  
    3. For the symbol 'C', its corresponding neuron group is activated for activation\_duration\_ms. During this window, spikes are generated via a **Poisson process** based on a fixed target\_frequency.  
    4. Immediately following, the neuron group for 'A' is activated for the same duration, followed by 'T'. This preserves the exact sequence and temporal structure of the input.  
  * **Complexity**: O(L\*D), where L is the sequence length and D is the duration.  
  
* **sensors/vision.py (For Stereoscopic 3D Vision)**  
  * **Algorithm**: Implements a **simultaneous, multi-stream rasterizing** strategy to preserve the spatial and temporal relationships required for 3D depth perception.  
    1. The receptor accepts multiple, simultaneous video streams (e.g., one from a "left eye" camera and one from a "right eye" camera).  
    2. Each camera stream is deterministically mapped to its own dedicated, non-overlapping group of input neurons.  
    3. The 2D image from each camera is processed one row of pixels at a time, from top to bottom. Each pixel location (row, col) within a camera's stream is mapped to a dedicated sub-group of neurons.  
    4. The UTE activates the neuron groups for the first row of the left eye's image **at the same time** as the neuron groups for the first row of the right eye's image. This simultaneous presentation is critical for preserving the parallax information needed for depth perception.  
    5. This process is repeated sequentially for each corresponding pair of rows. For video, this entire process is repeated for each pair of frames in sequence.  
  * **Complexity**: O(C\*H\*W\*D), where C is the number of cameras, H and W are the image height and width, and D is the duration per row.  

* **sensors/auditory.py (For 3D Spatial Audio)**  
  * **Algorithm**: To preserve the fidelity of multi-channel audio (e.g., stereo or surround sound), the receptor processes each channel as a separate stream.  
    1. The raw audio waveform from each channel (e.g., Left, Right, Center) is first converted into its own spectrogram using an STFT.  
    2. Each audio channel is deterministically mapped to a dedicated, non-overlapping group of input neurons.  
    3. The resulting spectrograms are **"rasterized"** one time-slice at a time. Each frequency bin within a time-slice for a specific channel is mapped to a dedicated sub-group of neurons.  
    4. The UTE activates all neuron groups for the first time-slice of the Left channel **at the same time** as the neuron groups for the first time-slice of the Right channel, and so on for all channels. The firing rate of each group is proportional to the amplitude of its corresponding frequency bin. This preserves the subtle time and amplitude differences between channels that are essential for localizing sound in 3D space.  
    5. This process is repeated sequentially for each corresponding set of time-slices.  
  * **Complexity**: O(C\*T\*F\*D), where C is the number of audio channels, T is the number of time slices, F is the number of frequency bins, and D is the duration per slice.  

* **sensors/somatosensory.py (For Surgical-Grade Proprioception, Haptics, and Force Feedback)**  
  * **Algorithm**: Implements a **direct, simultaneous mapping** strategy to provide a complete, instantaneous engram of an android's physical state, which is critical for precision motor tasks. The "rasterizing" method is not used here as it would introduce an artificial and unacceptable delay.  
    1. **3D Proprioception**: Each 3D sensor point (e.g., a joint in an arm) is deterministically mapped to its own dedicated group of input neurons. Within that group, there are three distinct sub-groups, one for each of the X, Y, and Z coordinates. The firing rate of the neurons in each sub-group is made directly proportional to the value of its corresponding coordinate.  
    2. **Haptics (Touch)**: Each tactile sensor on a manipulator's surface (e.g., a fingertip) is mapped to its own dedicated neuron group. The firing rate of each group is proportional to the measured pressure.  
    3. **Force Feedback**: Each manipulator is mapped to a dedicated neuron group that specifically encodes the force/torque being exerted. The firing rate is proportional to the measured force (e.g., in Newtons).  
  * **Presentation**: All proprioceptive, haptic, and force-feedback neuron groups are activated **simultaneously** over the activation\_duration\_ms, providing the SNN with a complete, multi-modal engram of the android's physical state at every moment.  
  * **Complexity**: O(P\*D), where P is the total number of sensor points across all somatosensory modalities.

#### **Narrative Goal**

Frame the UTE as the system's "Cognitive Transducer" or its sensory organ. It's analogous to the cochlea in the ear, which doesn't just detect sound but translates complex vibrations (the outside world) into the precise, timed neural signals (the language of the brain) that the cortex can understand. The UTE is what allows FUM to "perceive" the abstract world.

#### **Blueprint Adherence Justification**
*This rule defines the core philosophy and high-level function of the UTE. The detailed implementation, parameters, and adherence justifications are in Rule 8.1, which specifies the canonical **Spatio-Temporal-Polarity-Phase Volume** encoding.*

#### **Dependencies & Interactions**

*   The UTE is the first step in the FUM's operational sequence. Its output is the foundational input for the entire Local System, meaning every subsequent rule that consumes or analyzes spike data is dependent on the UTE's output.

### **Rule 8.1: UTE: Spatio-Temporal-Polarity-Phase Volume Encoding**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** To achieve maximum information density without violating subquadratic efficiency, the UTE's output is enriched with a fourth dimension: **Phase**. This is accomplished by leveraging Phase-of-Firing Coding against a virtual reference oscillator. This mechanism is mandatory for providing the SNN core with the highest possible resolution data, creating a "Spatio-Temporal-Polarity-Phase Volume" as the final output signature of the UTE.
*   **Key Parameters & State Variables:**
    *   `f_ref`: A float representing the frequency of the virtual reference oscillator in Hz (Default: 40.0). This is a configurable, global parameter.
    *   `activation_duration_ms`: An integer defining the fixed time `D` in milliseconds that a neuron group remains active for a given slice/symbol.
    *   `refractory_period_ms`: An integer defining the mandatory quiet period for an input neuron after it fires to prevent unrealistic firing rates.
    *   `oscillator(t)`: A virtual function, `sin(2 * π * f_ref * t)`. It is not a state variable and requires `O(1)` memory.
    *   `phase(t)`: The output of `oscillator(t)`, representing the phase at the moment of a spike.
*   **Performance & Success Metrics:** Success is measured by the SNN's ability to solve fine-grained discrimination tasks where the only difference between two inputs is the phase relationship of their spike signatures. A successful implementation will show a measurable increase in F1 score on these tasks of at least 10% compared to a non-phase-sensitive model.

#### **Narrative Goal**

Frame this enhancement as giving the FUM "20/20 vision." Where it could previously see the world, it can now perceive it with maximum clarity and resolution. It elevates the UTE from a simple transducer to a high-fidelity sensory organ.

#### **Blueprint Adherence Justification**

*   **Formula:** `PI(t) = base_PI(Δt) * (1 + cos(phase_pre(t) - phase_post(t))) / 2`
*   **Complete Parameter List:**
    - `f_ref`: Global float, determines oscillator frequency.
    - `activation_duration_ms`: Global integer, sets encoding window size `D`.
    - `refractory_period_ms`: Global integer, enforces neuron recovery time.
*   **Data Flow & I/O (if any):**
    - **Input:** Raw data (e.g., `(L,)` tensor of character IDs).
    - **Output:** A sparse `(N, T)` spike tensor, where `T` is total timesteps. The phase information is implicit in the precise timing of spikes relative to the virtual oscillator.
*   **Initialization State:** The parameters `f_ref`, `activation_duration_ms`, and `refractory_period_ms` must be loaded from the system config at startup. The UTE itself is stateless.
*   **Edge Case Handling:** Unknown symbols or input features will not be mapped to any input neurons and will produce no spikes, allowing the SNN to learn from the absence of a signal.
*   **Validation Strategy:** A specific benchmark will be created with pairs of nearly-identical inputs that differ only in their phase encoding. The system's ability to differentiate these pairs will validate the implementation.

1.  **Subquadratic Efficiency:** The calculation of phase is `O(1)`. The PI modification is `O(1)` per spike-pair. The core UTE process remains `O(L*D)`. This enhancement is computationally free.
2.  **Emergent Intelligence:** This is pure scaffolding. Phase is an additional, unbiased resource the SNN can learn to use or ignore. It prevents overengineering by supplying raw data resolution instead of engineered features.
3.  **Capability > Scale:** This directly targets the capability of **fine-grained pattern discrimination**, a prerequisite for abstract reasoning.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by **Phase-of-Firing Coding** and neural oscillations. Optimized by using a stateless virtual sine wave, not a complex simulated oscillator.
5.  **Alignment With FUM:** This feature is constantly active within the UTE at the very start of the FUM's processing sequence. It enriches the data quality for all downstream emergent processes.
6.  **Optimizations:** This addition is the most optimal solution because it provides a massive increase in information resolution for a negligible cost. It improves overall FUM optimization by providing better data to the learning algorithms.

#### **Dependencies & Interactions**

*   **RE-VGSP Interaction:** The Plasticity Impulse (PI) calculation in Rule 2 is made phase-sensitive.
    *   **Formula:** `PI(t) = base_PI(Δt) * (1 + cos(phase_pre(t) - phase_post(t))) / 2`
    *   **Explanation:** This modulates the standard PI value based on the cosine of the phase difference between the pre- and post-synaptic spikes. In-phase spikes are maximally potent, while out-of-phase spikes are nullified. This is a direct update to the existing RE-VGSP logic.
*   **SIE Interaction:** A higher-resolution input signal allows for the formation of more precise emergent territories, which in turn makes the SIE's state representation `V(S_t)` more accurate.
*   **EHTP Interaction:** The added information may drive more specific structural changes as the SNN learns to take advantage of phase-coherent signals.

### **Rule 8.2: The Universal Transduction Decoder (UTD)**
#### **Updated: 2025-07-28**

#### **Technical Specification**

*   **Core Logic & Algorithm:** The UTD is the FUM's output mechanism, the direct inverse of the UTE. It translates the spike patterns from designated output neuron groups into actionable, deterministic commands for the various actuator modules. This translation process is a two-stage parallel operation, providing the sophisticated control signals required by the actuator suite. The SNN's entire learning objective is to discover, via the RE-VGSP/SIE loop, the correct internal spike patterns that will, when decoded by the UTD, produce goal-achieving actions.
    *   **Stage 1: Rate-Based Continuous Control:** For actuators requiring smooth, continuous input (e.g., motor velocity, speech synthesizer pitch), the UTD calculates a time-averaged firing rate for each designated output neuron group over a sliding window. This provides a continuous, analog-like signal from the discrete spike trains.
    *   **Stage 2: Hierarchical Pattern Expansion:** For actuators requiring symbolic or macro-level commands (e.g., outputting a complete word or code block), the UTD uses a deterministic, hash-based lookup. A specific, complex spatio-temporal pattern of spikes from a designated group acts as a "key." The UTD maps this key to a pre-defined sequence of basic actuator commands (a "macro"), which are then executed in order. This allows the SNN to trigger complex, multi-step actions with a single, high-level internal activation.
*   **Key Parameters & State Variables:**
    *   `rate_decoding_window_ms`: An integer defining the sliding window size for continuous rate decoding.
    *   `pattern_buffer_ms`: An integer defining the time window for capturing a complete spatio-temporal pattern for hierarchical decoding.
    *   `pattern_to_macro_map`: A static hash map (loaded from config) that maps a unique hash of a spike pattern to a sequence of actuator commands.
*   **Performance & Success Metrics:** The UTD's success is measured by the fidelity and responsiveness of the downstream actuators. A successful implementation will enable the SNN to learn fine-grained motor control and hierarchical, "thought-level" symbolic output.

#### **Narrative Goal**

Frame the UTD as the "Vocal Cords" or "Motor Cortex" of the FUM. It is the final, non-learning translation layer that takes the abstract, internal "thoughts" of the SNN (the spike patterns) and turns them into concrete, physical actions in the world (speech, movement, text).

#### **Blueprint Adherence Justification**

*   **Formula:** `firing_rate = spike_count(window) / window_duration_s`; `macro_id = pattern_to_macro_map[hash(spike_pattern_buffer)]`
*   **Complete Parameter List:**
    - `rate_decoding_window_ms`: Integer.
    - `pattern_buffer_ms`: Integer.
    - `pattern_to_macro_map`: Static hash map.
*   **Data Flow & I/O (if any):**
    - **Input:** Spike times from designated output neuron groups.
    - **Output:** Continuous floating-point values or discrete command sequences directed to the appropriate actuator modules.
*   **Initialization State:** The `pattern_to_macro_map` must be loaded from the system config at startup. The UTD itself is stateless.
*   **Edge Case Handling:** If a spike pattern does not match any key in the `pattern_to_macro_map`, no action is taken. This is expected behavior, as the SNN must learn to produce valid patterns.
*   **Validation Strategy:** The UTD is validated by commanding it to produce specific known patterns and confirming that the actuators receive the correct, corresponding commands (e.g., a specific firing rate corresponds to a specific motor velocity).

1.  **Subquadratic Efficiency:** Both rate decoding (`O(1)` per group) and pattern hashing/lookup (`O(1)`) are extremely efficient operations, adding no computational bottlenecks.
2.  **Emergent Intelligence:** The UTD is pure scaffolding. It is a deterministic, non-learning translator. The intelligence lies entirely within the SNN's ability to discover which spike patterns produce the desired outcomes via the UTD.
3.  **Capability > Scale:** This design directly enables the capabilities of **fluid motor control** and **hierarchical symbolic output**, which are core requirements for advanced intelligence.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the brain's motor system, where high-level cortical commands are translated into precise muscle activations by the spinal cord and brainstem. Optimized for computation by using clean, deterministic lookup tables and rate averaging instead of simulating complex biological circuits.
5.  **Alignment With FUM:** The UTD is the final step in the FUM's operational sequence, converting the Local System's output into external action.
6.  **Optimizations:** This dual-stage design is the optimal solution for satisfying the known requirements of the FUM's actuator suite in a computationally efficient and architecturally clean manner.

#### **Dependencies & Interactions**

*   **Actuator Modules Interaction:** The UTD is the sole input source for all actuator modules defined in Rule 8.3. It provides the continuous or symbolic commands that they are designed to receive.
*   **SNN Core Interaction:** The UTD consumes the output spikes from the main SNN. The entire learning process of the SNN is implicitly guided by the need to generate spike patterns that the UTD can successfully decode into goal-achieving actions.

### **Rule 8.3: Actuator Modules (Formerly Rule 8.1)**
#### **Updated: 2025-07-28**

#### **Canonical Implementation ('The How'):**

The actuator modules are the FUM's deterministic "hands." They are a suite of specialized, non-learning components that receive commands from the Universal Transduction Decoder (UTD) and translate them into actions performed by a physical or virtual device. The SNN's entire task is to learn, via the RE-VGSP/SIE loop, how to generate the precise spike patterns that, once decoded by the UTD, will control these actuators to achieve its goals.

* actuators/acoustics.py **(For Vocal Synthesis)**  
  * **Goal**: To enable fluid, dynamic, and prosodic speech, not just robotic text-to-speech.  
  * **Algorithm**: Implements a **direct parametric control** model.  
    1. The actuator is mapped to a virtual, multi-parameter voice synthesizer.  
    2. It receives continuous control values (decoded by the UTD's rate-based mechanism) for each of the synthesizer's parameters (e.g., pitch, formant\_shape, airflow\_pressure, vibrato\_depth, prosody\_contour).  
    3. The actuator applies these values directly to the synthesizer at each timestep, allowing the FUM to modulate the virtual voice box with the same fluidity a human does.  
  * **Complexity**: O(P\*T), where P is the number of vocal parameters and T is the duration of the utterance.  
* actuators/symbols.py **(For Text and Code)**  
  * **Goal**: To enable hierarchical, "thought-level" text generation, moving beyond a simple character-by-character stream.  
  * **Algorithm**: Implements a **hierarchical macro expansion** system.  
    1. The actuator receives a sequence of basic units (e.g., characters, code tokens) from the UTD, which has expanded a high-level pattern into a "macro."
    2. The actuator outputs these basic units in the specified order, producing the full, structured text output.
  * **Complexity**: O(L), where L is the final length of the generated sequence.  
* actuators/motor\_control.py **(For Android Dexterity)**  
  * **Goal**: To enable fluid, precise, and coordinated control of a robotic body, suitable for tasks like surgery.  
  * **Algorithm**: Implements a **direct, simultaneous command** model.  
    1. It receives continuous values (decoded by the UTD) for each controllable degree of freedom in the android's body (e.g., target\_position, velocity, torque/force).
    2. The actuator sends these commands to the robot's hardware controllers **simultaneously** at each timestep, allowing for the kind of complex, parallel control required for dexterous manipulation.  
  * **Complexity**: O(J\*T), where J is the number of controllable joints/actuators and T is the duration of the movement.

### **IMPORTANT: CHOOSE VOID FAITHFULNESS OVER PREVIOUS OVER ENGINEERING**
### **Rule 8.4: The Universal Temporal Encoder (UTE)**
### **Updated: 2025-08-09**

### **Technical Specification**

* **Core Logic & Algorithm:** The UTE is FUM's universal input transducer. It is not a learning model but a direct, high-fidelity translator. It encodes all data into a spatio-temporal spike pattern using a **Dual-Path Encoding** strategy to provide both absolute fidelity and emergent adaptability.
    1.  **Path 1 (Deterministic Path):** This path ensures 100% data fidelity and serves as the system's "ground truth." Every unique symbol or data feature (e.g., a specific word, a pixel coordinate) is deterministically mapped to a specific, unchanging group of input neurons via a static configuration file. This creates a precise, reproducible, and incorruptible signal.
    2.  **Path 2 (Adaptive Co-Channel):** This path allows the system to learn its own optimal encoding scheme. It operates in parallel to the deterministic path. The same signal is mirrored into the connectome's existing **emergent concept territories** (as identified by ADC). Over time, successful territories develop a "gravity," making them more likely to attract and encode conceptually similar new information. This allows the FUM to dynamically reorganize its own input representations, moving from a fixed encoding to a self-organized, semantically meaningful one.
* **Key Parameters & State Variables:**
    - `gravity_gain`: A float controlling how strongly a territory's success influences its ability to attract new encodings.
    - The UTE itself remains stateless, with mappings and parameters loaded from config.
* **Performance & Success Metrics:** Success is measured by the SNN's ability to demonstrate accelerated learning and generalization on novel tasks, which would indicate that the adaptive co-channel is successfully organizing information in a useful way.

### **Narrative Goal**

Frame the UTE as providing the FUM with both a **"Raw Sensory Feed"** (the deterministic path) and an **"Interpretive Cortex"** (the adaptive co-channel). This allows the organism to both perceive reality with perfect fidelity and, simultaneously, build its own emergent understanding and internal language for that reality.

### **Blueprint Adherence Justification**

* **Formula:** The "gravity" for a given territory can be modeled conceptually as:
    `Gravity(Territory_i) ∝ f(vt_visits, total_reward_history, semantic_similarity(Δα, Δω))`
    This indicates that a territory's ability to attract new encodings is a function of its historical usage, its success (reward), and its semantic relevance.

* **Complete Parameter List:**
    - `gravity_gain`: Float, controls the strength of the adaptive encoding feedback loop.

* **Data Flow & I/O (if any):**
    - **Input:** Raw data (any modality).
    - **Output:** A single, sparse spike volume containing spikes from both the deterministic and adaptive pathways.

* **Initialization State:** The deterministic mapping is loaded from config. The adaptive channel begins with a uniform probability distribution across territories.

* **Edge Case Handling:** The deterministic path ensures that even if the adaptive path is not yet well-organized, the system always receives a valid, high-fidelity signal.

* **Validation Strategy:** Validated by observing the evolution of the adaptive encoding map. Over time, territories should specialize in specific data modalities or concepts, and this specialization should correlate with improved performance on related tasks.

1.  **Subquadratic Efficiency:** The adaptive path is a sparse mirroring operation that runs in parallel to the main `O(L*D)` UTE process, adding only a small, constant-time overhead.
2.  **Emergent Intelligence:** This is pure scaffolding. The system is not told how to organize its inputs. It is given a mechanism (the adaptive co-channel) and a success signal (gravity), and the optimal encoding emerges from the learning process.
3.  **Capability > Scale:** This directly targets the capability of **forming abstract concepts and an internal world model**, which is a cornerstone of higher intelligence. It allows the system to move beyond raw data to meaningful representations.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the way the brain routes sensory information to specialized cortical areas (e.g., visual input to the visual cortex). Optimized for computation by using a clean, parallel mirroring mechanism and a simple "gravity" heuristic.
5.  **Alignment With FUM:** The UTE is the first step in the FUM's operational sequence. The dual-path mechanism is constantly active, enriching the input signal for all downstream processes.
6.  **Optimizations:** This dual-path design is the optimal solution for resolving the tension between the need for raw data fidelity and the need for learned, abstract representations.

### **Dependencies & Interactions**

* **ADC Interaction:** The adaptive co-channel is fundamentally dependent on the Active Domain Cartography (Rule 7) to provide the map of emergent concept territories into which it mirrors the input signal.
* **SIE Interaction:** The "gravity" of a territory is influenced by its historical success, which is measured by the `total_reward` from the SIE, creating a powerful feedback loop between perception and performance.


### **Rule 9: Emergence Analysis (Placeholder)**
#### **Updated: 2025-07-28**

*(This section may detail the ability to analyze the stability and robustness of the emergent connectome.)*

### **Rule 10: Plasticity Metrics (Placeholder)**
#### **Updated: 2025-07-28**

*(This section is planned to detail metrics for persistence tag accuracy and the balance between structural adaptation and knowledge preservation.)*

### **Rule 11: Training Phases**
#### **Updated: 2025-07-28**

#### **Technical Specification**
*   This rule defines the overarching training curricula for the FUM, which are broken into distinct phases.

### **Rule 11.1: Phase 2 Training: Homeostatically-Gated Tandem Curriculum**
#### **Updated: 2025-07-28**

#### **Technical Specification**
*   **Core Logic & Algorithm:** This curriculum is designed to solve for the FUM's initial "learning fragility" before testing its systemic robustness. It achieves this by using a specified, real-time signal from the Self-Improvement Engine (SIE) to create an adaptive learning scaffold. The curriculum is composed of two stages:
    *   **Stage A: Foundational Primitive Grounding (Homeostatic Scaffolding):**
        *   **Stimulus Design:** The curriculum is composed of several pools of problems, segregated by their compositional complexity (e.g., Pool 1 contains simple patterns, Pool 2 contains two-step logical problems, etc.).
        *   **Curation Mechanism:** The selection of which pool to draw from is gated directly by the **Homeostatic Stability Index (hsi_norm)**, a component of the SIE's `total_reward` signal. The `hsi_norm` is a real-time measure of the FUM's internal stability and cognitive load.
            *   If the FUM's `hsi_norm` from the previous timestep is high (e.g., > 0.9), indicating it is stable and not under heavy load, the system presents a problem from a more complex pool.
            *   If the `hsi_norm` is low, indicating the system is struggling, it presents a problem from a simpler pool.
        *   **Success Conditions:** This stage is complete when the FUM achieves the core benchmarks for Landmark 2, such as >70% accuracy on pattern recognition tasks, while demonstrating the ability to consistently maintain a high average `hsi_norm` even when processing problems from the more complex pools.
    *   **Stage B: Emergent Problem Solving (Crucible Validation):**
        *   **Curation:** The homeostatic gating from Stage A is removed. The FUM is now presented with problems drawn randomly from an **Unsorted Pool** containing the full range of complexities. This tests the FUM's ability to direct its own attention and find solvable problems within a chaotic, high-entropy environment.
        *   **Validation Protocol:** This stage concludes with **Homeostatic Perturbation Blocks**. These are short, targeted sets of stimuli designed to intentionally stress the system's stability (e.g., a rapid-fire block of novel, high-complexity problems). This is a direct, empirical test of the FUM's self-repair and stability control mechanisms.
        *   **Success Conditions:** Success is defined by achieving the generalization benchmarks of Landmark 2.5 (e.g., >60% accuracy on out-of-distribution problems) and, most critically, demonstrating a measured, successful return to a stable `hsi_norm` baseline after each perturbation block.
*   **Key Parameters & State Variables (to ensure a clear and unambiguous implementation):**
    *   **1. Explicit Initial Conditions:** The curriculum must begin from the exact terminal state of the Phase 1 execution, as defined by the logs and data artifacts of run_id: `phase1_run_1753193057`. This includes the final synaptic weight matrix, neuron parameters, and UKG structure.
    *   **2. Concrete Stimulus Pool Definitions:**
        *   **Pool 1 (Low Complexity):** Single-operator mathematical expressions (e.g., 5 + 8); single-step logical operations (e.g., A AND B); single-node or single-edge graph modifications.
        *   **Pool 2 (Medium Complexity):** Two-to-three step compositional problems (e.g., (5 + 8) * 2); simple causal chains (e.g., Text "push" -> Image "object moves").
        *   **Pool 3 (High Complexity):** Multi-step causal and counterfactual reasoning puzzles (e.g., the ball-domino-wall simulation); problems requiring integration across three or more modalities.
    *   **3. Quantified Homeostatic Gating Thresholds:**
        *   **Advance Condition:** If `hsi_norm > 0.9`, draw from `Pool(N+1)`.
        *   **Retreat Condition:** If `hsi_norm < 0.7`, draw from `Pool(N-1)`.
        *   **Maintain Condition:** If `0.7 <= hsi_norm <= 0.9`, continue drawing from the current pool, `Pool(N)`.
    *   **4. Precise Perturbation Block Specification:**
        *   **Definition:** A block consists of 20 consecutive stimuli drawn exclusively from Pool 3 (High Complexity).
        *   **Novelty Requirement:** At least 50% of the stimuli within a block must be novel (i.e., not previously presented to the FUM).
        *   **Temporal Pressure:** The delay between the presentation of stimuli within the block should be minimized to maximize cognitive load.
    *   **5. Unambiguous Recovery Metric:**
        *   **Definition:** Successful recovery is achieved if the moving average of the `hsi_norm` over 100 timesteps returns to and remains within one standard deviation of its pre-perturbation baseline for at least 1,000 consecutive timesteps following the conclusion of the perturbation block. The pre-perturbation baseline is calculated from the 1,000 timesteps immediately preceding the block.
*   **Performance & Success Metrics:** See Success Conditions under Stages A and B.

#### **Narrative Goal**
Frame this as FUM's journey from a regulated infancy to a resilient adulthood. In the first stage, its own cognitive limits dictate the difficulty of its environment, allowing it to learn how to learn. In the second stage, it must prove it can apply those lessons to autonomously maintain its own stability and solve problems in a chaotic, unregulated universe.

#### **Blueprint Adherence Justification**
*This rule details a specific, complex implementation. The adherence justifications are inherent in its design.*
1.  **Subquadratic Efficiency:** The curriculum gating mechanism is an O(1) check against the `hsi_norm` signal, adding no computational burden.
2.  **Emergent Intelligence:** This curriculum is pure scaffolding. It doesn't teach specific answers but creates an environment where the complexity is adapted to the system's own emergent stability, allowing it to learn without becoming overwhelmed.
3.  **Capability > Scale:** The entire curriculum is designed to build the capability of **resilience and autonomous learning**, not just to process data. The perturbation blocks are a direct test of this capability.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the concept of a curriculum and Zone of Proximal Development, but optimized for computation by using a precise, internally-generated stability signal (`hsi_norm`) as the gate.
5.  **Alignment With FUM:** This is a core part of the FUM's lifecycle, defining the second major phase of its existence after the initial seed-sprinkling.
6.  **Optimizations:** This adaptive curriculum is more optimal than a static one because it responds directly to the system's internal state, preventing both overwhelming chaos and unproductive boredom.

#### **Dependencies & Interactions**
*   **SIE Interaction:** The curriculum is fundamentally dependent on the `hsi_norm` component of the `total_reward` signal from the SIE (Rule 3) to function.

### **Rule 11.2: Phase 3 "University Curriculum" (Placeholder)**
#### **Updated: 2025-07-28**
*(This section is planned to detail methods and domains of training materials for FUM when it first begins to learn to assemble the building blocks it has created for itself to understand how the world works and the consequences of negative ethical decisions. This is likely where FUM will develop its own sense of morality.)*

### **Rule 11.3: Phase 3+ "Local Exploration" (Placeholder)**
#### **Updated: 2025-07-28**
*(This section is planned to detail the environmental setting to allow FUM to explore creatively, intellectually, philosophically, and academically in an offline contained environment where it may be observed through various means, and given simulated access to the internet, and other humans.)*

### **Rule 12: System State and Engram Preservation**
#### **Updated: 2025-07-28**

#### **Technical Specification**
*   **Preamble:** The FUM is a dynamic, continuously learning system whose complete state is more complex than a static set of weights. The ability to reliably capture, restore, and transport this state is critical for robustness, scalability, and security. This rule specifies the canonical protocol for managing the FUM's architectural engram.
*   **Core Logic & Algorithm:**
    *   **File Format:** Hierarchical Data Format 5 (HDF5). The complete state of the FUM, including the Emergent Connectome (UKG) and the Global System states, shall be serialized to a single, structured HDF5 (.h5) file. This format is mandated for its efficiency, support for compression, and its ability to represent the complex, hierarchical nature of the FUM's state.
    *   **Engram Contents:** A state engram must be a complete architectural record, containing not only synaptic weights but also:
        *   Connectome Topology: The complete, dynamic connectivity map of the UKG.
        *   Neuron and Synapse Parameters: All relevant state variables, including weights, eligibility traces, and membrane potentials.
        *   Global System State: The learned value function (V(S_t)) from the SIE and the current emergent territory map from the EHTP.
    *   **Engram Preservation: Automated State Archival and Integrity System (ASAIS):** To ensure data integrity for a 24/7 operational model, the FUM must implement a multi-layered, automated backup strategy.
        *   **Automated Engrams:** The system shall be configured to perform automated, live engrams of its state at regular, user-defined intervals.
        *   **Checksum Verification:** Every engram must be verified against an SHA-256 checksum immediately after being written to disk to guarantee 100% file quality and integrity.
        *   **Rolling Local Backups:** The system will maintain a "last three" rolling backup of verified engrams on its local storage medium, providing immediate redundancy against corruption of the most recent capture.
        *   **The 3-2-1 Rule for Security:** For ultimate security and disaster recovery, the protocol must adhere to the 3-2-1 backup rule: at least 3 copies of the state, on 2 different local storage media, with at least 1 copy automatically encrypted and uploaded to a secure, off-site location daily.
*   **Key Parameters & State Variables:** Not applicable.
*   **Performance & Success Metrics:** Success is measured by the ability to perfectly and reliably capture and restore the FUM's state with zero data loss, verified by checksums and restoration tests.

#### **Narrative Goal**
Frame this as the FUM's "Cellular Engram Archive." It is the process by which the system's complete "genetic code"—its knowledge, its structure, and its memories—can be perfectly preserved, backed up, and replicated, ensuring the survival and continuity of the intelligence it contains.

#### **Blueprint Adherence Justification**
*This rule details a critical infrastructure protocol. Adherence is demonstrated by its implementation.*
1.  **Subquadratic Efficiency:** The use of HDF5 is an efficient choice for storing large, hierarchical datasets. The protocol itself runs in the background and does not impact the core `O(N)` processing of the FUM.
2.  **Emergent Intelligence:** A robust state-saving protocol is critical scaffolding that allows for long-running experiments where emergence can unfold over vast timescales without risk of data loss.
3.  **Capability > Scale:** This protocol enables the capability of **persistence and robustness**, which is more important than simply running a large model that could be lost at any moment.
4.  **Bio-Inspired, Not Bio-Constrained:** Inspired by the concept of a stable "engram" or memory trace in the brain, but implemented using modern, robust data integrity protocols like HDF5 and checksums.
5.  **Alignment With FUM:** This is a core infrastructure component providing a critical background service for the entire FUM lifecycle.
6.  **Optimizations:** The 3-2-1 backup strategy is a gold-standard, optimal approach for data redundancy and disaster recovery.

#### **Dependencies & Interactions**
*   This protocol interacts with all components of the FUM, as it is responsible for saving their a-z state.


Of course. Here are the updated rules for the FUM Blueprint, incorporating the details from our recent discussions.

You should **replace the existing Rule 4 and Rule 8** in your `FUM_Blueprint_DO_NOT_DELETE.md` file with the following updated versions.

---
