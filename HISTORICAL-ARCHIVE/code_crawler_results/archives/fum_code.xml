<?xml version="1.0" ?>
<fum_code>
  <metadata>
    <global_stats>
      <total_files>215</total_files>
      <total_size_bytes>1104287</total_size_bytes>
      <total_loc>29252</total_loc>
    </global_stats>
    <chunk_stats>
      <files_in_chunk>215</files_in_chunk>
      <size_in_chunk_bytes>1104287</size_in_chunk_bytes>
      <loc_in_chunk>29252</loc_in_chunk>
    </chunk_stats>
  </metadata>
  <ascii_map><![CDATA[
fum_rt/
├── README.md
│   (LOC: 312, Size: 15.2 KB)
├── __init__.py
│   (LOC: 2, Size: 47 B)
├── api/
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── llms/
│   │   ├── __init__.py
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── providers/
│   │   │   ├── google/
│   │   │   │   ├── gemini_client.py
│   │   │   │   │   (LOC: 0, Size: 0 B)
│   │   │   │   ├── model_loader.py
│   │   │   │   │   (LOC: 0, Size: 0 B)
│   │   │   │   └── models/
│   │   │   ├── ollama/
│   │   │   │   ├── models/
│   │   │   │   └── ollama_client.py
│   │   │   │       (LOC: 0, Size: 0 B)
│   │   │   ├── openai/
│   │   │   │   ├── models/
│   │   │   │   └── openai_client.py
│   │   │   │       (LOC: 0, Size: 0 B)
│   │   │   ├── openrouter/
│   │   │   │   └── openrouter_client.py
│   │   │   │       (LOC: 0, Size: 0 B)
│   │   │   └── xai/
│   │   │       ├── models/
│   │   │       └── xai_client.py
│   │   │           (LOC: 0, Size: 0 B)
│   │   └── router.py
│   │       (LOC: 0, Size: 0 B)
│   ├── routes/
│   └── server/
├── ck/
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── build.sh
│   │   (LOC: 29, Size: 659 B)
│   └── hip_spmv.cpp
│       (LOC: 70, Size: 2.6 KB)
├── cli/
│   └── args.py
│       (LOC: 81, Size: 4.1 KB)
├── core/
│   ├── README.md
│   │   (LOC: 1, Size: 25 B)
│   ├── __init__.py
│   │   (LOC: 0, Size: 0 B)
│   ├── adc.py
│   │   (LOC: 205, Size: 8.0 KB)
│   ├── announce.py
│   │   (LOC: 76, Size: 3.0 KB)
│   ├── bus.py
│   │   (LOC: 62, Size: 2.0 KB)
│   ├── connectome.py
│   │   (LOC: 402, Size: 16.9 KB)
│   ├── control_server.py
│   │   (LOC: 296, Size: 10.4 KB)
│   ├── cortex/
│   │   ├── __init__.py
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── maps/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 6, Size: 207 B)
│   │   │   ├── base_decay_map.py
│   │   │   │   (LOC: 130, Size: 4.6 KB)
│   │   │   ├── coldmap.py
│   │   │   │   (LOC: 165, Size: 5.7 KB)
│   │   │   ├── excitationmap.py
│   │   │   │   (LOC: 65, Size: 2.2 KB)
│   │   │   ├── heatmap.py
│   │   │   │   (LOC: 70, Size: 2.5 KB)
│   │   │   ├── inhibitionmap.py
│   │   │   │   (LOC: 65, Size: 2.2 KB)
│   │   │   ├── memorymap.py
│   │   │   │   (LOC: 324, Size: 11.4 KB)
│   │   │   └── trailmap.py
│   │   │       (LOC: 102, Size: 3.9 KB)
│   │   ├── scouts.py
│   │   │   (LOC: 78, Size: 2.5 KB)
│   │   └── void_walkers/
│   │       ├── base.py
│   │       │   (LOC: 241, Size: 8.2 KB)
│   │       ├── frontier_scout.py
│   │       │   (LOC: 10, Size: 266 B)
│   │       ├── runner.py
│   │       │   (LOC: 139, Size: 4.6 KB)
│   │       ├── void_cold_scout.py
│   │       │   (LOC: 55, Size: 1.6 KB)
│   │       ├── void_cycle_scout.py
│   │       │   (LOC: 161, Size: 4.9 KB)
│   │       ├── void_excitation_scout.py
│   │       │   (LOC: 113, Size: 4.1 KB)
│   │       ├── void_frontier_scout.py
│   │       │   (LOC: 293, Size: 9.1 KB)
│   │       ├── void_heat_scout.py
│   │       │   (LOC: 267, Size: 8.5 KB)
│   │       ├── void_inhibition_scout.py
│   │       │   (LOC: 113, Size: 4.0 KB)
│   │       ├── void_memory_ray_scout.py
│   │       │   (LOC: 251, Size: 7.9 KB)
│   │       ├── void_ray_scout.py
│   │       │   (LOC: 238, Size: 7.4 KB)
│   │       └── void_sentinel_scout.py
│   │           (LOC: 165, Size: 5.3 KB)
│   ├── diagnostics.py
│   │   (LOC: 292, Size: 10.5 KB)
│   ├── engine/
│   │   ├── __init__.py
│   │   │   (LOC: 10, Size: 243 B)
│   │   ├── core_engine.py
│   │   │   (LOC: 532, Size: 21.4 KB)
│   │   ├── evt_snapshot.py
│   │   │   (LOC: 123, Size: 3.5 KB)
│   │   └── maps_frame.py
│   │       (LOC: 130, Size: 4.0 KB)
│   ├── fum_growth_arbiter.py
│   │   (LOC: 148, Size: 6.2 KB)
│   ├── fum_sie.py
│   │   (LOC: 298, Size: 13.1 KB)
│   ├── fum_structural_homeostasis.py
│   │   (LOC: 166, Size: 6.8 KB)
│   ├── global_system.py
│   │   (LOC: 251, Size: 9.7 KB)
│   ├── guards/
│   │   └── invariants.py
│   │       (LOC: 296, Size: 9.2 KB)
│   ├── memory/
│   │   ├── __init__.py
│   │   │   (LOC: 17, Size: 482 B)
│   │   ├── engram_io.py
│   │   │   (LOC: 440, Size: 16.3 KB)
│   │   └── field.py
│   │       (LOC: 353, Size: 11.9 KB)
│   ├── metrics.py
│   │   (LOC: 115, Size: 4.4 KB)
│   ├── neuroplasticity/
│   │   ├── __init__.py
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── gdsp.py
│   │   │   (LOC: 522, Size: 21.9 KB)
│   │   └── revgsp.py
│   │       (LOC: 293, Size: 11.5 KB)
│   ├── primitives/
│   │   └── dsu.py
│   │       (LOC: 101, Size: 3.3 KB)
│   ├── proprioception/
│   │   ├── events.py
│   │   │   (LOC: 477, Size: 15.2 KB)
│   │   └── territory.py
│   │       (LOC: 184, Size: 6.4 KB)
│   ├── sie_v2.py
│   │   (LOC: 100, Size: 3.9 KB)
│   ├── signals.py
│   │   (LOC: 317, Size: 9.6 KB)
│   ├── sparse_connectome.py
│   │   (LOC: 678, Size: 27.3 KB)
│   ├── substrate/
│   │   ├── README.md
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── growth_arbiter.py
│   │   │   (LOC: 108, Size: 4.6 KB)
│   │   ├── neurogenesis.py
│   │   │   (LOC: 112, Size: 6.0 KB)
│   │   ├── structural_homeostasis.py
│   │   │   (LOC: 92, Size: 3.9 KB)
│   │   └── substrate.py
│   │       (LOC: 240, Size: 10.4 KB)
│   ├── text_utils.py
│   │   (LOC: 122, Size: 4.0 KB)
│   ├── visualizer.py
│   │   (LOC: 89, Size: 3.3 KB)
│   ├── void_b1.py
│   │   (LOC: 377, Size: 13.0 KB)
│   └── void_dynamics_adapter.py
│       (LOC: 69, Size: 3.1 KB)
├── curriculum/
│   ├── phase1_primitives/
│   ├── phase2_homeostatic_gated/
│   ├── phase3_university/
│   └── phase4_self_discovery/
├── electron-frontend/
│   └── README.md
│       (LOC: 0, Size: 0 B)
├── frontend/
│   ├── DEV_PLANS.md
│   │   (LOC: 0, Size: 0 B)
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── __init__.py
│   │   (LOC: 18, Size: 779 B)
│   ├── __main__.py
│   │   (LOC: 21, Size: 484 B)
│   ├── app.py
│   │   (LOC: 194, Size: 8.6 KB)
│   ├── callbacks/
│   │   ├── charts.py
│   │   │   (LOC: 43, Size: 1.7 KB)
│   │   ├── chat.py
│   │   │   (LOC: 117, Size: 4.1 KB)
│   │   ├── engram.py
│   │   │   (LOC: 72, Size: 2.6 KB)
│   │   ├── feed.py
│   │   │   (LOC: 62, Size: 2.3 KB)
│   │   ├── file_picker/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 20, Size: 497 B)
│   │   │   ├── common.py
│   │   │   │   (LOC: 414, Size: 16.3 KB)
│   │   │   └── registrars.py
│   │   │       (LOC: 188, Size: 6.7 KB)
│   │   ├── interval.py
│   │   │   (LOC: 37, Size: 1.1 KB)
│   │   ├── logs.py
│   │   │   (LOC: 72, Size: 2.5 KB)
│   │   ├── perf.py
│   │   │   (LOC: 91, Size: 2.7 KB)
│   │   ├── process.py
│   │   │   (LOC: 316, Size: 9.3 KB)
│   │   ├── profile.py
│   │   │   (LOC: 223, Size: 7.9 KB)
│   │   ├── runtime.py
│   │   │   (LOC: 131, Size: 4.1 KB)
│   │   └── workspace.py
│   │       (LOC: 62, Size: 2.0 KB)
│   ├── components/
│   │   ├── charts.py
│   │   │   (LOC: 18, Size: 443 B)
│   │   ├── chat.py
│   │   │   (LOC: 62, Size: 2.1 KB)
│   │   ├── feed.py
│   │   │   (LOC: 40, Size: 1.3 KB)
│   │   ├── layout.py
│   │   │   (LOC: 71, Size: 2.5 KB)
│   │   ├── perf.py
│   │   │   (LOC: 95, Size: 3.4 KB)
│   │   ├── run_config/
│   │   │   ├── __init__.py
│   │   │   │   (LOC: 61, Size: 1.5 KB)
│   │   │   └── sections.py
│   │   │       (LOC: 212, Size: 8.8 KB)
│   │   ├── run_config.py
│   │   │   (LOC: 42, Size: 1.3 KB)
│   │   ├── runtime_controls.py
│   │   │   (LOC: 220, Size: 7.9 KB)
│   │   ├── widgets/
│   │   │   ├── file_breadcrumbs.py
│   │   │   │   (LOC: 51, Size: 1.8 KB)
│   │   │   ├── file_picker.py
│   │   │   │   (LOC: 236, Size: 10.0 KB)
│   │   │   ├── file_tree.py
│   │   │   │   (LOC: 86, Size: 3.0 KB)
│   │   │   └── graph.py
│   │   │       (LOC: 11, Size: 317 B)
│   │   └── workspace.py
│   │       (LOC: 72, Size: 2.3 KB)
│   ├── controllers/
│   │   ├── __init__.py
│   │   │   (LOC: 17, Size: 352 B)
│   │   ├── charts_controller.py
│   │   │   (LOC: 289, Size: 11.1 KB)
│   │   ├── chat_controller.py
│   │   │   (LOC: 119, Size: 3.6 KB)
│   │   ├── file_picker_controller.py
│   │   │   (LOC: 147, Size: 4.9 KB)
│   │   ├── file_picker_ctx.py
│   │   │   (LOC: 30, Size: 867 B)
│   │   ├── file_picker_status.py
│   │   │   (LOC: 83, Size: 2.6 KB)
│   │   └── runtime_controller.py
│   │       (LOC: 156, Size: 5.6 KB)
│   ├── debug_ui.py
│   │   (LOC: 429, Size: 17.1 KB)
│   ├── models/
│   │   └── series.py
│   │       (LOC: 170, Size: 5.3 KB)
│   ├── plugins/
│   │   ├── fum_reporting_v1/
│   │   └── fum_visualizer_v1/
│   │       └── fum_visualizer.py
│   │           (LOC: 535, Size: 19.9 KB)
│   ├── services/
│   │   ├── process_manager.py
│   │   │   (LOC: 308, Size: 12.4 KB)
│   │   └── status_client.py
│   │       (LOC: 87, Size: 2.9 KB)
│   ├── styles/
│   │   ├── README.md
│   │   │   (LOC: 128, Size: 4.8 KB)
│   │   ├── __init__.py
│   │   │   (LOC: 64, Size: 1.8 KB)
│   │   ├── base.py
│   │   │   (LOC: 59, Size: 2.5 KB)
│   │   ├── components.py
│   │   │   (LOC: 57, Size: 1.7 KB)
│   │   ├── layout.py
│   │   │   (LOC: 101, Size: 2.6 KB)
│   │   └── theme.py
│   │       (LOC: 53, Size: 1.6 KB)
│   ├── utilities/
│   │   ├── fs_utils.py
│   │   │   (LOC: 98, Size: 3.1 KB)
│   │   ├── pdf_utils.py
│   │   │   (LOC: 145, Size: 4.2 KB)
│   │   ├── profiles.py
│   │   │   (LOC: 142, Size: 4.9 KB)
│   │   └── tail.py
│   │       (LOC: 164, Size: 4.7 KB)
│   ├── view_models/
│   └── views/
├── io/
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── __init__.py
│   │   (LOC: 0, Size: 0 B)
│   ├── actuators/
│   │   ├── __init__.py
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── macros.py
│   │   │   (LOC: 133, Size: 4.9 KB)
│   │   ├── motor_control.py
│   │   │   (LOC: 9, Size: 383 B)
│   │   ├── symbols.py
│   │   │   (LOC: 7, Size: 348 B)
│   │   ├── thoughts.py
│   │   │   (LOC: 87, Size: 3.3 KB)
│   │   ├── visualize.py
│   │   │   (LOC: 7, Size: 348 B)
│   │   └── vocalizer.py
│   │       (LOC: 7, Size: 348 B)
│   ├── cognition/
│   │   ├── composer.py
│   │   │   (LOC: 84, Size: 2.8 KB)
│   │   ├── speaker.py
│   │   │   (LOC: 98, Size: 2.5 KB)
│   │   └── stimulus.py
│   │       (LOC: 54, Size: 1.7 KB)
│   ├── lexicon/
│   │   ├── idf.py
│   │   │   (LOC: 87, Size: 2.9 KB)
│   │   └── store.py
│   │       (LOC: 160, Size: 5.6 KB)
│   ├── logging/
│   │   └── rolling_jsonl.py
│   │       (LOC: 536, Size: 20.6 KB)
│   ├── maps_ring.py
│   │   (LOC: 14, Size: 389 B)
│   ├── sensors/
│   │   ├── __init__.py
│   │   │   (LOC: 0, Size: 0 B)
│   │   ├── auditory.py
│   │   │   (LOC: 7, Size: 348 B)
│   │   ├── somatosensory.py
│   │   │   (LOC: 7, Size: 348 B)
│   │   ├── symbols.py
│   │   │   (LOC: 7, Size: 348 B)
│   │   └── vision.py
│   │       (LOC: 7, Size: 348 B)
│   ├── utd.py
│   │   (LOC: 115, Size: 4.5 KB)
│   ├── ute.py
│   │   (LOC: 94, Size: 3.4 KB)
│   └── visualization/
│       ├── __init__.py
│       │   (LOC: 8, Size: 297 B)
│       ├── maps_ring.py
│       │   (LOC: 113, Size: 3.7 KB)
│       └── websocket_server.py
│           (LOC: 314, Size: 10.4 KB)
├── nexus.py
│   (LOC: 385, Size: 17.5 KB)
├── physics/
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── __init__.py
│   │   (LOC: 29, Size: 802 B)
│   ├── memory_steering/
│   │   ├── memory_steering.py
│   │   │   (LOC: 461, Size: 16.7 KB)
│   │   ├── memory_steering_experiments.py
│   │   │   (LOC: 931, Size: 38.4 KB)
│   │   └── plot_memory_steering.py
│   │       (LOC: 466, Size: 20.0 KB)
│   ├── outputs/
│   │   ├── figures/
│   │   └── logs/
│   ├── rd_dispersion_runner.py
│   │   (LOC: 135, Size: 6.0 KB)
│   ├── rd_front_speed_runner.py
│   │   (LOC: 139, Size: 5.8 KB)
│   └── tachyonic_condensation/
│       ├── condense_tube.py
│       │   (LOC: 293, Size: 9.2 KB)
│       └── cylinder_modes.py
│           (LOC: 328, Size: 10.3 KB)
├── run_nexus.py
│   (LOC: 99, Size: 4.5 KB)
├── runtime/
│   ├── __init__.py
│   │   (LOC: 14, Size: 386 B)
│   ├── emitters.py
│   │   (LOC: 60, Size: 2.1 KB)
│   ├── events_adapter.py
│   │   (LOC: 176, Size: 6.6 KB)
│   ├── helpers/
│   │   ├── __init__.py
│   │   │   (LOC: 39, Size: 1.2 KB)
│   │   ├── checkpointing.py
│   │   │   (LOC: 52, Size: 1.8 KB)
│   │   ├── emission.py
│   │   │   (LOC: 58, Size: 2.0 KB)
│   │   ├── engram.py
│   │   │   (LOC: 75, Size: 2.6 KB)
│   │   ├── ingest.py
│   │   │   (LOC: 99, Size: 3.5 KB)
│   │   ├── macro_board.py
│   │   │   (LOC: 51, Size: 1.5 KB)
│   │   ├── maps_ws.py
│   │   │   (LOC: 86, Size: 3.0 KB)
│   │   ├── redis_out.py
│   │   │   (LOC: 160, Size: 5.3 KB)
│   │   ├── smoke.py
│   │   │   (LOC: 56, Size: 2.5 KB)
│   │   ├── speak.py
│   │   │   (LOC: 133, Size: 4.2 KB)
│   │   ├── status_http.py
│   │   │   (LOC: 194, Size: 6.5 KB)
│   │   └── viz.py
│   │       (LOC: 34, Size: 1.1 KB)
│   ├── loop/
│   │   ├── __init__.py
│   │   │   (LOC: 48, Size: 1.4 KB)
│   │   └── main.py
│   │       (LOC: 934, Size: 40.0 KB)
│   ├── orchestrator.py
│   │   (LOC: 100, Size: 4.0 KB)
│   ├── phase.py
│   │   (LOC: 282, Size: 10.9 KB)
│   ├── retention.py
│   │   (LOC: 105, Size: 3.7 KB)
│   ├── state.py
│   │   (LOC: 86, Size: 2.8 KB)
│   ├── stepper.py
│   │   (LOC: 136, Size: 4.1 KB)
│   └── telemetry.py
│       (LOC: 652, Size: 25.8 KB)
├── substrate/
│   ├── README.md
│   │   (LOC: 0, Size: 0 B)
│   ├── fum_growth_arbiter.py
│   │   (LOC: 108, Size: 4.5 KB)
│   ├── fum_hypertrophy.py
│   │   (LOC: 112, Size: 6.0 KB)
│   ├── fum_structural_homeostasis.py
│   │   (LOC: 92, Size: 3.9 KB)
│   └── fum_substrate.py
│       (LOC: 240, Size: 10.4 KB)
├── tests/
│   ├── api/
│   ├── conftest.py
│   │   (LOC: 30, Size: 839 B)
│   ├── core/
│   │   ├── test_active_graph_regression.py
│   │   │   (LOC: 79, Size: 3.2 KB)
│   │   ├── test_core_boundary_guards.py
│   │   │   (LOC: 188, Size: 5.7 KB)
│   │   ├── test_maps_frame.py
│   │   │   (LOC: 73, Size: 3.0 KB)
│   │   ├── test_runner_budget.py
│   │   │   (LOC: 106, Size: 3.4 KB)
│   │   ├── test_scouts.py
│   │   │   (LOC: 51, Size: 1.8 KB)
│   │   ├── test_territory_uf.py
│   │   │   (LOC: 60, Size: 1.8 KB)
│   │   └── test_void_faithful_guards.py
│   │       (LOC: 66, Size: 3.0 KB)
│   ├── data/
│   ├── electron-frontend/
│   ├── frontend/
│   │   └── test_process_manager_runs_root.py
│   │       (LOC: 108, Size: 3.7 KB)
│   ├── guards/
│   │   └── test_no_scheduler.py
│   │       (LOC: 92, Size: 2.9 KB)
│   ├── io/
│   ├── physics/
│   │   ├── test_invariants.py
│   │   │   (LOC: 140, Size: 4.2 KB)
│   │   ├── test_memory_kernel_event_local.py
│   │   │   (LOC: 77, Size: 2.6 KB)
│   │   └── test_steering_ab.py
│   │       (LOC: 95, Size: 3.3 KB)
│   ├── runtime/
│   │   ├── test_events_adapter_inhibition.py
│   │   │   (LOC: 127, Size: 4.7 KB)
│   │   ├── test_phase_legacy_key.py
│   │   │   (LOC: 38, Size: 1.2 KB)
│   │   └── test_telemetry_frame_v2.py
│   │       (LOC: 87, Size: 2.7 KB)
│   └── substrate/
└── utils/
    ├── README.md
    │   (LOC: 0, Size: 0 B)
    └── logging_setup.py
        (LOC: 54, Size: 1.9 KB)]]></ascii_map>
  <files>
    <file>
      <path>README.md</path>
      <content><![CDATA[
# FUM Real‑Time Runtime (Scaffold v3)

This is a **minimal, production‑oriented** runtime that matches your Nexus ⇄ UTE/UTD vision.
It runs continuously, ingests input, updates the connectome with your *void equations* if
present (`Void_Equations.py`), logs metrics, and renders dashboards and connectome images.

> Entry point: `python -m fum_rt.run_nexus`

## Quick start

```bash
pip install -r requirements.txt
export PYTHONPATH=.
python -m fum_rt.run_nexus --neurons 800 --hz 10 --domain biology_consciousness --viz-every 5
```

Artifacts land in `runs/<timestamp>/`:
- `events.jsonl`   - structured logs
- `dashboard.png`  - metrics (updated)
- `connectome.png` - graph snapshot (updated)
- `state_<step>.h5` (or `.npz` fallback) - checkpointed engram state (see `--checkpoint-every`, `--checkpoint-keep`)

### Where to put your functions
If your repo already contains `Void_Equations.py` and `Void_Debt_Modulation.py` on `PYTHONPATH`,
this runtime will import them automatically.

If not, drop those files at the project root (next to `fum_rt/`) **or** copy them into `fum_rt/core/`.
The adapter will prefer your versions and only fall back to an internal stub if not found.

### What “real‑time” means here
The Nexus loop ticks at `--hz` (default 10 Hz). Each tick:
1. UTE collects any inbound messages (stdin/queue/synthetic “tick” generator).
2. Connectome applies your void dynamics to the node field vector `W` (vectorized).
3. Metrics -> logs; UTD can emit text events opportunistically.
4. On schedule it saves a dashboard and a connectome image.

### Checkpoints
Engram checkpoints are saved as HDF5 (`.h5`) by default when `h5py` is available; otherwise snapshots fall back to `.npz`.
Use `--checkpoint-every S` to enable periodic saves; files live in `runs/<timestamp>/` as `state_<step>.h5` (or `.npz`).
Use `--checkpoint-keep K` to keep only the last K checkpoints (per format); set `0` to disable retention.

---

## CLI

```
python -m fum_rt.run_nexus [--neurons N] [--k K] [--hz HZ] [--domain NAME]
                           [--viz-every S] [--log-every S] [--checkpoint-every S]
                           [--duration S] [--use-time-dynamics/--no-time-dynamics]
                           [--seed SEED]
```

Domains supported (for auto‑modulation): `quantum`, `standard_model`, `dark_matter`,
`biology_consciousness`, `cosmogenesis`, `higgs`. You can override the factor in code if desired.

---

## Layout

- `fum_rt/run_nexus.py` - CLI entrypoint.
- `fum_rt/nexus.py` - the real‑time orchestrator.
- `fum_rt/core/void_dynamics_adapter.py` - loads your void functions or a minimal stub.
- `fum_rt/core/connectome.py` - kNN‑ish graph + vectorized update step.
- `fum_rt/core/metrics.py` - sparsity/cohesion/complexity metrics.
- `fum_rt/core/visualizer.py` - dashboard & graph rendering (matplotlib).
- `fum_rt/core/memory.py` - engram snapshots (.npz).
- `fum_rt/io/ute.py` - Universal Temporal Encoder (stdin & synthetic tick sources).
- `fum_rt/io/utd.py` - Universal Transduction Decoder (stdout & file sink).
- `fum_rt/utils/logging_setup.py` - structured logger helper.
- `requirements.txt` - only `numpy`, `networkx`, `matplotlib`.

All modules are tiny and documented so you can extend fast.

---

## Event scanning (UTD/Nexus)

During runs, UTD writes macro/text emissions to `runs/<timestamp>/utd_events.jsonl`.
Nexus writes structured logs (including speak gating) to `runs/<timestamp>/events.jsonl`.

A helper scanner is provided:

- Script: `tools/utd_event_scan.py`
- Purpose: Extract UTD “macro” (e.g., say) and “text” records, optionally include Nexus `speak_suppressed` events
- Output: NDJSON (default) or CSV

Examples:

```bash
# 1) Scan a specific run for “say” macros and print NDJSON
python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --macro say

# 2) Scan all runs, include Nexus speak_suppressed, write CSV
python tools/utd_event_scan.py runs --macro say --include-nexus --format csv --out say_events.csv

# 3) Include UTD status text payloads as well
python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --macro say --include-text

# 4) Persist a macro board synthesized from observed macros for a run
python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --emit-macro-board runs/2025-08-10_21-00-00/macro_board.json

# 5) Build a simple vocabulary from “say” texts
python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --emit-lexicon runs/2025-08-10_21-00-00/lexicon.json
```

### Macro board persistence

- At runtime, newly used macro names are automatically registered and persisted to `runs/<timestamp>/macro_board.json`.
- On startup, the Nexus also registers macro keys from:
  1) the run’s `macro_board.json` (preferred), or
  2) `fum_rt/io/lexicon/macro_board_min.json` (fallback).
- This allows new macro keys (e.g., `say`, `status`) to accumulate across runs without additional configuration.



---

## Language output: Macro board, phrase bank, and lexicon

Overview
- Macro board is a simple on-disk registry of macro names used by the UTD output path.
- Phrase bank provides optional sentence templates the runtime can use to compose richer “say” messages.
- Lexicon persists a lightweight vocabulary, learned from inbound text and emitted speech.

Files and where they live
- Macro board (auto‑persisted each run): runs/&lt;timestamp&gt;/macro_board.json
  - Runtime source: [fum_rt/io/utd.py](fum_rt/io/utd.py)
  - Nexus reads macros at boot: [fum_rt/nexus.py](fum_rt/nexus.py)
- Phrase bank (optional source of sentence templates, loaded at boot):
  - Per‑run: runs/&lt;timestamp&gt;/phrase_bank.json
  - Fallback: [fum_rt/io/lexicon/phrase_bank_min.json](fum_rt/io/lexicon/phrase_bank_min.json)
- Lexicon (auto‑learned vocabulary from inputs/outputs): runs/&lt;timestamp&gt;/lexicon.json
  - Grows during the run; periodically saved by the runtime

How the macro board populates
- Whenever the runtime emits a macro that is not already registered, it is auto‑registered and persisted to macro_board.json by the UTD.
- This requires no configuration. The file will appear in the active run directory as soon as a new macro key is used.

Example macro_board.json
```json
{
  "status": { "desc": "Emit structured status payload" },
  "say": {
    "desc": "Emit plain text line",
    "templates": [
      "Topology discovery: {keywords}",
      "Observation: {top1}, {top2} (vt={vt_entropy:.2f}, v={valence:.2f})",
      "Emergent structure: {keywords} (b1_z={b1_z:.2f})"
    ]
  }
}
```

Phrase bank (richer sentences)
- The runtime loads optional sentence templates for the “say” macro from either:
  - runs/&lt;timestamp&gt;/phrase_bank.json, or
  - [fum_rt/io/lexicon/phrase_bank_min.json](fum_rt/io/lexicon/phrase_bank_min.json)
- Expected shape:
```json
{
  "say": [
    "Topology discovery: {keywords}",
    "Exploration reveals {top1} linked to {top2} (coverage={vt_coverage:.2f})",
    "Coherent loop near {top1} ↔ {top2} (b1_z={b1_z:.2f})"
  ]
}
```
- Supported placeholders the runtime will fill from current metrics/context:
  - {keywords}, {top1}, {top2}
  - {vt_entropy}, {vt_coverage}, {b1_z}, {connectome_entropy}, {valence}

Lexicon (word bank)
- The runtime maintains runs/&lt;timestamp&gt;/lexicon.json as a simple token frequency store learned from:
  - Inbound UTE text messages (your input stream)
  - Emitted “say” lines
- The lexicon is used to extract keyword summaries and top tokens to slot into templates.
- You can also bootstrap a lexicon from logs using the scanner:
  - Script: [tools/utd_event_scan.py](tools/utd_event_scan.py)
  - Build lexicon JSON from observed “say” macros:
    ```bash
    python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --macro say --emit-lexicon runs/2025-08-10_21-00-00/lexicon.json
    ```

How to get more words and whole sentences
1) Feed more text into the UTE
- Stream domain texts to grow the lexicon automatically (no config needed).
- The more diverse and structured your input, the richer the learned vocabulary.

2) Provide more sentence templates
- Create runs/&lt;timestamp&gt;/phrase_bank.json with many “say” templates (see placeholders above).
- Templates rotate deterministically each emission; you can include longer, multi‑clause sentences.

3) Seed macro board metadata
- Add a “templates” array under the “say” key in macro_board.json (see example above). The runtime will use these at boot.

4) Tune self‑speak gating to allow more emissions
- Lower the valence threshold and tweak spike detector parameters via CLI:
  ```bash
  python -m fum_rt.run_nexus \
    --speak-auto \
    --speak-valence-thresh 0.35 \
    --speak-z 2.0 \
    --speak-hysteresis 0.5
  ```
- Emissions are still gated by topology spikes (B1 proxy) and valence for stability.

Where to see the outputs
- UTD events: runs/&lt;timestamp&gt;/utd_events.jsonl (type: "macro", macro: "say", args.text: "...").
- Optional Nexus logs (gating decisions, e.g., speak_suppressed): runs/&lt;timestamp&gt;/events.jsonl.

Scanner quick start (to audit what the model “tried to say”)
```bash
# Scan a run for “say” macros (NDJSON)
python tools/utd_event_scan.py runs/2025-08-10_21-00-00 --macro say

# Include Nexus speak_suppressed and write CSV
python tools/utd_event_scan.py runs --macro say --include-nexus --format csv --out say_events.csv
```

Operational notes
- Macro names are persisted automatically when first used; no manual step required.
- Phrase bank and macro board are complementary; phrase bank supplies sentence templates, macro board is the registry of macro keys and optional metadata like “templates”.
- The runtime keeps everything compute‑light: deterministic template filling with keywords/tokens from the live lexicon and metrics.


---

## Domain, Phase Control, and Cycles (Topology Complexity)

This section explains three runtime concepts that appear in profiles and logs: domain, phase control, and the “cycles” metric.

### Domain

- Purpose: Selects a modulation factor for the void equations, scaling both the growth and decay elemental deltas before each tick. The value is computed by [get_domain_modulation()](fum_rt/core/void_dynamics_adapter.py:46), and is passed into both Δα and Δω inside the adapter.
- How it works:
  - If you provide your own Void_Debt_Modulation on PYTHONPATH (class with get_universal_domain_modulation), the adapter uses it to obtain domain_modulation.
  - Otherwise, a safe fallback mapping is used internally and the modulation is computed from built‑in targets and the ALPHA/BETA ratio of the void equations.
- Supported presets (fallback path) include: quantum, standard_model, dark_matter, biology_consciousness, cosmogenesis, higgs. Any unknown string (e.g., "math_physics") resolves to a baseline default in the fallback path unless your module overrides it.
- Where it is applied: The scalar is fed into the void equations through the adapter and then consumed by the Connectome during step() each tick.

Examples
- CLI:
  python -m fum_rt.run_nexus --domain biology_consciousness
- Profile JSON (run_profiles/*.json):
  "domain": "math_physics"

To customize the mapping:
- Add a Python module on PYTHONPATH that exposes a class VoidDebtModulation with get_universal_domain_modulation(domain) → {"domain_modulation": float}. See the adapter’s import logic in [get_domain_modulation()](fum_rt/core/void_dynamics_adapter.py:46) for how it is discovered. A reference template for domain modulation also exists in [computational_proofs/Void_Debt_Modulation.py](computational_proofs/Void_Debt_Modulation.py).

---

### Phase control

- Purpose: A simple, file‑driven control plane that lets you switch between pre‑tuned “profiles” at runtime without restarting. It adjusts:
  - Speak gates (z threshold, hysteresis, cooldown, valence threshold)
  - Connectome traversal/homeostasis parameters (walkers, hops, bundle_size, prune_factor)
  - Optional structural knobs (threshold, lambda_omega, candidates)
- Where: See default profile definitions in [Nexus._default_phase_profiles()](fum_rt/nexus.py:339).
- How it’s applied:
  - The runtime polls runs/<timestamp>/phase.json each tick via [Nexus._poll_control()](fum_rt/nexus.py:403).
  - When the file exists and its mtime changes, the profile is merged and applied immediately. The path is set at startup in [Nexus.__init__ → phase_file](fum_rt/nexus.py:183).
- On vs Off:
  - OFF: If runs/<timestamp>/phase.json does not exist, no phase control is applied (runtime uses current CLI values and defaults).
  - ON: Create runs/<timestamp>/phase.json and write a profile (see example below). Edits to the file are picked up live.

Example phase.json
{
  "phase": 1,
  "speak": {
    "speak_z": 2.5,
    "speak_hysteresis": 0.8,
    "speak_cooldown_ticks": 10,
    "speak_valence_thresh": 0.35
  },
  "connectome": {
    "walkers": 384,
    "hops": 4,
    "bundle_size": 3,
    "prune_factor": 0.10,
    "threshold": 0.15,
    "lambda_omega": 0.10,
    "candidates": 64
  }
}

Notes
- Simple toggle: Create the file to enable; remove/rename to disable.
- Merging: If you only specify {"phase": n}, the defaults for that phase are loaded; any extra fields you include override those defaults.
- Safety: All updates are range‑checked and applied only if a matching attribute exists on the current connectome.

---

### “Cycles” metric (complexity_cycles)

- Definition: A topology‑only proxy for the number of simple cycles in the active subgraph. It is computed from the active graph induced by W[i]*W[j] > threshold. The proxy is the cyclomatic complexity formula:
  cycles = E_active - N + C_active
  where E_active is the number of active edges, N is the number of nodes, and C_active is the number of connected components over the active nodes.
- Implementations:
  - Dense backend: [Connectome.cyclomatic_complexity()](fum_rt/core/connectome.py:375)
  - Sparse backend: [SparseConnectome.cyclomatic_complexity()](fum_rt/core/sparse_connectome.py:393)
- How it’s used:
  - The Nexus uses complexity_cycles each tick as a “B1 proxy” input to a streaming z‑score detector. See the B1 update inside [Nexus.run()](fum_rt/nexus.py:442) where b1_value is taken from m["complexity_cycles"] and fed to the z‑spike detector to gate speaking.
  - It can be augmented with additional cycle signals from the traversal/ADC subsystem; the Nexus folds such findings into the metric before gating.

Why it matters
- Phase control does not “only affect cycles.” It adjusts traversal/homeostasis/speak gates that indirectly influence many structure‑and‑dynamics metrics (coverage, entropy, cohesion components, active edges, density, and thus cycles). Cycles is highlighted because it’s an effective, void‑native trigger for salient topology events and is used to gate autonomous speaking.

Toggle summary
- Domain: CLI/profile string selecting void‑equation modulation; fallback mapping is internal unless you provide your own implemention. See [get_domain_modulation()](fum_rt/core/void_dynamics_adapter.py:46).
- Phase control: Enabled when runs/<timestamp>/phase.json exists; disabled when it doesn’t. Profiles are defined in [Nexus._default_phase_profiles()](fum_rt/nexus.py:339), polled by [Nexus._poll_control()](fum_rt/nexus.py:403), and applied live.
- Cycles: Topology‑only cycle count proxy computed each tick (dense/sparse backends above). Used by the streaming detector to gate “say” macro emissions.
]]></content>
    </file>
    <file>
      <path>__init__.py</path>
      <content><![CDATA[# FUM real-time runtime package
__all__ = ["*"]]]></content>
    </file>
    <file>
      <path>api/README.md</path>
      <content/>
    </file>
    <file>
      <path>api/llms/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/google/gemini_client.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/google/model_loader.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/ollama/ollama_client.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/openai/openai_client.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/openrouter/openrouter_client.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/providers/xai/xai_client.py</path>
      <content/>
    </file>
    <file>
      <path>api/llms/router.py</path>
      <content/>
    </file>
    <file>
      <path>ck/README.md</path>
      <content/>
    </file>
    <file>
      <path>ck/build.sh</path>
      <content><![CDATA[
    #!/usr/bin/env bash
    set -euo pipefail
    python3 - <<'PY'
import sys, os, subprocess, textwrap
from pathlib import Path
code = r"""
from setuptools import setup, Extension
from setuptools.command.build_ext import build_ext
import os

hipcc = os.environ.get("HIPCC", "hipcc")

ext = Extension(
    "fum_ck",
    sources=["ck/hip_spmv.cpp"],
    extra_compile_args=[f"-std=c++17"],
    # You may need to add include/library paths for pybind11 and HIP here.
)

setup(
    name="fum_ck",
    version="0.0.1",
    ext_modules=[ext],
)
"""
Path("setup.py").write_text(code)
subprocess.check_call([sys.executable, "setup.py", "build_ext", "--inplace"])
PY
]]></content>
    </file>
    <file>
      <path>ck/hip_spmv.cpp</path>
      <content><![CDATA[
// Minimal HIP CSR SpMV placeholder (A*y = x) with pybind11 bindings.
// Replace with tuned Composable Kernel kernels as you iterate.
#include <hip/hip_runtime.h>
#include <pybind11/pybind11.h>
#include <pybind11/numpy.h>
namespace py = pybind11;

__global__ void spmv_csr_kernel(
    const int N,
    const int* __restrict__ indptr,
    const int* __restrict__ indices,
    const float* __restrict__ data,
    const float* __restrict__ x,
    float* __restrict__ y
){
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N){
        float sum = 0.f;
        int start = indptr[row];
        int end   = indptr[row+1];
        for (int p=start; p<end; ++p){
            sum += data[p] * x[ indices[p] ];
        }
        y[row] = sum;
    }
}

py::array_t<float> spmv_csr(py::array_t<int> indptr,
                            py::array_t<int> indices,
                            py::array_t<float> data,
                            py::array_t<float> x){
    auto buf_indptr = indptr.request();
    auto buf_indices = indices.request();
    auto buf_data = data.request();
    auto buf_x = x.request();

    int N = buf_indptr.shape[0] - 1;
    auto y = py::array_t<float>(N);
    auto buf_y = y.request();

    // NOTE: This placeholder copies to device each call. For production,
    // allocate device buffers once and stream updates.
    int *d_indptr, *d_indices;
    float *d_data, *d_x, *d_y;
    hipMalloc(&d_indptr, buf_indptr.size * sizeof(int));
    hipMalloc(&d_indices, buf_indices.size * sizeof(int));
    hipMalloc(&d_data,   buf_data.size   * sizeof(float));
    hipMalloc(&d_x,      buf_x.size      * sizeof(float));
    hipMalloc(&d_y,      buf_y.size      * sizeof(float));

    hipMemcpy(d_indptr, buf_indptr.ptr, buf_indptr.size * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_indices, buf_indices.ptr, buf_indices.size * sizeof(int), hipMemcpyHostToDevice);
    hipMemcpy(d_data, buf_data.ptr, buf_data.size * sizeof(float), hipMemcpyHostToDevice);
    hipMemcpy(d_x, buf_x.ptr, buf_x.size * sizeof(float), hipMemcpyHostToDevice);

    int threads = 256;
    int blocks = (N + threads - 1) / threads;
    hipLaunchKernelGGL(spmv_csr_kernel, dim3(blocks), dim3(threads), 0, 0, N, d_indptr, d_indices, d_data, d_x, d_y);
    hipDeviceSynchronize();

    hipMemcpy(buf_y.ptr, d_y, buf_y.size * sizeof(float), hipMemcpyDeviceToHost);
    hipFree(d_indptr); hipFree(d_indices); hipFree(d_data); hipFree(d_x); hipFree(d_y);
    return y;
}

PYBIND11_MODULE(fum_ck, m){
    m.doc() = "FUM HIP CSR SpMV (placeholder)";
    m.def("spmv_csr", &spmv_csr, "CSR SpMV via HIP");
}
]]></content>
    </file>
    <file>
      <path>cli/args.py</path>
      <content><![CDATA[from __future__ import annotations

"""
CLI argument parser for the FUM runtime.

Behavior:
- Mirrors the legacy make_parser() previously defined in fum_rt/nexus.py exactly.
- Kept here to reduce nexus.py size and improve separation of concerns.
"""

import argparse


def make_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser()
    p.add_argument('--neurons', type=int, default=1000)
    p.add_argument('--k', type=int, default=12)
    p.add_argument('--hz', type=int, default=10)
    p.add_argument('--domain', type=str, default='biology_consciousness')
    p.add_argument('--viz-every', type=int, default=10)
    p.add_argument('--log-every', type=int, default=1)
    p.add_argument('--checkpoint-every', type=int, default=0)
    p.add_argument('--checkpoint-keep', type=int, default=5)
    p.add_argument('--duration', type=int, default=None)
    p.add_argument('--use-time-dynamics', dest='use_time_dynamics', action='store_true')
    p.add_argument('--no-time-dynamics', dest='use_time_dynamics', action='store_false')
    p.set_defaults(use_time_dynamics=True)
    p.add_argument('--seed', type=int, default=0)

    # Ultra-scale/sparse flags
    p.add_argument('--sparse-mode', dest='sparse_mode', action='store_true')
    p.add_argument('--dense-mode', dest='sparse_mode', action='store_false')
    # Aliases
    p.add_argument('--sparse', dest='sparse_mode', action='store_true')
    p.add_argument('--dense', dest='sparse_mode', action='store_false')
    p.set_defaults(sparse_mode=None)
    p.add_argument('--threshold', type=float, default=0.15)
    p.add_argument('--lambda-omega', dest='lambda_omega', type=float, default=0.1)
    p.add_argument('--candidates', type=int, default=64)
    p.add_argument('--walkers', type=int, default=256)
    p.add_argument('--hops', type=int, default=3)
    p.add_argument('--status-interval', dest='status_interval', type=int, default=1)
    p.add_argument('--bundle-size', dest='bundle_size', type=int, default=3)
    p.add_argument('--prune-factor', dest='prune_factor', type=float, default=0.10)

    # Text→connectome stimulation (symbol→group)
    p.add_argument('--stim-group-size', dest='stim_group_size', type=int, default=4)
    p.add_argument('--stim-amp', dest='stim_amp', type=float, default=0.05)
    p.add_argument('--stim-decay', dest='stim_decay', type=float, default=0.90)
    p.add_argument('--stim-max-symbols', dest='stim_max_symbols', type=int, default=64)

    # Self-speak and topology-spike detection (void-native)
    p.add_argument('--speak-auto', dest='speak_auto', action='store_true')
    p.add_argument('--no-speak-auto', dest='speak_auto', action='store_false')
    p.set_defaults(speak_auto=True)
    p.add_argument('--speak-z', dest='speak_z', type=float, default=1.0)
    p.add_argument('--speak-hysteresis', dest='speak_hysteresis', type=float, default=1.0)
    p.add_argument('--speak-cooldown-ticks', dest='speak_cooldown_ticks', type=int, default=10)
    p.add_argument('--speak-valence-thresh', dest='speak_valence_thresh', type=float, default=0.01)
    p.add_argument('--b1-half-life-ticks', dest='b1_half_life_ticks', type=int, default=50)

    # Announcement bus / ADC tuning
    p.add_argument('--bus-capacity', dest='bus_capacity', type=int, default=65536)
    p.add_argument('--bus-drain', dest='bus_drain', type=int, default=2048)
    p.add_argument('--r-attach', dest='r_attach', type=float, default=0.25)
    p.add_argument('--ttl-init', dest='ttl_init', type=int, default=120)
    p.add_argument('--split-patience', dest='split_patience', type=int, default=6)

    # Engram loader (optional)
    p.add_argument('--load-engram', dest='load_engram', type=str, default=None)
    # Optional embedded control server (disabled by default to avoid duplicate UI)
    p.add_argument('--control-server', dest='control_server', action='store_true')
    p.add_argument('--no-control-server', dest='control_server', action='store_false')
    p.set_defaults(control_server=False)
    # Allow explicit reuse of an existing run directory (resume), otherwise a new timestamp dir is used
    p.add_argument('--run-dir', dest='run_dir', type=str, default=None)

    return p


__all__ = ["make_parser"]]]></content>
    </file>
    <file>
      <path>core/README.md</path>
      <content><![CDATA[### fum_rt/core/README.md]]></content>
    </file>
    <file>
      <path>core/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>core/adc.py</path>
      <content><![CDATA[# adc.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Active Domain Cartography (ADC) - incremental, void-faithful reducer.

Design
- ADC consumes compact Observation events from the void-walker announcement bus.
- It never inspects raw W or dense adjacency; all inputs are announcements.
- Territories and boundaries are updated locally per event (O(1) per event).
- Provides lightweight map metrics for Nexus logging and self-speak decisions.

Territories
- Coarse "concept regions" indexed by a composite key (domain_hint, coverage_id).
- Each territory tracks EWMA stats (w_mean/var, s_mean), a mass (support), a confidence,
  and a TTL (decays unless reinforced).

Boundaries
- Abstract edges between territories tracking cut_strength EWMA, churn, and TTL.

Events
- region_stat: assimilation into a nearest territory (or create).
- boundary_probe: update boundary signal (if 2+ territories exist).
- cycle_hit: bumps a cycle counter (B1 proxy) surfaced in map metrics.
- novel_frontier: creates/boosts a new/sibling territory with low initial confidence.

This is a minimal, safe baseline. You can evolve the territory identity function
from (domain_hint, coverage_id) to a learned centroid distance when you add one.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Dict, Tuple, Optional, Iterable
import math
from .announce import Observation


@dataclass
class _EWMA:
    """Numerically stable EWMA with alpha in (0,1]."""
    alpha: float
    mean: float = 0.0
    var: float = 0.0
    init: bool = False

    def update(self, x: float):
        a = self.alpha
        if not self.init:
            self.mean = float(x)
            self.var = 0.0
            self.init = True
            return
        # Welford-style EMA
        delta = float(x) - self.mean
        self.mean += a * delta
        self.var = (1 - a) * (self.var + a * delta * delta)


@dataclass
class Territory:
    key: Tuple[str, int]  # (domain_hint, coverage_id)
    id: int
    mass: float = 0.0
    conf: float = 0.0
    ttl: int = 120
    w_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.15))
    s_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.15))

    def reinforce(self, w_mean: float, s_mean: float, add_mass: float, add_conf: float, ttl_init: int):
        self.w_stats.update(w_mean)
        self.s_stats.update(s_mean)
        self.mass += max(0.0, add_mass)
        self.conf = min(1.0, self.conf + max(0.0, add_conf))
        self.ttl = max(self.ttl, int(ttl_init))


@dataclass
class Boundary:
    a: int
    b: int
    cut_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.2))
    churn: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.2))
    ttl: int = 120

    def reinforce(self, cut_strength: float, ttl_init: int):
        prev = float(self.cut_stats.mean) if self.cut_stats.init else 0.0
        self.cut_stats.update(cut_strength)
        self.churn.update(abs(float(self.cut_stats.mean) - prev))
        self.ttl = max(self.ttl, int(ttl_init))


class ADC:
    def __init__(self, r_attach: float = 0.25, ttl_init: int = 120, split_patience: int = 6):
        self.r_attach = float(r_attach)
        self.ttl_init = int(max(1, ttl_init))
        self.split_patience = int(max(1, split_patience))

        self._territories: Dict[Tuple[str, int], Territory] = {}
        self._id_seq: int = 1
        self._boundaries: Dict[Tuple[int, int], Boundary] = {}
        self._frontier_counter: Dict[Tuple[str, int], int] = {}
        self._cycle_events: int = 0  # accumulated since last metrics call

    # --- Public API ---

    def update_from(self, observations: Iterable[Observation]) -> None:
        for o in observations:
            kind = getattr(o, "kind", "")
            if kind == "region_stat":
                self._accumulate_region(o)
            elif kind == "boundary_probe":
                self._accumulate_boundary(o)
            elif kind == "cycle_hit":
                self._note_cycle(o)
            elif kind == "novel_frontier":
                self._note_frontier(o)
        self._decay()

    def get_metrics(self) -> Dict[str, float]:
        """Return a small metrics dict and reset transient counters."""
        terr_count = len(self._territories)
        bnd_count = len(self._boundaries)
        cycles = self._cycle_events
        self._cycle_events = 0
        return {
            "adc_territories": int(terr_count),
            "adc_boundaries": int(bnd_count),
            "adc_cycle_hits": int(cycles),
        }

    # --- Internals ---

    def _territory_for(self, domain_hint: str, cov_id: int) -> Territory:
        key = (str(domain_hint or ""), int(cov_id))
        t = self._territories.get(key)
        if t is None:
            t = Territory(key=key, id=self._id_seq, ttl=self.ttl_init)
            self._id_seq += 1
            self._territories[key] = t
        return t

    def _accumulate_region(self, o: Observation):
        t = self._territory_for(o.domain_hint, o.coverage_id)
        # Attach if "close": here closeness is discretized by coverage bin match via key.
        # When you add a real centroid, use a distance threshold compared to r_attach.
        add_mass = max(1.0, float(len(o.nodes)))
        add_conf = 0.02
        t.reinforce(w_mean=float(o.w_mean), s_mean=float(o.s_mean),
                    add_mass=add_mass, add_conf=add_conf, ttl_init=self.ttl_init)

    def _accumulate_boundary(self, o: Observation):
        # Pick two "closest" territories by coverage bin neighborhood:
        # Here we approximate by choosing (domain_hint, cov_id) and (domain_hint, cov_id±1)
        t1 = self._territory_for(o.domain_hint, o.coverage_id)
        # neighbor bin
        neighbor_cov = int(max(0, min(9, int(o.coverage_id + (1 if (o.coverage_id % 2 == 0) else -1)))))
        t2 = self._territory_for(o.domain_hint, neighbor_cov)
        a, b = (t1.id, t2.id) if t1.id < t2.id else (t2.id, t1.id)
        key = (a, b)
        bnd = self._boundaries.get(key)
        if bnd is None:
            bnd = Boundary(a=a, b=b, ttl=self.ttl_init)
            self._boundaries[key] = bnd
        bnd.reinforce(float(o.cut_strength), ttl_init=self.ttl_init)

    def _note_cycle(self, o: Observation):
        self._cycle_events += 1
        # Optionally: attach cycles to a territory using coverage bin
        t = self._territory_for(o.domain_hint, o.coverage_id)
        t.conf = min(1.0, t.conf + 0.01)
        t.ttl = max(t.ttl, self.ttl_init)

    def _note_frontier(self, o: Observation):
        key = (str(o.domain_hint or ""), int(o.coverage_id))
        cnt = self._frontier_counter.get(key, 0) + 1
        self._frontier_counter[key] = cnt
        if cnt >= self.split_patience:
            # Create or boost a new sibling territory by nudging coverage bin
            sib_cov = int(max(0, min(9, int(o.coverage_id + 1))))
            sib = self._territory_for(o.domain_hint, sib_cov)
            sib.reinforce(w_mean=float(o.w_mean), s_mean=float(o.s_mean),
                          add_mass=max(1.0, float(len(o.nodes))), add_conf=0.05, ttl_init=self.ttl_init)
            self._frontier_counter[key] = 0

    def _decay(self):
        # TTL decay and garbage collection for stale items with low confidence/mass
        drop_terr = []
        for key, t in self._territories.items():
            t.ttl -= 1
            if t.ttl <= 0 and (t.conf < 0.05 or t.mass < 5.0):
                drop_terr.append(key)
        for key in drop_terr:
            self._territories.pop(key, None)

        drop_bnd = []
        for key, b in self._boundaries.items():
            b.ttl -= 1
            if b.ttl <= 0 and (not b.cut_stats.init or b.cut_stats.mean < 1e-4):
                drop_bnd.append(key)
        for key in drop_bnd:
            self._boundaries.pop(key, None)]]></content>
    </file>
    <file>
      <path>core/announce.py</path>
      <content><![CDATA[# announce.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event schema for the void-walker announcement bus (Active Domain Cartography input).

Blueprint alignment:
- Use void equations for traversal/measuring: walkers traverse using your RE-VGSP/GDSP deltas
  and publish compact observations only (no W dumps).
- ADC consumes only these observations to maintain territories/boundaries incrementally.
- This keeps introspection cost proportional to the number of announcements, not to N.

Kinds:
- "region_stat": aggregate stats for a small visited set (mean/var over W, mean coupling S_ij)
- "boundary_probe": evidence of a low-coupling cut between neighborhoods (candidate boundary)
- "cycle_hit": a loop was closed during a walk (B1 proxy event)
- "novel_frontier": sustained novelty ridge / new subdomain frontier

Notes:
- All floats are plain Python floats to keep JSON-friendly if logged.
- nodes is small (sampled IDs visited during the walk). Keep it compact (<= ~64).
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Dict, Any


@dataclass
class Observation:
    tick: int
    kind: str  # "region_stat" | "boundary_probe" | "cycle_hit" | "novel_frontier"
    # Small, representative subset of node ids touched during the walk or boundary sample
    nodes: List[int] = field(default_factory=list)

    # Optional centroid in an embedding space if available (not required)
    centroid: Optional[Tuple[float, float, float]] = None

    # Aggregate stats over the local visited set
    w_mean: float = 0.0
    w_var: float = 0.0
    s_mean: float = 0.0  # mean positive coupling encountered during the walk

    # Boundary-specific signal (strength of cut across a small boundary sample)
    cut_strength: float = 0.0

    # Cycle-specific fields
    loop_len: int = 0
    loop_gain: float = 0.0  # accumulated positive transition weights along the loop

    # Coverage bin for ADC scheduling and map updates (e.g., int(vt_coverage*10))
    coverage_id: int = 0

    # Optional hint from the domain/cartographer
    domain_hint: str = ""

    # Extra metadata bag for future-proofing (small; JSON-serializable only)
    meta: Dict[str, Any] = field(default_factory=dict)


def validate_observation(o: Observation) -> bool:
    """Light sanity checks to avoid corrupting the bus/ADC with malformed events."""
    if not isinstance(o.tick, int) or o.tick < 0:
        return False
    if o.kind not in ("region_stat", "boundary_probe", "cycle_hit", "novel_frontier"):
        return False
    if not isinstance(o.nodes, list):
        return False
    # keep nodes tiny to avoid large payloads by accident
    if len(o.nodes) > 256:
        return False
    return True]]></content>
    </file>
    <file>
      <path>core/bus.py</path>
      <content><![CDATA[# bus.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Lock-free announcement bus for void-walker observations (ADC input).

Blueprint alignment:
- Walkers traverse with void equations and publish compact Observation packets.
- ADC consumes only these announcements to maintain territories/boundaries incrementally.
- Cost is proportional to number of announcements, not to graph size.

Usage
- Producer (connectome walkers):
    bus.publish(observation)
- Consumer (Nexus/ADC loop):
    batch = bus.drain(max_items=2048)

Behavior
- Bounded deque with overwrite-on-full semantics (drop oldest) to keep runtime stable.
"""

from __future__ import annotations
from collections import deque
from typing import Deque, List, Any, Optional


class AnnounceBus:
    """Bounded, overwrite-on-full FIFO for Observation events."""
    def __init__(self, capacity: int = 65536):
        self._q: Deque[Any] = deque(maxlen=int(max(1, capacity)))

    @property
    def capacity(self) -> int:
        return int(self._q.maxlen or 0)

    def size(self) -> int:
        return len(self._q)

    def publish(self, obs: Any) -> None:
        """
        Append an event; when full, the oldest is dropped automatically.
        This keeps the system stable under load without backpressure deadlocks.
        """
        self._q.append(obs)

    def drain(self, max_items: int = 2048) -> List[Any]:
        """
        Pop up to max_items from the left, returning them in arrival order.
        """
        n = min(int(max_items), len(self._q))
        out: List[Any] = []
        append = out.append
        for _ in range(n):
            append(self._q.popleft())
        return out

    def clear(self) -> None:
        self._q.clear()]]></content>
    </file>
    <file>
      <path>core/connectome.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os as _os

# Dense backend is validation-only; forbid import unless FORCE_DENSE is explicitly set.
# This prevents accidental dense usage in runtime paths and enforces sparse-first policy.
if str(_os.getenv("FORCE_DENSE", "0")).strip().lower() not in ("1", "true", "yes", "on", "y", "t"):
    raise RuntimeError("Dense connectome is validation-only. Set FORCE_DENSE=1 to enable for tests.")

import numpy as np
import networkx as nx
from .void_dynamics_adapter import universal_void_dynamics, delta_re_vgsp, delta_gdsp
from .fum_structural_homeostasis import perform_structural_homeostasis
from .announce import Observation  # event schema for ADC bus

class Connectome:
    def __init__(self, N: int, k: int, seed: int = 0, threshold: float = 0.15, lambda_omega: float = 0.1, candidates: int = 64, structural_mode: str = "alias", traversal_walkers: int = 256, traversal_hops: int = 3, bundle_size: int = 3, prune_factor: float = 0.10):
        self.N = N
        self.k = k
        self.rng = np.random.default_rng(seed)
        self.threshold = threshold  # active synapse threshold on edge weights
        self.lambda_omega = lambda_omega  # penalty weight for structural plasticity (Ω)
        self.candidates = int(max(1, candidates))  # candidate samples per node for structure search
        self.structural_mode = structural_mode      # "alias" (default) | "dense"
        # Node state
        self.W = self.rng.uniform(0.0, 1.0, size=(N,)).astype(np.float32)
        # Start with empty topology; growth is dictated purely by Void Equations
        self.A = np.zeros((N, N), dtype=np.int8)
        # Graph objects are built on demand for visualization/metrics
        self.G = nx.Graph()
        # Edge weights derived from node states initially (zeros)
        self.E = self._edge_weights_from_W()
        # Void‑equation traversal configuration (Blueprint: "void equations for traversal and measuring")
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))
        # Findings propagated each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Void‑equation traversal configuration (Blueprint: "void equations for traversal and measuring")
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))
        # Findings propagated each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Local tick counter for announcements (incremented each step)
        self._tick = 0
        # Homeostasis tuning (Blueprint Rule 4/4.1)
        self.bundle_size = int(max(1, bundle_size))
        self.prune_factor = float(max(0.0, prune_factor))
        # External stimulation buffer (deterministic symbol→group)
        self._stim = np.zeros(N, dtype=np.float32)
        self._stim_decay = 0.90

    def _ring_lattice(self, N, k):
        # Each node connected to k nearest neighbors (k must be even)
        k = max(2, k + (k % 2))
        A = np.zeros((N, N), dtype=np.int8)
        for i in range(N):
            for d in range(1, k//2 + 1):
                j = (i + d) % N
                h = (i - d) % N
                A[i, j] = 1
                A[i, h] = 1
        return A

    def _edge_weights_from_W(self):
        # Symmetric edge weights derived from node states
        E = (np.outer(self.W, self.W) * self.A).astype(np.float32)
        return E

    def stimulate_indices(self, idxs, amp: float = 0.05):
        """
        Deterministic stimulus injection:
        - idxs: iterable of neuron indices to stimulate
        - amp: additive boost to the stimulus buffer (decays each tick)
        """
        try:
            if idxs is None:
                return
            idxs = np.asarray(list(set(int(i) % self.N for i in idxs)), dtype=np.int64)
            if idxs.size == 0:
                return
            # accumulate into a decaying stimulus buffer that feeds ReLU(Δalpha)
            self._stim[idxs] = np.clip(self._stim[idxs] + float(amp), 0.0, 1.0)
            # small immediate bump to W to seed associations
            self.W[idxs] = np.clip(self.W[idxs] + 0.01 * float(amp), 0.0, 1.0)
        except Exception:
            # maintain runtime continuity
            pass

    # --- Alias sampler (Vose) to sample candidates ~ ReLU(Δalpha) in O(N) build + O(1) draw ---
    def _build_alias(self, p: np.ndarray):
        n = p.size
        if n == 0:
            return np.array([], dtype=np.float32), np.array([], dtype=np.int32)
        p = p.astype(np.float64, copy=False)
        s = float(p.sum())
        if s <= 0:
            p = np.full(n, 1.0 / n, dtype=np.float64)
        else:
            p = p / s
        prob = np.zeros(n, dtype=np.float64)
        alias = np.zeros(n, dtype=np.int32)
        scaled = p * n
        small = [i for i, v in enumerate(scaled) if v < 1.0]
        large = [i for i, v in enumerate(scaled) if v >= 1.0]
        while small and large:
            s_idx = small.pop()
            l_idx = large.pop()
            prob[s_idx] = scaled[s_idx]
            alias[s_idx] = l_idx
            scaled[l_idx] = scaled[l_idx] - (1.0 - prob[s_idx])
            if scaled[l_idx] < 1.0:
                small.append(l_idx)
            else:
                large.append(l_idx)
        for i in large:
            prob[i] = 1.0
        for i in small:
            prob[i] = 1.0
        return prob.astype(np.float32), alias

    def _alias_draw(self, prob: np.ndarray, alias: np.ndarray, s: int):
        n = prob.size
        if n == 0 or s <= 0:
            return np.array([], dtype=np.int64)
        k = self.rng.integers(0, n, size=s, endpoint=False)
        u = self.rng.random(s)
        choose_alias = (u >= prob[k])
        out = k.copy()
        out[choose_alias] = alias[k[choose_alias]]
        return out.astype(np.int64)

    def _void_traverse(self, a: np.ndarray, om: np.ndarray):
        """
        Continuous void‑equation traversal (Blueprint mandate: use void equations for traversal/measuring)
        - Seeds walkers proportionally to ReLU(Δalpha)
        - Transition weight to neighbor j: max(0, a[i]*a[j] - λ*|ω_i-ω_j|)
        - Simulates small number of hops per walker; accumulates visit histogram
        Complexity: ~O(N*k + walkers*hops). k is current degree bound per node.
        Produces findings propagated to metrics/SIE/ADC: coverage, entropy, unique_nodes, mean_a/omega.
        Also publishes compact Observation events to the ADC bus if present.
        """
        N = self.N
        walkers = self.traversal_walkers
        hops = self.traversal_hops

        # Seed distribution ~ a (ReLU(Δalpha))
        prob, alias = self._build_alias(a)
        seeds = self._alias_draw(prob, alias, walkers)
        visit = np.zeros(N, dtype=np.int32)

        # Optional ADC bus
        bus = getattr(self, "bus", None)
        tick = int(getattr(self, "_tick", 0))

        # Event accumulators (kept small)
        sample_cap = 64
        sel_w_sum = 0.0
        sel_steps = 0
        cycle_events = 0
        sample_nodes = set()

        for s in seeds:
            i = int(s)
            cur = i
            seen = {cur: 0}  # for simple loop detection within this walk
            path = [cur]
            for step_idx in range(1, hops + 1):
                nbrs = np.nonzero(self.A[cur])[0]
                if nbrs.size == 0:
                    break
                # weights by void affinity
                w = a[cur] * a[nbrs] - self.lambda_omega * np.abs(om[cur] - om[nbrs])
                w = np.clip(w, 0.0, None)
                if np.all(w <= 0):
                    break
                # sample next neighbor proportional to w
                wp = w / (w.sum() + 1e-12)
                r = self.rng.random()
                cdf = np.cumsum(wp)
                idx = int(np.searchsorted(cdf, r, side="right"))
                nxt = int(nbrs[min(idx, nbrs.size - 1)])
                visit[nxt] += 1

                # Accumulate event stats
                sel_w = float(w[min(idx, nbrs.size - 1)])
                sel_w_sum += max(0.0, sel_w)
                sel_steps += 1
                if len(sample_nodes) < sample_cap:
                    sample_nodes.add(nxt)

                # Simple cycle detection: return to a previously visited node on this walk
                if nxt in seen:
                    cycle_events += 1
                    # Publish cycle_hit immediately if bus is present (use lightweight payload)
                    if bus is not None:
                        try:
                            obs = Observation(
                                tick=tick,
                                kind="cycle_hit",
                                nodes=[cur, nxt],
                                w_mean=float(a.mean()),
                                w_var=float(a.var()),
                                s_mean=0.0,
                                loop_len=int(len(path) - seen[nxt] + 1),
                                loop_gain=float(sel_w),
                                coverage_id=0,
                                domain_hint=""
                            )
                            bus.publish(obs)
                        except Exception:
                            pass
                else:
                    seen[nxt] = step_idx
                    path.append(nxt)

                cur = nxt

        total_visits = int(visit.sum())
        unique = int(np.count_nonzero(visit))
        coverage = float(unique) / float(max(1, N))
        if total_visits > 0:
            p = visit.astype(np.float64) / float(total_visits)
            p = p[p > 0]
            vt_entropy = float(-(p * np.log(p)).sum())
        else:
            vt_entropy = 0.0

        # Store findings for external consumers (Nexus metrics/SIE/ADC)
        self.findings = {
            "vt_visits": total_visits,
            "vt_unique": unique,
            "vt_coverage": coverage,
            "vt_entropy": vt_entropy,
            "vt_walkers": float(walkers),
            "vt_hops": float(hops),
            "a_mean": float(a.mean()),
            "omega_mean": float(om.mean()),
        }

        # Publish a compact region_stat at end of traversal
        if bus is not None:
            try:
                s_mean = float(sel_w_sum / max(1, sel_steps))
                cov_id = int(min(9, max(0, int(coverage * 10.0))))
                obs = Observation(
                    tick=tick,
                    kind="region_stat",
                    nodes=list(sample_nodes),
                    w_mean=float(self.W.mean()),
                    w_var=float(self.W.var()),
                    s_mean=s_mean,
                    coverage_id=cov_id,
                    domain_hint=""
                )
                bus.publish(obs)
            except Exception:
                pass
    
            # Increment local tick for announcement timestamps
            try:
                self._tick += 1
            except Exception:
                pass


    def step(self, t: float, domain_modulation: float, sie_drive: float = 1.0, use_time_dynamics: bool=True):
        """
        Apply one update tick driven entirely by Void Equations:
        - Structural growth/rewiring: candidates sampled via alias table built from ReLU(Δalpha)
        - Affinity S_ij = ReLU(Δalpha_i) * ReLU(Δalpha_j) - λ * |Δomega_i - Δomega_j|
        - Top‑k neighbors per node; symmetric adjacency
        - Node field update via universal_void_dynamics, multiplicatively gated by SIE valence (Rule 3)
        """
        # 1) Compute elemental deltas from your void equations
        d_alpha = delta_re_vgsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        d_omega = delta_gdsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        a = np.maximum(0.0, d_alpha.astype(np.float32))  # ReLU(Δalpha)
        om = d_omega.astype(np.float32)
        # External stimulation: add and decay deterministic symbol→group drive
        try:
            a = np.clip(a + self._stim, 0.0, None)
            self._stim *= getattr(self, "_stim_decay", 0.90)
        except Exception:
            pass

        # 2) Build candidate sampler ~ a (ReLU(Δalpha))
        prob, alias = self._build_alias(a)

        # 3) Per-node top-k neighbors by sampling; complexity ~ O(N * candidates)
        N = self.N
        k = int(max(1, self.k))
        s = int(max(self.candidates, 2 * k))
        A_new = np.zeros((N, N), dtype=np.int8)

        if self.structural_mode == "dense" and N <= 4096:
            # Exact affinity (validation/small N)
            S = a[:, None] * a[None, :] - self.lambda_omega * np.abs(om[:, None] - om[None, :])
            np.fill_diagonal(S, -np.inf)
            idx_topk = np.argpartition(S, -k, axis=1)[:, -k:]
            rows = np.repeat(np.arange(N), k)
            cols = idx_topk.reshape(-1)
            A_new[rows, cols] = 1
        else:
            # Alias sampling per node (default, efficient, void‑guided traversal)
            for i in range(N):
                js = self._alias_draw(prob, alias, s)
                # drop self and dupes
                js = js[js != i]
                if js.size == 0:
                    continue
                js = np.unique(js)  # s is small; set semantics OK
                # score by void affinity
                Si = a[i] * a[js] - self.lambda_omega * np.abs(om[i] - om[js])
                take = min(k, Si.size)
                idx = np.argpartition(Si, -take)[-take:]
                nbrs = js[idx]
                A_new[i, nbrs] = 1

        # Undirected symmetrization
        A_new = np.maximum(A_new, A_new.T)
        self.A = A_new

        # 4) Update node field with combined universal dynamics, gated by SIE valence (in [0,1])
        dW = universal_void_dynamics(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        dW = (float(max(0.0, min(1.0, sie_drive))) * dW).astype(np.float32)
        self.W = np.clip(self.W + dW, 0.0, 1.0)

        # 4.1) SIE v2 intrinsic reward/valence from W and dW (void-native)
        try:
            if not hasattr(self, "_sie2"):
                from .sie_v2 import SIECfg, SIEState
                self._sie2 = SIEState(self.N, SIECfg())
            from .sie_v2 import sie_step
            r_vec, v01 = sie_step(self._sie2, self.W, dW)
            self._last_sie2_reward = float(np.mean(r_vec))
            self._last_sie2_valence = float(v01)
        except Exception:
            pass

        # 5) Edge weights follow nodes on the updated topology
        self.E = self._edge_weights_from_W()

        # 5.1) Stage‑1 cohesion repair + pruning via structural homeostasis (void‑affinity)
        try:
            labels = self.component_labels()
            perform_structural_homeostasis(
                self,
                labels=labels,
                d_alpha=d_alpha,
                d_omega=d_omega,
                lambda_omega=self.lambda_omega,
                bundle_size=int(getattr(self, "bundle_size", 3)),
                prune_factor=float(getattr(self, "prune_factor", 0.10))
            )
        except Exception:
            # Keep runtime alive even if homeostasis step fails
            pass
        
        # 6) Continuous void‑equation traversal to propagate findings for Global System
        try:
            self._void_traverse(a, om)
        except Exception:
            # Keep system alive even if traversal fails; findings may be stale
            pass

    def active_edge_count(self):
        return int((self.E > self.threshold).sum() // 2)  # undirected

    def connected_components(self):
        # Cohesion via topology-only graph (Stage 1)
        G = nx.from_numpy_array(self.A.astype(int), create_using=nx.Graph)
        return nx.number_connected_components(G)
    
    def component_labels(self):
        # Labels for topology-only cohesion (Stage 1)
        G = nx.from_numpy_array(self.A.astype(int), create_using=nx.Graph)
        labels = np.zeros(self.N, dtype=int)
        for idx, comp in enumerate(nx.connected_components(G)):
            for n in comp:
                labels[int(n)] = idx
        return labels
    
    def cyclomatic_complexity(self):
        # For the active subgraph: cycles = E - N + C
        mask = (self.E > self.threshold)
        G_act = nx.from_numpy_array(mask.astype(int), create_using=nx.Graph)
        n = G_act.number_of_nodes()
        e = G_act.number_of_edges()
        c = nx.number_connected_components(G_act)
        return max(0, e - n + c)
    
    def snapshot_graph(self):
        # Return a NetworkX graph (active subgraph) for drawing
        mask = (self.E > self.threshold)
        G_act = nx.from_numpy_array(mask.astype(int), create_using=nx.Graph)
        return G_act
]]></content>
    </file>
    <file>
      <path>core/control_server.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

# control_server.py
# Lightweight local control server to expose a "Load Engram" button/page.
# - Serves a minimal HTML UI at http://127.0.0.1:<port>/
# - Accepts POST /api/load_engram with JSON {"path": "<engram file path>"}
# - Writes/updates runs/<ts>/phase.json with {"load_engram": "<path>"} so Nexus control plane will pick it up.

import os
import json
import threading
from http.server import HTTPServer, BaseHTTPRequestHandler
import socketserver
from urllib.parse import urlparse

_HTML = r'''<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FUM Control - Load Engram</title>
  <style>
    :root { --fg: #e6edf3; --bg: #0d1117; --muted: #8b949e; --accent: #2f81f7; --danger: #f85149; }
    body { background: var(--bg); color: var(--fg); font: 14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica Neue, Arial, sans-serif; margin: 0; padding: 0; }
    .wrap { max-width: 880px; margin: 32px auto; padding: 16px 20px; }
    h1 { margin: 0 0 16px 0; font-size: 20px; }
    p.note { color: var(--muted); }
    .card { border: 1px solid #30363d; border-radius: 8px; padding: 16px; margin: 16px 0; background: #161b22; }
    label { display: block; margin-bottom: 6px; color: var(--muted); }
    input[type=text] {
      width: 100%; padding: 10px 12px; border-radius: 6px; border: 1px solid #30363d; background: #0d1117; color: var(--fg);
    }
    .row { display: flex; gap: 8px; align-items: center; margin-top: 10px; }
    button {
      padding: 10px 16px; border-radius: 6px; border: 1px solid #30363d; background: var(--accent); color: white; cursor: pointer;
    }
    button:disabled { opacity: 0.6; cursor: not-allowed; }
    .status { margin-top: 12px; min-height: 20px; }
    .ok { color: #3fb950; }
    .err { color: var(--danger); }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, monospace; }
    .footer { margin-top: 24px; color: var(--muted); font-size: 12px; }
    code { background: #0b1220; padding: 2px 6px; border-radius: 4px; border: 1px solid #30363d; }
  </style>
</head>
<body>
  <div class="wrap">
    <h1>FUM Control - Load Engram</h1>
    <p class="note">Run directory: <span class="mono" id="runDir"></span></p>

    <div class="card">
      <label for="engram">Engram path (.h5 or .npz)</label>
      <input id="engram" type="text" placeholder="e.g. runs/20250811_155023/state_23220.h5" />
      <div class="row">
        <button id="btnLoad">Load Engram</button>
        <span class="mono" id="busy" style="display:none">loading…</span>
      </div>
      <div class="status" id="status"></div>
    </div>

    <div class="footer">
      The button sets <code>load_engram</code> in your run's <code>phase.json</code>; Nexus will hot-load it on the next poll and then clear the field.
    </div>
  </div>

  <script>
    const runDirSpan = document.getElementById('runDir');
    fetch('/api/status').then(r => r.json()).then(js => {
      runDirSpan.textContent = js.run_dir || '(unknown)';
    }).catch(() => { runDirSpan.textContent = '(unknown)'; });

    const el = (id) => document.getElementById(id);
    el('btnLoad').addEventListener('click', async () => {
      const path = el('engram').value.trim();
      const btn = el('btnLoad');
      const busy = el('busy');
      const status = el('status');
      status.textContent = '';
      status.className = 'status';
      if (!path) {
        status.textContent = 'Please enter a file path.';
        status.classList.add('err');
        return;
      }
      btn.disabled = true; busy.style.display = 'inline';
      try {
        const res = await fetch('/api/load_engram', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ path })
        });
        const js = await res.json().catch(() => ({}));
        if (res.ok && js.ok) {
          status.textContent = 'Queued load_engram: ' + (js.path || path);
          status.classList.add('ok');
        } else {
          status.textContent = 'Error: ' + (js.error || ('HTTP ' + res.status));
          status.classList.add('err');
        }
      } catch (err) {
        status.textContent = 'Request failed';
        status.classList.add('err');
      } finally {
        btn.disabled = false; busy.style.display = 'none';
      }
    });
  </script>
</body>
</html>
'''

class ThreadingHTTPServer(socketserver.ThreadingMixIn, HTTPServer):
    daemon_threads = True
    allow_reuse_address = True


class _Handler(BaseHTTPRequestHandler):
    # Server context attached at runtime: self.server.ctx = { 'run_dir': ..., 'phase_file': ... }

    def _json(self, code: int, obj: dict):
        try:
            payload = json.dumps(obj).encode("utf-8")
        except Exception:
            payload = b'{}'
        self.send_response(code)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.send_header("Cache-Control", "no-store")
        self.end_headers()
        try:
            self.wfile.write(payload)
        except Exception:
            pass

    def _text(self, code: int, html: str):
        try:
            data = html.encode("utf-8")
        except Exception:
            data = b""
        self.send_response(code)
        self.send_header("Content-Type", "text/html; charset=utf-8")
        self.end_headers()
        try:
            self.wfile.write(data)
        except Exception:
            pass

    def do_GET(self):
        try:
            path = urlparse(self.path).path
        except Exception:
            path = "/"
        if path in ("/", "/index", "/index.html"):
            # Fill in run_dir client-side via /api/status
            return self._text(200, _HTML)
        if path == "/api/status":
            ctx = getattr(self.server, "ctx", {})
            run_dir = ctx.get("run_dir", "")
            return self._json(200, {"ok": True, "run_dir": run_dir})
        return self._json(404, {"ok": False, "error": "not_found"})

    def do_POST(self):
        try:
            path = urlparse(self.path).path
        except Exception:
            path = "/"
        if path != "/api/load_engram":
            return self._json(404, {"ok": False, "error": "not_found"})

        # parse JSON
        try:
            length = int(self.headers.get("Content-Length", "0"))
        except Exception:
            length = 0
        try:
            body = self.rfile.read(length) if length > 0 else b"{}"
        except Exception:
            body = b"{}"
        try:
            data = json.loads(body.decode("utf-8"))
        except Exception:
            data = {}
        raw_path = data.get("path")
        if not isinstance(raw_path, str) or not raw_path.strip():
            return self._json(400, {"ok": False, "error": "path_missing"})

        # Normalize path
        p = raw_path.strip()
        try:
            p = os.path.expanduser(p)
        except Exception:
            pass
        # Allow relative paths; make them absolute relative to CWD
        try:
            if not os.path.isabs(p):
                p = os.path.abspath(p)
        except Exception:
            pass

        # Optional existence check to reduce confusion
        if not os.path.exists(p):
            return self._json(400, {"ok": False, "error": "path_not_found", "path": p})

        # Write/merge phase.json with load_engram directive
        ctx = getattr(self.server, "ctx", {})
        phase_file = ctx.get("phase_file")
        if not isinstance(phase_file, str) or not phase_file:
            return self._json(500, {"ok": False, "error": "phase_file_unavailable"})

        obj = {}
        try:
            if os.path.exists(phase_file):
                with open(phase_file, "r", encoding="utf-8") as fh:
                    obj = json.load(fh)
                    if not isinstance(obj, dict):
                        obj = {}
        except Exception:
            obj = {}

        obj["load_engram"] = p
        try:
            os.makedirs(os.path.dirname(phase_file), exist_ok=True)
        except Exception:
            pass

        try:
            with open(phase_file, "w", encoding="utf-8") as fh:
                json.dump(obj, fh, ensure_ascii=False, indent=2)
        except Exception as e:
            return self._json(500, {"ok": False, "error": "write_failed", "detail": str(e)})

        return self._json(200, {"ok": True, "path": p})

    # Quiet server logs
    def log_message(self, fmt, *args):
        try:
            # Suppress default stderr chatter
            return
        except Exception:
            pass


class ControlServer:
    """
    Spawn a local HTTP control server in a background thread.
    Exposes:
      - url: http://127.0.0.1:<port>/
      - stop(): shutdown server
    """
    def __init__(self, run_dir: str, host: str = "127.0.0.1", port: int = 8765):
        self.run_dir = run_dir
        self.phase_file = os.path.join(run_dir, "phase.json")
        self.host = host
        self.port = None
        self._server = None
        self._thread = None

        # Bind first available port in a small range
        last_err = None
        for p in range(int(port), int(port) + 16):
            try:
                server = ThreadingHTTPServer((host, p), _Handler)
                server.ctx = {"run_dir": self.run_dir, "phase_file": self.phase_file}
                self._server = server
                self.port = p
                break
            except OSError as e:
                last_err = e
                continue

        if self._server is None:
            raise RuntimeError(f"Failed to bind control server on {host}:{port} (+15) - last error: {last_err}")

        t = threading.Thread(target=self._server.serve_forever, name="fum-control-server", daemon=True)
        t.start()
        self._thread = t

        self.url = f"http://{host}:{self.port}/"

    def stop(self):
        try:
            if self._server:
                self._server.shutdown()
        except Exception:
            pass
        try:
            if self._server:
                self._server.server_close()
        except Exception:
            pass
        self._server = None]]></content>
    </file>
    <file>
      <path>core/cortex/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>core/cortex/maps/__init__.py</path>
      <content><![CDATA[from .coldmap import ColdMap
from .heatmap import HeatMap
from .excitationmap import ExcitationMap
from .inhibitionmap import InhibitionMap

__all__ = ["ColdMap", "HeatMap", "ExcitationMap", "InhibitionMap"]]]></content>
    </file>
    <file>
      <path>core/cortex/maps/base_decay_map.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.base_decay_map
Purpose: Shared bounded, exponential-decay event-driven map base for Heat/Exc/Inh reducers.

Void-faithful constraints:
- Event-driven folding only (no global scans over W or neighbors).
- Bounded working set via keep_max with sample-based pruning.
- O(#events) time per tick; snapshot is cheap and bounded by head_k/keep_max.
"""

from typing import Dict, Iterable, List
import math
import random


class BaseDecayMap:
    """
    Bounded, per-node exponentially decaying accumulator.
    Score_t(node) = Score_{t-Δ} * 2^(-Δ/half_life_ticks) + sum(increments at t)

    Snapshot:
      - head (top-16 [node, score] pairs by default; bounded by head_k)
      - p95, p99, max, count summaries

    Notes:
    - Subclasses must implement fold(events, tick) and call add(node, tick, inc).
    - No I/O/logging; pure core.
    """

    __slots__ = ("head_k", "half_life", "keep_max", "rng", "_val", "_last_tick")

    def __init__(self, head_k: int = 256, half_life_ticks: int = 200, keep_max: int | None = None, seed: int = 0) -> None:
        self.head_k = int(max(8, head_k))
        self.half_life = int(max(1, half_life_ticks))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._val: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

    # ------------- core updates -------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            factor = 2.0 ** (-(dt / float(self.half_life)))
            try:
                self._val[node] *= factor
            except Exception:
                self._val[node] = float(self._val.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def add(self, node: int, tick: int, inc: float) -> None:
        try:
            n = int(node)
            t = int(tick)
            dv = float(inc)
        except Exception:
            return
        if n < 0:
            return
        if n in self._val:
            self._decay_to(n, t)
            self._val[n] += dv
        else:
            self._val[n] = max(0.0, dv)
            self._last_tick[n] = t
        if len(self._val) > self.keep_max:
            self._prune()

    def _prune(self) -> None:
        # Drop a sampled set of the smallest entries (cheap; avoids full O(N) sort)
        size = len(self._val)
        target = size - self.keep_max
        if target <= 0:
            return
        keys = list(self._val.keys())
        sample_size = min(len(keys), max(256, target * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        sample.sort(key=lambda k: self._val.get(k, 0.0))  # ascending by score
        for k in sample[:target]:
            self._val.pop(k, None)
            self._last_tick.pop(k, None)

    # ------------- folding & snapshots -------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Subclasses override and call add(node, tick, inc) appropriately.
        """
        raise NotImplementedError

    def snapshot(self, head_n: int = 16) -> dict:
        if not self._val:
            return {"head": [], "p95": 0.0, "p99": 0.0, "max": 0.0, "count": 0}
        # head top-k by score
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._val.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._val.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]
        # quick percentiles over working set
        vals = sorted(float(v) for v in self._val.values())
        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])
        return {
            "head": [[int(k), float(v)] for k, v in head],
            "p95": q(0.95),
            "p99": q(0.99),
            "max": float(vals[-1]),
            "count": int(len(vals)),
        }


__all__ = ["BaseDecayMap"]]]></content>
    </file>
    <file>
      <path>core/cortex/maps/coldmap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.coldmap
Purpose: Persistent, bounded coldness tracker keyed by node id (telemetry-only, read-only).
Design: Pure core module; no IO/logging; compatible with existing CoreEngine usage.
"""

from typing import List
import random
import math


class ColdMap:
    """
    Persistent, bounded coldness tracker keyed by node id.

    Coldness score (monotonic in idle time, bounded in [0,1)):
        age = max(0, t - last_seen[node])
        score = 1 - 2^(-age / half_life_ticks)

    Snapshot fields:
      - cold_head: top-16 [node_id, score] pairs (most cold first)
      - cold_p95, cold_p99, cold_max: distribution summaries across tracked nodes

    Notes:
    - API-compatible with existing CoreEngine usage:
        * touch(node: int, tick: int) to record activity
        * snapshot(tick: int, head_n: int = 16) -> dict with fields listed above
    - Constructor accepts (head_k, half_life_ticks, keep_max, seed) to match current wiring.
    """
    __slots__ = ("head_k", "half_life", "keep_max", "rng", "_last_seen")

    def __init__(self, head_k: int = 256, half_life_ticks: int = 200, keep_max: int | None = None, seed: int = 0) -> None:
        self.head_k = int(max(8, head_k))
        self.half_life = int(max(1, half_life_ticks))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._last_seen: dict[int, int] = {}

    # ------------- updates -------------

    def touch(self, node: int, tick: int) -> None:
        """
        Record a touch for node at tick. Node ids must be non-negative ints.
        """
        try:
            n = int(node)
            t = int(tick)
        except Exception:
            return
        if n < 0:
            return
        self._last_seen[n] = t
        if len(self._last_seen) > self.keep_max:
            self._prune(t)

    def _prune(self, tick: int) -> None:
        """
        Reduce tracked set to keep_max entries, preferentially dropping the most recently seen nodes.
        Uses sampling to avoid O(N) passes.
        """
        try:
            size = len(self._last_seen)
            if size <= self.keep_max:
                return
            target = size - self.keep_max
            keys = list(self._last_seen.keys())
            sample_size = min(len(keys), max(256, target * 4))
            sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
            # Sort sample by recency (most recent first) and drop up to target from this set.
            sample.sort(key=lambda k: self._last_seen.get(k, -10**12), reverse=True)
            to_remove = min(target, len(sample))
            for k in sample[:to_remove]:
                self._last_seen.pop(k, None)
        except Exception:
            # Conservative fallback: random removals until within bound
            while len(self._last_seen) > self.keep_max:
                try:
                    k = self.rng.choice(tuple(self._last_seen.keys()))
                    self._last_seen.pop(k, None)
                except Exception:
                    break

    # ------------- scoring -------------

    def _score(self, age: int) -> float:
        a = max(0, int(age))
        # score in [0, 1): 1 - 2^(-age / half_life)
        try:
            return float(1.0 - math.pow(0.5, float(a) / float(self.half_life)))
        except Exception:
            return 0.0

    # ------------- snapshot -------------

    def snapshot(self, tick: int, head_n: int = 16) -> dict:
        """
        Compute a coldness snapshot at tick.

        Returns:
          {
            "cold_head": list[[node_id, score], ...]           # top head_n by score
            "cold_p95": float,
            "cold_p99": float,
            "cold_max": float,
          }
        """
        try:
            t = int(tick)
        except Exception:
            t = 0

        if not self._last_seen:
            return {"cold_head": [], "cold_p95": 0.0, "cold_p99": 0.0, "cold_max": 0.0}

        # Compute scores for all tracked nodes (bounded by keep_max)
        pairs: List[tuple[int, float]] = []
        for node, ts in self._last_seen.items():
            try:
                age = t - int(ts)
            except Exception:
                age = 0
            s = self._score(age)
            pairs.append((int(node), float(s)))

        # Top head_n by score
        head_n = max(1, min(int(head_n), self.head_k))
        try:
            import heapq as _heapq
            head = _heapq.nlargest(head_n, pairs, key=lambda kv: kv[1])
        except Exception:
            head = sorted(pairs, key=lambda kv: kv[1], reverse=True)[:head_n]

        # Percentiles over full tracked set (bounded)
        vals = [s for _, s in pairs]
        vals.sort()

        def _pct(p: float) -> float:
            if not vals:
                return 0.0
            i = int(max(0, min(len(vals) - 1, round((len(vals) - 1) * p))))
            return float(vals[i])

        p95 = _pct(0.95)
        p99 = _pct(0.99)
        vmax = float(vals[-1]) if vals else 0.0

        head_out: List[List[float]] = [[int(n), float(s)] for n, s in head]
        return {
            "cold_head": head_out,
            "cold_p95": float(p95),
            "cold_p99": float(p99),
            "cold_max": float(vmax),
        }


__all__ = ["ColdMap"]
]]></content>
    </file>
    <file>
      <path>core/cortex/maps/excitationmap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.excitationmap
Purpose: Excitatory-only activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import SpikeEvent, DeltaWEvent


class ExcitationMap(BaseDecayMap):
    """
    Excitatory-only activity map.
    Filters by sign>0 (spikes) and dw>0 (ΔW).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * dw for DeltaWEvent (dw > 0 only)
    """
    __slots__ = ("spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "spike" and isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) > 0:
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                dw = float(getattr(e, "dw", 0.0))
                if dw > 0.0:
                    self.add(int(e.node), int(tick), self.dW_gain * dw)

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "exc_head": s["head"],
            "exc_p95": s["p95"],
            "exc_p99": s["p99"],
            "exc_max": s["max"],
            "exc_count": s["count"],
        }


__all__ = ["ExcitationMap"]
]]></content>
    </file>
    <file>
      <path>core/cortex/maps/heatmap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.heatmap
Purpose: Recency-weighted activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import VTTouchEvent, SpikeEvent, DeltaWEvent


class HeatMap(BaseDecayMap):
    """
    Recency-weighted activity map (short half-life).
    Increments on vt_touch (small) and any spike/ΔW (scaled).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - vt_touch_gain: increment per vt_touch (e.g., 0.25)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * |dw| for DeltaWEvent (e.g., 0.5)

    Void-faithful: folds events only; never scans global structures.
    """
    __slots__ = ("vt_touch_gain", "spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        vt_touch_gain: float = 0.25,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.vt_touch_gain = float(vt_touch_gain)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                self.add(int(e.token), int(tick), self.vt_touch_gain * float(getattr(e, "w", 1.0)))
            elif k == "spike" and isinstance(e, SpikeEvent):
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                self.add(int(e.node), int(tick), self.dW_gain * abs(float(e.dw)))

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "heat_head": s["head"],
            "heat_p95": s["p95"],
            "heat_p99": s["p99"],
            "heat_max": s["max"],
            "heat_count": s["count"],
        }


__all__ = ["HeatMap"]
]]></content>
    </file>
    <file>
      <path>core/cortex/maps/inhibitionmap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.inhibitionmap
Purpose: Inhibitory-only activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import SpikeEvent, DeltaWEvent


class InhibitionMap(BaseDecayMap):
    """
    Inhibitory-only activity map.
    Filters by sign<0 (spikes) and dw<0 (ΔW).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * |dw| for DeltaWEvent (dw < 0 only; absolute value applied)
    """
    __slots__ = ("spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "spike" and isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) < 0:
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                dw = float(getattr(e, "dw", 0.0))
                if dw < 0.0:
                    self.add(int(e.node), int(tick), self.dW_gain * abs(dw))

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "inh_head": s["head"],
            "inh_p95": s["p95"],
            "inh_p99": s["p99"],
            "inh_max": s["max"],
            "inh_count": s["count"],
        }


__all__ = ["InhibitionMap"]
]]></content>
    </file>
    <file>
      <path>core/cortex/maps/memorymap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.memorymap
Purpose: Memory map view for scouts/UI with bounded head/dict (void-faithful, no scans).

Design
- Single source of truth for slow memory field m[i] must live outside maps/ (e.g., core/memory/field.py).
- This class acts as a thin VIEW/ADAPTER over that field when provided (preferred).
- If no field is provided, it can optionally run as a bounded reducer proxy (Pattern B) that folds events
  but only retains a small working set (no full-N vector).

Contracts
- snapshot() returns:
    {
      "memory_head": list[[node, value], ...],   # top-k bounded
      "memory_p95": float,
      "memory_p99": float,
      "memory_max": float,
      "memory_count": int,
      "memory_dict": {node: value}               # bounded dictionary
    }

Guardrails
- No global scans; bounded working set only when operating in reducer-proxy mode.
- When a field is attached, fold() is a no-op; view delegates to the field snapshot.
"""

from typing import Any, Dict, Iterable, List
import math
import random

from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class MemoryMap:
    """
    Thin view over MemoryField (preferred), with bounded fallback reducer (proxy) if field absent.

    Parameters:
      - field: optional memory field owner (preferred). When set, this map only adapts snapshots.
      - head_k: top-k head size for "memory_head"
      - dict_cap: maximum items to include in "memory_dict"
      - keep_max: maximum retained working-set size when operating in proxy mode (defaults to 16×head_k)
      - gamma/delta/kappa/touch_gain/spike_gain/dW_gain: only used in proxy mode
    """

    __slots__ = (
        "field",
        "head_k",
        "dict_cap",
        "keep_max",
        "rng",
        "_m",           # proxy-mode working set (absent when field provided)
        "_last_tick",   # proxy-mode last-tick tracker
        "gamma",
        "delta",
        "kappa",
        "touch_gain",
        "spike_gain",
        "dW_gain",
    )

    def __init__(
        self,
        field: Any | None = None,
        *,
        head_k: int = 256,
        dict_cap: int = 2048,
        keep_max: int | None = None,
        seed: int = 0,
        # proxy-mode dynamics
        gamma: float = 0.05,
        delta: float = 0.01,
        kappa: float = 0.10,
        touch_gain: float = 1.0,
        spike_gain: float = 0.20,
        dW_gain: float = 0.10,
    ) -> None:
        self.field = field
        self.head_k = int(max(8, head_k))
        self.dict_cap = int(max(8, dict_cap))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))

        # proxy-mode state (only used when field is None)
        self._m: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

        # proxy-mode parameters
        self.gamma = float(max(0.0, gamma))
        self.delta = float(max(0.0, min(1.0, delta)))
        self.kappa = float(max(0.0, kappa))
        self.touch_gain = float(max(0.0, touch_gain))
        self.spike_gain = float(max(0.0, spike_gain))
        self.dW_gain = float(max(0.0, dW_gain))

    # ---------------- adapter path (preferred) ----------------

    def _snapshot_from_field(self) -> Dict[str, object]:
        """Delegate snapshot to the owning field and adapt keys/caps."""
        fld = self.field
        if fld is None:
            return {}
        try:
            snap = fld.snapshot(head_n=self.head_k)  # expects keys memory_head/memory_dict/etc.
        except Exception:
            return {}
        if not isinstance(snap, dict):
            return {}

        head = snap.get("memory_head", []) or []
        dct = snap.get("memory_dict", {}) or {}

        # Cap dictionary size deterministically by highest values
        if isinstance(dct, dict) and len(dct) > self.dict_cap:
            try:
                import heapq as _heapq
                items = list(dct.items())
                top = _heapq.nlargest(int(self.dict_cap), items, key=lambda kv: kv[1])
                dct = {int(k): float(v) for k, v in top}
            except Exception:
                # Fallback: arbitrary trim
                keys = list(dct.keys())[: self.dict_cap]
                dct = {int(k): float(dct[k]) for k in keys if k in dct}

        p95 = snap.get("memory_p95", 0.0)
        p99 = snap.get("memory_p99", 0.0)
        vmax = snap.get("memory_max", 0.0)
        cnt = snap.get("memory_count", len(dct) if isinstance(dct, dict) else 0)

        return {
            "memory_head": head,
            "memory_p95": float(p95),
            "memory_p99": float(p99),
            "memory_max": float(vmax),
            "memory_count": int(cnt),
            "memory_dict": dct,
        }

    # ---------------- proxy-mode helpers (no field) ----------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            try:
                base = max(0.0, 1.0 - self.delta)
                factor = base ** dt
            except Exception:
                factor = math.exp(-self.delta * float(dt))
            self._m[node] = float(self._m.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def _ensure_and_decay(self, node: int, tick: int) -> None:
        n = int(node)
        if n not in self._m:
            self._m[n] = 0.0
            self._last_tick[n] = int(tick)
        else:
            self._decay_to(n, int(tick))

    def _prune(self) -> None:
        size = len(self._m)
        target_drop = size - self.keep_max
        if target_drop <= 0:
            return
        keys = list(self._m.keys())
        sample_size = min(len(keys), max(256, target_drop * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        sample.sort(key=lambda k: self._m.get(k, 0.0))
        for k in sample[:target_drop]:
            self._m.pop(k, None)
            self._last_tick.pop(k, None)

    # ---------------- API ----------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events.
        - If a field is attached, this is a no-op (owner already folds).
        - If no field, run bounded proxy updates (no scans).
        """
        if self.field is not None:
            return  # delegate model dynamics elsewhere

        t = int(tick)
        γ = self.gamma
        κ = self.kappa
        tg = self.touch_gain
        sg = self.spike_gain
        wg = self.dW_gain

        for e in events:
            k = getattr(e, "kind", None)

            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                try:
                    i = int(e.token)
                except Exception:
                    continue
                if i < 0:
                    continue
                self._ensure_and_decay(i, t)
                r_i = float(getattr(e, "w", 1.0))
                self._m[i] = float(self._m.get(i, 0.0)) + float(γ * tg * r_i)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                try:
                    u = int(getattr(e, "u", -1))
                    v = int(getattr(e, "v", -1))
                except Exception:
                    continue
                if u < 0 or v < 0:
                    continue
                self._ensure_and_decay(u, t)
                self._ensure_and_decay(v, t)
                mu = float(self._m.get(u, 0.0))
                mv = float(self._m.get(v, 0.0))
                d = float(κ * (mv - mu))
                self._m[u] = mu + d
                self._m[v] = mv - d
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "spike" and isinstance(e, SpikeEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                amp = float(getattr(e, "amp", 1.0))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * sg * amp)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                dw = abs(float(getattr(e, "dw", 0.0)))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * wg * dw)
                if len(self._m) > self.keep_max:
                    self._prune()

    def snapshot(self, head_n: int = 16) -> Dict[str, object]:
        """
        Return bounded snapshot dictionary per contract.
        - If field is present: adapt its snapshot and cap dict to dict_cap.
        - Else: produce from proxy working set (bounded by keep_max).
        """
        if self.field is not None:
            out = self._snapshot_from_field()
            if out:
                return out
            # fallthrough on error to proxy-mode snapshot (empty)

        if not self._m:
            return {
                "memory_head": [],
                "memory_p95": 0.0,
                "memory_p99": 0.0,
                "memory_max": 0.0,
                "memory_count": 0,
                "memory_dict": {},
            }

        # Head top-k
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._m.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._m.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]

        vals = sorted(float(v) for v in self._m.values())

        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])

        # Cap dictionary size
        if len(self._m) > self.dict_cap:
            try:
                import heapq as _heapq
                items = list(self._m.items())
                top = _heapq.nlargest(int(self.dict_cap), items, key=lambda kv: kv[1])
                out_dict: Dict[int, float] = {int(k): float(v) for k, v in top}
            except Exception:
                keys = list(self._m.keys())[: self.dict_cap]
                out_dict = {int(k): float(self._m[k]) for k in keys if k in self._m}
        else:
            out_dict = {int(k): float(v) for k, v in self._m.items()}

        return {
            "memory_head": [[int(k), float(v)] for k, v in head],
            "memory_p95": q(0.95),
            "memory_p99": q(0.99),
            "memory_max": float(vals[-1]),
            "memory_count": int(len(vals)),
            "memory_dict": out_dict,
        }


__all__ = ["MemoryMap"]]]></content>
    </file>
    <file>
      <path>core/cortex/maps/trailmap.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.trailmap
Purpose: Short half-life trail/repulsion map updated only by events (void-faithful, no scans).

Design:
- Event-driven only; folds vt_touch and edge_on into a fast-decaying accumulator.
- Intended as a light repulsion field to discourage immediate retracing (fan-out).
- Bounded working set via BaseDecayMap.keep_max (no global scans).

Snapshot keys:
- trail_head: top-k [[node, score], ...]
- trail_dict: bounded dict {node: score} over current working set (len ≤ keep_max)
"""

from typing import Iterable, Dict

from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class TrailMap(BaseDecayMap):
    """
    Short half-life trail/repulsion map.

    Parameters:
      - half_life_ticks: decay half-life in ticks (defaults short, e.g., 50)
      - vt_touch_gain: increment for a node touch (small, e.g., 0.15)
      - edge_gain: increment applied to both endpoints of an edge_on (very small, e.g., 0.05)
      - spike_gain / dW_gain: optional small contributions to treat bursts as footprints
    """

    __slots__ = ("vt_touch_gain", "edge_gain", "spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 50,
        keep_max: int | None = None,
        seed: int = 0,
        vt_touch_gain: float = 0.15,
        edge_gain: float = 0.05,
        spike_gain: float = 0.05,
        dW_gain: float = 0.02,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.vt_touch_gain = float(vt_touch_gain)
        self.edge_gain = float(edge_gain)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events into the trail accumulator.

        Void-faithful:
        - Only uses provided events; no adjacency/weight scans.
        - Updates are strictly local to the nodes appearing in events.
        """
        t = int(tick)
        for e in events:
            k = getattr(e, "kind", None)
            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                self.add(int(e.token), t, self.vt_touch_gain * float(getattr(e, "w", 1.0)))
            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                # Apply a small footprint on both endpoints
                u = int(getattr(e, "u", -1))
                v = int(getattr(e, "v", -1))
                if u >= 0:
                    self.add(u, t, self.edge_gain)
                if v >= 0:
                    self.add(v, t, self.edge_gain)
            elif k == "spike" and isinstance(e, SpikeEvent):
                self.add(int(e.node), t, self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                self.add(int(e.node), t, self.dW_gain * abs(float(e.dw)))

    def snapshot(self, head_n: int = 16) -> dict:
        """
        Export a bounded snapshot including both head list and the working-set dictionary.
        """
        s = super().snapshot(head_n=head_n)
        # Working-set dict is bounded by keep_max by construction
        d: Dict[int, float] = {int(k): float(v) for k, v in getattr(self, "_val", {}).items()}
        return {
            "trail_head": s["head"],
            "trail_p95": s["p95"],
            "trail_p99": s["p99"],
            "trail_max": s["max"],
            "trail_count": s["count"],
            "trail_dict": d,
        }


__all__ = ["TrailMap"]]]></content>
    </file>
    <file>
      <path>core/cortex/scouts.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.scouts (facade)

This module is now a thin aggregator that re-exports modular scout classes and maps.
It preserves legacy import paths while enforcing void-faithful, read-only traversal.

Key points:
- No global scans; scouts only use local neighbor reads and bounded TTL/budgets.
- This facade exposes:
    * VoidColdScoutWalker (ColdScout)
    * HeatScout, ExcitationScout, InhibitionScout
    * ColdMap (from maps.coldmap)
    * BaseScout (interface) via void_walkers.base
    * GDSPActuator / RevGSP re-exported from core.neuroplasticity (for legacy imports)

Contract compliance:
- Scouts emit only foldable events: vt_touch, edge_on, and (optionally) spike(+/-)
- They do not mutate the connectome (read-only), no scans, no schedulers.
"""

# Prefer modular implementations
from .void_walkers.void_cold_scout import ColdScout as VoidColdScoutWalker
from .void_walkers.void_heat_scout import HeatScout
from .void_walkers.void_ray_scout import VoidRayScout
from .void_walkers.void_memory_ray_scout import MemoryRayScout
from .void_walkers.void_frontier_scout import FrontierScout
from .void_walkers.void_cycle_scout import CycleHunterScout
from .void_walkers.void_sentinel_scout import SentinelScout
try:
    from .void_walkers.void_excitation_scout import ExcitationScout
except Exception:  # pragma: no cover - optional during staged migration
    class ExcitationScout:  # type: ignore
        pass
try:
    from .void_walkers.void_inhibition_scout import InhibitionScout
except Exception:  # pragma: no cover - optional during staged migration
    class InhibitionScout:  # type: ignore
        pass

# Maps
try:
    from .maps.coldmap import ColdMap
except Exception:  # pragma: no cover
    ColdMap = None  # type: ignore

# Base interface (allow both "scouts.base" and "scouts: BaseScout" import styles)
try:
    from .void_walkers.base import BaseScout  # type: ignore
except Exception:  # pragma: no cover
    BaseScout = None  # type: ignore

# Neuroplasticity re-exports for legacy imports
try:
    from ..neuroplasticity.gdsp import GDSPActuator
except Exception:  # pragma: no cover
    GDSPActuator = None  # type: ignore
try:
    from ..neuroplasticity.revgsp import RevGSP
except Exception:  # pragma: no cover
    RevGSP = None  # type: ignore

__all__ = [
    "VoidColdScoutWalker",
    "HeatScout",
    "ExcitationScout",
    "InhibitionScout",
    "VoidRayScout",
    "MemoryRayScout",
    "FrontierScout",
    "CycleHunterScout",
    "SentinelScout",
    "ColdMap",
    "BaseScout",
    "GDSPActuator",
    "RevGSP",
]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/base.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.base

Void-faithful, read-only scout base class.
- No global scans or dense conversions; no direct access to raw weight arrays or external graph libraries.
- Operates only on local neighbor reads provided by the active graph.
- Emits only small, foldable events for reducers and telemetry.

Contract:
- step(connectome, bus, maps, budget) - returns a list[BaseEvent]
  * connectome: object exposing N and neighbors/get_neighbors or adj mapping
  * bus: opaque (optional) announce bus; NOT used for writes here (read-only scouts emit events to return)
  * maps: optional dict-like snapshots; subclasses may consult e.g. {"heat_head":[[n,score],...]}
  * budget: optional dict with keys:
      - "visits": int (node touches to attempt)
      - "edges": int (edge probes to attempt)
      - "ttl":   int (max walk depth per seed)
      - "tick":  int (current tick for event timestamps)
      - "seeds": Sequence[int] (preferred start nodes; bounded; falls back to map heads or uniform)

Returned events use only core event types:
- VTTouchEvent(kind="vt_touch", t, token=node, w=1.0)
- EdgeOnEvent(kind="edge_on", t, u, v)
- Subclasses may add SpikeEvent with sign bias (still event-only).

This module defines the common, safe scaffolding. Heuristics live in subclasses.
"""

from typing import Any, Iterable, List, Optional, Sequence, Set, Dict
import random

from fum_rt.core.proprioception.events import (
    BaseEvent,
    VTTouchEvent,
    EdgeOnEvent,
    SpikeEvent,  # subclasses may use; base does not emit spikes
)


class BaseScout:
    __slots__ = ("budget_visits", "budget_edges", "ttl", "rng")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
    ) -> None:
        self.budget_visits = int(max(0, budget_visits))
        self.budget_edges = int(max(0, budget_edges))
        self.ttl = int(max(1, ttl))
        self.rng = random.Random(int(seed))

    # ---------------------- connectome helpers (read-only) ----------------------

    @staticmethod
    def _get_N(C: Any) -> int:
        try:
            N = int(getattr(C, "N", 0))
            if N > 0:
                return N
        except Exception:
            pass
        try:
            W = getattr(C, "W", None)
            shp = getattr(W, "shape", None)
            if shp and isinstance(shp, (tuple, list)) and len(shp) >= 1:
                n = int(shp[0])
                return n if n > 0 else 0
        except Exception:
            pass
        return 0

    @staticmethod
    def _neighbors(C: Any, u: int) -> List[int]:
        # Prefer explicit methods
        try:
            for meth in ("neighbors", "get_neighbors"):
                fn = getattr(C, meth, None)
                if callable(fn):
                    xs = fn(int(u))
                    if xs:
                        try:
                            return [int(x) for x in list(xs)]
                        except Exception:
                            return []
        except Exception:
            pass
        # Fallback: adjacency mapping
        try:
            adj = getattr(C, "adj", None)
            if isinstance(adj, dict):
                vals = adj.get(int(u), [])
                if isinstance(vals, dict):
                    return [int(x) for x in vals.keys()]
                return [int(x) for x in list(vals)]
        except Exception:
            pass
        return []

    # --------------------------- heuristic hooks --------------------------------

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        """
        Subclasses may override to bias routing locally using heads from reducers.
        Returns a bounded set of node indices to prefer when available.
        """
        return set()

    # ------------------------------ main API ------------------------------------

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused (read-only)
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        """
        Bounded, TTL-limited local exploration that returns foldable events.
        Default strategy:
        - Touch up to 'visits' seeds (uniform from [0..N) if no priority).
        - For each, walk up to TTL steps, emitting vt_touch on the current node
          and best-effort edge_on to a locally chosen neighbor (biased by priority set).
        - Edge probes total bounded by 'edges'.
        """
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        # Seed pool: prefer explicit seeds, else priority, else uniform
        seeds = None
        if isinstance(budget, dict):
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None
        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()
        pool: Sequence[int]
        if seeds:
            try:
                pool = tuple(int(s) for s in seeds if 0 <= int(s) < N)
                if not pool:
                    pool = tuple(priority) if priority else tuple(range(N))
            except Exception:
                pool = tuple(priority) if priority else tuple(range(N))
        else:
            pool = tuple(priority) if priority else tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            # TTL-limited micro-walk starting at u
            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node (coverage)
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor(neigh, priority)
                        if v is not None and v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            # random hop when no preference applies
                            try:
                                cur = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                    else:
                        break
                else:
                    break

                depth += 1

        return events

    # -------------------------- local routing policy ----------------------------

    def _pick_neighbor(self, neigh: Sequence[int], priority: Set[int]) -> Optional[int]:
        """
        Choose a neighbor biased toward 'priority' set when available, else blue-noise hop.
        """
        try:
            # Filter by priority first
            pref = [int(x) for x in neigh if int(x) in priority]
            if pref:
                return int(self.rng.choice(pref))
            # Blue-noise hop (random choice)
            return int(self.rng.choice(tuple(neigh)))
        except Exception:
            return None]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/frontier_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Shim module for naming convention.
Use void-prefixed class from [void_frontier_scout.py](fum_rt/core/cortex/void_walkers/void_frontier_scout.py).
"""

from .void_frontier_scout import FrontierScout

__all__ = ["FrontierScout"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/runner.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.runner

Stateless, per-tick scout executor (void-faithful, no schedulers).
- Runs a bounded list of read-only scouts exactly once per tick.
- Enforces a micro time budget (microseconds) across all scouts.
- Accepts optional seeds (e.g., recent UTE indices) and map heads (heat/exc/inh/cold).
- Emits only foldable events (vt_touch, edge_on, optional spike/delta_w); no writes.

Usage (in runtime loop per tick):
    from fum_rt.core.cortex.void_walkers.runner import run_scouts_once as _run_scouts_once
    evs = _run_scouts_once(connectome, scouts, maps, budget, bus, max_us)

Notes:
- No timers, no cadence, no background threads. This is a pure function called once per tick.
- Drop-oldest behavior is delegated to the downstream bus implementation when publish_many is used.
"""

from typing import Any, Dict, Iterable, List, Optional, Sequence
from time import perf_counter_ns
import os as _os

from fum_rt.core.proprioception.events import BaseEvent


def _truthy(x: Any) -> bool:
    try:
        if isinstance(x, (int, float, bool)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False


def run_scouts_once(
    connectome: Any,
    scouts: Sequence[Any],
    maps: Optional[Dict[str, Any]] = None,
    budget: Optional[Dict[str, int]] = None,
    bus: Any = None,
    max_us: int = 2000,
) -> List[BaseEvent]:
    """
    Execute a bounded batch of scouts exactly once for this tick.

    Parameters:
      - connectome: object exposing read-only neighbor access (N, neighbors/get_neighbors or adj mapping)
      - scouts: sequence of instantiated scout objects with .step(connectome, bus, maps, budget) -> list[BaseEvent]
      - maps: optional dict of map heads: {"heat_head": [[node,score],...], "exc_head": [...], "inh_head": [...], "cold_head": [...]}
      - budget: {"visits": int, "edges": int, "ttl": int, "tick": int, "seeds": list[int]} (any subset)
      - bus: optional announce bus; when present, publish_many(evs) is invoked once at end
      - max_us: total per-tick microsecond budget across all scouts

    Returns:
      - list of BaseEvent emitted by all scouts within budget
    """
    evs: List[BaseEvent] = []
    if not scouts:
        return evs

    # Ensure safe numeric bounds
    try:
        max_us = int(max(0, int(max_us)))
    except Exception:
        max_us = 0  # 0 → gather but still permit at least the first scout call if desired

    t0 = perf_counter_ns()

    # Fairness: rotate starting scout by tick (round-robin) to avoid starvation
    start_idx = 0
    try:
        if isinstance(budget, dict):
            start_idx = int(budget.get("tick", 0))
    except Exception:
        start_idx = 0

    ordered: List[Any] = list(scouts or [])
    n_sc = len(ordered)
    if n_sc > 0 and start_idx:
        try:
            k = start_idx % n_sc
            ordered = ordered[k:] + ordered[:k]
        except Exception:
            # fallback: keep original order
            ordered = list(scouts or [])

    # Optional per-scout micro-slice (still one-shot runner; no schedulers)
    per_us = 0
    try:
        per_us = int(_os.getenv("SCOUTS_PER_SCOUT_US", "0"))
    except Exception:
        per_us = 0
    if per_us <= 0 and max_us > 0 and n_sc > 0:
        per_us = int(max_us // max(1, n_sc))

    for sc in ordered:
        # Global time guard (drop rest on over-budget)
        if max_us > 0:
            elapsed_us = (perf_counter_ns() - t0) // 1000
            if elapsed_us >= max_us:
                break

        sc_t0 = perf_counter_ns()
        try:
            out = sc.step(connectome=connectome, bus=None, maps=maps, budget=budget) or []
        except Exception:
            out = []
        if out:
            evs.extend(out)

        # Per-scout guard (best-effort; cannot preempt inside step)
        if per_us > 0:
            sc_elapsed_us = (perf_counter_ns() - sc_t0) // 1000
            if sc_elapsed_us > per_us:
                # soft-guard only: we don't penalize the scout, but this informs future tuning
                pass

    # Publish once (drop-oldest semantics live in bus implementation)
    if evs and bus is not None:
        try:
            if hasattr(bus, "publish_many"):
                bus.publish_many(evs)
            else:
                # bounded fallback
                for e in evs:
                    try:
                        bus.publish(e)  # type: ignore[attr-defined]
                    except Exception:
                        break
        except Exception:
            pass

    return evs


__all__ = ["run_scouts_once"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_cold_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_cold_scout

ColdScout (read-only, void-faithful):
- Prefers neighbors whose node ids appear in ColdMap snapshot head ("cold_head").
- Emits only vt_touch and edge_on events.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Compatibility:
- Provides alias VoidColdScoutWalker for existing imports in legacy code paths.
"""

from typing import Any, Dict, List, Optional, Sequence, Set
from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded head nodes from map snapshot structure: [[node, score], ...]
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class ColdScout(BaseScout):
    """
    Coldness-driven scout: routes toward nodes with higher coldness (less recently seen).
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        return _extract_head_nodes(maps, "cold_head", cap=max(64, self.budget_visits * 8))


# Back-compat alias used by runtime/engine wiring in existing code
VoidColdScoutWalker = ColdScout

__all__ = ["ColdScout", "VoidColdScoutWalker"]
]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_cycle_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_cycle_scout

CycleHunterScout (read-only, void-faithful):
- Seeks short cycles (3-6 hops) using a TTL-limited walk with a tiny path window.
- Purely local: only neighbor lists are read; no global scans or dense conversions.
- Emits vt_touch and edge_on events; reducers can infer cycle hits from returned edge traces.

Heuristic:
- Maintain a small deque of the recent path (window ~ 5).
- Prefer stepping to a neighbor that is already in the recent window (closes a short cycle).
- Otherwise, hop randomly (blue-noise) among neighbors.

Guardrails:
- No schedulers; executes once per tick under the runner.
- Bounded budgets: visits, edges, ttl.
- No writes; events only.
"""

from typing import Any, Dict, Optional, Sequence, Set, List, Deque
from collections import deque
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


class CycleHunterScout(BaseScout):
    """
    Short-cycle finder with tiny path memory.
    """

    __slots__ = ("window",)

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        window: int = 5,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.window = int(max(2, window))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Neutral: let runner seeds drive locality; no external heads required.
        return set()

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        # Seed pool: prefer runner-provided seeds; else uniform
        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(range(N))
            except Exception:
                pool = tuple(range(N))
        else:
            pool = tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            path: Deque[int] = deque(maxlen=self.window)

            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                path.append(int(cur))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if not neigh:
                        break

                    # Prefer neighbors that are in the recent path window (cycle closure)
                    try:
                        path_set = set(path)
                    except Exception:
                        path_set = set(int(x) for x in path) if path else set()

                    pref = [int(v) for v in neigh if int(v) in path_set and int(v) != int(cur)]
                    if pref:
                        v = int(self.rng.choice(pref))
                    else:
                        # Blue-noise hop
                        try:
                            v = int(self.rng.choice(tuple(neigh)))
                        except Exception:
                            break

                    if v != cur:
                        events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                        edges_emitted += 1
                        cur = int(v)
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["CycleHunterScout"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_excitation_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_excitation_scout

ExcitationScout (read-only, void-faithful):
- Duty: Map excitatory corridors, feeding ExcitationMap strictly via events.
- Strategy: Seed from ExcitationMap.exc_head; during walk, emit VTTouchEvent per visit and
            synthesize SpikeEvent(node, amp≈local_exc, sign=+1) with bounded amplitude in [0,1].
            Occasional EdgeOnEvent samples are produced by BaseScout's bounded walk; no global scans.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Physics alignment (docs in /derivation):
- finite_tube_mode_analysis.md, discrete_to_continuum.md: fast φ-fronts (c^2 = 2 J a^2) guide recent activity.
- memory_steering.md: scouts do not write; they only observe and announce, keeping the φ sector void-faithful.
"""

from typing import Any, Dict, List, Optional, Set

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, SpikeEvent


def _head_lookup(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Dict[int, float]:
    """
    Build a bounded lookup {node: norm_score in [0,1]} from a head list [[node, score], ...].
    Normalization: divide by max(score) over the truncated head; empty -> {}.
    """
    if not isinstance(maps, dict):
        return {}
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        pairs: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                pairs.append((n, s))
        if not pairs:
            return {}
        vmax = max(s for _, s in pairs) or 1.0
        return {n: max(0.0, min(1.0, s / vmax)) for (n, s) in pairs}
    except Exception:
        return {}


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded set of node ids from a head list [[node, score], ...].
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class ExcitationScout(BaseScout):
    """
    Excitation-driven scout: routes toward nodes with higher ExcitationMap scores.
    Adds SpikeEvent(sign=+1) upon each vt_touch visit with amplitude ~ local excitation.
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer ExcitationMap head indices
        return _extract_head_nodes(maps, "exc_head", cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Use BaseScout bounded walk to generate vt_touch and edge_on
        base_events = super().step(connectome, bus=bus, maps=maps, budget=budget)

        # Build local excitation amplitude lookup from snapshot head (bounded, read-only)
        exc_lookup = _head_lookup(maps, "exc_head", cap=max(64, self.budget_visits * 8))

        out: List[BaseEvent] = []
        for e in base_events:
            out.append(e)
            if getattr(e, "kind", None) == "vt_touch":
                token = getattr(e, "token", None)
                try:
                    node = int(token)
                except Exception:
                    node = None
                if node is not None and node >= 0:
                    # Amplitude in [0,1]; default to 0.5 when not found
                    amp = float(exc_lookup.get(node, 0.5))
                    out.append(SpikeEvent(kind="spike", t=getattr(e, "t", None), node=node, amp=amp, sign=+1))
        return out


__all__ = ["ExcitationScout"]
]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_frontier_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_frontier_scout

FrontierScout (read-only, void-faithful):
- Skims component boundaries and likely bridge frontiers to refresh cohesion/cycle estimators.
- Purely local heuristics; no scans. Emits vt_touch and edge_on only.

Local neighbor score for hop u→j (bounded, read-only):
    s_j = + w_cold * cold[j]
          - w_heat * heat[j]
          - w_shn  * shared_neighbors(u, j)
          + w_deg  * I[deg(j) != deg(u)]

Inputs (optional):
- maps["cold_head"] / maps["heat_head"] to derive small dicts (bounded).
- Only local neighbor lists are read; no global adjacency or dense-array access.

Guardrails:
- No schedulers; TTL/budgets enforce bounds.
- No writes; events only.
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_dict(maps: Optional[Dict[str, Any]], key: str, cap: int = 1024) -> Dict[int, float]:
    out: Dict[int, float] = {}
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        vmax = 0.0
        tmp: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                tmp.append((n, s))
                vmax = max(vmax, s)
        if vmax <= 0.0:
            for n, s in tmp:
                out[n] = 1.0
        else:
            for n, s in tmp:
                out[n] = max(0.0, min(1.0, s / vmax))
    except Exception:
        return out
    return out


class FrontierScout(BaseScout):
    """
    Boundary/cohesion probe: prefer edges that look like weak cuts or cross-degree boundaries.
    """

    __slots__ = ("w_cold", "w_heat", "w_shn", "w_deg", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        w_cold: float = 1.0,
        w_heat: float = 0.5,
        w_shn: float = 0.25,
        w_deg: float = 0.5,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.w_cold = float(max(0.0, w_cold))
        self.w_heat = float(max(0.0, w_heat))
        self.w_shn = float(max(0.0, w_shn))
        self.w_deg = float(max(0.0, w_deg))
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer coldest tiles as starting seeds
        out: Set[int] = set()
        if not isinstance(maps, dict):
            return out
        try:
            head = maps.get("cold_head", []) or []
            for pair in head[: max(64, self.budget_visits * 8)]:
                try:
                    n = int(pair[0])
                    if n >= 0:
                        out.add(n)
                except Exception:
                    continue
        except Exception:
            return out
        return out

    @staticmethod
    def _shared_neighbors(connectome: Any, u: int, v: int, cap: int = 128) -> int:
        try:
            nu = set(int(x) for x in (connectome.neighbors(u) or []))  # type: ignore[attr-defined]
        except Exception:
            try:
                nu = set(int(x) for x in (connectome.get_neighbors(u) or []))  # type: ignore[attr-defined]
            except Exception:
                nu = set()
        try:
            nv_list = (connectome.neighbors(v) or [])  # type: ignore[attr-defined]
        except Exception:
            try:
                nv_list = (connectome.get_neighbors(v) or [])  # type: ignore[attr-defined]
            except Exception:
                nv_list = []
        # Bound cost: only check up to 'cap' neighbors of v
        cnt = 0
        for x in list(nv_list)[: max(0, int(cap))]:
            try:
                if int(x) in nu:
                    cnt += 1
            except Exception:
                continue
        return int(cnt)

    @staticmethod
    def _deg(connectome: Any, u: int) -> int:
        try:
            xs = connectome.neighbors(u)  # type: ignore[attr-defined]
        except Exception:
            try:
                xs = connectome.get_neighbors(u)  # type: ignore[attr-defined]
            except Exception:
                xs = []
        try:
            return int(len(xs or []))
        except Exception:
            return 0

    def _pick_neighbor_scored(
        self,
        cur: int,
        neigh: Sequence[int],
        connectome: Any,
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        cold = _head_to_dict(maps, "cold_head", cap=1024)
        heat = _head_to_dict(maps, "heat_head", cap=1024)

        du = self._deg(connectome, int(cur))
        logits: List[tuple[int, float]] = []
        inv_tau = 1.0 / self.tau
        for v in neigh:
            j = int(v)
            shn = float(self._shared_neighbors(connectome, int(cur), j, cap=64))
            dj = self._deg(connectome, j)
            s = (
                (self.w_cold * float(cold.get(j, 0.0)))
                - (self.w_heat * float(heat.get(j, 0.0)))
                - (self.w_shn * shn)
                + (self.w_deg * (1.0 if dj != du else 0.0))
            )
            logits.append((j, s * inv_tau))

        # Softmax
        try:
            m = max(l for _, l in logits)
            ws = [math.exp(l - m) for _, l in logits]
            Z = sum(ws)
            if Z <= 0.0:
                return int(logits[0][0])
            r = (hash((cur, du, len(neigh))) & 0xFFFF) / 65535.0 * Z
            acc = 0.0
            for (i, _), w in zip(logits, ws):
                acc += w
                if r <= acc:
                    return i
            return int(logits[-1][0])
        except Exception:
            try:
                return int(logits[0][0])
            except Exception:
                return None

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(int(cur), neigh, connectome, maps)
                        if v is None or v == cur:
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["FrontierScout"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_heat_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_heat_scout

HeatScout (read-only, void-faithful):
- Local-only neighbor selection using a softmax over map signals.
- Supports trail repulsion (short-term) and optional memory steering (long-term).
- Emits vt_touch and edge_on events; no writes; no scans.

Logit per neighbor j:
    logit_j = theta_mem * m_j - rho_trail * htrail_j + gamma_heat * h_j

Where:
- m_j: slow memory value (maps.get("memory_dict", {})[j]) if provided; else 0.
- htrail_j: short-term trail/heat value (maps["trail_dict"] if present, else maps["heat_dict"]; fallback 0).
- h_j: HeatMap score (maps["heat_dict"] if present; fallback 0).
- theta_mem (can be ±) sets attraction (>) or repulsion (<) to memory.
- rho_trail >= 0 repels recently traversed/hot nodes.
- gamma_heat >= 0 biases toward heat fronts when desired (default 1.0).
- tau > 0 is temperature (lower tau = sharper decisions).

Notes:
- If maps dicts are absent, falls back to priority head nodes (if any), then blue-noise hop.
- Priority seed set still used for initial pool bias via _priority_set().
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


def _head_to_dict(maps: Optional[Dict[str, Any]], key: str, cap: int = 2048) -> Dict[int, float]:
    d: Dict[int, float] = {}
    if not isinstance(maps, dict):
        return d
    try:
        head = maps.get(key, []) or []
        if isinstance(head, dict):
            # already a dict
            for k, v in list(head.items())[: cap]:
                try:
                    d[int(k)] = float(v)
                except Exception:
                    continue
            return d
        for pair in head[: cap]:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                d[n] = s
    except Exception:
        return d
    return d


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            return random.choice([i for i, _ in pairs])
        r = random.random() * Z
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(random.choice([i for i, _ in pairs]))
        except Exception:
            return None


class HeatScout(BaseScout):
    """
    Activity-driven scout with optional memory steering and trail repulsion.
    Defaults preserve legacy behavior (follow heat; no memory, no repulsion).
    """

    __slots__ = ("theta_mem", "rho_trail", "gamma_heat", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        theta_mem: float = 0.0,
        rho_trail: float = 0.0,
        gamma_heat: float = 1.0,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.theta_mem = float(theta_mem)
        self.rho_trail = float(max(0.0, rho_trail))
        self.gamma_heat = float(max(0.0, gamma_heat))
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer HeatMap head indices for seeds
        return _head_to_set(maps, "heat_head", cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        neigh: Sequence[int],
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        # Pull small dicts from maps (bounded heads or dict snapshots)
        md = maps.get("memory_dict", {}) if isinstance(maps, dict) else {}
        hd = maps.get("heat_dict", {}) if isinstance(maps, dict) else {}
        td = maps.get("trail_dict", {}) if isinstance(maps, dict) else {}
        # Allow fallback to heat as trail if explicit trail absent
        use_td = td if td else hd

        logits: List[tuple[int, float]] = []
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            try:
                htrail_j = float(use_td.get(j, 0.0))
            except Exception:
                htrail_j = 0.0
            try:
                h_j = float(hd.get(j, 0.0))
            except Exception:
                h_j = 0.0
            s = (self.theta_mem * m_j) - (self.rho_trail * htrail_j) + (self.gamma_heat * h_j)
            logits.append((j, s / self.tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Inline copy of BaseScout.step to insert map-aware neighbor choice.
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(neigh, maps)
                        if v is None or v == cur:
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break
                depth += 1

        return events


__all__ = ["HeatScout"]
]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_inhibition_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_inhibition_scout

InhibitionScout (read-only, void-faithful):
- Duty: Map inhibitory ridges, feeding InhibitionMap strictly via events.
- Strategy: Seed from InhibitionMap.inh_head; during walk, emit VTTouchEvent per visit and
            synthesize SpikeEvent(node, amp≈local_inh, sign=-1) with bounded amplitude in [0,1].
            Occasional EdgeOnEvent samples are produced by BaseScout's bounded walk; no global scans.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Physics alignment (docs in /derivation):
- finite_tube_mode_analysis.md, discrete_to_continuum.md: fast φ-fronts inform recent activity.
- memory_steering.md: scouts only observe and announce (no writes), keeping φ sector void-faithful.
"""

from typing import Any, Dict, List, Optional, Set

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, SpikeEvent


def _head_lookup(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Dict[int, float]:
    """
    Build a bounded lookup {node: norm_score in [0,1]} from a head list [[node, score], ...].
    Normalization: divide by max(score) over the truncated head; empty -> {}.
    """
    if not isinstance(maps, dict):
        return {}
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        pairs: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                pairs.append((n, s))
        if not pairs:
            return {}
        vmax = max(s for _, s in pairs) or 1.0
        return {n: max(0.0, min(1.0, s / vmax)) for (n, s) in pairs}
    except Exception:
        return {}


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded set of node ids from a head list [[node, score], ...].
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class InhibitionScout(BaseScout):
    """
    Inhibition-driven scout: routes toward nodes with higher InhibitionMap scores.
    Adds SpikeEvent(sign=-1) upon each vt_touch visit with amplitude ~ local inhibition.
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer InhibitionMap head indices
        return _extract_head_nodes(maps, "inh_head", cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Use BaseScout bounded walk to generate vt_touch and edge_on
        base_events = super().step(connectome, bus=bus, maps=maps, budget=budget)

        # Build local inhibition amplitude lookup from snapshot head (bounded, read-only)
        inh_lookup = _head_lookup(maps, "inh_head", cap=max(64, self.budget_visits * 8))

        out: List[BaseEvent] = []
        for e in base_events:
            out.append(e)
            if getattr(e, "kind", None) == "vt_touch":
                token = getattr(e, "token", None)
                try:
                    node = int(token)
                except Exception:
                    node = None
                if node is not None and node >= 0:
                    # Amplitude in [0,1]; default to 0.5 when not found
                    amp = float(inh_lookup.get(node, 0.5))
                    out.append(SpikeEvent(kind="spike", t=getattr(e, "t", None), node=node, amp=amp, sign=-1))
        return out


__all__ = ["InhibitionScout"]
]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_memory_ray_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_memory_ray_scout

MemoryRayScout (read-only, void-faithful):
- Implements refractive-index steering using a slow memory field m.
- Local selection: P(i→j) ∝ exp(Theta * m[j]) with temperature tau (Boltzmann choice).
- Falls back to HeatMap head/dict when memory is absent to keep behavior useful OOTB.
- Emits vt_touch and edge_on events; no writes; no scans.

Signals (read-only):
- maps["memory_dict"] (preferred): bounded dict {node: value}
- maps["memory_head"] (optional): head list [[node, score], ...] for seeds
- Fallbacks:
  * maps["heat_dict"] / maps["heat_head"] used when memory is not available

Guardrails:
- No global scans or dense conversions; neighbors only.
- No schedulers; TTL/budget bounded; emits compact events only.

Fork law (two-branch junction):
- P(A) = sigmoid(Theta * (m_A - m_B)) for tau = 1, aligning with derivation/memory_steering.md
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], keys: Sequence[str], cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    for key in keys:
        try:
            head = maps.get(key, []) or []
            for pair in head[: cap]:
                try:
                    n = int(pair[0])
                except Exception:
                    continue
                if n >= 0:
                    out.add(n)
        except Exception:
            continue
    return out


def _dict_from_maps(maps: Optional[Dict[str, Any]], keys: Sequence[str]) -> Dict[int, float]:
    if not isinstance(maps, dict):
        return {}
    for key in keys:
        try:
            d = maps.get(key, {}) or {}
            # Accept dict snapshots directly; if head list was mistakenly passed, adapt minimally
            if isinstance(d, dict):
                return {int(k): float(v) for k, v in d.items()}  # type: ignore[arg-type]
            if isinstance(d, list):
                out: Dict[int, float] = {}
                for pair in d:
                    try:
                        n = int(pair[0])
                        s = float(pair[1]) if len(pair) > 1 else 1.0
                    except Exception:
                        continue
                    if n >= 0:
                        out[n] = s
                if out:
                    return out
        except Exception:
            continue
    return {}


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            return random.choice([i for i, _ in pairs])
        r = random.random() * Z
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(random.choice([i for i, _ in pairs]))
        except Exception:
            return None


class MemoryRayScout(BaseScout):
    """
    Memory-driven scout: routes toward neighbors with higher memory values m[j].
    """

    __slots__ = ("theta_mem", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        theta_mem: float = 0.8,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.theta_mem = float(theta_mem)
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer memory head; fallback to heat head for useful boot behavior
        return _head_to_set(maps, keys=("memory_head", "heat_head"), cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        neigh: Sequence[int],
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        # Prefer memory; fallback to heat as slow proxy
        md = _dict_from_maps(maps, keys=("memory_dict", "heat_dict"))
        logits: List[tuple[int, float]] = []
        th = float(self.theta_mem)
        inv_tau = 1.0 / float(self.tau)
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            s = th * m_j
            logits.append((j, s * inv_tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(neigh, maps)
                        if v is None or v == cur:
                            # fallback to blue-noise hop
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["MemoryRayScout"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_ray_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_ray_scout

VoidRayScout (read-only, void-faithful):
- Physics-aware routing that prefers neighbors with favorable local change in a fast field φ.
- Local scoring (no scans): for hop i→j, s_j = lambda_phi * (φ[j] - φ[i]) + theta_mem * m[j]
- Temperatured choice via softmax over neighbors; strictly local reads.
- Emits vt_touch and edge_on events; optional spike can be added by subclasses if needed.

Signals (read-only):
- connectome.phi: per-node scalar field (Sequence/ndarray-like) when present; otherwise treated as zeros.
- maps["memory_dict"]: optional slow memory field (bounded dict snapshot), default empty.

Guardrails:
- No global scans or dense conversions.
- Operates only on local neighbor lists and small map snapshots.
- No schedulers; TTL/budget bounded; emits compact events only.

References:
- Refractive-index steering law: P(i→j) ∝ exp(Θ · m[j]) with logistic 2-way fork (see derivation/memory_steering.md).
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            # fallback to uniform pick among candidates
            return pairs[0][0]
        r = (hash((len(pairs), m, Z)) & 0xFFFF) / 65535.0 * Z  # deterministic-ish fallback
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(pairs[0][0])
        except Exception:
            return None


class VoidRayScout(BaseScout):
    """
    Physics-aware scout: routes along favorable local φ gradients with optional memory steering.
    """

    __slots__ = ("lambda_phi", "theta_mem", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        lambda_phi: float = 1.0,
        theta_mem: float = 0.0,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.lambda_phi = float(lambda_phi)
        self.theta_mem = float(theta_mem)
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer HeatMap head when available for initial seeds (bounded, read-only)
        return _head_to_set(maps, "heat_head", cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        cur: int,
        neigh: Sequence[int],
        connectome: Any,
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None

        # φ values: read only local entries (no scans)
        phi = getattr(connectome, "phi", None)

        def _phi(idx: int) -> float:
            try:
                if phi is None:
                    return 0.0
                return float(phi[idx])
            except Exception:
                return 0.0

        phi_i = _phi(int(cur))
        md = maps.get("memory_dict", {}) if isinstance(maps, dict) else {}

        logits: List[tuple[int, float]] = []
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            s = (self.lambda_phi * (_phi(j) - phi_i)) + (self.theta_mem * m_j)
            logits.append((j, s / self.tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(cur, neigh, connectome, maps)
                        if v is None or v == cur:
                            # fallback to blue-noise hop
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["VoidRayScout"]]]></content>
    </file>
    <file>
      <path>core/cortex/void_walkers/void_sentinel_scout.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_sentinel_scout

SentinelScout (read-only, void-faithful):
- Blue-noise reseeder / de-trample walker.
- Purpose: prevent path lock-in by sampling uniformly across space and announcing coverage.
- Emits vt_touch for coverage and opportunistic edge_on (one hop) when neighbors exist.

Behavior:
- Seeds = budget["seeds"] when provided (e.g., recent UTE indices) else uniform random nodes.
- TTL kept minimal (default 1) to avoid trampling and keep cost bounded.
- Local reads only (neighbors of the current node); no scans; no writes.

Optional inputs (maps):
- "visit_head" or "cold_head" can bias seeds slightly when present; still bounded heads.
"""

from typing import Any, Dict, Optional, Sequence, Set, List
from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], keys: Sequence[str], cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    for key in keys:
        try:
            head = maps.get(key, []) or []
            for pair in head[: cap]:
                try:
                    n = int(pair[0])
                except Exception:
                    continue
                if n >= 0:
                    out.add(n)
        except Exception:
            continue
    return out


class SentinelScout(BaseScout):
    """
    Blue-noise reseeding walker with minimal TTL to refresh coverage.
    """

    __slots__ = ()

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 1,   # one hop per seed by default
        seed: int = 0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer low-visit or cold heads when available; bounded and read-only
        return _head_to_set(maps, keys=("visit_head", "cold_head"), cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                # Sentinel is intentionally shallow; cap TTL to 1 even if provided larger
                ttl = max(1, min(1, int(budget.get("ttl", ttl))))
            except Exception:
                ttl = 1
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = 1  # enforce single-step walks to reduce trampling

        # Seeds: prefer explicit seeds; else priority; else uniform domain
        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Announce coverage
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Opportunistic single hop
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        try:
                            v = int(self.rng.choice(tuple(neigh)))
                        except Exception:
                            break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["SentinelScout"]]]></content>
    </file>
    <file>
      <path>core/diagnostics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Physics-informed diagnostics for the FUM runtime.

This module provides:
- Mass-gap estimation from two-point correlations on the runtime graph
- Pulse-speed (group velocity) estimation from time-resolved activity

References:
- [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:125-193)
- [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134)
- [derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1)
- [derivation/fum_voxtrium_mapping.md](derivation/fum_voxtrium_mapping.md:44-121)

Conventions:
- We treat graph shortest-path distance (in hops) as the discrete spatial metric r
  when geometric embedding is unavailable. This is a standard surrogate on networks.
- The continuum prediction for the static two-point correlator is
  C(r) ~ exp(-r / xi) with mass gap m_eff = 1 / xi (dimensionless units).
- The wave speed c enters the EOM via c^2 = 2 J a^2 (per-site convention), see
  [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134).
  We estimate an effective group velocity v_g from an expanding activity front.

Author: Justin K. Lietz
Date: 2025-08-09
"""

from __future__ import annotations

import math
from typing import Dict, List, Optional, Tuple

import numpy as np


def _ensure_adjacency(connectome) -> np.ndarray:
    """
    Extract a dense binary adjacency matrix A (int8) from the runtime connectome.

    Expected connectome interface (from fum_rt.core.*):
      - connectome.A : np.ndarray (N x N), int8, 0/1
      - connectome.N : int, number of nodes

    Returns:
      A : np.ndarray (N x N) int8 in {0,1}
    """
    if not hasattr(connectome, "A"):
        raise AttributeError("connectome must expose .A (adjacency)")

    A = connectome.A
    if not isinstance(A, np.ndarray):
        A = np.asarray(A)
    # Ensure binary
    A = (A != 0).astype(np.int8)
    return A


def _shortest_path_distances(A: np.ndarray, seeds: np.ndarray, max_d: int) -> Dict[int, List[Tuple[int, int, int]]]:
    """
    Compute shortest-path distances from a subset of seed nodes using BFS.

    Args:
      A: binary adjacency (N x N)
      seeds: array of seed node indices
      max_d: maximum distance to consider

    Returns:
      distances_by_d: mapping d -> list of (src, dst, d) pairs observed with shortest-path distance d,
                      with 1 <= d <= max_d
    """
    N = A.shape[0]
    neighbors = [np.where(A[i] != 0)[0] for i in range(N)]
    distances_by_d: Dict[int, List[Tuple[int, int, int]]] = {d: [] for d in range(1, max_d + 1)}

    for s in seeds:
        dist = np.full(N, -1, dtype=np.int32)
        dist[s] = 0
        q = [s]
        head = 0
        while head < len(q):
            u = q[head]
            head += 1
            du = dist[u]
            if du >= max_d:
                continue
            for v in neighbors[u]:
                if dist[v] == -1:
                    dist[v] = du + 1
                    q.append(v)
                    if 1 <= dist[v] <= max_d:
                        distances_by_d[dist[v]].append((s, v, dist[v]))
    return distances_by_d


def estimate_mass_gap_from_phi(
    connectome,
    phi: Optional[np.ndarray] = None,
    sample_fraction: float = 0.1,
    max_d: int = 10,
    min_counts_per_d: int = 50,
) -> Dict[str, float]:
    """
    Estimate the correlation length xi (in graph hops) and mass gap m_eff = 1/xi
    from a static snapshot of a scalar field on the graph.

    Inputs:
      connectome: runtime connectome with fields .A (binary adjacency) and .N
      phi: optional np.ndarray (N,) scalar field per node. If None, use a weight-derived proxy:
           phi_i := sum_j |E_ij| if connectome.E available, else degree (sum of A_i*).
      sample_fraction: fraction of nodes to use as BFS seeds (subsamples for speed)
      max_d: maximum graph distance to consider
      min_counts_per_d: require at least this many pairs per distance bin for inclusion

    Outputs (in a dict):
      {
        "xi": correlation length (hops),
        "m_eff": 1/xi,
        "r_values": number of bins used,
        "fit_slope": slope of log C(d) vs d,
        "fit_intercept": intercept,
      }

    Notes:
      - Two-point connected correlator defined as C(d) = mean_{pairs at dist d}[ (phi_i - mu)(phi_j - mu) ],
        normalized by var to reduce scale dependence. We then fit log C(d) ~ -d/xi + const.
      - If no sufficient bins, returns NaNs.
    """
    A = _ensure_adjacency(connectome)
    N = A.shape[0]

    # Field proxy if none provided
    if phi is None:
        if hasattr(connectome, "E") and isinstance(connectome.E, np.ndarray):
            # node scalar = L1 sum of incident weights (absolute)
            phi = np.sum(np.abs(connectome.E), axis=1).astype(np.float64)
        else:
            # fallback: degree
            phi = np.sum(A, axis=1).astype(np.float64)

    phi = np.asarray(phi, dtype=np.float64)
    if phi.shape[0] != N:
        raise ValueError("phi length must equal connectome.N")

    mu = float(np.mean(phi))
    var = float(np.var(phi))
    if var <= 1e-18:
        return {"xi": float("nan"), "m_eff": float("nan"), "r_values": 0, "fit_slope": float("nan"), "fit_intercept": float("nan")}

    # Subsample seeds
    rng = np.random.default_rng(12345)
    seeds = np.arange(N)
    rng.shuffle(seeds)
    keep = max(1, int(sample_fraction * N))
    seeds = seeds[:keep]

    distances_by_d = _shortest_path_distances(A, seeds, max_d=max_d)

    # Compute correlator per distance
    C_vals = []
    d_vals = []
    for d in range(1, max_d + 1):
        pairs = distances_by_d[d]
        if len(pairs) < min_counts_per_d:
            continue
        # average over pairs: connected correlator normalized by var
        num = 0.0
        for (i, j, _) in pairs:
            num += (phi[i] - mu) * (phi[j] - mu)
        C_d = (num / len(pairs)) / var
        if C_d > 1e-12:
            C_vals.append(max(C_d, 1e-12))
            d_vals.append(d)

    if len(d_vals) < 2:
        return {"xi": float("nan"), "m_eff": float("nan"), "r_values": 0, "fit_slope": float("nan"), "fit_intercept": float("nan")}

    # Fit log C(d) ~ - d/xi + const
    y = np.log(np.asarray(C_vals))
    x = np.asarray(d_vals, dtype=np.float64)
    # Least squares fit
    A_fit = np.vstack([x, np.ones_like(x)]).T
    slope, intercept = np.linalg.lstsq(A_fit, y, rcond=None)[0]  # y = slope*x + intercept
    # slope should be negative: slope = -1/xi
    if slope >= -1e-12:
        xi = float("inf")
    else:
        xi = -1.0 / slope
    m_eff = 1.0 / xi if xi != float("inf") else 0.0

    return {
        "xi": float(xi),
        "m_eff": float(m_eff),
        "r_values": int(len(d_vals)),
        "fit_slope": float(slope),
        "fit_intercept": float(intercept),
    }


class PulseSpeedEstimator:
    """
    Online estimator for a pulse (activity front) group velocity on a graph.

    Usage:
      pse = PulseSpeedEstimator(connectome)
      pse.begin(center_node=some_index, tick=t0)
      for each tick t:
          pse.observe(tick=t, active_mask=spikes or thresholded field)
      result = pse.finalize()

    The estimator computes the mean geodesic radius of active nodes relative to the chosen center,
    then fits a linear model radius(t) ~ v_g * (t - t0) + const to recover v_g.
    """

    def __init__(self, connectome, max_radius: Optional[int] = None):
        self.A = _ensure_adjacency(connectome)
        self.N = self.A.shape[0]
        self.max_radius = max_radius if max_radius is not None else max(10, self.N // 10)
        self._dist_cache_center: Optional[int] = None
        self._dist_from_center: Optional[np.ndarray] = None
        self._t0: Optional[float] = None
        self._ts: List[float] = []
        self._radii: List[float] = []

    def _bfs_from_center(self, c: int) -> np.ndarray:
        dist = np.full(self.N, -1, dtype=np.int32)
        dist[c] = 0
        q = [c]
        head = 0
        rows = self.A
        neighbors = [np.where(rows[i] != 0)[0] for i in range(self.N)]
        while head < len(q):
            u = q[head]
            head += 1
            du = dist[u]
            if du >= self.max_radius:
                continue
            for v in neighbors[u]:
                if dist[v] == -1:
                    dist[v] = du + 1
                    q.append(v)
        return dist

    def begin(self, center_node: int, tick: float):
        self._dist_cache_center = int(center_node)
        self._dist_from_center = self._bfs_from_center(self._dist_cache_center)
        self._t0 = float(tick)
        self._ts.clear()
        self._radii.clear()

    def observe(self, tick: float, active_mask: np.ndarray):
        """
        Record the mean radius of currently active nodes.

        Args:
          tick: current time (integer tick or float time)
          active_mask: boolean array shape (N,) marking active nodes at this tick
                       (e.g., spikes, or |delta phi| > threshold)
        """
        if self._dist_from_center is None or self._t0 is None:
            raise RuntimeError("PulseSpeedEstimator.begin(...) must be called before observe(...)")

        active_mask = np.asarray(active_mask, dtype=bool)
        if active_mask.shape[0] != self.N:
            raise ValueError("active_mask length must equal number of nodes")

        idx = np.where(active_mask)[0]
        if idx.size == 0:
            return  # skip empty frames
        d = self._dist_from_center[idx]
        d = d[d >= 0]  # ignore unreachable or -1
        if d.size == 0:
            return
        mean_r = float(np.mean(d))
        self._ts.append(float(tick) - self._t0)
        self._radii.append(mean_r)

    def finalize(self) -> Dict[str, float]:
        """
        Fit radius(t) ~ v_g * (t - t0) + const. Return v_g and fit diagnostics.
        """
        if len(self._ts) < 2:
            return {"v_g": float("nan"), "frame_count": int(len(self._ts)), "slope": float("nan"), "intercept": float("nan")}
        x = np.asarray(self._ts, dtype=np.float64)
        y = np.asarray(self._radii, dtype=np.float64)
        A_fit = np.vstack([x, np.ones_like(x)]).T
        slope, intercept = np.linalg.lstsq(A_fit, y, rcond=None)[0]
        return {"v_g": float(max(0.0, slope)), "frame_count": int(len(self._ts)), "slope": float(slope), "intercept": float(intercept)}]]></content>
    </file>
    <file>
      <path>core/engine/__init__.py</path>
      <content><![CDATA["""
Core Engine package initializer.

Exports CoreEngine from the in-package implementation module to avoid any
cross-file redirects. Implementation resides under this package.
"""

from .core_engine import CoreEngine

__all__ = ["CoreEngine"]]]></content>
    </file>
    <file>
      <path>core/engine/core_engine.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Core seam: temporary adapter that forwards to existing Nexus internals without changing behavior.

Phase B goal:
- Define a stable Core API now to avoid rework later.
- Do NOT move logic yet; keep Nexus as source of truth.
- These methods either delegate to existing functions or act as explicit stubs.

Separation policy:
- This module must not import from fum_rt.io.* or fum_rt.runtime.* to keep core isolated.
- Only depend on fum_rt.core.* and the Nexus-like object passed at construction.
"""

from typing import Any, Dict, Optional, Tuple

from fum_rt.core.metrics import compute_metrics
from fum_rt.core.memory import (
    load_engram as _load_engram_state,
    save_checkpoint as _save_checkpoint,
)
from fum_rt.core.proprioception.events import EventDrivenMetrics as _EvtMetrics
from fum_rt.core.cortex.scouts import VoidColdScoutWalker as _VoidScout, ColdMap as _ColdMap
from fum_rt.core.cortex.maps.heatmap import HeatMap as _HeatMap
from fum_rt.core.cortex.maps.excitationmap import ExcitationMap as _ExcMap
from fum_rt.core.cortex.maps.inhibitionmap import InhibitionMap as _InhMap
from fum_rt.core.cortex.maps.trailmap import TrailMap as _TrailMap
from fum_rt.core.cortex.maps.memorymap import MemoryMap as _MemMap
from fum_rt.core.signals import (
    compute_active_edge_density as _sig_density,
    compute_td_signal as _sig_td,
    compute_firing_var as _sig_fvar,
)

# Local helpers (telemetry-only; remain inside core boundary)
from .maps_frame import stage_maps_frame
from .evt_snapshot import build_evt_snapshot


class CoreEngine:
    """
    Temporary adapter (seam) to the current runtime.

    - step(): folds event-driven reducers (no IO/logging) and stages maps/frame for telemetry.
    - snapshot(): exposes a minimal, safe snapshot using current metrics.
    - engram_load(): pass-through to the legacy loader.
    - engram_save(): pass-through to the legacy saver (saves into run_dir; path argument is advisory).
    """

    def __init__(self, nexus_like: Any) -> None:
        """
        nexus_like: an instance exposing the attributes currently used by the runtime:
          - connectome, adc, run_dir, checkpoint_format (optional), logger (optional), _phase (optional)
        """
        self._nx = nexus_like
        # Public alias for tests and adapters that expect a public handle
        try:
            self.nx = self._nx  # test convenience: allows eng.nx access
        except Exception:
            pass
        # Event-driven stack (lazy-initialized)
        self._evt_metrics: Optional[_EvtMetrics] = None
        self._void_scout: Optional[_VoidScout] = None
        self._cold_map: Optional[_ColdMap] = None
        self._heat_map: Optional[_HeatMap] = None
        self._exc_map: Optional[_ExcMap] = None
        self._inh_map: Optional[_InhMap] = None
        self._memory_map: Optional[_MemMap] = None
        self._trail_map: Optional[_TrailMap] = None
        self._last_evt_snapshot: Dict[str, Any] = {}

    # ---- Event-driven fold and telemetry staging ----
    def step(self, dt_ms: int, ext_events: list) -> None:
        """
        Fold provided core events and cold-scout events into event-driven reducers.
        Pure core; no IO/logging. Read-only against connectome.
        """
        # lazy init local reducers and VOID scout
        try:
            self._ensure_evt_init()
        except Exception:
            pass

        if getattr(self, "_evt_metrics", None) is None:
            return

        # latest tick observed this step (from ext events or scout)
        latest_tick = None
        collected_events: list = []

        # 1) fold external events (already core BaseEvent subclasses from runtime adapter)
        try:
            for ev in (ext_events or []):
                try:
                    # accept any object exposing 'kind' attribute (duck-typed BaseEvent)
                    if hasattr(ev, "kind"):
                        self._evt_metrics.update(ev)
                        collected_events.append(ev)
                        # update cold-map on node touches/endpoints when possible
                        if getattr(self, "_cold_map", None) is not None:
                            try:
                                kind = getattr(ev, "kind", "")
                                t_ev = getattr(ev, "t", None)
                                if t_ev is not None:
                                    if kind == "vt_touch":
                                        token = getattr(ev, "token", None)
                                        if isinstance(token, int) and token >= 0:
                                            self._cold_map.touch(int(token), int(t_ev))
                                    elif kind == "edge_on":
                                        u = getattr(ev, "u", None)
                                        v = getattr(ev, "v", None)
                                        if isinstance(u, int) and u >= 0:
                                            self._cold_map.touch(int(u), int(t_ev))
                                        if isinstance(v, int) and v >= 0:
                                            self._cold_map.touch(int(v), int(t_ev))
                            except Exception:
                                pass
                        # track latest tick seen
                        try:
                            tv = getattr(ev, "t", None)
                            if tv is not None:
                                if latest_tick is None or int(tv) > int(latest_tick):
                                    latest_tick = int(tv)
                        except Exception:
                            pass
                except Exception:
                    continue
        except Exception:
            pass

        # 2) fold VOID cold-scout reads (read-only traversal)
        try:
            if getattr(self, "_void_scout", None) is not None:
                # Prefer explicit tick from external events; fallback to predicted next tick.
                tick_hint = None
                try:
                    if ext_events:
                        # Pick the last event with a valid 't' (most recent)
                        for _e in reversed(ext_events):
                            tv = getattr(_e, "t", None)
                            if tv is not None:
                                tick_hint = int(tv)
                                break
                except Exception:
                    tick_hint = None
                if tick_hint is None:
                    try:
                        # Use next tick relative to last emitted step (updated later in runtime loop)
                        tick_hint = int(getattr(self._nx, "_emit_step", -1)) + 1
                    except Exception:
                        tick_hint = 0
                C = getattr(getattr(self, "_nx", None), "connectome", None)
                for _ev in self._void_scout.step(C, int(tick_hint)) or []:
                    try:
                        self._evt_metrics.update(_ev)
                        collected_events.append(_ev)
                        # update cold-map for scout-generated events
                        if getattr(self, "_cold_map", None) is not None:
                            try:
                                kind = getattr(_ev, "kind", "")
                                if kind == "vt_touch":
                                    token = getattr(_ev, "token", None)
                                    if isinstance(token, int) and token >= 0:
                                        self._cold_map.touch(int(token), int(tick_hint))
                                elif kind == "edge_on":
                                    u = getattr(_ev, "u", None)
                                    v = getattr(_ev, "v", None)
                                    if isinstance(u, int) and u >= 0:
                                        self._cold_map.touch(int(u), int(tick_hint))
                                    if isinstance(v, int) and v >= 0:
                                        self._cold_map.touch(int(v), int(tick_hint))
                            except Exception:
                                pass
                    except Exception:
                        continue
                # update latest tick from scout pass
                try:
                    if latest_tick is None or int(tick_hint) > int(latest_tick):
                        latest_tick = int(tick_hint)
                except Exception:
                    pass
        except Exception:
            pass

        # 2.5) fold heat/excitation/inhibition (+memory/trail) maps with collected events (telemetry-only)
        try:
            try:
                fold_tick = int(latest_tick) if latest_tick is not None else int(getattr(self._nx, "_emit_step", -1)) + 1
            except Exception:
                fold_tick = 0
            if getattr(self, "_heat_map", None) is not None:
                try:
                    self._heat_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_exc_map", None) is not None:
                try:
                    self._exc_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_inh_map", None) is not None:
                try:
                    self._inh_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_memory_map", None) is not None:
                try:
                    self._memory_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_trail_map", None) is not None:
                try:
                    self._trail_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
        except Exception:
            pass

        # 2.75) stage maps/frame payload for UI bus (header JSON + Float32 LE payload)
        try:
            stage_maps_frame(
                nx=self._nx,
                heat_map=self._heat_map,
                exc_map=self._exc_map,
                inh_map=self._inh_map,
                fold_tick=int(
                    latest_tick
                    if latest_tick is not None
                    else (int(getattr(self._nx, "_emit_step", -1)) + 1)
                ),
            )
        except Exception:
            pass

        # 3) refresh cached evt snapshot
        try:
            self._last_evt_snapshot = build_evt_snapshot(
                evt_metrics=self._evt_metrics,
                cold_map=self._cold_map,
                heat_map=self._heat_map,
                exc_map=self._exc_map,
                inh_map=self._inh_map,
                memory_map=getattr(self, "_memory_map", None),
                trail_map=getattr(self, "_trail_map", None),
                latest_tick=(
                    int(latest_tick)
                    if latest_tick is not None
                    else (int(getattr(self._nx, "_emit_step", -1)) + 1)
                ),
                nx=self._nx,
            )
        except Exception:
            self._last_evt_snapshot = {}

    def _ensure_evt_init(self) -> None:
        """
        Initialize event-driven reducers (EventDrivenMetrics) and VOID scout lazily
        using configuration exposed by the nexus-like object when available.
        """
        # reducers
        if getattr(self, "_evt_metrics", None) is None:
            try:
                det = getattr(self._nx, "b1_detector", None)
                z_spike = float(getattr(det, "z_spike", 1.0)) if det is not None else 1.0
                hysteresis = float(getattr(det, "hysteresis", 1.0)) if det is not None else 1.0
                half_life = int(getattr(self._nx, "b1_half_life_ticks", 50))
                seed = int(getattr(self._nx, "seed", 0))
                self._evt_metrics = _EvtMetrics(
                    z_half_life_ticks=max(1, half_life),
                    z_spike=z_spike,
                    hysteresis=hysteresis,
                    seed=seed,
                )
            except Exception:
                self._evt_metrics = None
        # VOID scout
        if getattr(self, "_void_scout", None) is None:
            try:
                sv = int(getattr(self._nx, "scout_visits", 16))
            except Exception:
                sv = 16
            try:
                se = int(getattr(self._nx, "scout_edges", 8))
            except Exception:
                se = 8
            try:
                seed = int(getattr(self._nx, "seed", 0))
            except Exception:
                seed = 0
            try:
                self._void_scout = _VoidScout(
                    budget_visits=max(0, sv), budget_edges=max(0, se), seed=seed
                )
            except Exception:
                self._void_scout = None
        # Cold-map reducer
        if getattr(self, "_cold_map", None) is None:
            try:
                ck = int(getattr(self._nx, "cold_head_k", 256))
            except Exception:
                ck = 256
            try:
                hl = int(getattr(self._nx, "cold_half_life_ticks", 200))
            except Exception:
                hl = 200
            try:
                seed = int(getattr(self._nx, "seed", 0))
            except Exception:
                seed = 0
            try:
                self._cold_map = _ColdMap(
                    head_k=max(8, ck), half_life_ticks=max(1, hl), keep_max=None, seed=seed
                )
            except Exception:
                self._cold_map = None
        # Heat/Excitation/Inhibition reducers (mirror cold-map settings; telemetry-only)
        try:
            hk = int(getattr(self._nx, "cold_head_k", 256))
        except Exception:
            hk = 256
        try:
            hl2 = int(getattr(self._nx, "cold_half_life_ticks", 200))
        except Exception:
            hl2 = 200
        try:
            seed = int(getattr(self._nx, "seed", 0))
        except Exception:
            seed = 0
        if getattr(self, "_heat_map", None) is None:
            try:
                self._heat_map = _HeatMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 1
                )
            except Exception:
                self._heat_map = None
        if getattr(self, "_exc_map", None) is None:
            try:
                self._exc_map = _ExcMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 2
                )
            except Exception:
                self._exc_map = None
        if getattr(self, "_inh_map", None) is None:
            try:
                self._inh_map = _InhMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 3
                )
            except Exception:
                self._inh_map = None
        # Memory/Trail reducers (event-driven steering fields; telemetry-only exposure)
        if getattr(self, "_memory_map", None) is None:
            try:
                self._memory_map = _MemMap(
                    head_k=max(8, hk), keep_max=None, seed=seed + 4
                )
                # expose a read-only pointer for local getters without scans
                try:
                    C = getattr(self._nx, "connectome", None)
                    if C is not None:
                        setattr(C, "_memory_map", self._memory_map)
                except Exception:
                    pass
            except Exception:
                self._memory_map = None
        if getattr(self, "_trail_map", None) is None:
            try:
                self._trail_map = _TrailMap(
                    head_k=max(8, hk),
                    half_life_ticks=max(1, int(max(1, hl2 // 4))),
                    keep_max=None,
                    seed=seed + 5,
                )
            except Exception:
                self._trail_map = None

    # --- Connectome interface (single entrypoint for runtime) ---
    def stimulate_indices(self, indices, amp: float = 0.05) -> None:
        try:
            self._nx.connectome.stimulate_indices(list(indices), amp=float(amp))
        except Exception:
            pass

    def step_connectome(
        self,
        t: float,
        domain_modulation: float = 1.0,
        sie_gate: float = 0.0,
        use_time_dynamics: bool = True,
    ) -> None:
        try:
            self._nx.connectome.step(
                t,
                domain_modulation=float(domain_modulation),
                sie_drive=float(sie_gate),
                use_time_dynamics=bool(use_time_dynamics),
            )
        except Exception:
            pass

    def compute_metrics(self) -> Dict[str, Any]:
        try:
            return compute_metrics(self._nx.connectome)
        except Exception:
            return {}

    def snapshot_graph(self):
        try:
            return self._nx.connectome.snapshot_graph()
        except Exception:
            return None

    # --- Numeric helpers (wrap core.signals) ---
    def compute_active_edge_density(self) -> Tuple[int, float]:
        try:
            N = int(getattr(self._nx, "N", 0))
        except Exception:
            N = 0
        try:
            return _sig_density(getattr(self._nx, "connectome", None), N)
        except Exception:
            return 0, 0.0

    def compute_td_signal(
        self, prev_E: int | None, E: int, vt_prev: float | None = None, vt_last: float | None = None
    ) -> float:
        try:
            return float(_sig_td(prev_E, E, vt_prev, vt_last))
        except Exception:
            return 0.0

    def compute_firing_var(self):
        try:
            return _sig_fvar(getattr(self._nx, "connectome", None))
        except Exception:
            return None

    def get_homeostasis_counters(self) -> Tuple[int, int]:
        try:
            pruned = int(getattr(self._nx.connectome, "_last_pruned_count", 0))
            bridged = int(getattr(self._nx.connectome, "_last_bridged_count", 0))
            return pruned, bridged
        except Exception:
            return 0, 0

    def get_findings(self) -> Dict[str, Any]:
        try:
            f = getattr(self._nx.connectome, "findings", None)
            return dict(f) if isinstance(f, dict) else {}
        except Exception:
            return {}

    def get_last_sie2_valence(self) -> float:
        try:
            return float(getattr(self._nx.connectome, "_last_sie2_valence", 0.0))
        except Exception:
            return 0.0

    def snapshot(self) -> Dict[str, Any]:
        """
        Build a minimal state snapshot via current compute_metrics without mutating the model.
        Adds common context fields used by Why providers when available.
        Also merges cached event-driven metrics under an 'evt_' prefix to preserve canonical fields.
        """
        nx = self._nx
        m = compute_metrics(nx.connectome)
        # Attach minimal, non-intrusive context
        try:
            m["t"] = int(getattr(nx, "_emit_step", 0))
        except Exception:
            pass
        try:
            m["phase"] = int(getattr(nx, "_phase", {}).get("phase", 0))
        except Exception:
            pass
        # Merge event-driven snapshot without overriding canonical keys
        try:
            evs = getattr(self, "_last_evt_snapshot", None)
            if isinstance(evs, dict):
                for k, v in evs.items():
                    try:
                        # preserve existing canonical b1_* if present
                        if str(k).startswith("b1_") and k in m:
                            continue
                        m[f"evt_{k}"] = v
                    except Exception:
                        continue
        except Exception:
            pass
        return m

    def engram_load(self, path: str) -> None:
        """
        Pass-through to the existing engram loader with ADC included when available.
        Mirrors the call used in Nexus, preserving logs/events and behavior.
        """
        nx = self._nx
        _load_engram_state(str(path), nx.connectome, adc=getattr(nx, "adc", None))
        # Optional: let the caller log; we keep core side-effect free except the actual load.

    def engram_save(
        self, path: Optional[str] = None, step: Optional[int] = None, fmt: Optional[str] = None
    ) -> str:
        """
        Pass-through to the existing checkpoint saver. Saves into nx.run_dir using the legacy naming scheme.
        Arguments:
          - path: advisory only (ignored by the legacy saver, which chooses its own path under run_dir)
          - step: when None, the caller should provide an explicit step; if missing, a safe default is used (0)
          - fmt: optional override for format (e.g., 'h5' or 'npz'); defaults to nx.checkpoint_format or 'h5'

        Returns:
          The filesystem path returned by the legacy saver.
        """
        nx = self._nx
        use_step = int(step if step is not None else getattr(nx, "_emit_step", 0))
        use_fmt = str(
            fmt if fmt is not None else getattr(nx, "checkpoint_format", "h5") or "h5"
        )
        return _save_checkpoint(
            nx.run_dir, use_step, nx.connectome, fmt=use_fmt, adc=getattr(nx, "adc", None)
        )


__all__ = ["CoreEngine"]]]></content>
    </file>
    <file>
      <path>core/engine/evt_snapshot.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event-driven snapshot builder (core-local).

- Aggregates lightweight telemetry from:
  * EventDrivenMetrics.snapshot()
  * ColdMap.snapshot(t)
  * Heat/Excitation/Inhibition reducer snapshots
- Pure function; no IO, no scans, no side-effects outside returning a dict.
"""

from __future__ import annotations

from typing import Any, Dict, Optional


def _safe_merge(dst: Dict[str, Any], src: Optional[Dict[str, Any]]) -> None:
    if not isinstance(src, dict):
        return
    for k, v in src.items():
        try:
            dst[k] = v
        except Exception:
            continue


def build_evt_snapshot(
    *,
    evt_metrics: Optional[Any],
    cold_map: Optional[Any],
    heat_map: Optional[Any],
    exc_map: Optional[Any],
    inh_map: Optional[Any],
    memory_map: Optional[Any] = None,
    trail_map: Optional[Any] = None,
    latest_tick: int = 0,
    nx: Any = None,
) -> Dict[str, Any]:
    """
    Construct a consolidated event-driven snapshot without mutating model state.

    Parameters:
      evt_metrics: EventDrivenMetrics instance (or None)
      cold_map: ColdMap reducer (or None)
      heat_map/exc_map/inh_map: reducers exposing snapshot() -> dict
      latest_tick: integer tick associated with this fold/snapshot
      nx: nexus-like handle (unused; reserved for future keys)

    Returns:
      dict of event-driven fields (raw keys) to be prefixed by the caller when merging
      into the canonical telemetry map (e.g., "evt_*" in CoreEngine.snapshot()).
    """
    out: Dict[str, Any] = {}

    # 1) Base event-driven metrics
    try:
        if evt_metrics is not None:
            evs = evt_metrics.snapshot()
            if isinstance(evs, dict):
                _safe_merge(out, evs)
    except Exception:
        pass

    # 2) Cold map snapshot at the current tick (bounded head only; no scans)
    try:
        if cold_map is not None:
            cs = cold_map.snapshot(int(latest_tick))
            if isinstance(cs, dict):
                _safe_merge(out, cs)
    except Exception:
        pass

    # 3) Heat/Exc/Inh reducer snapshots (bounded heads; telemetry-only)
    try:
        if heat_map is not None:
            hs = heat_map.snapshot()
            if isinstance(hs, dict):
                _safe_merge(out, hs)
    except Exception:
        pass

    try:
        if exc_map is not None:
            es = exc_map.snapshot()
            if isinstance(es, dict):
                _safe_merge(out, es)
    except Exception:
        pass

    try:
        if inh_map is not None:
            ins = inh_map.snapshot()
            if isinstance(ins, dict):
                _safe_merge(out, ins)
    except Exception:
        pass

    # 4) Optional steering fields (views): memory/trail (bounded heads/dicts; no scans)
    try:
        if memory_map is not None:
            ms = memory_map.snapshot()
            if isinstance(ms, dict):
                _safe_merge(out, ms)
    except Exception:
        pass

    try:
        if trail_map is not None:
            ts = trail_map.snapshot()
            if isinstance(ts, dict):
                _safe_merge(out, ts)
    except Exception:
        pass

    return out


__all__ = ["build_evt_snapshot"]]]></content>
    </file>
    <file>
      <path>core/engine/maps_frame.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Telemetry maps/frame builder (core-local, no IO).

- Builds Float32 LE arrays for heat/excitation/inhibition from bounded reducer working sets.
- Computes header with contract:
  {topic:'maps/frame', tick, n, shape, channels:['heat','exc','inh'], dtype:'f32', endianness:'LE', stats}
- Stages result onto nx._maps_frame_ready for runtime telemetry emitters to publish.
- Strictly avoids any W/CSR/adjacency scans; operates only on small reducer dictionaries.
"""

from __future__ import annotations

from typing import Any, Dict, Optional

import numpy as _np


def _max_from(d: Dict[int, float]) -> float:
    try:
        if not d:
            return 0.0
        # Mirror payload dtype (float32 LE): cast values to float32 before max to ensure
        # header['stats']['max'] ≥ observed max from the serialized payload.
        return float(max((_np.float32(v) for v in d.values())))
    except Exception:
        return 0.0


def _fill_array_from_map(arr: _np.ndarray, d: Dict[int, float]) -> None:
    try:
        n = int(arr.shape[0])
    except Exception:
        n = len(arr)
    try:
        for k, v in (d or {}).items():
            try:
                ik = int(k)
                if 0 <= ik < n:
                    arr[ik] = float(v)
            except Exception:
                continue
    except Exception:
        pass


def stage_maps_frame(
    nx: Any,
    heat_map: Optional[Any],
    exc_map: Optional[Any],
    inh_map: Optional[Any],
    fold_tick: int,
) -> None:
    """
    Construct and stage the maps/frame payload on nx._maps_frame_ready.

    Parameters:
      nx: nexus-like object, must provide integer attribute N (<= few 10^6) for shape.
      heat_map/exc_map/inh_map: reducers exposing a _val: Dict[int,float] working set (bounded).
      fold_tick: integer tick associated to this fold (monotonic).
    """
    try:
        N = int(getattr(nx, "N", 0))
    except Exception:
        N = 0
    if N <= 0:
        return

    # Allocate arrays (Float32 LE by frombuffer/tobytes contract downstream)
    heat_arr = _np.zeros(N, dtype=_np.float32)
    exc_arr = _np.zeros(N, dtype=_np.float32)
    inh_arr = _np.zeros(N, dtype=_np.float32)

    # Fill from bounded dictionaries (no global scans)
    try:
        _fill_array_from_map(heat_arr, getattr(heat_map, "_val", {}))
    except Exception:
        pass
    try:
        _fill_array_from_map(exc_arr, getattr(exc_map, "_val", {}))
    except Exception:
        pass
    try:
        _fill_array_from_map(inh_arr, getattr(inh_map, "_val", {}))
    except Exception:
        pass

    # Sanitize non-finite
    for arr in (heat_arr, exc_arr, inh_arr):
        try:
            _np.nan_to_num(arr, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
        except Exception:
            pass

    # Square-ish shape heuristic
    try:
        side = int(max(1, int(_np.ceil(_np.sqrt(N)))))
    except Exception:
        side = int(max(1, int((N or 1) ** 0.5)))
    shape = [side, side]

    # Stats from bounded dictionaries (min fixed to 0.0 by construction)
    stats = {
        "heat": {"min": 0.0, "max": _max_from(getattr(heat_map, "_val", {}))},
        "exc": {"min": 0.0, "max": _max_from(getattr(exc_map, "_val", {}))},
        "inh": {"min": 0.0, "max": _max_from(getattr(inh_map, "_val", {}))},
    }

    header = {
        "topic": "maps/frame",
        "tick": int(fold_tick),
        "n": int(N),
        "shape": shape,
        "channels": ["heat", "exc", "inh"],
        "dtype": "f32",
        "endianness": "LE",
        "stats": stats,
    }

    payload = heat_arr.tobytes() + exc_arr.tobytes() + inh_arr.tobytes()

    try:
        setattr(nx, "_maps_frame_ready", (header, payload))
    except Exception:
        pass]]></content>
    </file>
    <file>
      <path>core/fum_growth_arbiter.py</path>
      <content><![CDATA[# fum_growth_arbiter.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Purpose
- Unified growth/cull arbiter driven by the void-equation philosophy.
- Extends your original GrowthArbiter with explicit culling and debt accounting.
- Keeps growth pressure as "void debt" when stable and releases it into organic growth.
- When unstable, trends toward culling and reduces debt accordingly.

Blueprint references
- Rule 1: Parallel Local & Global Systems (arbiter is part of the Global System).
- Rule 3: SIE total_reward is the global pressure input (valence/drive).
- Rule 4/4.1: Structural homeostasis and repair triggers.
Time complexity: O(1) per tick (deque pushes and simple checks).
"""

from collections import deque
from typing import Dict
import numpy as np

# Try to align defaults with your universal constants; fall back to safe values.
try:
    from Void_Equations import get_universal_constants  # noqa: F401
    _UC = get_universal_constants()
    _ALPHA_DEF = float(_UC.get("ALPHA", 0.25))
    _BETA_DEF = float(_UC.get("BETA", 0.10))
except Exception:
    _ALPHA_DEF = 0.25
    _BETA_DEF = 0.10


class GrowthArbiter:
    """
    Monitors rolling metrics to decide when and how much to grow or cull.

    Parameters
    - stability_window: ticks considered to test for a "flat" plateau.
    - trend_threshold: max delta across the window to be considered flat.
    - debt_growth_factor: scale accumulated void debt into new neurons when stable.
    - alpha_growth: growth rate scaling (maps to ALPHA).
    - beta_cull: cull rate scaling (maps to BETA).

    Returns from accumulate_and_decide()
    - dict(grow=int, cull=int, void_debt=float, stable=bool)

    Notes
    - Debt accumulates only while stable; culling reduces debt (cannot drop below 0).
    - Actual target selection for cull/growth (which neurons/edges) is done by the connectome,
      guided by void pulses and S_ij; this arbiter only decides magnitudes and timing.
    """

    def __init__(self,
                 stability_window: int = 10,
                 trend_threshold: float = 0.001,
                 debt_growth_factor: float = 0.10,
                 alpha_growth: float = _ALPHA_DEF,
                 beta_cull: float = _BETA_DEF):
        self.stability_window = int(stability_window)
        self.trend_threshold = float(trend_threshold)
        self.debt_growth_factor = float(debt_growth_factor)
        self.alpha_growth = float(alpha_growth)
        self.beta_cull = float(beta_cull)

        self.weight_history = deque(maxlen=self.stability_window)
        self.synapse_history = deque(maxlen=self.stability_window)
        self.complexity_history = deque(maxlen=self.stability_window)
        self.cohesion_history = deque(maxlen=self.stability_window)

        self.is_stable: bool = False
        self.void_debt_accumulator: float = 0.0  # grows when stable; reduced on cull

    def clear_history(self):
        self.weight_history.clear()
        self.synapse_history.clear()
        self.complexity_history.clear()
        self.cohesion_history.clear()

    def update_metrics(self, metrics: Dict):
        """
        Update historical metrics and recompute stability flag.

        Expected keys (robust to naming differences used elsewhere):
        - avg_weight: float
        - active_synapses: int
        - total_b1_persistence or complexity_cycles: float
        - cohesion_components or cluster_count: int
        """
        self.weight_history.append(float(metrics.get("avg_weight", 0.0)))
        self.synapse_history.append(int(metrics.get("active_synapses", 0)))

        complexity = metrics.get("total_b1_persistence", metrics.get("complexity_cycles", 0.0))
        self.complexity_history.append(float(complexity))

        cohesion = metrics.get("cohesion_components", metrics.get("cluster_count", 1))
        self.cohesion_history.append(int(cohesion))

        if len(self.weight_history) < self.stability_window:
            self.is_stable = False
            return

        is_cohesive = all(c == 1 for c in self.cohesion_history)
        is_weight_flat = abs(self.weight_history[0] - self.weight_history[-1]) < self.trend_threshold
        is_synapse_flat = abs(self.synapse_history[0] - self.synapse_history[-1]) < 3
        is_complexity_flat = abs(self.complexity_history[0] - self.complexity_history[-1]) < self.trend_threshold

        self.is_stable = bool(is_cohesive and is_weight_flat and is_synapse_flat and is_complexity_flat)

    def accumulate_and_decide(self, valence_signal: float) -> Dict:
        """
        Unified decision surface for growth/cull driven by a global pressure (valence).

        Logic
        - If stable: accumulate void debt with |valence|. When debt > 1.0, grow:
            grow = ceil(debt * debt_growth_factor * alpha_growth)
            reset debt, mark unstable, clear history (system re-equilibrates).
        - If unstable: propose cull proportional to beta_cull and |valence|:
            cull = floor(|valence| * beta_cull)
            reduce debt by beta_cull * cull (clamped at 0).
        """
        grow = 0
        cull = 0
        pressure = abs(float(valence_signal))

        if self.is_stable:
            self.void_debt_accumulator += pressure
            if self.void_debt_accumulator > 1.0:
                grow = int(np.ceil(self.void_debt_accumulator * self.debt_growth_factor * self.alpha_growth))
                grow = max(0, grow)
                self.void_debt_accumulator = 0.0
                self.is_stable = False  # perturbation from growth expected
                self.clear_history()
        else:
            cull = int(np.floor(pressure * self.beta_cull))
            cull = max(0, cull)
            if cull > 0:
                self.void_debt_accumulator = max(0.0, self.void_debt_accumulator - (self.beta_cull * cull))

        return {
            "grow": grow,
            "cull": cull,
            "void_debt": float(self.void_debt_accumulator),
            "stable": bool(self.is_stable),
        }]]></content>
    </file>
    <file>
      <path>core/fum_sie.py</path>
      <content><![CDATA[# fum_sie.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
from scipy.sparse import csc_matrix

# --- FUM Modules (optional external helpers) ---
# Keep copyright and your original API intact; add robust fallbacks so fum_rt
# does not depend on external paths being on PYTHONPATH.
try:
    from fum_validated_math import (
        calculate_modulation_factor as _ext_calculate_modulation_factor,
        calculate_stabilized_reward as _ext_calculate_stabilized_reward,
    )
    _HAVE_VALIDATED_MATH = True
except Exception:
    _HAVE_VALIDATED_MATH = False
    _ext_calculate_modulation_factor = None
    _ext_calculate_stabilized_reward = None


def _sigmoid(x: float) -> float:
    x = float(np.clip(x, -500.0, 500.0))
    return 1.0 / (1.0 + np.exp(-x))


def _calculate_modulation_factor(total_reward: float) -> float:
    """
    Blueprint Rule 3 helper: squash to [-1, 1].
    Falls back to internal implementation if the external helper is not available.
    """
    if _HAVE_VALIDATED_MATH and _ext_calculate_modulation_factor is not None:
        try:
            return float(_ext_calculate_modulation_factor(total_reward))
        except Exception:
            pass
    return 2.0 * _sigmoid(total_reward) - 1.0


def _calculate_stabilized_reward(td_error, novelty, habituation, self_benefit, external_reward):
    """
    Blueprint Rule 3 helper: stabilized reward blend (weights + damping).
    Mirrors early_FUM_tests/FUM_Demo/fum_validated_math.py semantics when available.
    """
    if _HAVE_VALIDATED_MATH and _ext_calculate_stabilized_reward is not None:
        try:
            return float(_ext_calculate_stabilized_reward(td_error, novelty, habituation, self_benefit, external_reward))
        except Exception:
            pass
    # Internal fallback (mirrors the reference file)
    W_TD, W_NOVELTY, W_HABITUATION, W_SELF_BENEFIT, W_EXTERNAL = 0.5, 0.2, 0.1, 0.2, 0.8
    td_norm = float(np.clip(td_error, -1.0, 1.0))
    alpha_damping = 1.0 - np.tanh(abs(novelty - self_benefit))
    damped_novelty_term = alpha_damping * (W_NOVELTY * novelty - W_HABITUATION * habituation)
    damped_self_benefit_term = alpha_damping * (W_SELF_BENEFIT * self_benefit)
    if external_reward is not None:
        w_r = W_EXTERNAL if external_reward > 0 else (1.0 - W_EXTERNAL)
        total_reward = w_r * external_reward
    else:
        total_reward = (W_TD * td_norm) + damped_novelty_term + damped_self_benefit_term
    return float(total_reward)

class SelfImprovementEngine:
    """
    The FUM's Self-Improvement Engine (SIE).
    
    This module is the system's intrinsic motivation. It generates the
    internal, multi-objective valence signal that guides all learning and
    adaptation within the Substrate.
    """
    def __init__(self, num_neurons):
        self.num_neurons = num_neurons
        # --- Core Valence Components ---
        self.td_error = 0.0      # Represents unexpectedness or prediction error
        self.novelty = 0.0       # The drive to explore new informational states
        self.habituation = np.zeros(num_neurons) # Counter-force to Novelty
        self.self_benefit = 0.0  # The drive for efficiency and stability

        # --- Phase 2+ / buffers kept intact (preserve your original API/state) ---
        self.cret_buffer = np.zeros(num_neurons, dtype=np.float32)        # CRET
        self.td_value_function = np.zeros(num_neurons, dtype=np.float32)  # TD V

        # Internal bookkeeping (not externally required)
        self.last_reward_time = -1
        self.last_drive = None  # stores the latest computed drive packet (see get_drive)
        self._prev_density = None  # for intrinsic TD proxy (density delta)

    def update_and_calculate_valence(self, W: csc_matrix, external_signal: float, time_step: int) -> float:
        """
        Updates the Core's internal state and returns a unified valence signal in [0, 1].
        Backward-compatible with your original API, while also computing a Rule 3 drive packet.
        """
        drive = self.get_drive(W=W, external_signal=external_signal, time_step=time_step)
        # Preserve legacy behavior: return the [0,1] valence (VGSP-compatible magnitude)
        return float(drive["valence_01"])

    def update_from_runtime_metrics(self, density: float, external_signal: float, time_step: int) -> float:
        """
        Lightweight Rule 3 drive update that avoids requiring a CSC matrix.
        Preserves novelty decay and self_benefit semantics; returns valence in [0,1]
        for VGSP gating. Also updates self.last_drive for introspection.

        Args:
            density: float in [0,1] computed as W.nnz / possible_edges; self_benefit = 1 - density
            external_signal: task or environment feedback, can be None
            time_step: current tick

        Returns:
            float in [0,1]: valence magnitude for gating RE‑VGSP.
        """
        try:
            self.self_benefit = float(1.0 - float(density))
        except Exception:
            self.self_benefit = 0.0

        self.td_error = 0.0 if external_signal is None else float(external_signal)
        habituation_mean = float(self.habituation.mean() if self.habituation.size else 0.0)

        total_reward = _calculate_stabilized_reward(
            td_error=self.td_error,
            novelty=self.novelty,
            habituation=habituation_mean,
            self_benefit=self.self_benefit,
            external_reward=None if external_signal is None else float(external_signal),
        )
        modulation = _calculate_modulation_factor(total_reward)

        # Preserve your novelty trigger dynamics
        if modulation > 0.5 and (time_step - self.last_reward_time) > 150:
            self.novelty = 0.9
            self.last_reward_time = int(time_step)
        else:
            self.novelty *= 0.98

        valence_01 = max(0.0, abs((modulation + self.novelty) / 2.0))

        # Maintain a compact drive packet for downstream consumers
        self.last_drive = {
            "total_reward": float(np.clip(total_reward, -1.0, 1.0)),
            "modulation_factor": float(np.clip(modulation, -1.0, 1.0)),
            "valence_01": float(np.clip(valence_01, 0.0, 1.0)),
            "components": {
                "td_error": float(self.td_error),
                "novelty": float(self.novelty),
                "habituation_mean": float(habituation_mean),
                "self_benefit": float(self.self_benefit),
                "density": float(density),
            }
        }
        return float(self.last_drive["valence_01"])

    # --- Blueprint Rule 3 canonical helpers (kept additive to your API) ---

    def _compute_hsi_norm(self, firing_var: float = None, target_var: float = 0.15) -> float:
        """
        Rule 3 HSI component: higher when firing variance is close to target.
        Returns value in [-1, 1].
        """
        if firing_var is None:
            return 0.0
        target = max(1e-6, float(target_var))
        # Map proximity to target into [-1,1] where exact match -> +1, far -> negative
        prox = 1.0 - min(1.0, abs(float(firing_var) - target) / target)
        # Center around 0; maintain symmetry
        return float(2.0 * prox - 1.0)

    def get_drive(self, W: csc_matrix, external_signal: float, time_step: int,
                  firing_var: float = None, target_var: float = 0.15,
                  weights: dict | None = None,
                  density_override: float | None = None,
                  novelty_idf_scale: float = 1.0) -> dict:
        """
        Compute the canonical Rule 3 drive packet, preserving your novelty and sparsity logic.
        Returns:
            {
              'total_reward': [-1,1],
              'modulation_factor': [-1,1],
              'valence_01': [0,1],            # legacy-compatible magnitude
              'components': {
                   'td_error': ..., 'novelty': ..., 'habituation_mean': ...,
                   'self_benefit': ..., 'hsi_norm': ..., 'density': ...
              }
            }
        """
        # TD error and sparsity (self_benefit) as in your code
        # Allow None to mean "no external reward signal" (intrinsic-only blending)
        ext_val = 0.0 if (external_signal is None) else float(external_signal)

        # Density can be provided directly to avoid converting W; falls back to W if available
        if density_override is not None:
            density = float(min(1.0, max(0.0, density_override)))
        else:
            if W is not None:
                n = int(W.shape[0])
                num_possible_connections = n * max(0, (W.shape[1] - 1))
                density = (W.nnz / num_possible_connections) if num_possible_connections > 0 else 0.0
            else:
                density = 0.0

        # Intrinsic TD proxy from density change if external is absent/negligible
        prev = getattr(self, "_prev_density", None)
        ddens = 0.0 if prev is None else float(density - prev)
        try:
            self._prev_density = float(density)
        except Exception:
            pass
        intrinsic_td = float(np.clip(ddens * 10.0, -1.0, 1.0))
        td = float(ext_val) if abs(float(ext_val)) > 1e-9 else intrinsic_td
        self.td_error = float(td)

        # Self-benefit and a very-light EMA toward topology saturation as a habituation proxy
        self.self_benefit = float(1.0 - density)
        try:
            self.habituation = (0.995 * self.habituation) + (0.005 * float(density))
        except Exception:
            pass

        # Habituation (aggregate for now; you already maintain the vector)
        habituation_mean = float(self.habituation.mean() if self.habituation.size else 0.0)

        # HSI via variance target
        hsi_norm = self._compute_hsi_norm(firing_var=firing_var, target_var=target_var)

        # Stabilized total reward (signed), then squashed to modulation factor
        # Use intrinsic blend so TD/novelty/habituation/self_benefit all contribute
        total_reward = _calculate_stabilized_reward(
            td_error=self.td_error,
            novelty=self.novelty,
            habituation=habituation_mean,
            self_benefit=self.self_benefit,
            external_reward=None,
        )
        modulation_factor = _calculate_modulation_factor(total_reward)

        # Novelty dynamics: trigger on modulation or topology change spikes
        trigger = (modulation_factor > 0.5) or (abs(ddens) > 1e-3) or (abs(self.td_error) > 0.05)
        # IDF rarity scale ∈ [0.5, 2.0] modulates novelty toward rare, content-bearing tokens
        scale = float(max(0.5, min(2.0, 1.0 if novelty_idf_scale is None else novelty_idf_scale)))
        if trigger and (time_step - self.last_reward_time) > 50:
            # proportional to spike, capped and with partial retention; scaled by rarity
            self.novelty = float(min(0.95, max(self.novelty * 0.5, scale * (0.3 + 3.0 * abs(intrinsic_td)))))
            self.last_reward_time = int(time_step)
        else:
            self.novelty *= 0.995

        # Legacy [0,1] valence magnitude for VGSP gating as needed
        valence_01 = max(0.0, abs((modulation_factor + self.novelty) / 2.0))

        packet = {
            "total_reward": float(np.clip(total_reward, -1.0, 1.0)),
            "modulation_factor": float(np.clip(modulation_factor, -1.0, 1.0)),
            "valence_01": float(np.clip(valence_01, 0.0, 1.0)),
            "components": {
                "td_error": float(self.td_error),
                "novelty": float(self.novelty),
                "habituation_mean": float(habituation_mean),
                "self_benefit": float(self.self_benefit),
                "hsi_norm": float(hsi_norm),
                "density": float(density),
                "novelty_scale": float(scale),
            }
        }
        self.last_drive = packet
        return packet

    def resize_buffers(self, new_num_neurons: int) -> None:
        """
        Resizes the internal buffers to accommodate a new number of neurons after growth.
        """
        old_num_neurons = int(self.num_neurons)
        if int(new_num_neurons) <= old_num_neurons:
            return

        # Calculate the number of neurons added
        num_added = int(new_num_neurons) - old_num_neurons

        # Create zero arrays for the new neurons
        zeros_to_add = np.zeros(num_added, dtype=np.float32)

        # Add the new zero elements to the end of the existing buffers
        self.cret_buffer = np.concatenate([self.cret_buffer, zeros_to_add])
        self.td_value_function = np.concatenate([self.td_value_function, zeros_to_add])
        # Keep dtype stable for habituation buffer
        self.habituation = np.concatenate(
            [self.habituation, np.zeros(num_added, dtype=self.habituation.dtype if hasattr(self.habituation, "dtype") else np.float32)]
        )

        # Update the neuron count
        self.num_neurons = int(new_num_neurons)
        try:
            print(f"--- SIE buffers resized to accommodate {self.num_neurons} neurons. ---")
        except Exception:
            pass]]></content>
    </file>
    <file>
      <path>core/fum_structural_homeostasis.py</path>
      <content><![CDATA[# fum_structural_homeostasis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

# Rule reference (Blueprint)
# - Rule 4 / 4.1: EHTP + Global Directed Synaptic Plasticity (GDSP)
#   Stage‑1 cohesion repair and light, adaptive pruning. Subquadratic; act
#   only on loci/components provided by the runtime, not full graph scans.
#
# Time complexity:
# - O(N + M) to get component labels upstream (Connectome already computes count).
# - Bridging: O(B * k) where B is number of bridges (small), k is per‑node neighbor cap.
# - Pruning: O(M) over active edges via masked thresholding (vectorized).
#
# Formulae:
# - S_ij = ReLU(Δalpha_i) * ReLU(Δalpha_j) - λ * |Δomega_i - Δomega_j|
# - Prune if |E_ij| < prune_threshold (adaptive fraction of |E| mean)
#
# Parameters:
# - bundle_size: number of parallel edges to reinforce a bridge (encourages fusion)
# - lambda_omega: penalty weight in S_ij
# - prune_factor: fraction of mean(|E|) below which edges are removed


def _compute_affinity(a: np.ndarray, w: np.ndarray, lambda_omega: float) -> np.ndarray:
    """Compute void‑affinity matrix S_ij from Δalpha (a) and Δomega (w)."""
    a_relu = np.maximum(0.0, a.astype(np.float32))
    w = w.astype(np.float32)
    # S_ij = relu(a_i) * relu(a_j) - λ |Δω_i - Δω_j|
    S = a_relu[:, None] * a_relu[None, :] - lambda_omega * np.abs(w[:, None] - w[None, :])
    np.fill_diagonal(S, -np.inf)
    return S


def _select_bridge_pairs(labels: np.ndarray,
                         S: np.ndarray,
                         degrees: np.ndarray,
                         bundle_size: int = 3):
    """
    Choose bridge node pairs (u,v) across different components by maximizing S_ij.
    Tie‑break to prefer strong→weak: high degree u, low degree v.
    Returns a list of (u, v) pairs of length up to bundle_size per component pair.
    """
    pairs = []
    unique = np.unique(labels)
    if unique.size < 2:
        return pairs

    # Generate candidate component pairs (greedy: connect largest to others)
    # Rank components by size descending.
    comp_sizes = {c: int((labels == c).sum()) for c in unique}
    order = sorted(unique, key=lambda c: comp_sizes[c], reverse=True)
    root = order[0]
    others = order[1:]

    for tgt in others:
        idx_root = np.where(labels == root)[0]
        idx_tgt = np.where(labels == tgt)[0]
        if idx_root.size == 0 or idx_tgt.size == 0:
            continue

        # Submatrix of S over the boundary (root x tgt)
        S_block = S[np.ix_(idx_root, idx_tgt)]
        if np.all(~np.isfinite(S_block)):
            continue

        # For bundle, iteratively pick max, then suppress chosen rows/cols lightly
        local_pairs = []
        S_copy = S_block.copy()
        for _ in range(bundle_size):
            i_flat = np.nanargmax(S_copy)  # works since -inf stays < any finite
            r, c = divmod(i_flat, S_copy.shape[1])
            u = idx_root[r]
            v = idx_tgt[c]
            local_pairs.append((u, v))
            # soft suppression to diversify selection
            S_copy[r, :] = -np.inf
            S_copy[:, c] = -np.inf
            if not np.isfinite(S_copy).any():
                break

        # Apply strong→weak preference reordering (optional)
        local_pairs.sort(key=lambda p: (-degrees[p[0]], degrees[p[1]]))
        pairs.extend(local_pairs)

    return pairs


def perform_structural_homeostasis(connectome,
                                   labels: np.ndarray,
                                   d_alpha: np.ndarray,
                                   d_omega: np.ndarray,
                                   lambda_omega: float = 0.1,
                                   bundle_size: int = 3,
                                   prune_factor: float = 0.10):
    """
    Perform cohesion healing (bridging) and light pruning on the runtime connectome.

    Args:
        connectome: fum_rt.core.connectome.Connectome instance (current runtime).
        labels: np.ndarray of component labels per node for the ACTIVE subgraph.
        d_alpha: np.ndarray Δalpha (delta_re_vgsp) for current tick.
        d_omega: np.ndarray Δomega (delta_gdsp) for current tick.
        lambda_omega: float; S_ij penalty weight.
        bundle_size: int; number of parallel edges to reinforce each bridge.
        prune_factor: float; fraction of mean(|E|) used as adaptive pruning threshold.

    Effects:
        - Modifies connectome.A (adjacency) by adding symmetric bridge edges between
          components using S_ij max rule.
        - Updates connectome.E to follow nodes after topology change.
        - Prunes edges whose |E_ij| < prune_threshold (adaptive).
    """
    N = connectome.N
    if N <= 1:
        return

    # 1) Pruning (adaptive to current edge weights)
    E = connectome.E  # float32 N x N but sparse via threshold
    if E.size > 0:
        mean_w = float(np.mean(np.abs(E[E != 0]))) if np.any(E != 0) else 0.0
        prune_threshold = prune_factor * mean_w if mean_w > 0 else 0.0
        if prune_threshold > 0.0:
            mask_keep = np.abs(E) >= prune_threshold
            # keep symmetry shape
            connectome.A = np.where(mask_keep, connectome.A, 0).astype(np.int8)
            connectome.E = np.where(mask_keep, E, 0.0).astype(np.float32)
            # Expose pruning stats for diagnostics (undirected edges)
            try:
                pruned_count = int(np.count_nonzero((~mask_keep) & (E != 0)) // 2)
                setattr(connectome, "_last_pruned_count", int(pruned_count))
            except Exception:
                pass

    # 2) Bridging if multiple components
    unique = np.unique(labels) if labels is not None else np.array([0], dtype=int)
    if unique.size > 1:
        # Compute S_ij over current nodes
        S = _compute_affinity(d_alpha, d_omega, lambda_omega=lambda_omega)

        # degrees for strong→weak preference
        degrees = connectome.A.sum(axis=1).astype(np.int32)

        bridge_pairs = _select_bridge_pairs(labels, S, degrees, bundle_size=bundle_size)

        # Add symmetric edges for selected pairs
        for u, v in bridge_pairs:
            if u == v:
                continue
            connectome.A[u, v] = 1
            connectome.A[v, u] = 1

        # Edge weights follow nodes (reuse existing vectorized function)
        connectome.E = (np.outer(connectome.W, connectome.W) * connectome.A).astype(np.float32)
        # Expose bridging stats for diagnostics
        try:
            setattr(connectome, "_last_bridged_count", int(len(bridge_pairs)))
        except Exception:
            pass]]></content>
    </file>
    <file>
      <path>core/global_system.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.


Global System Components: Active Domain Cartography (ADC) and Self‑Improvement Engine (SIE)
Blueprint References:
- Rule 1: Core Architectural Principle (Parallel Local & Global Systems)
- Rule 3: The Self-Improvement Engine (SIE) and Its Components
- Rule 4.1: Pathology Detection Mechanisms (connectome_entropy input)
- Rule 7: Active Domain Cartography (ADC) with adaptive scheduling
Time Complexity:
- ADC (1D k-means over W): O(N * k * iters) with small k-range and few iterations (subquadratic)
- SIE (per tick updates): O(N + k_states) dominated by simple reductions (subquadratic)
Formulas: documented inline per method docstrings
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict
import numpy as np

# -------------------------------
# Active Domain Cartography (ADC)
# -------------------------------

@dataclass
class ADC:
    """
    Active Domain Cartography (Rule 7)
    - Maps neurons to territories (State S) using a bespoke, efficient process.
    - Scheduling: t_cartography = schedule_base * exp(-alpha * connectome_entropy)
    - Optimization: narrow k search [k_min, max_k]; 1D k-means on node field W
    - Complexity: O(N * k * iters) per trial k; k-range is small; iters small (default 5)
    Parameters:
        k_min: minimum number of territories to consider (>=2)
        max_k: maximum number of territories to consider
        alpha: decay constant for adaptive cadence
        schedule_base: base interval for cartography
        iters: k-means iterations (small constant)
        performance_threshold: cohesion score threshold for reactive adaptation
    """
    k_min: int = 2
    max_k: int = 16
    alpha: float = 0.30
    schedule_base: int = 100_000
    iters: int = 5
    performance_threshold: float = 1e-2
    next_t: int = 0
    last_k: int = 0

    def should_run(self, step: int, connectome_entropy: float) -> bool:
        """Blueprint Rule 7: scheduling. Returns True if step reached next_t."""
        if step >= self.next_t:
            # t_cartography = schedule_base * exp(-alpha * entropy)
            interval = int(max(1, round(self.schedule_base * np.exp(-self.alpha * float(connectome_entropy)))))
            self.next_t = step + interval
            return True
        return False

    @staticmethod
    def _kmeans_1d(x: np.ndarray, k: int, iters: int, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:
        """
        1D k-means over x (shape (N,)), returns (centroids, labels)
        Complexity: O(N * k * iters)
        """
        N = x.size
        # Init centroids from quantiles (stable)
        qs = np.linspace(0.0, 1.0, num=k+2, endpoint=True)[1:-1]
        c = np.quantile(x, qs) if k > 1 else np.array([float(np.median(x))], dtype=np.float32)
        c = np.asarray(c, dtype=np.float32)
        labels = np.zeros(N, dtype=np.int32)
        for _ in range(max(1, iters)):
            # Assign
            dist = np.abs(x[:, None] - c[None, :])  # (N,k)
            labels = np.argmin(dist, axis=1).astype(np.int32)
            # Update
            for j in range(k):
                sel = (labels == j)
                if np.any(sel):
                    c[j] = float(np.mean(x[sel]))
        return c, labels

    @staticmethod
    def _cohesion_score(x: np.ndarray, labels: np.ndarray, k: int) -> float:
        """
        Blueprint Rule 7 ('The How'): overall cohesion = mean over territories of inverse intra-variance.
        We use a numerically stable version: mean(1 / (var + eps)).
        """
        eps = 1e-8
        score = 0.0
        for j in range(k):
            sel = (labels == j)
            if not np.any(sel):
                continue
            v = float(np.var(x[sel]))
            score += 1.0 / (v + eps)
        return score / float(max(1, k))

    def run(self, W: np.ndarray, rng: np.random.Generator) -> Tuple[np.ndarray, int, float]:
        """
        Perform cartography over 1D feature W (node field).
        Returns: (territory_ids, k_opt, cohesion_score)
        - Trials k in [k_min, max_k]; pick best by cohesion score (higher is better).
        - Reactive adaptation (Rule 7): if best cohesion below threshold, try k+1 once.
        """
        N = W.size
        k_min = max(2, int(self.k_min))
        k_max = max(k_min, int(self.max_k))
        best = (-np.inf, None, None)  # (score, labels, k)

        for k in range(k_min, min(k_max, N) + 1):
            _, lbl = self._kmeans_1d(W, k, self.iters, rng)
            s = self._cohesion_score(W, lbl, k)
            if s > best[0]:
                best = (s, lbl, k)

        score, labels, k_opt = best
        # Reactive adaptation (bifurcation)
        if score < self.performance_threshold and (k_opt + 1) <= min(k_max, N):
            _, lbl = self._kmeans_1d(W, k_opt + 1, self.iters, rng)
            s2 = self._cohesion_score(W, lbl, k_opt + 1)
            if s2 > score:
                score, labels, k_opt = s2, lbl, k_opt + 1

        self.last_k = int(k_opt)
        return labels.astype(np.int32), self.last_k, float(score)


# -----------------------------------
# Self‑Improvement Engine (SIE) Rule 3
# -----------------------------------

@dataclass
class SIE:
    """
    Self‑Improvement Engine (Rule 3)
    total_reward = w_td * TD_error_norm + w_nov * novelty_norm - w_hab * habituation_norm + w_hsi * hsi_norm
    State:
        V_states: value function per territory id (dense vector sized on demand)
        visit_counts: visitation counts per territory (for novelty/habituation)
    Complexity:
        Per tick: O(N) to aggregate territory stats (+ O(k_states) bookkeeping)
    """
    w_td: float = 0.35
    w_nov: float = 0.25
    w_hab: float = 0.15
    w_hsi: float = 0.25
    alpha: float = 0.10  # value function learning rate
    gamma: float = 0.95  # discount for TD
    target_var: float = 0.05  # target firing variance for HSI

    V_states: Dict[int, float] = field(default_factory=dict)
    visit_counts: Dict[int, int] = field(default_factory=dict)
    last_state: Optional[int] = None
    last_value: float = 0.0

    def _ensure_state(self, s: int):
        if s not in self.V_states:
            self.V_states[s] = 0.0
        if s not in self.visit_counts:
            self.visit_counts[s] = 0

    @staticmethod
    def _normalize(z: float) -> float:
        # Map arbitrary scalar to [-1, 1] with tanh
        return float(np.tanh(z))

    def _hsi_norm(self, W: np.ndarray) -> float:
        """
        Homeostatic Stability Index (proxy): 1 - |var(W) - target| / (target + eps), clipped to [-1,1]
        Cheap O(N) measure aligned with Rule 3 inputs.
        """
        eps = 1e-8
        v = float(np.var(W))
        diff = abs(v - self.target_var) / (self.target_var + eps)
        return float(np.clip(1.0 - diff, -1.0, 1.0))

    def compute(self, territories: Optional[np.ndarray], W: np.ndarray, external_R: Optional[float] = None) -> Dict[str, float]:
        """
        Compute SIE components and total_reward.
        Inputs:
            territories: array of length N with territory id per neuron (from ADC). If None, uses a single implicit state 0.
            W: node field (for HSI proxy)
            external_R: optional external reward R_t
        Returns dict with components and total_reward.
        """
        if territories is None or territories.size == 0:
            # Single territory fallback
            S_t = 0
            terr_ids = np.array([0], dtype=np.int32)
        else:
            # Choose current territory by majority (cheap proxy)
            # In future wire from UTE stream tagging
            vals, counts = np.unique(territories, return_counts=True)
            S_t = int(vals[np.argmax(counts)])
            terr_ids = vals.astype(np.int32)

        # Ensure state containers
        self._ensure_state(S_t)

        # Novelty/Habituation from visitation counts
        n_vis = self.visit_counts.get(S_t, 0)
        novelty_norm = self._normalize(1.0 / np.sqrt(max(1, n_vis)))
        habituation_norm = self._normalize(n_vis / (n_vis + 10.0))  # increases with repeated visits

        # HSI from W variance
        hsi_norm = self._hsi_norm(W)

        # TD error for current state (no external reward by default)
        R_t = 0.0 if external_R is None else float(external_R)
        V_s = self.V_states.get(S_t, 0.0)
        V_next = V_s  # single-state proxy unless territories change next tick

        # If previous state differs, approximate bootstrapping using its value
        if self.last_state is not None and self.last_state != S_t:
            V_next = self.V_states.get(self.last_state, 0.0)

        td_error = R_t + self.gamma * V_next - V_s
        # Normalize TD error to [-1,1] via tanh
        TD_error_norm = self._normalize(td_error)

        # Update value function
        self.V_states[S_t] = V_s + self.alpha * td_error

        # Update visit counts
        self.visit_counts[S_t] = n_vis + 1

        # Total reward per Rule 3
        total_reward = (
            self.w_td * TD_error_norm
            + self.w_nov * novelty_norm
            - self.w_hab * habituation_norm
            + self.w_hsi * hsi_norm
        )

        # Track last
        self.last_state = S_t
        self.last_value = self.V_states[S_t]

        return {
            "S_t": float(S_t),
            "TD_error_norm": float(TD_error_norm),
            "novelty_norm": float(novelty_norm),
            "habituation_norm": float(habituation_norm),
            "hsi_norm": float(hsi_norm),
            "total_reward": float(total_reward),
        }]]></content>
    </file>
    <file>
      <path>core/guards/invariants.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.guards.invariants

Physics ↔ code guard helpers (CI/runtime-safe, no IO, no scans).

Purpose
- Provide minimal, deterministic checks that connect implementation to physics notes.
- Intended for CI tests and optional runtime warnings (callers decide policy).

Included
- check_site_constant_of_motion: sample-based drift check of a simple constant-of-motion proxy Q_FUM.
- compute_memory_groups: expose dimensionless memory steering groups from MemoryField.

Notes
- Q_FUM here uses a conservative, implementation-agnostic proxy over the on-site state vector W:
    Q_i = 0.5 * W_i^2 + α * W_i + β
  The exact analytical form depends on the chosen on-site law; callers can tune (alpha, beta) and tolerances.
- Sampling is caller-controlled and bounded; no global scans required.

Void-faithful
- Pure numeric helpers; no external imports beyond typing/math/statistics.
- O(#samples) time; callers pass bounded samples (e.g., 256-2048 indices).
"""

from typing import Dict, Iterable, Optional, Sequence, Tuple
import math


def _percentile(xs: Sequence[float], p: float) -> float:
    if not xs:
        return 0.0
    if p <= 0.0:
        return float(min(xs))
    if p >= 1.0:
        return float(max(xs))
    xs_sorted = sorted(float(v) for v in xs)
    i = int(math.floor(p * (len(xs_sorted) - 1)))
    return float(xs_sorted[max(0, min(len(xs_sorted) - 1, i))])


def check_site_constant_of_motion(
    W_prev: Sequence[float],
    W_curr: Sequence[float],
    *,
    alpha: float = 0.0,
    beta: float = 0.0,
    dt: float = 1.0,
    samples: Optional[Iterable[int]] = None,
    tol_abs: float = 1e-6,
    tol_p99: float = 1e-5,
) -> Dict[str, float | int | bool]:
    """
    Sample-based constant-of-motion proxy drift check.

    Definitions (per-site):
      Q_prev = 0.5 * W_prev^2 + alpha * W_prev + beta
      Q_curr = 0.5 * W_curr^2 + alpha * W_curr + beta
      dQ = (Q_curr - Q_prev)

    Returns dict with:
      {
        "count": int,          # number of sampled sites evaluated
        "dQ_mean": float,
        "dQ_p95": float,
        "dQ_p99": float,
        "dQ_max": float,
        "pass_abs": bool,      # max |dQ| <= tol_abs
        "pass_p99": bool,      # p99 |dQ| <= tol_p99
      }

    Notes
    - This is a conservative drift check. Tighten tolerances as your on-site ODE is finalized.
    - dt is accepted for API symmetry; current proxy is discrete in time and uses raw differences.
    """
    try:
        n_prev = len(W_prev)
        n_curr = len(W_curr)
    except Exception:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}
    if n_prev <= 0 or n_prev != n_curr:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    idxs: Iterable[int]
    if samples is None:
        # bounded default: first min(1024, N)
        k = min(1024, n_prev)
        idxs = range(k)
    else:
        idxs = samples

    dqs_abs: list[float] = []
    s = 0.0
    c = 0
    for i in idxs:
        try:
            ii = int(i)
            if ii < 0 or ii >= n_prev:
                continue
            wp = float(W_prev[ii])
            wc = float(W_curr[ii])
            q_prev = 0.5 * wp * wp + alpha * wp + beta
            q_curr = 0.5 * wc * wc + alpha * wc + beta
            dq = q_curr - q_prev
            dqs_abs.append(abs(float(dq)))
            s += float(dq)
            c += 1
        except Exception:
            continue

    if c <= 0:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    dQ_mean = float(s) / float(c)
    dQ_p95 = _percentile(dqs_abs, 0.95)
    dQ_p99 = _percentile(dqs_abs, 0.99)
    dQ_max = float(max(dqs_abs)) if dqs_abs else 0.0

    return {
        "count": int(c),
        "dQ_mean": float(dQ_mean),
        "dQ_p95": float(dQ_p95),
        "dQ_p99": float(dQ_p99),
        "dQ_max": float(dQ_max),
        "pass_abs": bool(dQ_max <= float(tol_abs)),
        "pass_p99": bool(dQ_p99 <= float(tol_p99)),
    }


def compute_memory_groups(field: object) -> Dict[str, float]:
    """
    Read dimensionless memory steering groups from MemoryField (if present).

    Expected MemoryField properties (read-only):
      - Theta   (steering coefficient placeholder; walkers use it when applicable)
      - D_a     (write gain, γ in dimensionless units)
      - Lambda  (decay, δ)
      - Gamma   (one-edge smoothing, κ)

    Returns dict: {"mem_Theta":..., "mem_Da":..., "mem_Lambda":..., "mem_Gamma":...}
    Missing properties default to 0.0.
    """
    def _get(obj: object, name: str) -> float:
        try:
            return float(getattr(obj, name, 0.0))
        except Exception:
            return 0.0

    return {
        "mem_Theta": _get(field, "Theta"),
        "mem_Da": _get(field, "D_a") if hasattr(field, "D_a") else _get(field, "Da"),
        "mem_Lambda": _get(field, "Lambda"),
        "mem_Gamma": _get(field, "Gamma"),
    }


# --- Physics invariants additions (CI helpers; pure numeric, no scans) ---

def qfum_logistic_value(w: float, t: float, *, alpha: float, beta: float, eps: float = 1e-12) -> float:
    """
    Closed-form on-site invariant for the logistic law:
        dW/dt = (α - β) W - α W^2  ≡  k W (1 - W/K)
    with k = α - β, K = (α - β)/α (assuming α > 0).

    Time-invariant quantity:
        Q = t - (1/α) ln( W / (K - W) ) - (β/α) t

    Justification:
      Using W/(K - W) = e^{k t}/A, we obtain
        Q = t - (k/α) t + (1/α) ln A - (β/α) t = (1/α) ln A (constant).
    This helper computes Q(w, t) robustly with safe clamping at boundaries.
    If α ≤ 0 or K ≤ 0, returns 0.0 (fail-soft) since the invariant is undefined.
    """
    try:
        a = float(alpha)
        b = float(beta)
        tt = float(t)
        if not (a > eps):
            return 0.0
        K = (a - b) / a
        if not (K > eps):
            return 0.0
        # clamp w to (eps, K - eps) to avoid singularities at 0 and K
        wv = float(w)
        if wv <= eps:
            wv = eps
        Km = float(K) - eps
        if wv >= Km:
            wv = Km
        base = tt - (1.0 / a) * math.log(wv / (float(K) - wv))
        return base - (b / a) * tt
    except Exception:
        return 0.0


def check_qfum_logistic(
    W_prev: Sequence[float],
    W_curr: Sequence[float],
    *,
    t_prev: float,
    t_curr: float,
    alpha: float,
    beta: float,
    samples: Optional[Iterable[int]] = None,
    tol_abs: float = 1e-6,
    tol_p99: float = 1e-5,
) -> Dict[str, float | int | bool]:
    """
    Sample-based drift check for the analytic Q_FUM constant of motion under the logistic on-site law.

    Definitions:
      Q_i(t, W) = t - (1/α) ln( W / (K - W) ), with K = (α - β)/α, α>0.
      ΔQ_i = Q_i(t_curr, W_curr[i]) - Q_i(t_prev, W_prev[i])

    Returns dict analogous to check_site_constant_of_motion with p95/p99/max of |ΔQ|.
    """
    try:
        n_prev = len(W_prev)
        n_curr = len(W_curr)
    except Exception:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}
    if n_prev <= 0 or n_prev != n_curr:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    idxs: Iterable[int]
    if samples is None:
        k = min(1024, n_prev)
        idxs = range(k)
    else:
        idxs = samples

    dqs_abs: list[float] = []
    s = 0.0
    c = 0
    for i in idxs:
        try:
            ii = int(i)
            if ii < 0 or ii >= n_prev:
                continue
            q0 = qfum_logistic_value(float(W_prev[ii]), float(t_prev), alpha=alpha, beta=beta)
            q1 = qfum_logistic_value(float(W_curr[ii]), float(t_curr), alpha=alpha, beta=beta)
            dq = float(q1 - q0)
            dqs_abs.append(abs(dq))
            s += dq
            c += 1
        except Exception:
            continue

    if c <= 0:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    dQ_mean = float(s) / float(c)
    dQ_p95 = _percentile(dqs_abs, 0.95)
    dQ_p99 = _percentile(dqs_abs, 0.99)
    dQ_max = float(max(dqs_abs)) if dqs_abs else 0.0

    return {
        "count": int(c),
        "dQ_mean": float(dQ_mean),
        "dQ_p95": float(dQ_p95),
        "dQ_p99": float(dQ_p99),
        "dQ_max": float(dQ_max),
        "pass_abs": bool(dQ_max <= float(tol_abs)),
        "pass_p99": bool(dQ_p99 <= float(tol_p99)),
    }


def kinetic_c2_from_kappa(kappa: float, a: float) -> float:
    """
    Kinetic normalization: c^2 = κ a^2, where κ is the spatial coupling in L_K.
    """
    try:
        return float(kappa) * float(a) * float(a)
    except Exception:
        return 0.0


def kinetic_c2_from_J(J: float, a: float) -> float:
    """
    Site-coupling convention: c^2 = 2 J a^2. Useful equivalence check against κ=2J.
    """
    try:
        return 2.0 * float(J) * float(a) * float(a)
    except Exception:
        return 0.0


__all__ = [
    "check_site_constant_of_motion",
    "compute_memory_groups",
    "qfum_logistic_value",
    "check_qfum_logistic",
    "kinetic_c2_from_kappa",
    "kinetic_c2_from_J",
]]]></content>
    </file>
    <file>
      <path>core/memory/__init__.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.core.memory package

Exports:
- MemoryField: event-driven memory field owner (from .field)
- load_engram, save_checkpoint: engram IO (from .engram_io)

This resolves the prior module/package name conflict by making
fum_rt.core.memory a proper package namespace with explicit re-exports.
"""

from .field import MemoryField
from .engram_io import load_engram, save_checkpoint

__all__ = ["MemoryField", "load_engram", "save_checkpoint"]]]></content>
    </file>
    <file>
      <path>core/memory/engram_io.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os
import json
import numpy as np
from typing import List, Tuple

# Optional HDF5 backend (preferred)
try:
    import h5py  # type: ignore
    HAVE_H5 = True
except Exception:
    HAVE_H5 = False

# Import ADC dataclasses for (de)serialization
try:
    from .adc import Territory as ADCTerritory, Boundary as ADCBoundary, _EWMA as _ADC_EWMA  # type: ignore
except Exception:
    ADCTerritory = None  # type: ignore
    ADCBoundary = None  # type: ignore
    _ADC_EWMA = None  # type: ignore


def _adj_to_csr(adj: List[np.ndarray], N: int) -> Tuple[np.ndarray, np.ndarray]:
    """Convert neighbor-lists (sparse adjacency) to CSR arrays: row_ptr, col_idx."""
    row_ptr = np.zeros(N + 1, dtype=np.int64)
    total = 0
    for i in range(N):
        deg = int(adj[i].size)
        row_ptr[i] = total
        total += deg
    row_ptr[N] = total
    col_idx = np.zeros(total, dtype=np.int32)
    pos = 0
    for i in range(N):
        nbrs = adj[i]
        if nbrs.size:
            k = nbrs.size
            col_idx[pos : pos + k] = nbrs.astype(np.int32, copy=False)
            pos += k
    return row_ptr, col_idx


def _csr_to_adj(row_ptr: np.ndarray, col_idx: np.ndarray, N: int) -> List[np.ndarray]:
    """Convert CSR arrays to neighbor-lists (sparse adjacency)."""
    adj = []
    for i in range(N):
        start = int(row_ptr[i])
        end = int(row_ptr[i + 1])
        if end > start:
            adj.append(col_idx[start:end].astype(np.int32, copy=False))
        else:
            adj.append(np.zeros(0, dtype=np.int32))
    return adj


# -----------------------
# ADC (de)serialization
# -----------------------
def _ewma_to_dict(w) -> dict:
    try:
        return {
            "alpha": float(getattr(w, "alpha", 0.15)),
            "mean": float(getattr(w, "mean", 0.0)),
            "var": float(getattr(w, "var", 0.0)),
            "init": bool(getattr(w, "init", False)),
        }
    except Exception:
        return {"alpha": 0.15, "mean": 0.0, "var": 0.0, "init": False}


def _ewma_from_dict(d):
    try:
        a = float(d.get("alpha", 0.15))
        m = float(d.get("mean", 0.0))
        v = float(d.get("var", 0.0))
        ini = bool(d.get("init", False))
        if _ADC_EWMA is not None:
            return _ADC_EWMA(alpha=a, mean=m, var=v, init=ini)  # type: ignore
    except Exception:
        pass
    if _ADC_EWMA is not None:
        return _ADC_EWMA(alpha=0.15)  # type: ignore
    return None


def _adc_to_dict(adc) -> dict:
    """Serialize ADC internals into a JSON-friendly dict."""
    try:
        terr = []
        for key, t in getattr(adc, "_territories", {}).items():
            try:
                dom, cov = key
            except Exception:
                dom, cov = "", 0
            terr.append({
                "key": [str(dom), int(cov)],
                "id": int(getattr(t, "id", 0)),
                "mass": float(getattr(t, "mass", 0.0)),
                "conf": float(getattr(t, "conf", 0.0)),
                "ttl": int(getattr(t, "ttl", 0)),
                "w_stats": _ewma_to_dict(getattr(t, "w_stats", None)),
                "s_stats": _ewma_to_dict(getattr(t, "s_stats", None)),
            })
        bounds = []
        for key, b in getattr(adc, "_boundaries", {}).items():
            try:
                a, c = key
            except Exception:
                a, c = 0, 0
            bounds.append({
                "a": int(a),
                "b": int(c),
                "ttl": int(getattr(b, "ttl", 0)),
                "cut_stats": _ewma_to_dict(getattr(b, "cut_stats", None)),
                "churn": _ewma_to_dict(getattr(b, "churn", None)),
            })
        fcnt = []
        for key, cnt in getattr(adc, "_frontier_counter", {}).items():
            try:
                dom, cov = key
            except Exception:
                dom, cov = "", 0
            fcnt.append({
                "key": [str(dom), int(cov)],
                "count": int(cnt),
            })
        return {
            "id_seq": int(getattr(adc, "_id_seq", 1)),
            "territories": terr,
            "boundaries": bounds,
            "frontier_counter": fcnt,
        }
    except Exception:
        return {}


def _adc_load_from_dict(adc, state: dict) -> None:
    """Populate ADC internals from a previously serialized dict."""
    if adc is None or not isinstance(state, dict):
        return
    try:
        terr_d = {}
        max_id = 0
        for t in state.get("territories", []):
            try:
                dom, cov = t.get("key", ["", 0])
                tid = int(t.get("id", 0))
                max_id = max(max_id, tid)
                wj = t.get("w_stats", {})
                sj = t.get("s_stats", {})
                w_stats = _ewma_from_dict(wj)
                s_stats = _ewma_from_dict(sj)
                if ADCTerritory is not None:
                    terr = ADCTerritory(
                        key=(str(dom), int(cov)),
                        id=tid,
                        mass=float(t.get("mass", 0.0)),
                        conf=float(t.get("conf", 0.0)),
                        ttl=int(t.get("ttl", 0)),
                        w_stats=w_stats if w_stats is not None else (_ADC_EWMA(alpha=0.15) if _ADC_EWMA else None),  # type: ignore
                        s_stats=s_stats if s_stats is not None else (_ADC_EWMA(alpha=0.15) if _ADC_EWMA else None),  # type: ignore
                    )
                    terr_d[(str(dom), int(cov))] = terr
            except Exception:
                continue
        bnd_d = {}
        for b in state.get("boundaries", []):
            try:
                a = int(b.get("a", 0))
                c = int(b.get("b", 0))
                cut = _ewma_from_dict(b.get("cut_stats", {}))
                chrn = _ewma_from_dict(b.get("churn", {}))
                if ADCBoundary is not None:
                    bnd = ADCBoundary(
                        a=min(a, c),
                        b=max(a, c),
                        cut_stats=cut if cut is not None else (_ADC_EWMA(alpha=0.2) if _ADC_EWMA else None),  # type: ignore
                        churn=chrn if chrn is not None else (_ADC_EWMA(alpha=0.2) if _ADC_EWMA else None),  # type: ignore
                        ttl=int(b.get("ttl", 0)),
                    )
                    bnd_d[(min(a, c), max(a, c))] = bnd
            except Exception:
                continue
        fcnt = {}
        for fc in state.get("frontier_counter", []):
            try:
                dom, cov = fc.get("key", ["", 0])
                fcnt[(str(dom), int(cov))] = int(fc.get("count", 0))
            except Exception:
                continue
        setattr(adc, "_territories", terr_d)
        setattr(adc, "_boundaries", bnd_d)
        setattr(adc, "_frontier_counter", fcnt)
        try:
            id_seq = int(state.get("id_seq", max_id + 1))
        except Exception:
            id_seq = max_id + 1
        setattr(adc, "_id_seq", max(1, id_seq))
    except Exception:
        return


def save_checkpoint(run_dir: str, step: int, connectome, fmt: str = "h5", adc=None) -> str:
    """
    Save runtime state (engram) for dense or sparse backends.

    Args:
        run_dir: run directory
        step: tick index
        connectome: Connectome or SparseConnectome
        fmt: "h5" (preferred) or "npz" (compat)
        adc: Optional ADC instance to persist alongside the connectome
    """
    os.makedirs(run_dir, exist_ok=True)
    backend = "sparse" if hasattr(connectome, "adj") else "dense"

    if fmt.lower() == "h5":
        if not HAVE_H5:
            # Fallback transparently to npz if h5py isn't available
            fmt = "npz"
        else:
            path = os.path.join(run_dir, f"state_{step}.h5")
            _save_h5(path, connectome, backend, adc)
            return path

    # default/fallback npz
    path = os.path.join(run_dir, f"state_{step}.npz")
    _save_npz(path, connectome, backend, adc)
    return path


def _save_h5(path: str, connectome, backend: str, adc=None):
    with h5py.File(path, "w") as f:
        # Metadata as attributes
        f.attrs["backend"] = backend
        f.attrs["N"] = int(connectome.N)
        f.attrs["k"] = int(getattr(connectome, "k", 0))
        f.attrs["threshold"] = float(getattr(connectome, "threshold", 0.0))
        f.attrs["lambda_omega"] = float(getattr(connectome, "lambda_omega", 0.0))
        f.attrs["dtype"] = "float32"

        if backend == "dense":
            g = f.create_group("dense")
            g.create_dataset("W", data=connectome.W.astype(np.float32, copy=False), compression="gzip")
            g.create_dataset("A", data=connectome.A.astype(np.int8, copy=False), compression="gzip")
            g.create_dataset("E", data=connectome.E.astype(np.float32, copy=False), compression="gzip")
        else:
            # Sparse: store neighbor lists as CSR
            row_ptr, col_idx = _adj_to_csr(connectome.adj, int(connectome.N))
            g = f.create_group("sparse")
            g.create_dataset("W", data=connectome.W.astype(np.float32, copy=False), compression="gzip")
            g.create_dataset("row_ptr", data=row_ptr, compression="gzip")
            g.create_dataset("col_idx", data=col_idx, compression="gzip")

        # Optional: persist ADC in a single JSON dataset for portability
        if adc is not None:
            try:
                state_json = json.dumps(_adc_to_dict(adc))
                f.create_dataset("adc_json", data=state_json, dtype=h5py.string_dtype(encoding="utf-8"))
            except Exception:
                pass


def _save_npz(path: str, connectome, backend: str, adc=None):
    adc_json = None
    if adc is not None:
        try:
            adc_json = json.dumps(_adc_to_dict(adc))
        except Exception:
            adc_json = None

    if backend == "dense":
        if adc_json is None:
            np.savez_compressed(
                path,
                backend="dense",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                A=connectome.A.astype(np.int8, copy=False),
                E=connectome.E.astype(np.float32, copy=False),
            )
        else:
            np.savez_compressed(
                path,
                backend="dense",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                A=connectome.A.astype(np.int8, copy=False),
                E=connectome.E.astype(np.float32, copy=False),
                adc_json=adc_json,
            )
    else:
        row_ptr, col_idx = _adj_to_csr(connectome.adj, int(connectome.N))
        if adc_json is None:
            np.savez_compressed(
                path,
                backend="sparse",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                row_ptr=row_ptr,
                col_idx=col_idx,
            )
        else:
            np.savez_compressed(
                path,
                backend="sparse",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                row_ptr=row_ptr,
                col_idx=col_idx,
                adc_json=adc_json,
            )


def load_engram(path: str, connectome, adc=None) -> None:
    """
    Load an engram from .h5 or .npz and populate the provided connectome instance.
    If ADC state is present and 'adc' is provided, populate it as well.

    - Dense: sets W, A, E, threshold
    - Sparse: sets W, adj (neighbor lists), threshold
    - ADC (optional): territories, boundaries, counters
    """
    p = str(path)
    if p.lower().endswith(".h5"):
        if not HAVE_H5:
            raise RuntimeError("h5py not installed but .h5 requested")
        _load_h5(p, connectome, adc)
        return
    # npz fallback
    _load_npz(p, connectome, adc)


def _apply_common_attrs(meta: dict, connectome):
    # Resize N if needed (safe for our numpy arrays here)
    N = int(meta.get("N", connectome.N))
    connectome.N = N
    # threshold, lambda_omega if present
    if "threshold" in meta:
        connectome.threshold = float(meta["threshold"])
    if "lambda_omega" in meta:
        connectome.lambda_omega = float(meta["lambda_omega"])


def _load_h5(path: str, connectome, adc=None):
    with h5py.File(path, "r") as f:
        backend = f.attrs.get("backend", "dense")
        meta = {
            "N": int(f.attrs.get("N", connectome.N)),
            "threshold": float(f.attrs.get("threshold", getattr(connectome, "threshold", 0.0))),
            "lambda_omega": float(f.attrs.get("lambda_omega", getattr(connectome, "lambda_omega", 0.0))),
        }
        _apply_common_attrs(meta, connectome)

        if backend == "dense":
            g = f["dense"]
            connectome.W = g["W"][...].astype(np.float32, copy=False)
            connectome.A = g["A"][...].astype(np.int8, copy=False)
            connectome.E = g["E"][...].astype(np.float32, copy=False)
        else:
            g = f["sparse"]
            connectome.W = g["W"][...].astype(np.float32, copy=False)
            row_ptr = g["row_ptr"][...]
            col_idx = g["col_idx"][...]
            connectome.adj = _csr_to_adj(row_ptr, col_idx, int(connectome.N))

        # Load ADC if present
        if adc is not None:
            try:
                ds = f.get("adc_json", None)
                if ds is not None:
                    raw = ds[()]
                    if isinstance(raw, bytes):
                        raw = raw.decode("utf-8", errors="ignore")
                    state = json.loads(raw)
                    _adc_load_from_dict(adc, state)
            except Exception:
                pass


def _load_npz(path: str, connectome, adc=None):
    data = np.load(path, allow_pickle=False)
    backend = str(data.get("backend", "dense"))
    meta = {
        "N": int(data.get("N", connectome.N)),
        "threshold": float(data.get("threshold", getattr(connectome, "threshold", 0.0))),
        "lambda_omega": float(data.get("lambda_omega", getattr(connectome, "lambda_omega", 0.0))),
    }
    _apply_common_attrs(meta, connectome)
    if backend == "dense":
        connectome.W = data["W"].astype(np.float32, copy=False)
        connectome.A = data["A"].astype(np.int8, copy=False)
        connectome.E = data["E"].astype(np.float32, copy=False)
    else:
        connectome.W = data["W"].astype(np.float32, copy=False)
        row_ptr = data["row_ptr"]
        col_idx = data["col_idx"]
        connectome.adj = _csr_to_adj(row_ptr, col_idx, int(connectome.N))

    # Load ADC if present
    if adc is not None:
        try:
            if hasattr(data, "files") and "adc_json" in data.files:
                raw = data["adc_json"]
                # raw could be 0-d array of str/bytes
                if isinstance(raw, np.ndarray):
                    if raw.dtype.kind in ("U", "S") and raw.shape == ():
                        raw_val = raw.item()
                    else:
                        raw_val = raw.tolist()
                        if isinstance(raw_val, list) and raw_val:
                            raw_val = raw_val[0]
                else:
                    raw_val = raw
                if isinstance(raw_val, bytes):
                    raw_val = raw_val.decode("utf-8", errors="ignore")
                if isinstance(raw_val, (str,)):
                    state = json.loads(raw_val)
                    _adc_load_from_dict(adc, state)
        except Exception:
            pass
]]></content>
    </file>
    <file>
      <path>core/memory/field.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.memory.field
Purpose: Event-driven memory field with write-decay-spread dynamics (void-faithful).

Design constraints
- Sparse-first: no dense global passes; updates are event-local only.
- No schedulers: called from the per-tick CoreEngine fold path.
- No scans in core/ or maps/: bounded working set; pruning uses sampling.
- Maps/frame v1/v2 unchanged (telemetry-only); this reducer is for local steering.
- Guards pass; control-impact is minimal since this only folds small event batches.

Dynamics (per tick, event-driven)
- On node touch (vt_touch at node i):
    m[i] ← m[i] * exp(-δ·Δt) + γ · r_i · Δt
  where r_i is a small stimulus inferred from event weight (default 1.0).

- On edge_on(i, j) smoothing (one-edge local spread):
    δm = κ · (m[j] - m[i]) · Δt
    m[i] += δm
    m[j] -= δm

- Optional burst footprints:
    SpikeEvent(node=j, amp) → m[j] += γ_s · amp · Δt
    DeltaWEvent(node=j, dw) → m[j] += γ_w · |dw| · Δt

Snapshot
- memory_head: top-k [[node, value], ...] by current m value (k=head_k, default 256)
- memory_p95/p99/max/count: summaries of working set
- memory_dict: bounded dictionary {node: m} (size ≤ keep_max)

Tuning (dimensionless)
- gamma (γ): write gain
- delta (δ): exponential decay rate per tick
- kappa (κ): one-edge smoothing coupling
- touch_gain, spike_gain, dW_gain: per-event scalings feeding the write term
"""

from typing import Dict, Iterable, List, Tuple
import math
import random

from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class MemoryField:
    """
    Event-driven, bounded memory field.

    Parameters:
      - head_k: top-k head size for memory_head
      - keep_max: max retained working-set size (defaults to 16×head_k)
      - seed: RNG seed for pruning sampling
      - gamma: write gain (γ)
      - delta: decay rate (δ) per tick (0..1)
      - kappa: one-edge smoothing coupling (κ)
      - touch_gain/spike_gain/dW_gain: event-to-write scaling
    """

    __slots__ = (
        "head_k",
        "keep_max",
        "rng",
        "_m",
        "_last_tick",
        "gamma",
        "delta",
        "kappa",
        "touch_gain",
        "spike_gain",
        "dW_gain",
    )

    def __init__(
        self,
        head_k: int = 256,
        keep_max: int | None = None,
        seed: int = 0,
        *,
        gamma: float = 0.05,
        delta: float = 0.01,
        kappa: float = 0.10,
        touch_gain: float = 1.0,
        spike_gain: float = 0.20,
        dW_gain: float = 0.10,
    ) -> None:
        self.head_k = int(max(8, head_k))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._m: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

        # dynamics
        self.gamma = float(max(0.0, gamma))
        self.delta = float(max(0.0, min(1.0, delta)))
        self.kappa = float(max(0.0, kappa))
        self.touch_gain = float(max(0.0, touch_gain))
        self.spike_gain = float(max(0.0, spike_gain))
        self.dW_gain = float(max(0.0, dW_gain))

    # ---------------- internal helpers ----------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            # Exponential decay: m *= exp(-δ·Δt). Use (1-δ)^Δt for stability when δ small.
            try:
                base = max(0.0, 1.0 - self.delta)
                factor = base ** dt
            except Exception:
                factor = math.exp(-self.delta * float(dt))
            self._m[node] = float(self._m.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def _ensure_and_decay(self, node: int, tick: int) -> None:
        n = int(node)
        if n not in self._m:
            self._m[n] = 0.0
            self._last_tick[n] = int(tick)
        else:
            self._decay_to(n, int(tick))

    def _prune(self) -> None:
        size = len(self._m)
        target_drop = size - self.keep_max
        if target_drop <= 0:
            return
        keys = list(self._m.keys())
        sample_size = min(len(keys), max(256, target_drop * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        # Drop smallest m in the sample
        sample.sort(key=lambda k: self._m.get(k, 0.0))
        for k in sample[:target_drop]:
            self._m.pop(k, None)
            self._last_tick.pop(k, None)

    # ---------------- public API ----------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events at integer tick.
        """
        t = int(tick)
        γ = self.gamma
        δ = self.delta
        κ = self.kappa
        tg = self.touch_gain
        sg = self.spike_gain
        wg = self.dW_gain

        for e in events:
            k = getattr(e, "kind", None)

            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                try:
                    i = int(e.token)
                except Exception:
                    continue
                if i < 0:
                    continue
                # decay-then-write at node i
                self._ensure_and_decay(i, t)
                r_i = float(getattr(e, "w", 1.0))
                self._m[i] = float(self._m.get(i, 0.0)) + float(γ * tg * r_i)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                try:
                    u = int(getattr(e, "u", -1))
                    v = int(getattr(e, "v", -1))
                except Exception:
                    continue
                if u < 0 or v < 0:
                    continue
                # local smoothing on the edge (u, v)
                self._ensure_and_decay(u, t)
                self._ensure_and_decay(v, t)
                mu = float(self._m.get(u, 0.0))
                mv = float(self._m.get(v, 0.0))
                d = float(κ * (mv - mu))
                self._m[u] = mu + d
                self._m[v] = mv - d
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "spike" and isinstance(e, SpikeEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                amp = float(getattr(e, "amp", 1.0))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * sg * amp)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                dw = abs(float(getattr(e, "dw", 0.0)))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * wg * dw)
                if len(self._m) > self.keep_max:
                    self._prune()

    def snapshot(self, head_n: int = 16) -> Dict[str, object]:
        """
        Return bounded snapshot of the field.
        """
        if not self._m:
            return {
                "memory_head": [],
                "memory_p95": 0.0,
                "memory_p99": 0.0,
                "memory_max": 0.0,
                "memory_count": 0,
                "memory_dict": {},
            }

        # head top-k
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._m.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._m.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]

        vals = sorted(float(v) for v in self._m.values())

        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])

        out_dict: Dict[int, float] = {int(k): float(v) for k, v in self._m.items()}

        return {
            "memory_head": [[int(k), float(v)] for k, v in head],
            "memory_p95": q(0.95),
            "memory_p99": q(0.99),
            "memory_max": float(vals[-1]),
            "memory_count": int(len(vals)),
            "memory_dict": out_dict,
        }

    # ---------------- taps / adapters ----------------

    def get_m(self, i: int) -> float:
        """
        O(1) local read for node i.
        """
        try:
            return float(self._m.get(int(i), 0.0))
        except Exception:
            return 0.0

    def get_many(self, idxs) -> Dict[int, float]:
        """
        Return {i: m[i]} for a small iterable of indices.
        """
        out: Dict[int, float] = {}
        try:
            for j in idxs or []:
                try:
                    out[int(j)] = float(self._m.get(int(j), 0.0))
                except Exception:
                    continue
        except Exception:
            pass
        return out

    def update_from_events(self, events, dt_ms: int | float | None = None) -> None:
        """
        Alias for fold() to support adapter-style interfaces. dt_ms is ignored; tick should be provided in events or by the caller.
        """
        # Use last seen tick for touched nodes when missing; safe fallback 0.
        # Here we just pass 0; runtime passes proper tick to fold() already.
        self.fold(events, tick=0)

    def snapshot_head(self, head_k: int | None = None) -> List[List[float]]:
        """
        Convenience: return just the head list (top-k) [[node, value], ...]
        """
        k = int(self.head_k if head_k is None else max(1, head_k))
        snap = self.snapshot(head_n=k)
        head = snap.get("memory_head", []) if isinstance(snap, dict) else []
        return head or []

    def snapshot_dict(self, cap: int = 2048) -> Dict[int, float]:
        """
        Convenience: return a bounded dict {node:value}.
        """
        snap = self.snapshot(head_n=self.head_k)
        dct = snap.get("memory_dict", {}) if isinstance(snap, dict) else {}
        if not isinstance(dct, dict):
            return {}
        if len(dct) <= int(cap):
            return {int(k): float(v) for k, v in dct.items()}
        try:
            import heapq as _heapq
            items = list(dct.items())
            top = _heapq.nlargest(int(cap), items, key=lambda kv: kv[1])
            return {int(k): float(v) for k, v in top}
        except Exception:
            keys = list(dct.keys())[: int(cap)]
            return {int(k): float(dct[k]) for k in keys if k in dct}

    # ---------------- dimensionless knobs (read-only) ----------------

    @property
    def D_a(self) -> float:
        """Write gain (γ)"""
        return float(self.gamma)

    @property
    def Lambda(self) -> float:
        """Decay rate (δ)"""
        return float(self.delta)

    @property
    def Gamma(self) -> float:
        """One-edge smoothing (κ)"""
        return float(self.kappa)

    @property
    def Theta(self) -> float:
        """
        Steering coefficient placeholder (field does not steer directly).
        Provided for dimensional consistency; steering lives in walkers.
        """
        return 0.0


__all__ = ["MemoryField"]]]></content>
    </file>
    <file>
      <path>core/metrics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

def compute_metrics(connectome):
   """
   Rule Ref: Blueprint Rule 4.1 (Pathology Detection Mechanisms)
   - Adds connectome_entropy to support Active Domain Cartography (Rule 7) scheduling.
   - Prefers connectome.connectome_entropy() when available (sparse-mode), falling back to local function.
   """
   # TODO GET THESE FOR FREE FROM THE VOID WALKERS
   # Prefer a connectome-native entropy calculator for sparse-mode
   try:
       h = float(connectome.connectome_entropy())
   except Exception:
       h = float(connectome_entropy(connectome))
   return {
       "avg_weight": float(connectome.W.mean()),
       "active_synapses": int(connectome.active_edge_count()),
       "cohesion_components": int(connectome.connected_components()),
       "complexity_cycles": int(connectome.cyclomatic_complexity()),
       "connectome_entropy": h,
   }

def connectome_entropy(connectome) -> float:
   """
   Rule Ref: Blueprint Rule 4.1 - Global Pathological Structure (Connectome Entropy)
   Formula: H = -Σ p_i log(p_i), where p is degree distribution of the active subgraph.
   Returns 0.0 when no active edges are present.
   """
   # Active, undirected mask
   mask = (connectome.E > connectome.threshold) & (connectome.A == 1)
   # Degree per node (count upper+lower symmetrically from full mask)
   deg = mask.sum(axis=1).astype(np.float64)
   total = deg.sum()
   if total <= 0:
       return 0.0
   p = deg / total
   # Numerical stability
   p = np.clip(p, 1e-12, 1.0)
   return float(-(p * np.log(p)).sum())


# --- Streaming z-score detector for first-difference of a scalar series (tick-based) ---
# This keeps state across ticks to detect "spikes" in a topology metric such as
# cyclomatic complexity (a B1 proxy). It is void-faithful: no tokens, only graph-native signals.
import math as _math

class StreamingZEMA:
    """
    EMA-based z-score detector on first differences of a scalar time series.

    Parameters (tick-based):
    - half_life_ticks: EMA half-life in ticks (controls smoothing window)
    - z_spike: z-threshold to enter spiking
    - hysteresis: subtract from z_spike to exit spiking (prevents chatter)
    - min_interval_ticks: minimum ticks between spike fires (cooldown)
    """
    def __init__(self, half_life_ticks: int = 50, z_spike: float = 3.0, hysteresis: float = 1.0, min_interval_ticks: int = 10):
        self.alpha = 1.0 - _math.exp(_math.log(0.5) / float(max(1, int(half_life_ticks))))
        self.z_spike = float(z_spike)
        self.hysteresis = float(max(0.0, hysteresis))
        self.min_interval = int(max(1, int(min_interval_ticks)))

        self.mu = 0.0        # EMA mean of delta
        self.var = 1e-8      # EMA variance of delta
        self.prev = None     # previous value for delta computation
        self._spiking = False
        self.last_fire_tick = -10**12

    def update(self, value: float, tick: int):
        v = float(value)
        if self.prev is None:
            self.prev = v
            return {
                "value": v, "delta": 0.0, "mu": self.mu,
                "sigma": self.var ** 0.5, "z": 0.0, "spike": False
            }

        d = v - self.prev
        self.prev = v

        a = self.alpha
        # EMA on first-difference
        self.mu = (1.0 - a) * self.mu + a * d
        diff = d - self.mu
        self.var = (1.0 - a) * self.var + a * (diff * diff)
        sigma = (self.var if self.var > 1e-24 else 1e-24) ** 0.5
        z = diff / sigma

        # Hysteresis + cooldown
        fire = False
        high = self.z_spike
        low = max(0.0, self.z_spike - self.hysteresis)
        if not self._spiking and z >= high and (int(tick) - int(self.last_fire_tick)) >= self.min_interval:
            self._spiking = True
            self.last_fire_tick = int(tick)
            fire = True
        elif self._spiking and z <= low:
            self._spiking = False

        return {
            "value": v,
            "delta": float(d),
            "mu": float(self.mu),
            "sigma": float(sigma),
            "z": float(z),
            "spike": bool(fire),
        }
]]></content>
    </file>
    <file>
      <path>core/neuroplasticity/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>core/neuroplasticity/gdsp.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.neuroplasticity.gdsp
Purpose: Organism-native GDSP structural actuator with budgeted, territory-scoped, sparse-masked operations.

Design constraints
- One class per file; pure core; no IO/logging; NumPy + SciPy only.
- Budgeted algorithms:
  - Repairs: component-bridging with node/pair caps (no global mask sweeps).
  - Growth: reinforcement within territory by eligibility percentile; exploratory via similarity+eligibility prefilter.
  - Pruning: timer-based over weak, non-persistent synapses with CSR-safe operations.
"""

from typing import Any
import numpy as np


class GDSPActuator:
    """
    Goal-Directed Structural Plasticity (GDSP) actuator.

    - Homeostatic repairs (fragmentation healing; locus pruning)
    - Performance-driven growth (reinforcement, exploratory)
    - Maintenance pruning (weak, non-persistent synapses over time)

    Budget controls:
      bridge_budget_nodes: sample cap per component for gap bridging
      bridge_budget_pairs: max candidate (u,v) eligibility checks per tick
    """

    class _AdaptiveThresholds:
        def __init__(self) -> None:
            self.reward_threshold = 0.8
            self.td_error_threshold = 0.5
            self.novelty_threshold = 0.7
            self.sustained_window_size = 10

            self.structural_activity_counter = 0
            self.timesteps_since_growth = 0

            self.min_reward_threshold = 0.3
            self.max_reward_threshold = 0.9
            self.min_td_threshold = 0.1
            self.max_td_threshold = 0.8
            self.min_novelty_threshold = 0.2
            self.max_novelty_threshold = 0.9

            self.reward_history: list[float] = []
            self.td_error_history: list[float] = []
            self.novelty_history: list[float] = []

        def update_and_adapt(self, sie_report: dict, b1_persistence: float) -> None:
            self.reward_history.append(float(sie_report.get("total_reward", 0.0)))
            self.td_error_history.append(float(sie_report.get("td_error", 0.0)))
            self.novelty_history.append(float(sie_report.get("novelty", 0.0)))

            # truncate
            if len(self.reward_history) > 100:
                self.reward_history = self.reward_history[-100:]
                self.td_error_history = self.td_error_history[-100:]
                self.novelty_history = self.novelty_history[-100:]

            self.timesteps_since_growth += 1

            # encourage growth when stagnant
            if self.timesteps_since_growth > 500 and float(b1_persistence) <= 0.001:
                self.reward_threshold = max(self.min_reward_threshold, self.reward_threshold * 0.95)
                self.td_error_threshold = max(self.min_td_threshold, self.td_error_threshold * 0.95)
                self.novelty_threshold = max(self.min_novelty_threshold, self.novelty_threshold * 0.95)

            # dampen when activity is high
            elif self.structural_activity_counter > 20:
                self.reward_threshold = min(self.max_reward_threshold, self.reward_threshold * 1.05)
                self.td_error_threshold = min(self.max_td_threshold, self.td_error_threshold * 1.05)
                self.novelty_threshold = min(self.max_novelty_threshold, self.novelty_threshold * 1.05)
                self.structural_activity_counter = 0

            # statistical adaptation
            if len(self.reward_history) >= 50:
                r75 = float(np.percentile(self.reward_history, 75))
                td90 = float(np.percentile(self.td_error_history, 90))
                n75 = float(np.percentile(self.novelty_history, 75))

                target_reward = max(self.min_reward_threshold, min(self.max_reward_threshold, r75))
                target_td = max(self.min_td_threshold, min(self.max_td_threshold, td90))
                target_nov = max(self.min_novelty_threshold, min(self.max_novelty_threshold, n75))

                self.reward_threshold = 0.95 * self.reward_threshold + 0.05 * target_reward
                self.td_error_threshold = 0.95 * self.td_error_threshold + 0.05 * target_td
                self.novelty_threshold = 0.95 * self.novelty_threshold + 0.05 * target_nov

        def record_structural_activity(self) -> None:
            self.structural_activity_counter += 1
            self.timesteps_since_growth = 0

    def __init__(self, bridge_budget_nodes: int = 128, bridge_budget_pairs: int = 2048, rng_seed: int = 0) -> None:
        self._thr = GDSPActuator._AdaptiveThresholds()
        # Per-territory histories (keyed by frozenset(indices))
        from collections import deque
        self._reward_hist: dict[frozenset, Any] = {}
        self._td_hist: dict[frozenset, Any] = {}
        self._deque = deque  # constructor for deques

        # Budgets for homeostatic repairs
        self._bridge_nodes = int(max(1, int(bridge_budget_nodes)))
        self._bridge_pairs = int(max(1, int(bridge_budget_pairs)))
        self._rng = np.random.default_rng(int(rng_seed))

    # ---------------- Homeostatic repairs ----------------

    def _grow_connection_across_gap(self, substrate: Any) -> Any:
        """
        Bridge a topological gap by adding a single best edge evaluated under strict budgets.
        - Compute connected components once (O(N+E)).
        - Sample up to _bridge_nodes from each of the two largest components.
        - Evaluate up to _bridge_pairs candidate pairs by reading eligibility_traces[u,v].
        """
        try:
            from scipy.sparse.csgraph import connected_components
        except Exception:
            return substrate

        try:
            W = substrate.synaptic_weights
            E = substrate.eligibility_traces
        except Exception:
            return substrate

        n_components, labels = connected_components(csgraph=W, directed=False, connection="weak")
        if n_components <= 1:
            return substrate

        component_ids, counts = np.unique(labels, return_counts=True)
        if len(counts) < 2:
            return substrate
        idx = np.argsort(counts)[-2:]
        comp1_id, comp2_id = component_ids[idx[0]], component_ids[idx[1]]
        comp1_nodes = np.where(labels == comp1_id)[0]
        comp2_nodes = np.where(labels == comp2_id)[0]

        # Sample bounded node sets
        k1 = min(len(comp1_nodes), self._bridge_nodes)
        k2 = min(len(comp2_nodes), self._bridge_nodes)
        if k1 == 0 or k2 == 0:
            return substrate

        try:
            s1_idx = self._rng.choice(len(comp1_nodes), size=k1, replace=False)
            s2_idx = self._rng.choice(len(comp2_nodes), size=k2, replace=False)
            S1 = comp1_nodes[s1_idx]
            S2 = comp2_nodes[s2_idx]
        except Exception:
            S1 = comp1_nodes[:k1]
            S2 = comp2_nodes[:k2]

        # Generate candidate pairs within cap
        pairs: list[tuple[int, int]] = []
        for u in S1:
            for v in S2:
                if len(pairs) >= self._bridge_pairs:
                    break
                pairs.append((int(u), int(v)))
            if len(pairs) >= self._bridge_pairs:
                break

        best_val = None
        best_pair: tuple[int, int] | None = None
        for (u, v) in pairs:
            try:
                if W[u, v] != 0:
                    continue
                val = float(E[u, v])
                if best_val is None or val > best_val:
                    best_val = val
                    best_pair = (u, v)
            except Exception:
                continue

        if best_pair is None:
            return substrate

        uu, vv = best_pair
        try:
            W_lil = W.tolil()
            P_lil = substrate.persistent_synapses.tolil()
            W_lil[uu, vv] = 0.01
            P_lil[uu, vv] = True
            substrate.synaptic_weights = W_lil.tocsr()
            substrate.persistent_synapses = P_lil.tocsr()
        except Exception:
            pass
        return substrate

    @staticmethod
    def _prune_connections_in_locus(substrate: Any, locus_indices: np.ndarray) -> Any:
        if locus_indices is None or len(locus_indices) == 0:
            return substrate
        try:
            locus_mask = np.ix_(locus_indices, locus_indices)
            locus_weights_csr = substrate.synaptic_weights[locus_mask]
            if locus_weights_csr.nnz == 0:
                return substrate
            min_idx = int(np.argmin(np.abs(locus_weights_csr.data)))
            rows, cols = locus_weights_csr.nonzero()
            global_row = int(locus_indices[rows[min_idx]])
            global_col = int(locus_indices[cols[min_idx]])

            W = substrate.synaptic_weights.tolil()
            W[global_row, global_col] = 0
            substrate.synaptic_weights = W.tocsr()
        except Exception:
            pass
        return substrate

    def trigger_homeostatic_repairs(self, substrate: Any, probe_analysis: dict) -> Any:
        comp_cnt = int(probe_analysis.get("component_count", 1))
        # Attempt a single budgeted bridge per tick to bound cost
        if comp_cnt > 1:
            before = int(getattr(substrate.synaptic_weights, "nnz", 0))
            substrate = self._grow_connection_across_gap(substrate)
            after = int(getattr(substrate.synaptic_weights, "nnz", 0))
            # subsequent ticks will try again if still fragmented

        if float(probe_analysis.get("b1_persistence", 0.0)) > 0.9:
            locus = probe_analysis.get("locus_indices")
            if locus is not None:
                substrate = self._prune_connections_in_locus(substrate, locus)
        return substrate

    # ---------------- Performance-based growth ----------------

    def trigger_performance_growth(self, substrate: Any, sie_report: dict, territory_indices: np.ndarray, b1_persistence: float = 0.0) -> Any:
        self._thr.update_and_adapt(sie_report, b1_persistence)

        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        tid = frozenset(int(i) for i in territory_indices)

        if tid not in self._reward_hist:
            self._reward_hist[tid] = self._deque(maxlen=self._thr.sustained_window_size)
        if tid not in self._td_hist:
            self._td_hist[tid] = self._deque(maxlen=self._thr.sustained_window_size)

        self._reward_hist[tid].append(float(sie_report.get("total_reward", 0.0)))
        self._td_hist[tid].append(float(sie_report.get("td_error", 0.0)))
        novelty = float(sie_report.get("novelty", 0.0))

        # Reinforcement growth: strengthen existing connections with high eligibility
        if (len(self._reward_hist[tid]) == self._thr.sustained_window_size and
            all(r > self._thr.reward_threshold for r in self._reward_hist[tid])):
            substrate = self._execute_reinforcement_growth(substrate, territory_indices)
            self._thr.record_structural_activity()
            self._reward_hist[tid].clear()

        # Exploratory growth: persistent high error + novelty
        if (len(self._td_hist[tid]) == self._thr.sustained_window_size and
            all(e > self._thr.td_error_threshold for e in self._td_hist[tid]) and
            novelty > self._thr.novelty_threshold):
            substrate = self._execute_exploratory_growth(substrate, territory_indices)
            self._thr.record_structural_activity()
            self._td_hist[tid].clear()

        return substrate

    @staticmethod
    def _execute_reinforcement_growth(substrate: Any, territory_indices: np.ndarray) -> Any:
        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        try:
            W_lil = substrate.synaptic_weights.tolil()
            E_lil = substrate.eligibility_traces.tolil()

            mask = np.ix_(territory_indices, territory_indices)
            E_sub = E_lil[mask].tocsr()
            if E_sub.nnz > 0:
                thr = float(np.percentile(E_sub.data, 75))
                for r in territory_indices:
                    for c in territory_indices:
                        try:
                            if W_lil[r, c] != 0 and float(E_lil[r, c]) > thr:
                                W_lil[r, c] = float(W_lil[r, c]) * 1.1
                        except Exception:
                            continue
            substrate.synaptic_weights = W_lil.tocsr()
        except Exception:
            pass
        return substrate

    @staticmethod
    def _execute_exploratory_growth(substrate: Any, territory_indices: np.ndarray) -> Any:
        """
        Exploratory growth (budgeted, territory-scoped, sparse-masked):
          - Prefilter external candidates by firing-rate similarity (cheap)
          - Blend with sparse eligibility hint from territory boundary
          - Pick a tiny top-M set and create bidirectional edges (capped)
        """
        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        try:
            num_neurons = int(getattr(substrate.firing_rates, "shape", [0])[0]) if hasattr(substrate, "firing_rates") else 0
            if num_neurons <= len(territory_indices):
                return substrate

            all_neurons = np.arange(num_neurons, dtype=int)
            external = np.setdiff1d(all_neurons, territory_indices)
            if len(external) == 0:
                return substrate

            W_lil = substrate.synaptic_weights.tolil()
            P_lil = substrate.persistent_synapses.tolil()

            # 1) similarity prefilter
            terr_avg = float(np.mean(substrate.firing_rates[territory_indices])) if hasattr(substrate, "firing_rates") else 0.0
            ext_rates = substrate.firing_rates[external] if hasattr(substrate, "firing_rates") else np.zeros_like(external, dtype=float)
            diff = np.abs(ext_rates - terr_avg)

            prefilter_k = min(64, len(external))
            try:
                pf_idx = np.argpartition(diff, prefilter_k - 1)[:prefilter_k]
            except Exception:
                pf_idx = np.argsort(diff)[:prefilter_k]
            prefilter = external[pf_idx]
            diff_pf = diff[pf_idx]

            # 2) eligibility hint from territory boundary
            try:
                E_sub = substrate.eligibility_traces[territory_indices][:, prefilter]
                elig_hint = np.asarray(E_sub.max(axis=0)).ravel()
            except Exception:
                elig_hint = np.zeros_like(prefilter, dtype=float)

            # blend
            sim = 1.0 / (1.0 + diff_pf)
            try:
                emax = float(np.max(np.abs(elig_hint))) if elig_hint.size else 0.0
            except Exception:
                emax = 0.0
            elig_norm = (elig_hint / (emax + 1e-8)) if emax > 0.0 else np.zeros_like(elig_hint, dtype=float)
            score = 0.7 * sim + 0.3 * elig_norm

            # 3) top-M tiny set
            M = min(8, prefilter_k)
            try:
                chosen_idx = np.argpartition(score, -M)[-M:]
            except Exception:
                chosen_idx = np.argsort(score)[-M:]
            compat = prefilter[chosen_idx]

            # 4) add bidirectional edges under caps
            created = 0
            max_new = min(10, len(territory_indices) * max(1, len(compat)) // 4)
            for u in territory_indices[: min(3, len(territory_indices))]:
                for v in compat[: min(2, len(compat))]:
                    if created >= max_new:
                        break
                    try:
                        if W_lil[u, v] == 0:
                            W_lil[u, v] = 0.01
                            P_lil[u, v] = True
                            created += 1
                        if W_lil[v, u] == 0 and created < max_new:
                            W_lil[v, u] = 0.01
                            P_lil[v, u] = True
                            created += 1
                    except Exception:
                        continue

            substrate.synaptic_weights = W_lil.tocsr()
            substrate.persistent_synapses = P_lil.tocsr()
        except Exception:
            pass
        return substrate

    # ---------------- Maintenance pruning ----------------

    @staticmethod
    def trigger_maintenance_pruning(substrate: Any, T_prune: int, pruning_threshold: float = 0.01) -> Any:
        """
        Increment timers for weak, non-persistent synapses and prune when exceeding T_prune.
        """
        try:
            from scipy.sparse import csr_matrix
            W = substrate.synaptic_weights
            timers = substrate.synapse_pruning_timers.copy()
            P = substrate.persistent_synapses

            weak_mask = np.abs(W.data) < float(pruning_threshold)
            strong_mask = ~weak_mask

            persistent_bool = P.astype(bool)
            weak_mat = csr_matrix((weak_mask, W.nonzero()), shape=W.shape)
            eligible = weak_mat - weak_mat.multiply(persistent_bool)
            timers += eligible

            strong_mat = csr_matrix((strong_mask, W.nonzero()), shape=W.shape)
            timers = timers.multiply(strong_mat.astype(bool) == False)

            prune_mask = timers > int(T_prune)
            pruned = prune_mask.nnz
            if pruned > 0:
                W_lil = W.tolil()
                t_lil = timers.tolil()
                rows, cols = prune_mask.nonzero()
                if rows.size:
                    for r, c in zip(rows, cols):
                        try:
                            W_lil[r, c] = 0
                            t_lil[r, c] = 0
                        except Exception:
                            continue
                substrate.synaptic_weights = W_lil.tocsr()
                substrate.synapse_pruning_timers = t_lil.tocsr()
                substrate.synaptic_weights.eliminate_zeros()
            else:
                substrate.synapse_pruning_timers = timers
        except Exception:
            pass
        return substrate

    # ---------------- Orchestration ----------------

    def run(
        self,
        substrate: Any,
        introspection_report: dict | None = None,
        sie_report: dict | None = None,
        territory_indices: np.ndarray | None = None,
        T_prune: int = 100,
        pruning_threshold: float = 0.01,
    ) -> Any:
        b1_persistence = float(introspection_report.get("b1_persistence", 0.0)) if introspection_report else 0.0
        if introspection_report is not None and bool(introspection_report.get("repair_triggered", False)):
            substrate = self.trigger_homeostatic_repairs(substrate, introspection_report)
            self._thr.record_structural_activity()
            return substrate  # highest priority this tick

        if sie_report is not None and territory_indices is not None and len(territory_indices) > 0:
            substrate = self.trigger_performance_growth(substrate, sie_report, territory_indices, b1_persistence)

        substrate = self.trigger_maintenance_pruning(substrate, int(T_prune), float(pruning_threshold))
        return substrate

    @staticmethod
    def status_report(substrate: Any) -> dict:
        try:
            from scipy.sparse.csgraph import connected_components
            n_components, _ = connected_components(substrate.synaptic_weights, directed=False)
        except Exception:
            n_components = 1
        total_syn = int(getattr(substrate.synaptic_weights, "nnz", 0))
        total_neu = int(getattr(getattr(substrate, "firing_rates", None), "shape", [0])[0]) if hasattr(substrate, "firing_rates") else 0
        avg_deg = float(total_syn / total_neu) if total_neu > 0 else 0.0
        pers = int(getattr(substrate.persistent_synapses, "nnz", 0)) if hasattr(substrate, "persistent_synapses") else 0
        ratio = float(pers / total_syn) if total_syn > 0 else 0.0
        data = getattr(substrate.synaptic_weights, "data", np.array([], dtype=float))
        weight_stats = {
            "mean": float(np.mean(data)) if data.size > 0 else 0.0,
            "std": float(np.std(data)) if data.size > 0 else 0.0,
            "min": float(np.min(data)) if data.size > 0 else 0.0,
            "max": float(np.max(data)) if data.size > 0 else 0.0,
        }
        return {
            "total_neurons": int(total_neu),
            "total_synapses": int(total_syn),
            "persistent_synapses": int(pers),
            "persistent_ratio": float(ratio),
            "average_degree": float(avg_deg),
            "connected_components": int(n_components),
            "connectivity_health": "healthy" if n_components == 1 else "fragmented",
            "gdsp_operational": True,
        }


def run_gdsp_synaptic_actuator(
    substrate: Any,
    introspection_report: dict | None = None,
    sie_report: dict | None = None,
    territory_indices: Any | None = None,
    T_prune: int = 100,
    pruning_threshold: float = 0.01,
) -> Any:
    """
    Legacy-compatible wrapper (emergent-only trigger, no schedulers).
    Mirrors older runtime adapters by exposing a function entrypoint.

    Complexity: O(#bounded-ops) per tick (budgeted repairs/growth + pruning).
    """
    try:
        inst = getattr(run_gdsp_synaptic_actuator, "_inst", None)
        if inst is None:
            inst = GDSPActuator()
            setattr(run_gdsp_synaptic_actuator, "_inst", inst)
        return inst.run(
            substrate=substrate,
            introspection_report=introspection_report or {},
            sie_report=sie_report or {},
            territory_indices=territory_indices,
            T_prune=int(T_prune),
            pruning_threshold=float(pruning_threshold),
        )
    except Exception:
        return substrate


def get_gdsp_status_report(substrate: Any) -> dict:
    """
    Legacy-compatible status function.

    Returns a compact operational snapshot (component count, degree, weight stats).
    """
    try:
        return GDSPActuator.status_report(substrate)
    except Exception:
        return {"gdsp_operational": False}
__all__ = ["GDSPActuator"]
]]></content>
    </file>
    <file>
      <path>core/neuroplasticity/revgsp.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.neuroplasticity.revgsp
Purpose: Resonance‑Enhanced Valence‑Gated Synaptic Plasticity (REV‑GSP), organism‑native.

Design constraints
- Pure core; no IO/logging; NumPy + SciPy only.
- Budgeted pair sampling (no global candidate sweep).
- CSR‑safe updates for eligibility traces and weights.
"""

from typing import Any, Dict, List, Tuple
import math
import numpy as np


class RevGSP:
    """
    REV‑GSP learner (class form, organism‑native).
    - No IO/logging. Pure numeric state updates on a Substrate‑like object.
    - Accepts any 'substrate' exposing:
        synaptic_weights (CSR), eligibility_traces (CSR), neuron_polarities (ndarray)
    """

    def __init__(
        self,
        reward_sigmoid_scale: float = 1.5,
        pi_params: dict | None = None,
        rng_seed: int | None = None,
        max_pairs: int = 2048,
        sample_spikes_cap: int | None = None,
    ) -> None:
        """
        Parameters:
          - reward_sigmoid_scale: gain for eta_effective sigmoid
          - pi_params: base params for PI kernel (a±, tau±)
          - rng_seed: deterministic sampling for budgets
          - max_pairs: hard cap on pre/post spike candidate evaluations per tick
          - sample_spikes_cap: optional cap on filtered spike list (down-sample before pairing)
        """
        self.reward_sigmoid_scale = float(reward_sigmoid_scale)
        self.pi_params = pi_params or {
            "a_plus_base": 0.1,
            "a_minus_base": 0.1,
            "tau_plus_base": 20.0,
            "tau_minus_base": 20.0,
        }
        self.rng = np.random.default_rng(rng_seed)
        self.max_pairs = int(max(1, int(max_pairs)))
        self.sample_spikes_cap = None if sample_spikes_cap is None else int(max(1, int(sample_spikes_cap)))

    # --- helpers ---
    def _clamped_normal(self, mu: float, sigma: float, lo: float, hi: float) -> float:
        try:
            val = float(self.rng.normal(mu, sigma))
        except Exception:
            val = float(mu)
        if val < lo:
            val = lo
        if val > hi:
            val = hi
        return float(val)

    def _base_pi(self, delta_t: float) -> float:
        # STDP‑like impulse with constrained bio diversity per call
        a_plus = self._clamped_normal(self.pi_params["a_plus_base"], 0.01, 0.03, 0.07)
        a_minus = self._clamped_normal(self.pi_params["a_minus_base"], 0.01, 0.04, 0.08)
        tau_plus = self._clamped_normal(self.pi_params["tau_plus_base"], 2.0, 18.0, 22.0)
        tau_minus = self._clamped_normal(self.pi_params["tau_minus_base"], 2.0, 18.0, 22.0)
        if delta_t > 0:
            return float(a_plus * math.exp(-delta_t / tau_plus))
        return float(-a_minus * math.exp(delta_t / tau_minus))

    def _eta_effective(self, base_lr: float, total_reward: float) -> float:
        # Reward‑separated eta; falls back to base_lr when reward≈0 so early phases still learn
        if abs(total_reward) < 1e-10:
            return float(base_lr)
        k = self.reward_sigmoid_scale
        x = k * float(total_reward)
        mod = 2.0 / (1.0 + math.exp(-x)) - 1.0  # 2*sigmoid-1
        eta_mag = float(base_lr) * (1.0 + mod)
        return float(math.copysign(eta_mag, total_reward))

    @staticmethod
    def _gamma_from_plv(plv: float, base_decay: float = 0.95, sensitivity: float = 0.1) -> float:
        # PLV‑gated trace decay
        return float(base_decay + sensitivity * (float(plv) - 0.5))

    @staticmethod
    def _temporal_filter(spike_times: List[Tuple[int, int]], window_size: int = 5) -> List[Tuple[int, float]]:
        if len(spike_times) < window_size:
            return spike_times
        out: List[Tuple[int, float]] = []
        for i in range(len(spike_times) - window_size + 1):
            window = spike_times[i : i + window_size]
            avg_time = sum(t for _, t in window) / float(window_size)
            neuron_idx = window[-1][0]
            out.append((neuron_idx, avg_time))
        return out

    @staticmethod
    def _adaptive_window(base_ms: int, max_latency: float) -> int:
        return int(base_ms + float(max_latency))

    @staticmethod
    def _latency_scale(pi_value: float, latency_error: float, max_latency: float) -> float:
        if float(max_latency) > 0.0:
            return float(pi_value) * (1.0 - float(latency_error) / float(max_latency))
        return float(pi_value)

    # --- main API ---
    def adapt(
        self,
        substrate: Any,
        spike_train: List[Tuple[int, int]],
        spike_phases: Dict[Tuple[int, int], float],
        learning_rate: float,
        lambda_decay: float,
        total_reward: float,
        plv: float,
        network_latency_estimate: Dict[str, float],
        time_window_ms: int = 20,
    ) -> tuple[Any, dict]:
        """
        Update substrate in‑place using REV‑GSP rule; returns (substrate, metrics).
        Budgeted: samples pairs from recent spikes only; respects max_pairs and sample_spikes_cap.
        """
        try:
            from scipy.sparse import lil_matrix  # local import to avoid hard dependency at import-time
        except Exception:
            # Cannot operate without scipy
            return substrate, {"eta_effective": 0.0, "gamma": 0.0}

        filtered = self._temporal_filter(spike_train)
        win = self._adaptive_window(int(time_window_ms), float(network_latency_estimate.get("max", 0.0)))

        # Optional down‑sample of filtered spikes to respect complexity cap
        if self.sample_spikes_cap is not None and len(filtered) > self.sample_spikes_cap:
            try:
                idx = self.rng.choice(len(filtered), size=self.sample_spikes_cap, replace=False)
                filtered = [filtered[int(i)] for i in idx]
            except Exception:
                filtered = filtered[: self.sample_spikes_cap]

        W = getattr(substrate, "synaptic_weights", None)
        E = getattr(substrate, "eligibility_traces", None)
        P = getattr(substrate, "neuron_polarities", None)
        if W is None or E is None or P is None:
            return substrate, {"eta_effective": 0.0, "gamma": 0.0}

        # Build PI sparsely
        try:
            shape = W.shape
        except Exception:
            shape = (0, 0)
        PI = lil_matrix(shape, dtype=np.float32)

        # Budgeted pair evaluation to ensure sub‑quadratic behavior
        pairs_evaluated = 0
        break_outer = False
        for pre_neuron, pre_time in filtered:
            if break_outer:
                break
            for post_neuron, post_time in filtered:
                if pre_neuron == post_neuron:
                    continue
                try:
                    # Quick existence check (CSR O(1) average)
                    if W[pre_neuron, post_neuron] == 0:
                        continue
                    delta_t = float(post_time) - float(pre_time)
                    if 0.0 < abs(delta_t) < float(win):
                        base_pi = self._base_pi(delta_t)
                        phase_pre = float(spike_phases.get((pre_neuron, int(pre_time)), 0.0))
                        phase_post = float(spike_phases.get((post_neuron, int(post_time)), 0.0))
                        phase_mod = (1.0 + math.cos(phase_pre - phase_post)) * 0.5
                        pi_val = base_pi * phase_mod
                        pi_val = self._latency_scale(pi_val, float(network_latency_estimate.get("error", 0.0)), float(network_latency_estimate.get("max", 0.0)))
                        PI[pre_neuron, post_neuron] += float(pi_val)
                        pairs_evaluated += 1
                        if pairs_evaluated >= self.max_pairs:
                            break_outer = True
                            break
                except Exception:
                    continue

        PI_csr = PI.tocsr()

        # Eligibility update: E = gamma*E + PI
        gamma = self._gamma_from_plv(float(plv))
        try:
            E *= float(gamma)
        except Exception:
            # fallback reconstruct
            E = E.multiply(float(gamma))
        E += PI_csr

        # Row‑scale by neuron polarity (CSR‑friendly)
        try:
            indptr = E.indptr
            data = E.data
            for i in range(E.shape[0]):
                p = float(P[i])
                if p == 1.0:
                    continue
                start = indptr[i]
                end = indptr[i + 1]
                if end > start:
                    data[start:end] *= p
        except Exception:
            pass

        # Three‑factor update
        eta = self._eta_effective(float(learning_rate), float(total_reward))
        try:
            trace_update = E * float(eta)
            decay_update = W * float(lambda_decay)
            dW = trace_update - decay_update
            W += dW
            # clip
            try:
                W.data = np.clip(W.data, -1.0, 1.0)
            except Exception:
                pass
        except Exception:
            pass

        return substrate, {"eta_effective": float(eta), "gamma": float(gamma)}

    # Compatibility wrapper matching the task board signature
    def adapt_connectome(
        self,
        substrate: Any,
        spike_train: List[Tuple[int, int]],
        spike_phases: Dict[Tuple[int, int], float],
        total_reward: float,
        network_latency: Dict[str, float],
        *,
        max_pairs: int | None = None,
        spike_sampling_cap: int | None = None,
        pi_params: dict | None = None,
        lambda_decay: float = 1e-3,
        base_lr: float = 1e-2,
        plv: float | None = None,
        time_window_ms: int = 20,
    ) -> tuple[Any, dict]:
        """
        Thin compatibility wrapper:
        - Allows per‑call override of budgets and PI parameters.
        - Computes eta from total_reward via internal nonlinearity.
        - Uses network_latency['max'|'error'] to adapt the time window and latency scaling.
        - plv defaults to network_latency.get('plv', 0.5) if not provided.
        """
        # Stash old config and override temporarily
        old_mp = self.max_pairs
        old_cap = self.sample_spikes_cap
        old_pi = dict(self.pi_params) if isinstance(self.pi_params, dict) else self.pi_params

        try:
            if max_pairs is not None:
                self.max_pairs = int(max(1, int(max_pairs)))
            if spike_sampling_cap is not None:
                self.sample_spikes_cap = int(max(1, int(spike_sampling_cap)))
            if pi_params is not None:
                self.pi_params = dict(pi_params)

            use_plv = float(plv if plv is not None else float(network_latency.get("plv", 0.5)))
            return self.adapt(
                substrate=substrate,
                spike_train=spike_train,
                spike_phases=spike_phases,
                learning_rate=float(base_lr),
                lambda_decay=float(lambda_decay),
                total_reward=float(total_reward),
                plv=float(use_plv),
                network_latency_estimate=network_latency,
                time_window_ms=int(time_window_ms),
            )
        finally:
            # Restore config
            self.max_pairs = old_mp
            self.sample_spikes_cap = old_cap
            self.pi_params = old_pi


__all__ = ["RevGSP"]]]></content>
    </file>
    <file>
      <path>core/primitives/dsu.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Disjoint Set Union (Union-Find) primitive with O(1) component counting
and optional masked counting for rare telemetry.

Void-faithful: event-folded unions only; no global scans in hot path.
Copyright © 2025 Justin K. Lietz, Neuroca, Inc.
"""
from __future__ import annotations

from typing import Optional
import numpy as np


class DSU:
    """
    Path-compressed, union-by-rank disjoint set with size tracking and
    O(1) component counting.

    Hot-path methods:
      - find(x) -> int
      - union(a, b) -> bool            # True if merged (reduced components)
      - same_set(a, b) -> bool
      - count_sets() -> int            # O(1)

    Rare/telemetry method:
      - count_sets(mask: Optional[np.ndarray]) -> int
        When mask is provided, counts sets only across masked indices
        via a local scan of the mask (not for per-tick use).
    """
    __slots__ = ("parent", "rank", "size", "components")

    def __init__(self, n: int):
        n = int(n)
        self.parent = np.arange(n, dtype=np.int32)
        self.rank = np.zeros(n, dtype=np.int8)
        self.size = np.ones(n, dtype=np.int32)
        self.components = n

    def find(self, x: int) -> int:
        p = self.parent
        x = int(x)
        while p[x] != x:
            p[x] = p[p[x]]
            x = p[x]
        return int(x)

    def union(self, a: int, b: int) -> bool:
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return False
        # attach lower-rank to higher-rank
        if self.rank[ra] < self.rank[rb]:
            ra, rb = rb, ra
        self.parent[rb] = ra
        self.size[ra] += self.size[rb]
        if self.rank[ra] == self.rank[rb]:
            self.rank[ra] = self.rank[ra] + 1
        self.components -= 1
        return True

    def grow_to(self, n: int) -> None:
        """
        Grow DSU to cover indices [0, n), preserving existing sets.
        O(n - old_n) initialization; does not scan existing structure.
        """
        n = int(n)
        cur = int(self.parent.size)
        if n <= cur:
            return
        add = n - cur
        self.parent = np.concatenate([self.parent, np.arange(cur, n, dtype=np.int32)])
        self.rank = np.concatenate([self.rank, np.zeros(add, dtype=np.int8)])
        self.size = np.concatenate([self.size, np.ones(add, dtype=np.int32)])
        self.components += add

    def same_set(self, a: int, b: int) -> bool:
        return self.find(a) == self.find(b)
 
    def count_sets(self, mask: Optional[np.ndarray] = None) -> int:
        if mask is None:
            return int(self.components)
        m = np.asarray(mask)
        if m.dtype != np.bool_:
            m = m.astype(bool, copy=False)
        if m.shape[0] != self.parent.size:
            raise ValueError("mask length must equal DSU size")
        idx = np.nonzero(m)[0]
        if idx.size == 0:
            return 0
        roots = set(self.find(int(i)) for i in idx)
        return len(roots)
 
 
 
__all__ = ["DSU"]]]></content>
    </file>
    <file>
      <path>core/proprioception/events.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event schema and incremental reducers for event-driven metrics (Phase C scaffolding).

Design:
- Pure core module. No imports from fum_rt.io.* or fum_rt.runtime.*.
- Does not perform any I/O or logging. Export numbers only.
- Safe-by-default: Can be instantiated but not wired unless feature flags enable event-driven path.
- O(1) per-event update; no scans over W in the hot path.

Provided:
- Event types: DeltaEvent, VTTouchEvent, EdgeOnEvent, EdgeOffEvent, MotifEnterEvent, MotifExitEvent, ADCEvent
- Incremental reducers:
    - StreamingMeanVar: Welford online (mean/var/std)
    - EWMA: exponential moving average
    - CountMinSketchHead: CMS plus exact head for entropy/coverage approximation
    - UnionFindCohesion: incremental cohesion via union set on edge_on; marks edge_off as dirty
- EventDrivenMetrics: folds events and exposes snapshot() dict of numeric metrics

Integration plan:
- Connectome/walkers publish events on the announce bus (outside core).
- Runtime/orchestrator drains bus and forwards events to EventDrivenMetrics.update().
- Telemetry snapshot reads EventDrivenMetrics.snapshot() and packages 'why'.
- Old scan-based metrics remain the default until flags enable event-driven path.

Caveats:
- Edge_off is marked dirty; a low-cadence auditor is expected to reconcile connectivity.
- VT coverage/entropy are approximate via CMS+head; auditor can validate.
"""

from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple
import math
import random

# Allowed core import
from fum_rt.core.metrics import StreamingZEMA  # existing Z detector in core


# ----------------------------- Event Types -----------------------------------

@dataclass(frozen=True)
class BaseEvent:
    kind: str
    t: Optional[int] = None


@dataclass(frozen=True)
class DeltaEvent(BaseEvent):
    """
    Local structural/learning delta.
    Fields:
      - b1: contribution to B1-like topology signal (float)
      - novelty: novelty component in [0, +inf)
      - hab: habituation component in [0, +inf)
      - td: temporal-difference-like component (float)
      - hsi: homeostatic stability/instability component (float)
    """
    b1: float = 0.0
    novelty: float = 0.0
    hab: float = 0.0
    td: float = 0.0
    hsi: float = 0.0


@dataclass(frozen=True)
class VTTouchEvent(BaseEvent):
    """
    Vocabulary/feature touch (used for VT coverage/entropy approximations).
    Fields:
      - token: hashable token id or string
      - w: optional weight (float, default 1.0)
    """
    token: Any = ""
    w: float = 1.0


@dataclass(frozen=True)
class EdgeOnEvent(BaseEvent):
    u: int = 0
    v: int = 0


@dataclass(frozen=True)
class EdgeOffEvent(BaseEvent):
    u: int = 0
    v: int = 0


# Polarity-aware activity/spike event (void-faithful, event-driven only)
@dataclass(frozen=True)
class SpikeEvent(BaseEvent):
    node: int = 0       # neuron id
    amp: float = 1.0    # activity magnitude (or |ΔW| proxy)
    sign: int = +1      # +1 excitatory, -1 inhibitory, 0 unknown


# Optional signed weight delta event for local learning updates
@dataclass(frozen=True)
class DeltaWEvent(BaseEvent):
    node: int = 0
    dw: float = 0.0


@dataclass(frozen=True)
class MotifEnterEvent(BaseEvent):
    motif_id: int = 0


@dataclass(frozen=True)
class MotifExitEvent(BaseEvent):
    motif_id: int = 0


@dataclass(frozen=True)
class ADCEvent(BaseEvent):
    """
    ADC estimator readout event (fold metrics in place of reading raw structures).
    Suggested fields (all optional, numeric):
      - adc_territories
      - adc_boundaries
      - adc_cycle_hits
    """
    adc_territories: Optional[int] = None
    adc_boundaries: Optional[int] = None
    adc_cycle_hits: Optional[float] = None


# Optional hint for biasing exploration/actuation (not folded by metrics)
@dataclass(frozen=True)
class BiasHintEvent(BaseEvent):
    """
    Hint to bias exploration or actuation to a region/tile for a short TTL.
    - region: free-form label (e.g., "unknown", "tile:3,4")
    - nodes: bounded set of indices to hint (tuple for immutability)
    - ttl:   time-to-live in ticks (downstream consumer-managed)
    Note: EventDrivenMetrics ignores this; it travels on the bus for optional consumers.
    """
    region: str = "unknown"
    nodes: Tuple[int, ...] = tuple()
    ttl: int = 2


# -------------------------- Incremental Primitives ---------------------------

class StreamingMeanVar:
    """
    Welford's algorithm for streaming mean/variance/std.
    """
    __slots__ = ("n", "mean", "M2")

    def __init__(self) -> None:
        self.n = 0
        self.mean = 0.0
        self.M2 = 0.0

    def update(self, x: float) -> None:
        try:
            x = float(x)
        except Exception:
            return
        self.n += 1
        delta = x - self.mean
        self.mean += delta / self.n
        delta2 = x - self.mean
        self.M2 += delta * delta2

    def variance(self) -> float:
        if self.n < 2:
            return 0.0
        return self.M2 / (self.n - 1)

    def std(self, eps: float = 1e-9) -> float:
        v = self.variance()
        if v <= 0.0:
            return eps
        return math.sqrt(v) + eps


class EWMA:
    """
    Exponential Weighted Moving Average.
    """
    __slots__ = ("alpha", "y")

    def __init__(self, alpha: float = 0.05, init: float = 0.0) -> None:
        self.alpha = float(max(0.0, min(1.0, alpha)))
        self.y = float(init)

    def update(self, x: float) -> float:
        x = float(x)
        self.y = self.alpha * x + (1.0 - self.alpha) * self.y
        return self.y

    def value(self) -> float:
        return float(self.y)


class CountMinSketchHead:
    """
    Approximate frequency model for VT coverage/entropy.
    - CMS for tail, plus an exact head map for top-k tokens.
    - Entropy and coverage computed from head counts; tail mass estimated via CMS minimum row sum.

    Note: This is a lightweight approximation; an auditor can reconcile periodically.
    """
    def __init__(self, width: int = 256, depth: int = 3, head_k: int = 256, seed: int = 0) -> None:
        self.w = max(8, int(width))
        self.d = max(1, int(depth))
        self.head_k = max(8, int(head_k))
        rng = random.Random(int(seed))
        self._a = [rng.randrange(1, 2**61 - 1) for _ in range(self.d)]
        self._b = [rng.randrange(0, 2**61 - 1) for _ in range(self.d)]
        self._P = (2**61 - 1)
        self._M = [[0.0 for _ in range(self.w)] for _ in range(self.d)]
        self._head: Dict[Any, float] = {}
        self._total = 0.0

    def _h(self, i: int, key_hash: int) -> int:
        return ((self._a[i] * key_hash + self._b[i]) % self._P) % self.w

    def _hash_key(self, key: Any) -> int:
        # Stable hash across process; for best stability use explicit str
        return hash(str(key))

    def update(self, key: Any, w: float = 1.0) -> None:
        try:
            w = float(w)
        except Exception:
            return
        if w <= 0.0:
            return
        self._total += w
        kh = self._hash_key(key)
        for i in range(self.d):
            j = self._h(i, kh)
            self._M[i][j] += w
        # Update head exact counts; keep only top-K
        cur = self._head.get(key, 0.0) + w
        self._head[key] = cur
        if len(self._head) > self.head_k * 2:
            # prune lower half
            items = sorted(self._head.items(), key=lambda kv: kv[1], reverse=True)[: self.head_k]
            self._head = dict(items)

    def estimate(self, key: Any) -> float:
        if key in self._head:
            return float(self._head[key])
        kh = self._hash_key(key)
        est = min(self._M[i][self._h(i, kh)] for i in range(self.d))
        return float(est)

    def coverage(self) -> float:
        """
        Approximate coverage as fraction of head mass over total.
        """
        if self._total <= 0.0:
            return 0.0
        head_mass = sum(self._head.values())
        return float(max(0.0, min(1.0, head_mass / self._total)))

    def entropy(self, eps: float = 1e-12) -> float:
        """
        Shannon entropy over head distribution (tail ignored), in nats.
        """
        head_mass = sum(self._head.values())
        if head_mass <= 0.0:
            return 0.0
        H = 0.0
        for _, c in self._head.items():
            p = float(c / head_mass)
            if p > 0.0:
                H -= p * math.log(p + eps)
        return float(H)

    def snapshot(self) -> Dict[str, float]:
        return {
            "vt_coverage": self.coverage(),
            "vt_entropy": self.entropy(),
        }


class UnionFindCohesion:
    """
    Incremental cohesion via union-find on edge_on events.
    Edge_off events mark dirty; auditor should reconcile at low cadence.

    Exposes:
      - union(u,v) on edge_on
      - mark_dirty(u,v) on edge_off
      - components() approximate count (dirty edges may increase this transiently)
    """
    def __init__(self, n_hint: int = 0) -> None:
        self.parent: Dict[int, int] = {}
        self.size: Dict[int, int] = {}
        self._dirty = 0  # count of edge_off marks since last audit

    def _find(self, x: int) -> int:
        # path compression
        if self.parent.get(x, x) != x:
            self.parent[x] = self._find(self.parent[x])
        return self.parent.get(x, x)

    def _ensure(self, x: int) -> None:
        if x not in self.parent:
            self.parent[x] = x
            self.size[x] = 1

    def union(self, u: int, v: int) -> None:
        self._ensure(u)
        self._ensure(v)
        ru = self._find(u)
        rv = self._find(v)
        if ru == rv:
            return
        su = self.size.get(ru, 1)
        sv = self.size.get(rv, 1)
        if su < sv:
            ru, rv = rv, ru
            su, sv = sv, su
        self.parent[rv] = ru
        self.size[ru] = su + sv

    def mark_dirty(self, _u: int, _v: int) -> None:
        self._dirty += 1

    def components(self) -> int:
        roots = sum(1 for k, p in self.parent.items() if k == p)
        # naive dirty inflation (auditor should reconcile)
        return int(roots + 0)


# ---------------------------- Event-Driven Metrics ---------------------------

class EventDrivenMetrics:
    """
    O(1) per-event folding of key telemetry metrics.

    Maintained:
      - b1_value, b1_z (via StreamingZEMA)
      - vt_coverage, vt_entropy (via CountMinSketchHead)
      - cohesion_components (via UnionFindCohesion)
      - adc_territories, adc_boundaries (fold ADCEvent)
      - complexity_cycles proxy from ADC (adc_cycle_hits)

    Snapshot includes fields expected by telemetry packagers. Missing fields default to 0.0 or 0.
    """
    def __init__(
        self,
        z_half_life_ticks: int = 50,
        z_spike: float = 1.0,
        hysteresis: float = 1.0,
        vt_width: int = 256,
        vt_depth: int = 3,
        vt_head_k: int = 256,
        seed: int = 0,
    ) -> None:
        self.b1_detector = StreamingZEMA(
            half_life_ticks=int(max(1, z_half_life_ticks)),
            z_spike=float(z_spike),
            hysteresis=float(hysteresis),
            min_interval_ticks=1,
        )
        self._b1_value = 0.0
        self._b1_last: Dict[str, float] = {}
        self._vt = CountMinSketchHead(width=vt_width, depth=vt_depth, head_k=vt_head_k, seed=seed)
        self._cohesion = UnionFindCohesion()
        self._adc_territories = 0
        self._adc_boundaries = 0
        self._adc_cycle_hits = 0.0
        self._tick = 0

    def update(self, ev: BaseEvent) -> None:
        self._tick = int(getattr(ev, "t", self._tick))
        k = getattr(ev, "kind", None)
        if not k:
            return
        if k == "delta":
            dev: DeltaEvent = ev  # type: ignore[assignment]
            # b1_value as additive proxy; alternative mappings can be calibrated
            try:
                self._b1_value += float(dev.b1)
                z = self.b1_detector.update(self._b1_value, tick=self._tick)
                self._b1_last = {
                    "b1_value": float(z.get("value", 0.0)),
                    "b1_delta": float(z.get("delta", 0.0)),
                    "b1_z": float(z.get("z", 0.0)),
                    "b1_spike": bool(z.get("spike", False)),
                }
            except Exception:
                pass
            # SIE components can be folded externally; this class does not compute valence
        elif k == "vt_touch":
            tev: VTTouchEvent = ev  # type: ignore[assignment]
            try:
                self._vt.update(tev.token, w=float(getattr(tev, "w", 1.0)))
            except Exception:
                pass
        elif k == "edge_on":
            e: EdgeOnEvent = ev  # type: ignore[assignment]
            try:
                self._cohesion.union(int(e.u), int(e.v))
            except Exception:
                pass
        elif k == "edge_off":
            e: EdgeOffEvent = ev  # type: ignore[assignment]
            try:
                self._cohesion.mark_dirty(int(e.u), int(e.v))
            except Exception:
                pass
        elif k == "adc":
            a: ADCEvent = ev  # type: ignore[assignment]
            try:
                if a.adc_territories is not None:
                    self._adc_territories = int(a.adc_territories)
                if a.adc_boundaries is not None:
                    self._adc_boundaries = int(a.adc_boundaries)
                if a.adc_cycle_hits is not None:
                    self._adc_cycle_hits = float(a.adc_cycle_hits)
            except Exception:
                pass
        elif k in ("motif_enter", "motif_exit"):
            # Motif events can be used to refine b1_value or cohesion; placeholder noop.
            pass
        else:
            # Unknown event kinds are ignored (forward-compat)
            pass

    def snapshot(self) -> Dict[str, Any]:
        vt = self._vt.snapshot()
        snap = {
            # B1
            "b1_value": float(self._b1_last.get("b1_value", 0.0)),
            "b1_delta": float(self._b1_last.get("b1_delta", 0.0)),
            "b1_z": float(self._b1_last.get("b1_z", 0.0)),
            "b1_spike": bool(self._b1_last.get("b1_spike", False)),
            # VT
            "vt_coverage": float(vt.get("vt_coverage", 0.0)),
            "vt_entropy": float(vt.get("vt_entropy", 0.0)),
            # Cohesion
            "cohesion_components": int(self._cohesion.components()),
            # ADC readouts
            "adc_territories": int(self._adc_territories),
            "adc_boundaries": int(self._adc_boundaries),
            # Cycle proxy feed-through (can be added to complexity_cycles by caller)
            "adc_cycle_hits": float(self._adc_cycle_hits),
        }
        return snap


__all__ = [
    # events
    "BaseEvent",
    "DeltaEvent",
    "VTTouchEvent",
    "EdgeOnEvent",
    "EdgeOffEvent",
    "SpikeEvent",
    "DeltaWEvent",
    "MotifEnterEvent",
    "MotifExitEvent",
    "ADCEvent",
    "BiasHintEvent",
    # reducers
    "StreamingMeanVar",
    "EWMA",
    "CountMinSketchHead",
    "UnionFindCohesion",
    "EventDrivenMetrics",
]]]></content>
    </file>
    <file>
      <path>core/proprioception/territory.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Void-faithful cohesion territories (incremental, event-folded; no scans).

- Maintains a union-find structure over observed edge_on-like events
  to approximate connected components ("territories").
- Keeps a bounded per-component head (reservoir) of member indices to serve
  territory_indices to actuators (e.g., GDSP) without graph scans.
- O(1) amortized per observation; no reads of W/CSR/adjacency.

API
- fold(observations): consumes Observation-like objects (kind/nodes fields)
  and unions endpoints found in cycle_hit (first two nodes) or edge_on(u,v).
- components_count(): number of current components (approximate).
- sample_indices(component_id, k): returns up to k indices from the requested
  component (component_id can be any node in the component).
- sample_any(k): returns up to k indices across largest components (by UF size).
"""

from __future__ import annotations

from typing import Any, Dict, Iterable, List, Optional, Set


class TerritoryUF:
    def __init__(self, head_k: int = 512) -> None:
        self.parent: Dict[int, int] = {}
        self.size: Dict[int, int] = {}
        # bounded head members per root (kept small; no scans)
        self._head: Dict[int, List[int]] = {}
        self._head_k = max(8, int(head_k))
        self._dirty = 0  # for future auditors

    # ------------- UF core -------------

    def _find(self, x: int) -> int:
        p = self.parent.get(x, x)
        if p != x:
            self.parent[x] = self._find(p)
        return self.parent.get(x, x)

    def _ensure(self, x: int) -> int:
        if x not in self.parent:
            self.parent[x] = x
            self.size[x] = 1
            # seed head with the node itself
            self._head[x] = [x]
        return x

    def _merge_heads(self, r_to: int, r_from: int) -> None:
        """
        Merge bounded heads; keep uniqueness and cap to head_k.
        """
        h_to = self._head.get(r_to, [])
        h_from = self._head.get(r_from, [])
        if not h_from:
            self._head[r_to] = list(dict.fromkeys(h_to))[: self._head_k]
            return
        # Favor r_to contents then supplement with r_from
        merged: List[int] = []
        seen: Set[int] = set()
        for src in (h_to, h_from):
            for n in src:
                if n not in seen:
                    merged.append(n)
                    seen.add(n)
                    if len(merged) >= self._head_k:
                        break
            if len(merged) >= self._head_k:
                break
        self._head[r_to] = merged

    def _add_member(self, r: int, x: int) -> None:
        """
        Add a member to root r's head (bounded); no-ops on duplicates.
        """
        head = self._head.setdefault(r, [])
        if x in head:
            return
        if len(head) < self._head_k:
            head.append(x)
        # else: drop (bounded by design)

    def union(self, u: int, v: int) -> None:
        ru = self._find(self._ensure(u))
        rv = self._find(self._ensure(v))
        if ru == rv:
            return
        su = self.size.get(ru, 1)
        sv = self.size.get(rv, 1)
        # union by size
        if su < sv:
            ru, rv = rv, ru
            su, sv = sv, su
        self.parent[rv] = ru
        self.size[ru] = su + sv
        # merge bounded heads
        self._merge_heads(ru, rv)
        # cleanup from root moved
        try:
            if rv in self._head:
                del self._head[rv]
        except Exception:
            pass

    def mark_dirty(self, _u: int, _v: int) -> None:
        self._dirty += 1

    # ------------- public -------------

    def fold(self, observations: Iterable[Any]) -> None:
        """
        Fold Observation-like objects:
          - cycle_hit: if nodes has ≥ 2, union(nodes[0], nodes[1])
          - edge_on: union(u, v) if fields present
          - edge_off: mark_dirty (no reconciliation here)
        """
        if not observations:
            return
        for obs in observations:
            try:
                k = getattr(obs, "kind", None)
                if not k:
                    continue
                if k == "cycle_hit":
                    nodes = list(getattr(obs, "nodes", []) or [])
                    if len(nodes) >= 2:
                        u, v = int(nodes[0]), int(nodes[1])
                        self.union(u, v)
                        # seed members (bounded) for fast sampling
                        self._add_member(self._find(u), u)
                        self._add_member(self._find(v), v)
                elif k == "edge_on":
                    u = int(getattr(obs, "u", 0))
                    v = int(getattr(obs, "v", 0))
                    self.union(u, v)
                    self._add_member(self._find(u), u)
                    self._add_member(self._find(v), v)
                elif k == "edge_off":
                    u = int(getattr(obs, "u", 0))
                    v = int(getattr(obs, "v", 0))
                    self.mark_dirty(u, v)
            except Exception:
                continue

    def components_count(self) -> int:
        return sum(1 for n, p in self.parent.items() if n == p)

    def sample_indices(self, component_id: int, k: int) -> List[int]:
        """
        Sample up to k indices from the component containing 'component_id'.
        """
        if k <= 0:
            return []
        if component_id not in self.parent:
            return []
        r = self._find(int(component_id))
        head = self._head.get(r, [])
        return head[: int(k)]

    def sample_any(self, k: int) -> List[int]:
        """
        Sample up to k indices across largest components (bounded heads).
        """
        if k <= 0:
            return []
        # sort roots by UF size desc (bounded to number of heads)
        roots = list(self._head.keys())
        roots.sort(key=lambda r: self.size.get(r, len(self._head.get(r, []))), reverse=True)
        out: List[int] = []
        for r in roots:
            if len(out) >= k:
                break
            head = self._head.get(r, [])
            for n in head:
                out.append(n)
                if len(out) >= k:
                    break
        return out]]></content>
    </file>
    <file>
      <path>core/sie_v2.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
# sie_v2.py
# Void-faithful per-tick intrinsic drive computed directly from W and dW
# Produces a per-neuron reward vector and a smooth scalar valence in [0,1]
from __future__ import annotations
import math
import numpy as np
from dataclasses import dataclass

@dataclass
class SIECfg:
    td_w: float = 0.50
    nov_w: float = 0.20
    hab_w: float = 0.10
    hsi_w: float = 0.20
    half_life_ticks: int = 600     # EMA half-life for habituation stats
    target_var: float = 0.10       # desired variance of |dW|
    reward_clip: float = 1.0
    valence_beta: float = 0.30     # smoothing for scalar valence

class SIEState:
    def __init__(self, N: int, cfg: SIECfg):
        self.cfg = cfg
        self.mu = np.zeros(N, dtype=np.float32)   # EMA mean of |dW|
        self.var = np.zeros(N, dtype=np.float32)  # EMA var of |dW|
        self.prev_W = np.zeros(N, dtype=np.float32)
        self.valence = 0.0

def _ema_update(old: np.ndarray, new: np.ndarray, half_life_ticks: int) -> np.ndarray:
    # Convert half-life to per-tick EMA alpha
    a = 1.0 - math.exp(math.log(0.5) / float(max(1, int(half_life_ticks))))
    return (1.0 - a) * old + a * new

def _novelty_norm(spike_mag: np.ndarray) -> np.ndarray:
    m = float(np.max(spike_mag)) if spike_mag.size else 0.0
    if m <= 1e-12:
        return np.zeros_like(spike_mag, dtype=np.float32)
    return (spike_mag / m).astype(np.float32)

def _hsi_norm(mu: np.ndarray, var: np.ndarray, target_var: float) -> np.ndarray:
    # mean term high when mu ~ 0.5 after implicit normalization (here we use |dW| EMA; bias toward mid-range activity)
    mean_term = 1.0 - np.minimum(1.0, np.abs(mu - 0.5) * 2.0)
    # variance term high when var close to target_var
    tv = max(1e-8, float(target_var))
    var_term = 1.0 - np.minimum(1.0, np.abs(var - tv) / tv)
    return (0.5 * (mean_term + var_term)).astype(np.float32)

def sie_step(state: SIEState, W: np.ndarray, dW: np.ndarray):
    """
    Compute per-neuron reward and smooth scalar valence:
    - novelty from |dW|
    - habituation via EMA(mu,var) of |dW|
    - TD from W - γ·prev_W (γ≈0.99), normalized
    - HSI from closeness of (mu,var) to (0.5, target_var)
    Returns:
        (reward_vec: np.ndarray[float32], valence_01: float)
    """
    cfg = state.cfg
    spikes = np.abs(dW).astype(np.float32)

    # Update habituation statistics (EMA mean/var of |dW|)
    mu_new = _ema_update(state.mu, spikes, cfg.half_life_ticks)
    diff = spikes - mu_new
    var_new = _ema_update(state.var, diff * diff, cfg.half_life_ticks)
    state.mu = mu_new.astype(np.float32)
    state.var = var_new.astype(np.float32)

    nov = _novelty_norm(spikes)
    hab = state.mu

    # TD on field with light discount, normalized by max |td|
    td = (W.astype(np.float32) - 0.99 * state.prev_W.astype(np.float32))
    mtd = float(np.max(np.abs(td))) if td.size else 0.0
    if mtd > 1e-12:
        td = (td / mtd).astype(np.float32)
    else:
        td = np.zeros_like(td, dtype=np.float32)
    state.prev_W = W.astype(np.float32)

    # HSI stability indicator
    stab = _hsi_norm(state.mu, state.var, cfg.target_var)

    # Weighted reward, clipped
    r = (cfg.td_w * td) + (cfg.nov_w * nov) - (cfg.hab_w * hab) + (cfg.hsi_w * stab)
    r = np.clip(r, -cfg.reward_clip, cfg.reward_clip).astype(np.float32)

    # Scalar valence in [0,1], smoothed
    r_bar = float(np.mean(r)) if r.size else 0.0
    v_raw = 0.5 + 0.5 * (r_bar / (cfg.reward_clip + 1e-8))
    state.valence = (1.0 - cfg.valence_beta) * float(state.valence) + cfg.valence_beta * float(v_raw)
    # numerically clip
    state.valence = float(max(0.0, min(1.0, state.valence)))

    return r, state.valence]]></content>
    </file>
    <file>
      <path>core/signals.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Core signals seam (Phase B): stable function-level API for core numeric signals.

Intent:
- Define pure, numeric helpers that outside layers can depend on immediately.
- Initially forward to existing math/state (move-only). No logging/IO/emitters here.
- Safe defaults return 0.0/0 for unavailable signals to preserve behavior.

Rules:
- May import only fum_rt.core.* and numeric libs. Never import fum_rt.io.* or fum_rt.runtime.*.
- These helpers do not mutate external state; they read and derive scalars/dicts.

Migration path:
- Phase C will move incremental/event-driven implementations into core.{cortex,proprioception,neuroplasticity},
  and these wrappers will dispatch to the new implementations while preserving the same signatures.
"""

from typing import Any, Dict, Tuple
from fum_rt.core.metrics import compute_metrics


def _safe_getattr(obj: Any, name: str, default: float = 0.0) -> float:
    try:
        return float(getattr(obj, name))
    except Exception:
        return float(default)


def compute_b1_z(state: Any) -> float:
    """
    Derive b1_z scalar in a behavior-preserving way.

    Priority (non-mutating):
    1) Connectome intrinsic last b1_z if exposed by a detector cache (not guaranteed).
    2) Last computed runtime metrics if available on the 'state' (e.g., Nexus._emit_last_metrics).
    3) Recompute metrics via compute_metrics(connectome) and read 'b1_z' if exposed by runtime stack.
    4) Fallback to 0.0.

    Note: This is a seam; future implementations will obtain b1_z from event-driven reducers in core.
    """
    # 1) connectome-local cache (rare)
    try:
        cz = getattr(getattr(state, "connectome", None), "_last_b1_z", None)
        if cz is not None:
            return float(cz)
    except Exception:
        pass

    # 2) runtime snapshot cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "b1_z" in m:
            return float(m.get("b1_z", 0.0))
    except Exception:
        pass

    # 3) recompute metrics and read b1_z if runtime contributes it
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            return float(m2.get("b1_z", 0.0))
    except Exception:
        pass

    # 4) default
    return 0.0


def sie_valence(state: Any, dstate: Any = None) -> float:
    """
    Derive valence scalar in [0,1] using current prioritized sources:

    Priority:
    1) Connectome intrinsic SIE v2 snapshot (preferred): connectome._last_sie2_valence
    2) Runtime SieEngine legacy valence if exposed via last metrics or engine
    3) compute_metrics(connectome) field: 'sie_v2_valence_01' or 'sie_valence_01'
    4) Fallback 0.0

    This is read-only and does not alter SIE internals.
    """
    # 1) intrinsic v2
    try:
        v2 = getattr(getattr(state, "connectome", None), "_last_sie2_valence", None)
        if v2 is not None:
            return float(v2)
    except Exception:
        pass

    # 2) runtime last metrics cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict):
            if "sie_v2_valence_01" in m:
                return float(m.get("sie_v2_valence_01", 0.0))
            if "sie_valence_01" in m:
                return float(m.get("sie_valence_01", 0.0))
    except Exception:
        pass

    # 3) recompute metrics
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            if "sie_v2_valence_01" in m2:
                return float(m2.get("sie_v2_valence_01", 0.0))
            return float(m2.get("sie_valence_01", 0.0))
    except Exception:
        pass

    return 0.0


def compute_cohesion(state: Any) -> int:
    """
    Compute/derive cohesion_components (approximate number of connected components
    in active subgraph, as defined by the current runtime metrics layer).

    Priority:
    1) Use last metrics cache when present.
    2) Recompute via compute_metrics(connectome).
    3) Fallback 0.
    """
    # 1) cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "cohesion_components" in m:
            return int(m.get("cohesion_components", 0))
    except Exception:
        pass

    # 2) recompute
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            return int(m2.get("cohesion_components", 0))
    except Exception:
        pass

    return 0


def compute_vt_metrics(state: Any) -> Tuple[float, float]:
    """
    Derive (vt_coverage, vt_entropy).

    Priority:
    1) Last metrics cache if present on state
    2) compute_metrics(connectome)
    3) Fallback (0.0, 0.0)
    """
    # 1) cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict):
            if "vt_coverage" in m or "vt_entropy" in m:
                cov = float(m.get("vt_coverage", 0.0))
                ent = float(m.get("vt_entropy", 0.0))
                return (cov, ent)
    except Exception:
        pass

    # 2) recompute
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            cov = float(m2.get("vt_coverage", 0.0))
            ent = float(m2.get("vt_entropy", 0.0))
            return (cov, ent)
    except Exception:
        pass

    # 3) default
    return (0.0, 0.0)


def snapshot_numbers(state: Any) -> Dict[str, float]:
    """
    Convenience aggregator that composes the core snapshot dictionary expected
    by the runtime telemetry seam. Non-intrusive and read-only.

    Returns:
      {
        "b1_z": float, "vt_coverage": float, "vt_entropy": float,
        "cohesion_components": int, "sie_valence_01": float, "sie_v2_valence_01": float
      }
    """
    cov, ent = compute_vt_metrics(state)
    # Gather as many values as are cheaply available
    out: Dict[str, float] = {
        "b1_z": float(compute_b1_z(state)),
        "vt_coverage": float(cov),
        "vt_entropy": float(ent),
        "cohesion_components": float(compute_cohesion(state)),
        "sie_valence_01": 0.0,
        "sie_v2_valence_01": 0.0,
    }
    # attempt to fill valence fields
    try:
        v2 = sie_valence(state)
        out["sie_v2_valence_01"] = float(v2)
    except Exception:
        pass
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "sie_valence_01" in m:
            out["sie_valence_01"] = float(m.get("sie_valence_01", 0.0))
    except Exception:
        pass
    return out


def apply_b1_detector(state: Any, metrics: Dict[str, Any], step: int) -> Dict[str, Any]:
    """
    Behavior-preserving B1 detector update using state.b1_detector.
    Mutates metrics in place; returns metrics for convenience.

    This seam delegates to the existing StreamingZEMA instance configured in runtime (Nexus.b1_detector),
    avoiding any duplication of detector parameters and preserving gating behavior.
    """
    m = metrics if isinstance(metrics, dict) else {}
    try:
        b1_value = float(m.get("complexity_cycles", 0.0))
    except Exception:
        b1_value = 0.0
    try:
        det = getattr(state, "b1_detector", None)
        if det is not None:
            z = det.update(b1_value, tick=int(step))
            m["b1_value"] = float(z.get("value", 0.0))
            m["b1_delta"] = float(z.get("delta", 0.0))
            m["b1_z"] = float(z.get("z", 0.0))
            m["b1_spike"] = bool(z.get("spike", False))
    except Exception:
        # Leave metrics unchanged on failure
        pass
    return m


def compute_active_edge_density(connectome: Any, N: int) -> Tuple[int, float]:
    """
    Compute undirected active-edge density and return (E, density).

    Mirrors Nexus logic (behavior-preserving):
      E = max(0, active_edge_count)
      N = max(1, N)
      density = 2*E / (N*(N-1)) if denom > 0 else 0
    """
    try:
        E = max(0, int(connectome.active_edge_count()))
        Nn = max(1, int(N))
        denom = float(Nn * (Nn - 1))
        density = (2.0 * E / denom) if denom > 0.0 else 0.0
        return int(E), float(density)
    except Exception:
        return 0, 0.0


def compute_td_signal(prev_E: int | None, E: int, vt_prev: float | None = None, vt_last: float | None = None) -> float:
    """
    Compute TD-like signal combining structural change (delta_e) and traversal entropy change (vt_delta).

    Behavior-preserving mapping from Nexus:
      delta_e  = (E - prev_E) / max(1, E)                  # prev_E defaults to E on first use → 0
      vt_delta = 0.0 if missing else (vt_last - vt_prev)
      td_raw   = 4.0*delta_e + 1.5*vt_delta
      td       = clip(td_raw, -2.0,  2.0)
    """
    try:
        E_int = int(E)
        pE = E_int if prev_E is None else int(prev_E)
        delta_e = float(E_int - pE) / float(max(1, E_int))
    except Exception:
        delta_e = 0.0

    try:
        if vt_prev is None or vt_last is None:
            vt_delta = 0.0
        else:
            vt_delta = float(vt_last) - float(vt_prev)
    except Exception:
        vt_delta = 0.0

    td_raw = 4.0 * float(delta_e) + 1.5 * float(vt_delta)
    if td_raw > 2.0:
        return 2.0
    if td_raw < -2.0:
        return -2.0
    return float(td_raw)


def compute_firing_var(connectome: Any) -> float | None:
    """
    Compute variance of the field W; None on failure.
    """
    try:
        return float(connectome.W.var())
    except Exception:
        return None


__all__ = [
    "compute_b1_z",
    "sie_valence",
    "compute_cohesion",
    "compute_vt_metrics",
    "snapshot_numbers",
    "apply_b1_detector",
    "compute_active_edge_density",
    "compute_td_signal",
    "compute_firing_var",
]]]></content>
    </file>
    <file>
      <path>core/sparse_connectome.py</path>
      <content><![CDATA[# sparse_connectome.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.


Void-faithful sparse connectome for ultra-scale runs.

- Adjacency stored as neighbor lists (list[np.ndarray[int32]]) with symmetric edges
- No dense NxN matrices; all metrics computed by streaming over adjacency
- Traversal and measuring use void equations (Rule: use void equations for traversal/measuring)
- Stage-1 cohesion measured on topology-only adjacency (A_sparse)
- Active subgraph for cycle/entropy uses implicit edge weight W[i]*W[j] > threshold

API mirrors Connectome sufficiently for Nexus and metrics:
    - step(t, domain_modulation, sie_drive=1.0, use_time_dynamics=True)
    - active_edge_count()
    - connected_components()
    - cyclomatic_complexity()
    - snapshot_graph()  (safe for small N)
    - connectome_entropy()  (preferred by metrics if present)

Note: Stage‑1 healing/pruning here omits dense S_ij bridging to avoid NxN;
      bridging logic is already executed upstream in dense Connectome. For sparse,
      we rely on void‑guided rewiring each tick to fuse components (top‑k by S_ij).
"""

from __future__ import annotations
import numpy as np
import networkx as nx
from typing import List, Set
import os as _os
from .void_dynamics_adapter import universal_void_dynamics, delta_re_vgsp, delta_gdsp
from .announce import Observation


from .primitives.dsu import DSU as _DSU


class SparseConnectome:
    def __init__(self, N: int, k: int, seed: int = 0,
                 threshold: float = 0.15, lambda_omega: float = 0.1,
                 candidates: int = 64, structural_mode: str = "alias",
                 traversal_walkers: int = 256, traversal_hops: int = 3,
                 bundle_size: int = 3, prune_factor: float = 0.10):
        self.N = int(N)
        self.k = int(k)
        self.rng = np.random.default_rng(seed)
        self.threshold = float(threshold)
        self.lambda_omega = float(lambda_omega)
        self.candidates = int(max(1, candidates))
        self.structural_mode = structural_mode  # reserved; alias path used by default

        # Node state
        self.W = self.rng.uniform(0.0, 1.0, size=(self.N,)).astype(np.float32)

        # Sparse symmetric topology as neighbor lists (arrays of int32)
        self.adj: List[np.ndarray] = [np.zeros(0, dtype=np.int32) for _ in range(self.N)]

        # Traversal config
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))

        # Findings each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Homeostasis tuning (mirrors dense backend; currently stored for parity)
        self.bundle_size = int(max(1, bundle_size))
        self.prune_factor = float(max(0.0, prune_factor))
        # Local tick counter for announcement timestamps
        self._tick = 0
        # External stimulation buffer (deterministic symbol→group; decays each tick)
        self._stim = np.zeros(self.N, dtype=np.float32)
        self._stim_decay = 0.90
        # Active-edge/fragment trackers (incremental; void-faithful)
        self._edges_active = 0
        self._vertices_active = 0
        self._last_edges_active = 0
        self._last_vertices_active = 0
        self._frag_dsu = _DSU(self.N)
        self._frag_components_lb = self.N
        self._frag_dirty_since = None

    # --- Alias sampler (Vose) to sample candidates ~ ReLU(Δalpha) in O(N) build + O(1) draw ---
    def _build_alias(self, p: np.ndarray):
        n = p.size
        if n == 0:
            return np.array([], dtype=np.float32), np.array([], dtype=np.int32)
        p = p.astype(np.float64, copy=False)
        s = float(p.sum())
        if s <= 0:
            p = np.full(n, 1.0 / n, dtype=np.float64)
        else:
            p = p / s
        prob = np.zeros(n, dtype=np.float64)
        alias = np.zeros(n, dtype=np.int32)
        scaled = p * n
        small = [i for i, v in enumerate(scaled) if v < 1.0]
        large = [i for i, v in enumerate(scaled) if v >= 1.0]
        while small and large:
            s_idx = small.pop()
            l_idx = large.pop()
            prob[s_idx] = scaled[s_idx]
            alias[s_idx] = l_idx
            scaled[l_idx] = scaled[l_idx] - (1.0 - prob[s_idx])
            if scaled[l_idx] < 1.0:
                small.append(l_idx)
            else:
                large.append(l_idx)
        for i in large:
            prob[i] = 1.0
        for i in small:
            prob[i] = 1.0
        return prob.astype(np.float32), alias

    def _alias_draw(self, prob: np.ndarray, alias: np.ndarray, s: int):
        n = prob.size
        if n == 0 or s <= 0:
            return np.array([], dtype=np.int64)
        k = self.rng.integers(0, n, size=s, endpoint=False)
        u = self.rng.random(s)
        choose_alias = (u >= prob[k])
        out = k.copy()
        out[choose_alias] = alias[k[choose_alias]]
        return out.astype(np.int64)

    def stimulate_indices(self, idxs, amp: float = 0.05):
        """
        Deterministic stimulus injection for sparse backend:
        - idxs: iterable of neuron indices to stimulate
        - amp: additive boost to the stimulus buffer (decays each tick)
        """
        try:
            if idxs is None:
                return
            arr = np.asarray(list(set(int(i) % self.N for i in idxs)), dtype=np.int64)
            if arr.size == 0:
                return
            self._stim[arr] = np.clip(self._stim[arr] + float(amp), 0.0, 1.0)
            # small immediate bump to W to seed associations
            self.W[arr] = np.clip(self.W[arr] + 0.01 * float(amp), 0.0, 1.0)
        except Exception:
            pass

    def _void_traverse(self, a: np.ndarray, om: np.ndarray):
        """
        Continuous void‑equation traversal on sparse graph (neighbor lists).
        Seeds ~ ReLU(Δalpha). Transition weight to neighbor j: max(0, a[i]*a[j] - λ*|ω_i-ω_j|).

        Also publishes compact Observation events to the ADC bus if present.
        """
        N = self.N
        walkers = self.traversal_walkers
        hops = self.traversal_hops

        prob, alias = self._build_alias(a)
        seeds = self._alias_draw(prob, alias, walkers)
        visit = np.zeros(N, dtype=np.int32)

        # Optional ADC bus and tick
        bus = getattr(self, "bus", None)
        tick = int(getattr(self, "_tick", 0))

        # Event accumulators
        sample_cap = 64
        sel_w_sum = 0.0
        sel_steps = 0
        sample_nodes = set()

        for s in seeds:
            cur = int(s)
            seen = {cur: 0}
            path = [cur]
            for step_idx in range(1, hops + 1):
                nbrs = self.adj[cur]
                if nbrs.size == 0:
                    break
                w = a[cur] * a[nbrs] - self.lambda_omega * np.abs(om[cur] - om[nbrs])
                w = np.clip(w, 0.0, None)
                if np.all(w <= 0):
                    break
                wp = w / (w.sum() + 1e-12)
                r = self.rng.random()
                cdf = np.cumsum(wp)
                idx = int(np.searchsorted(cdf, r, side="right"))
                nxt = int(nbrs[min(idx, nbrs.size - 1)])
                visit[nxt] += 1

                # accumulate simple local stats
                sel_w = float(w[min(idx, nbrs.size - 1)])
                sel_w_sum += max(0.0, sel_w)
                sel_steps += 1
                if len(sample_nodes) < sample_cap:
                    sample_nodes.add(nxt)

                # simple loop detection on this walk
                if nxt in seen:
                    if bus is not None:
                        try:
                            obs = Observation(
                                tick=tick,
                                kind="cycle_hit",
                                nodes=[cur, nxt],
                                w_mean=float(a.mean()),
                                w_var=float(a.var()),
                                s_mean=0.0,
                                loop_len=int(len(path) - seen[nxt] + 1),
                                loop_gain=float(sel_w),
                                coverage_id=0,
                                domain_hint=""
                            )
                            bus.publish(obs)
                        except Exception:
                            pass
                else:
                    seen[nxt] = step_idx
                    path.append(nxt)

                cur = nxt

        total_visits = int(visit.sum())
        unique = int(np.count_nonzero(visit))
        coverage = float(unique) / float(max(1, N))
        if total_visits > 0:
            p = visit.astype(np.float64) / float(total_visits)
            p = p[p > 0]
            vt_entropy = float(-(p * np.log(p)).sum())
        else:
            vt_entropy = 0.0

        self.findings = {
            "vt_visits": total_visits,
            "vt_unique": unique,
            "vt_coverage": coverage,
            "vt_entropy": vt_entropy,
            "vt_walkers": float(walkers),
            "vt_hops": float(hops),
            "a_mean": float(a.mean()),
            "omega_mean": float(om.mean()),
        }

        # publish a compact region_stat
        if bus is not None:
            try:
                s_mean = float(sel_w_sum / max(1, sel_steps))
                cov_id = int(min(9, max(0, int(coverage * 10.0))))
                obs = Observation(
                    tick=tick,
                    kind="region_stat",
                    nodes=list(sample_nodes),
                    w_mean=float(self.W.mean()),
                    w_var=float(self.W.var()),
                    s_mean=s_mean,
                    coverage_id=cov_id,
                    domain_hint=""
                )
                bus.publish(obs)
            except Exception:
                pass

    def step(self, t: float, domain_modulation: float, sie_drive: float = 1.0, use_time_dynamics: bool = True):
        """
        Sparse, void‑faithful tick:
        - Compute Δalpha/Δomega by void equations
        - Build per-node candidate list via alias sampler ~ ReLU(Δalpha)
        - Score candidates by S_ij = ReLU(Δα_i)·ReLU(Δα_j) - λ·|Δω_i - Δω_j|
        - Take symmetric top‑k neighbors (undirected)
        - Update node field with universal_void_dynamics gated by SIE valence
        - Run traversal to publish vt_* findings
        """
        # 1) Elemental deltas from void equations
        d_alpha = delta_re_vgsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        d_omega = delta_gdsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        a = np.maximum(0.0, d_alpha.astype(np.float32))
        om = d_omega.astype(np.float32)
        # External stimulation: add and decay deterministic symbol→group drive
        try:
            a = np.clip(a + self._stim, 0.0, None)
            self._stim *= getattr(self, "_stim_decay", 0.90)
        except Exception:
            pass

        # 2) Candidate sampler ~ a
        prob, alias = self._build_alias(a)

        # 3) Per-node top-k selection from candidates by void affinity S_ij
        N = self.N
        k = int(max(1, self.k))
        s = int(max(self.candidates, 2 * k))
        neigh_sets: List[Set[int]] = [set() for _ in range(N)]

        for i in range(N):
            js = self._alias_draw(prob, alias, s)
            if js.size == 0:
                continue
            js = js[js != i]
            if js.size == 0:
                continue
            js = np.unique(js)
            Si = a[i] * a[js] - self.lambda_omega * np.abs(om[i] - om[js])
            take = min(k, Si.size)
            if take <= 0:
                continue
            idx = np.argpartition(Si, -take)[-take:]
            nbrs = js[idx]
            for j in nbrs:
                neigh_sets[i].add(int(j))

        # Undirected symmetrization
        for i in range(N):
            for j in neigh_sets[i]:
                neigh_sets[j].add(i)

        # Sparse structural maintenance (adaptive pruning, lightweight)
        # Rationale:
        # - In sparse mode, adjacency is rebuilt each tick; to expose real pruning dynamics and
        #   avoid permanent over-connection, we drop edges whose implicit weight |W_i*W_j| is
        #   below prune_factor * mean(|W_i*W_j|) over current edges.
        # - This keeps complexity bounded and allows components to split when pathologies exist.
        try:
            # Collect undirected effective weights for current edges
            weights = []
            for i in range(N):
                wi = float(self.W[i])
                for j in neigh_sets[i]:
                    jj = int(j)
                    if jj <= i:
                        continue
                    weights.append(abs(wi * float(self.W[jj])))

            pruned_pairs = 0
            if weights:
                mean_w = float(np.mean(np.asarray(weights, dtype=np.float64)))
                prune_factor = float(getattr(self, "prune_factor", 0.10))
                prune_threshold = (prune_factor * mean_w) if mean_w > 0.0 else 0.0
                if prune_threshold > 0.0:
                    # Remove edges below adaptive threshold; maintain undirected symmetry
                    for i in range(N):
                        wi = float(self.W[i])
                        # collect first to avoid mutating set during iteration
                        to_remove = []
                        for j in neigh_sets[i]:
                            jj = int(j)
                            if jj <= i:
                                continue
                            wij = abs(wi * float(self.W[jj]))
                            if wij < prune_threshold:
                                to_remove.append(jj)
                        for jj in to_remove:
                            if jj in neigh_sets[i]:
                                neigh_sets[i].remove(jj)
                            if i in neigh_sets[jj]:
                                neigh_sets[jj].remove(i)
                                pruned_pairs += 1
            # Expose pruning stats for diagnostics (undirected pairs)
            try:
                setattr(self, "_last_pruned_count", int(pruned_pairs))
            except Exception:
                pass
            # No bridging in sparse maintenance (keep light); report zero bridged edges
            try:
                setattr(self, "_last_bridged_count", 0)
            except Exception:
                pass
        except Exception:
            # Fail-soft to preserve runtime continuity
            pass

        # --- Sparse cohesion bridging (event-driven, budgeted, no scans) ---
        # Goal: when multiple components exist, propose up to B symmetric bridges using the
        # same void-affinity sampler used for growth. This keeps dynamics lively (cycles/components)
        # without any NxN work. Budget defaults to 8 per tick; can be tuned via instance attribute.
        try:
            # Use frag tracker lower-bound components (active graph), avoid structural scans
            comp_count = int(getattr(self, "_frag_components_lb", 1))
            _dirty = getattr(self, "_frag_dirty_since", None)
            # Optionally early-audit with a small budget to refresh active components
            try:
                _budget = int(_os.getenv("FRAG_AUDIT_EDGES", "200000"))
            except Exception:
                _budget = 200000
            try:
                if _dirty is not None and _budget > 0:
                    # Best-effort refresh of active components (bounded)
                    # This calls an internal audit that streams over up to _budget active edges.
                    self._maybe_audit_frag(int(_budget))
                    comp_count = int(getattr(self, "_frag_components_lb", comp_count))
            except Exception:
                pass
            dsu = getattr(self, "_frag_dsu", _DSU(N))

            bridged_pairs = 0
            if comp_count > 1:
                B = int(getattr(self, "bridge_budget", 8))
                B = max(0, B)
                if B > 0:
                    # Use alias sampler (a) to pick candidate endpoints cheaply
                    attempts = 0
                    max_attempts = int(max(32, B * 64))
                    while bridged_pairs < B and attempts < max_attempts:
                        attempts += 1
                        ui = self._alias_draw(prob, alias, 1)
                        vi = self._alias_draw(prob, alias, 1)
                        if ui.size == 0 or vi.size == 0:
                            continue
                        u = int(ui[0]); v = int(vi[0])
                        if u == v:
                            continue
                        # Skip if already adjacent
                        if (v in neigh_sets[u]) or (u in neigh_sets[v]):
                            continue
                        # Bridge only across distinct components
                        if dsu.find(u) == dsu.find(v):
                            continue
                        # Score by void affinity; require positive support
                        s_uv = float(a[u] * a[v] - self.lambda_omega * abs(om[u] - om[v]))
                        if s_uv <= 0.0:
                            continue
                        # Add symmetric bridge and union components
                        neigh_sets[u].add(v)
                        neigh_sets[v].add(u)
                        dsu.union(u, v)
                        try:
                            self._frag_dsu = dsu
                            if int(getattr(self, "_frag_components_lb", 1)) > 1:
                                self._frag_components_lb = int(self._frag_components_lb) - 1
                            self._frag_dirty_since = None
                        except Exception:
                            pass
                        bridged_pairs += 1
            # Expose bridged count for diagnostics
            try:
                setattr(self, "_last_bridged_count", int(bridged_pairs))
            except Exception:
                pass
        except Exception:
            # Fail-soft for bridging; keep prior counters if present
            try:
                _ = int(getattr(self, "_last_bridged_count", 0))
            except Exception:
                pass

        # Freeze adjacency
        self.adj = [np.fromiter(sorted(s), dtype=np.int32) if s else np.zeros(0, dtype=np.int32) for s in neigh_sets]

        # 4) Node field update via universal void dynamics, gated by SIE valence in [0,1]
        dW = universal_void_dynamics(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        gate = float(max(0.0, min(1.0, sie_drive)))
        dW_eff = gate * dW
        self.W = np.clip(self.W + dW_eff, 0.0, 1.0).astype(np.float32)

        # 4.1) SIE v2 intrinsic reward/valence from W and dW (void-native)
        try:
            if not hasattr(self, "_sie2"):
                from .sie_v2 import SIECfg, SIEState
                self._sie2 = SIEState(self.N, SIECfg())
            from .sie_v2 import sie_step
            r_vec, v01 = sie_step(self._sie2, self.W, dW_eff)
            self._last_sie2_reward = float(np.mean(r_vec))
            self._last_sie2_valence = float(v01)
        except Exception:
            pass

        # 4.2) Active-edge counters and frag tracker (void-faithful, streaming)
        try:
            E_new = 0
            _seen = set()
            for (ii, jj) in self._active_edge_iter():
                E_new += 1
                _seen.add(int(ii)); _seen.add(int(jj))
            V_new = int(len(_seen))
            # mark dirty on edge-off (E decreased)
            try:
                if int(getattr(self, "_edges_active", 0)) > int(E_new):
                    if getattr(self, "_frag_dirty_since", None) is None:
                        self._frag_dirty_since = int(getattr(self, "_tick", 0))
            except Exception:
                pass
            self._last_edges_active = int(getattr(self, "_edges_active", 0))
            self._last_vertices_active = int(getattr(self, "_vertices_active", 0))
            self._edges_active = int(E_new)
            self._vertices_active = int(V_new)
            # optional budgeted audit
            try:
                _audit_every = int(_os.getenv("FRAG_AUDIT_EVERY", "50"))
            except Exception:
                _audit_every = 50
            try:
                _budget = int(_os.getenv("FRAG_AUDIT_EDGES", "200000"))
            except Exception:
                _budget = 200000
            if (getattr(self, "_frag_dirty_since", None) is not None or (int(getattr(self, "_tick", 0)) % max(1, _audit_every) == 0)) and _budget > 0:
                self._maybe_audit_frag(int(_budget))
            # cycles estimate
            try:
                comp_lb = int(getattr(self, "_frag_components_lb", 1))
            except Exception:
                comp_lb = 1
            cycles_est = int(max(0, int(E_new) - int(V_new) + int(comp_lb)))
            # augment findings
            try:
                self.findings.update({
                    "edges_active": int(E_new),
                    "vertices_active": int(V_new),
                    "components_lb": int(comp_lb),
                    "frag_dirty_age": int((int(getattr(self, "_tick", 0)) - int(getattr(self, "_frag_dirty_since", 0)))) if getattr(self, "_frag_dirty_since", None) is not None else 0,
                    "cycles_est": int(cycles_est),
                })
            except Exception:
                pass
        except Exception:
            pass

        # 5) Continuous traversal to emit vt_* findings
        try:
            self._void_traverse(a, om)
        except Exception:
            pass

        # increment local tick for announcement timestamps
        try:
            self._tick += 1
        except Exception:
            pass

    def _active_edge_iter(self):
        """Yield undirected edges (i, j) with i < j whose implicit weight is active."""
        th = self.threshold
        W = self.W
        for i in range(self.N):
            wi = float(W[i])
            nbrs = self.adj[i]
            if nbrs.size == 0:
                continue
            for j in nbrs:
                j = int(j)
                if j <= i:
                    continue
                if (wi * float(W[j])) > th:
                    yield (i, j)

    def _maybe_audit_frag(self, budget_edges: int) -> None:
        """
        Budgeted active-fragment audit (void-faithful):
        - Rebuild a DSU over ACTIVE edges only, streaming via _active_edge_iter().
        - Only processes up to 'budget_edges' edges; computes a lower-bound component count
          across the 'seen' active vertices.
        - Clears the dirty flag only when processing completes before budget exhaust.
        """
        try:
            N = int(self.N)
            dsu = _DSU(N)
            seen: set[int] = set()
            processed = 0
            b = int(max(0, int(budget_edges)))
            for (i, j) in self._active_edge_iter():
                ii = int(i); jj = int(j)
                dsu.union(ii, jj)
                seen.add(ii); seen.add(jj)
                processed += 1
                if b > 0 and processed >= b:
                    break
            if seen:
                roots = set(int(dsu.find(idx)) for idx in seen)
                comp_lb = int(len(roots))
            else:
                # No active vertices observed → treat as fully fragmented across N nodes
                comp_lb = N
            # Update trackers
            self._frag_dsu = dsu
            self._frag_components_lb = int(comp_lb)
            # Clear dirty flag only if we did not exhaust budget
            if not (b > 0 and processed >= b):
                self._frag_dirty_since = None
        except Exception:
            # Fail-soft: keep previous DSU/lower-bound
            pass

    def active_edge_count(self) -> int:
        return sum(1 for _ in self._active_edge_iter())

    def connected_components(self) -> int:
        """Active-subgraph components (Stage‑1 cohesion) over active edges only."""
        dsu = _DSU(self.N)
        e_active = 0
        act_nodes = set()
        for (i, j) in self._active_edge_iter():
            dsu.union(i, j)
            e_active += 1
            act_nodes.add(int(i))
            act_nodes.add(int(j))
        return (len(set(int(dsu.find(idx)) for idx in act_nodes)) if e_active > 0 else self.N)

    def cyclomatic_complexity(self) -> int:
        """
        Active-subgraph cyclomatic complexity: cycles = E_active - N + C_active
        where unions are formed only by active edges (W[i]*W[j] > threshold).
        """
        dsu = _DSU(self.N)
        e_active = 0
        act_nodes = set()
        for (i, j) in self._active_edge_iter():
            dsu.union(i, j)
            e_active += 1
            act_nodes.add(int(i))
            act_nodes.add(int(j))
        c_active = (len(set(int(dsu.find(idx)) for idx in act_nodes)) if e_active > 0 else self.N)
        cycles = e_active - self.N + c_active
        return int(max(0, cycles))

    def snapshot_graph(self):
        """
        Build a NetworkX graph of the active subgraph for visualization.
        Guarded for scale: returns empty graph if N is large.
        """
        if self.N > 5000:
            return nx.Graph()
        G = nx.Graph()
        G.add_nodes_from(range(self.N))
        for (i, j) in self._active_edge_iter():
            G.add_edge(int(i), int(j))
        return G

    def get_phi(self, i: int) -> float:
        """O(1) local read of fast field φ at node i; returns 0.0 when absent."""
        try:
            phi = getattr(self, "phi", None)
            if phi is None:
                return 0.0
            return float(phi[int(i)])
        except Exception:
            return 0.0

    def get_memory(self, i: int) -> float:
        """O(1) local read of slow memory m at node i via attached field/map when present."""
        # Prefer an attached MemoryField
        try:
            mf = getattr(self, "_memory_field", None)
            if mf is not None and hasattr(mf, "get_m"):
                return float(mf.get_m(int(i)))
        except Exception:
            pass
        # Fallback to attached MemoryMap adapter
        try:
            mm = getattr(self, "_memory_map", None)
            if mm is not None:
                fld = getattr(mm, "field", None)
                if fld is not None and hasattr(fld, "get_m"):
                    return float(fld.get_m(int(i)))
                dct = getattr(mm, "_m", None)
                if isinstance(dct, dict):
                    return float(dct.get(int(i), 0.0))
        except Exception:
            pass
        return 0.0

    def connectome_entropy(self) -> float:
        """
        Global pathological structure metric on the active subgraph.
        H = -Σ p_i log p_i where p_i proportional to degree(i) in active subgraph.
        """
        deg = np.zeros(self.N, dtype=np.int64)
        for i in range(self.N):
            wi = float(self.W[i])
            cnt = 0
            for j in self.adj[i]:
                j = int(j)
                if (wi * float(self.W[j])) > self.threshold:
                    cnt += 1
            deg[i] = cnt
        total = int(deg.sum())
        if total <= 0:
            return 0.0
        p = deg.astype(np.float64) / float(total)
        p = np.clip(p, 1e-12, 1.0)
        return float(-(p * np.log(p)).sum())]]></content>
    </file>
    <file>
      <path>core/substrate/README.md</path>
      <content/>
    </file>
    <file>
      <path>core/substrate/growth_arbiter.py</path>
      <content><![CDATA[# fum_rt/core/substrate/growth_arbiter.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module contains the GrowthArbiter, a class responsible for deciding
when and *how much* to grow the network. It implements the "super saturation" 
and "void debt" principles, where growth is triggered by stability and the 
amount of growth is determined by accumulated system pressure.
"""
import numpy as np
from collections import deque

class GrowthArbiter:
    """
    Monitors network metrics to decide when and how much to grow.
    """
    def __init__(self, stability_window=10, trend_threshold=0.001, debt_growth_factor=0.1):
        """
        Initializes the GrowthArbiter.

        Args:
            stability_window (int): Number of recent steps to check for stability.
            trend_threshold (float): Max change for a metric to be "flat."
            debt_growth_factor(float): Scales accumulated debt to number of neurons.
        """
        self.stability_window = stability_window
        self.trend_threshold = trend_threshold
        self.debt_growth_factor = debt_growth_factor

        self.weight_history = deque(maxlen=stability_window)
        self.synapse_history = deque(maxlen=stability_window)
        self.complexity_history = deque(maxlen=stability_window)
        self.cohesion_history = deque(maxlen=stability_window)
        
        self.is_stable = False
        self.void_debt_accumulator = 0.0

    def update_metrics(self, metrics):
        """
        Updates the historical metrics and checks for system stability.
        
        Args:
            metrics (dict): A dictionary containing the latest network metrics.
        """
        self.weight_history.append(metrics.get('avg_weight', 0))
        self.synapse_history.append(metrics.get('active_synapses', 0))
        self.complexity_history.append(metrics.get('total_b1_persistence', 0))
        self.cohesion_history.append(metrics.get('cluster_count', -1))

        if len(self.weight_history) < self.stability_window:
            self.is_stable = False
            return

        is_cohesive = all(count == 1 for count in self.cohesion_history)
        is_weight_flat = abs(self.weight_history[0] - self.weight_history[-1]) < self.trend_threshold
        is_synapse_flat = abs(self.synapse_history[0] - self.synapse_history[-1]) < 3
        is_complexity_flat = abs(self.complexity_history[0] - self.complexity_history[-1]) < self.trend_threshold

        if is_cohesive and is_weight_flat and is_synapse_flat and is_complexity_flat:
            if not self.is_stable:
                print("\n--- GROWTH ARBITER: System has achieved STABILITY ---")
                print("--- Now accumulating 'void debt' from residual valence. ---")
            self.is_stable = True
        else:
            if self.is_stable:
                print("\n--- GROWTH ARBITER: System has left stable state. Resetting debt.---")
                self.void_debt_accumulator = 0.0 # Reset debt if stability is lost
            self.is_stable = False

    def accumulate_and_check_growth(self, valence_signal):
        """
        If the system is stable, accumulates void debt. If the debt crosses
        a threshold, returns the number of neurons to grow.

        Args:
            valence_signal (float): The residual system pressure signal.

        Returns:
            int: The number of new neurons to add, or 0.
        """
        if not self.is_stable:
            return 0

        self.void_debt_accumulator += abs(valence_signal) # Accumulate pressure

        # Check if the accumulated debt triggers growth
        # We'll use a simple linear threshold for now.
        if self.void_debt_accumulator > 1.0: 
            num_new_neurons = int(np.ceil(self.void_debt_accumulator * self.debt_growth_factor))
            
            print("\n--- GROWTH ARBITER: VOID DEBT THRESHOLD REACHED ---")
            print(f"Accumulated Debt: {self.void_debt_accumulator:.3f}")
            print(f"Triggering organic growth of {num_new_neurons} new neuron(s).")
            print("---------------------------------------------------\n")

            self.void_debt_accumulator = 0.0 # Reset debt after triggering
            self.is_stable = False # System will become unstable after growth, reset
            self.clear_history() # Clear history to re-evaluate stability
            return num_new_neurons

        return 0

    def clear_history(self):
        """Resets all metric histories."""
        self.weight_history.clear()
        self.synapse_history.clear()
        self.complexity_history.clear()
        self.cohesion_history.clear()]]></content>
    </file>
    <file>
      <path>core/substrate/neurogenesis.py</path>
      <content><![CDATA[# fum_rt/core/substrate/neurogenesis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module handles the growth of the FUM substrate, including the addition
of new computational units (neurons) and the formation of their initial
connections based on void dynamics.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix

from Void_Equations import universal_void_dynamics

class Neurogenesis:
    """
    Manages the growth of the Substrate's connectome.
    """
    def __init__(self, seed=42):
        """
        Initializes the growth manager.
        """
        self.rng = np.random.default_rng(seed=seed)

    def grow(self, substrate, num_new_neurons):
        """
        Grows the substrate by expanding the connectome and all associated state arrays.
        This method is designed to be called on a Substrate instance.

        Args:
            substrate: The Substrate instance to modify.
            num_new_neurons (int): The number of new neurons to add.
        """
        if num_new_neurons <= 0:
            return

        old_n = substrate.num_neurons
        new_n = old_n + num_new_neurons
        
        print(f"\n--- NEUROGENESIS: SUBSTRATE GROWTH ---")
        print(f"Adding {num_new_neurons} new neurons to the existing {old_n}.")

        # --- Create new neuron properties on CPU first ---
        new_is_excitatory = self.rng.choice([True, False], num_new_neurons, p=[0.8, 0.2])
        new_tau_m = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_thresh = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_rest = np.full(num_new_neurons, -65.0)
        new_refractory_period = np.full(num_new_neurons, 5.0)
        new_r_mem = np.full(num_new_neurons, 10.0)

        # --- Expand the connectome based on the backend ---
        if substrate.device_type == 'gpu':
            W_cpu = substrate.W.cpu().numpy()
        else:
            W_cpu = substrate.W.toarray() if isinstance(substrate.W, csc_matrix) else substrate.W

        new_W = np.zeros((new_n, new_n))
        new_W[:old_n, :old_n] = W_cpu

        # --- Connect new neurons using Void Dynamics ---
        # 1. Create a potential connection matrix for new neurons (outgoing)
        potential_connections_out = self.rng.random((num_new_neurons, old_n)) * 0.05 
        # 2. Evolve it with void dynamics
        delta_out = universal_void_dynamics(potential_connections_out, substrate.time_step)
        evolved_connections_out = potential_connections_out + delta_out
        # 3. Threshold to form actual connections
        new_connections_out = np.where(evolved_connections_out > 0.01, evolved_connections_out, 0)
        
        # 1. Create a potential connection matrix for new neurons (incoming)
        potential_connections_in = self.rng.random((old_n, num_new_neurons)) * 0.05
        # 2. Evolve it
        delta_in = universal_void_dynamics(potential_connections_in, substrate.time_step)
        evolved_connections_in = potential_connections_in + delta_in
        # 3. Threshold
        new_connections_in = np.where(evolved_connections_in > 0.01, evolved_connections_in, 0)

        # Add new connections to the main matrix
        new_W[old_n:new_n, :old_n] = new_connections_out
        new_W[:old_n, old_n:new_n] = new_connections_in
        
        # --- Handle backend-specific state expansions ---
        if substrate.device_type == 'gpu':
            substrate.W = torch.from_numpy(new_W).float().to(substrate.device)
            substrate.is_excitatory = torch.cat([substrate.is_excitatory, torch.from_numpy(new_is_excitatory).to(substrate.device)])
            substrate.tau_m = torch.cat([substrate.tau_m, torch.from_numpy(new_tau_m).float().to(substrate.device)])
            substrate.v_thresh = torch.cat([substrate.v_thresh, torch.from_numpy(new_v_thresh).float().to(substrate.device)])
            substrate.v_m = torch.cat([substrate.v_m, torch.from_numpy(new_v_rest).float().to(substrate.device)])
            substrate.refractory_time = torch.cat([substrate.refractory_time, torch.zeros(num_new_neurons, device=substrate.device)])
            substrate.refractory_period = torch.cat([substrate.refractory_period, torch.from_numpy(new_refractory_period).float().to(substrate.device)])
            substrate.r_mem = torch.cat([substrate.r_mem, torch.from_numpy(new_r_mem).float().to(substrate.device)])
            substrate.v_reset_tensor = torch.cat([substrate.v_reset_tensor, torch.from_numpy(np.full(num_new_neurons, -70.0)).float().to(substrate.device)])
            substrate.spikes = torch.cat([substrate.spikes, torch.zeros(num_new_neurons, dtype=torch.bool, device=substrate.device)])
        else: # CPU
            substrate.W = csc_matrix(new_W)
            substrate.is_excitatory = np.concatenate([substrate.is_excitatory, new_is_excitatory])
            substrate.tau_m = np.concatenate([substrate.tau_m, new_tau_m])
            substrate.v_thresh = np.concatenate([substrate.v_thresh, new_v_thresh])
            substrate.v_m = np.concatenate([substrate.v_m, new_v_rest])
            substrate.refractory_time = np.concatenate([substrate.refractory_time, np.zeros(num_new_neurons)])
            substrate.refractory_period = np.concatenate([substrate.refractory_period, new_refractory_period])
            substrate.r_mem = np.concatenate([substrate.r_mem, new_r_mem])
            substrate.v_reset = np.concatenate([substrate.v_reset, np.full(num_new_neurons, -70.0)])
            substrate.spikes = np.concatenate([substrate.spikes, np.zeros(num_new_neurons, dtype=bool)])
            substrate.neuron_polarities = np.concatenate([substrate.neuron_polarities, np.ones(num_new_neurons)])
            substrate.refractory_periods = np.concatenate([substrate.refractory_periods, np.zeros(num_new_neurons)])

        # Universal state expansions
        substrate.spike_times.extend([[] for _ in range(num_new_neurons)])
        substrate.num_neurons = new_n

        print(f"Growth complete. Total neurons: {substrate.num_neurons}")
        print("-------------------------------------\n")]]></content>
    </file>
    <file>
      <path>core/substrate/structural_homeostasis.py</path>
      <content><![CDATA[# fum_rt/core/substrate/structural_homeostasis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
from scipy.sparse import csc_matrix, lil_matrix

def perform_structural_homeostasis(W: csc_matrix, ccc_metrics: dict) -> csc_matrix:
    """
    Performs Structural Homeostasis on the Emergent Connectome (UKG).
    
    This is a purpose-driven, self-regulating process that uses TDA metrics
    (Cohesion and Complexity) to maintain the network's topological health,
    ensuring the FUM remains in a stable and efficient state.

    Args:
        W (csc_matrix): The current sparse weight matrix representing the UKG.
        ccc_metrics (dict): A dictionary of metrics from the CCC_Module.

    Returns:
        csc_matrix: The modified, healthier weight matrix.
    """
    num_neurons = W.shape[0]
    
    # To avoid performance warnings, all structural modifications (pruning and
    # growth) are performed on a `lil_matrix`, which is efficient for
    # changing sparsity structure.
    W_lil = W.tolil()
    
    # --- 1. Pruning (Complexity Homeostasis) ---
    # The pruning threshold is now adaptive, based on the current mean weight.
    # This prevents the network from getting stuck and allows for dynamic rearrangement.
    # We prune any synapse that is less than 10% of the mean strength.
    if W.nnz > 0:
        mean_weight = np.mean(np.abs(W.data))
        pruning_threshold = 0.1 * mean_weight
    else:
        pruning_threshold = 0.01 # Fallback for empty graph

    rows_cols = W_lil.rows
    data_rows = W_lil.data
    for i in range(num_neurons):
        to_prune_indices = [
            idx for idx, weight in enumerate(data_rows[i])
            if abs(weight) < pruning_threshold
        ]
        for idx in sorted(to_prune_indices, reverse=True):
            del rows_cols[i][idx]
            del data_rows[i][idx]

    # --- 2. Growth (Cohesion Homeostasis) ---
    component_count = ccc_metrics.get('cohesion_cluster_count', 1)
    if isinstance(component_count, np.integer):
        component_count = component_count.item()

    if component_count > 1 and 'cluster_labels' in ccc_metrics:
        # A "pathological" state of low cohesion has been detected. The system
        # implements the documented strategy of "biasing plasticity towards
        # growing connections" to heal the fragmentation.
        labels = ccc_metrics['cluster_labels']
        unique_labels = np.unique(labels)
        
        # To make the healing effective, we create a "bundle" of new connections
        # to ensure the clustering algorithm recognizes the new bridge.
        BUNDLE_SIZE = 3
        
        # We'll build one bridge for each excess cluster to encourage fusion.
        num_bridges_to_build = component_count - 1

        for _ in range(num_bridges_to_build):
            # Choose two different territories to bridge
            cluster_a, cluster_b = np.random.choice(unique_labels, 2, replace=False)
            
            indices_a = np.where(labels == cluster_a)[0]
            indices_b = np.where(labels == cluster_b)[0]
            
            if len(indices_a) > 0 and len(indices_b) > 0:
                # Create a bundle of connections between the two territories
                for _ in range(BUNDLE_SIZE):
                    neuron_u = np.random.choice(indices_a)
                    neuron_v = np.random.choice(indices_b)
                    if neuron_u != neuron_v and W_lil[neuron_u, neuron_v] == 0:
                        W_lil[neuron_u, neuron_v] = np.random.uniform(0.05, 0.1)
    
    # Convert back to csc_matrix once at the end for efficient calculations
    W_csc = W_lil.tocsc()
    W_csc.prune()
    return W_csc]]></content>
    </file>
    <file>
      <path>core/substrate/substrate.py</path>
      <content><![CDATA[# fum_rt/core/substrate/substrate.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix, find

# FUM Modules
from fum_initialization import create_knn_graph

class Substrate:
    """
    Represents the FUM's computational medium or "Substrate".
    
    VERSION 4 (MERGED): This version combines the stable V3 architecture with
    the GPU acceleration and Growth features from the V9 refactor.
    """
    def __init__(self, num_neurons: int, k: int, device: str = 'auto'):
        """
        Initializes the Substrate.

        Args:
            num_neurons (int): The number of Computational Units (CUs).
            k (int): The number of nearest neighbors for the initial k-NN graph.
            device (str): 'auto', 'gpu', or 'cpu'.
        """
        self.num_neurons = num_neurons
        self._setup_device(device)
        self.backend = np if self.device_type == 'cpu' else torch
        
        # --- Neuron Types (80% Excitatory, 20% Inhibitory) ---
        self.rng = np.random.default_rng(seed=42)
        is_excitatory_np = self.rng.choice([True, False], num_neurons, p=[0.8, 0.2])

        # --- CU parameters (vectorized, created on CPU first) ---
        tau_m_np = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_neurons)
        self.v_rest = np.full(num_neurons, -65.0)
        v_reset_np = np.full(num_neurons, -70.0)
        v_thresh_np = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_neurons)
        refractory_period_np = np.full(num_neurons, 5.0)
        r_mem_np = np.full(num_neurons, 10.0)

        # --- Parameters for Intrinsic Plasticity (A.6) ---
        self.ip_target_rate_min = 0.1 # Hz
        self.ip_target_rate_max = 0.5 # Hz
        self.ip_v_thresh_adjustment = 0.1 # mV
        self.ip_tau_m_adjustment = 0.1 # ms
        self.ip_v_thresh_bounds = (-60.0, -50.0)
        self.ip_tau_m_bounds = (15.0, 25.0)
        
        # --- Synaptic Pathways: k-NN Initialization (on CPU first) ---
        W_np = create_knn_graph(num_neurons, k, is_excitatory_np).toarray()
        
        # --- Create state vars and move to GPU if requested ---
        if self.device_type == 'gpu':
            self.is_excitatory = torch.from_numpy(is_excitatory_np).to(self.device)
            self.tau_m = torch.from_numpy(tau_m_np).float().to(self.device)
            self.v_thresh = torch.from_numpy(v_thresh_np).float().to(self.device)
            self.v_m = torch.from_numpy(self.v_rest).float().to(self.device)
            self.refractory_time = torch.zeros(num_neurons, device=self.device)
            self.refractory_period = torch.from_numpy(refractory_period_np).float().to(self.device)
            self.r_mem = torch.from_numpy(r_mem_np).float().to(self.device)
            self.v_reset_tensor = torch.from_numpy(v_reset_np).float().to(self.device)
            self.spikes = torch.zeros(num_neurons, dtype=torch.bool, device=self.device)
            self.W = torch.from_numpy(W_np).float().to(self.device)
        else: # cpu
            self.is_excitatory = is_excitatory_np
            self.tau_m = tau_m_np
            self.v_reset = v_reset_np
            self.v_thresh = v_thresh_np
            self.refractory_period = refractory_period_np
            self.r_mem = r_mem_np
            self.v_m = np.full(num_neurons, self.v_rest)
            self.refractory_time = np.zeros(num_neurons)
            self.neuron_polarities = np.ones(num_neurons)
            self.refractory_periods = np.zeros(num_neurons)
            self.W = csc_matrix(W_np)
            self.spikes = np.zeros(num_neurons, dtype=bool)
        self.spike_times = [[] for _ in range(num_neurons)]
        self.time_step = 0

    def run_step(self, external_currents, dt=1.0):
        """
        Runs one full step of the Substrate's dynamics, dispatching to the correct backend.
        """
        if self.device_type == 'gpu':
            self._run_step_gpu(external_currents, dt)
        else:
            self._run_step_cpu(external_currents, dt)
        
        self.time_step += 1

    def _run_step_cpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the CPU.
        """
        # Correctly apply membrane resistance only to synaptic currents inside the dv calculation
        synaptic_currents = self.W.dot(self.spikes.astype(np.float32))
        
        not_in_refractory = self.refractory_time <= 0
        
        # The full, correct ELIF update equation from the documentation
        dv = (
            -(self.v_m[not_in_refractory] - self.v_rest[not_in_refractory])
            + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory]
            + external_currents[not_in_refractory]
        ) / self.tau_m[not_in_refractory]
        
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt

        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = np.where(spiking_mask)[0]
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def _run_step_gpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the GPU.
        """
        external_currents_gpu = torch.from_numpy(external_currents).float().to(self.device)
        synaptic_currents = torch.mv(self.W, self.spikes.float())
        
        not_in_refractory = self.refractory_time <= 0
        v_rest_gpu = torch.from_numpy(self.v_rest).float().to(self.device)
        
        dv = (-(self.v_m[not_in_refractory] - v_rest_gpu[not_in_refractory]) + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory] + external_currents_gpu[not_in_refractory]) / self.tau_m[not_in_refractory]
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt
        self.refractory_time.clamp_(min=0)
        
        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset_tensor[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = torch.where(spiking_mask)[0].cpu().numpy()
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def apply_intrinsic_plasticity(self, window_ms=50, dt=1.0):
        """
        Applies intrinsic plasticity to neuron parameters based on their recent
        firing rate, as per documentation section A.6.
        """
        window_steps = int(window_ms / dt)
        analysis_start_time = max(0, (self.time_step - window_steps) * dt)
        window_duration_s = (self.time_step * dt - analysis_start_time) / 1000.0

        if window_duration_s == 0:
            return

        for i in range(self.num_neurons):
            spikes_in_window = [t for t in self.spike_times[i] if t >= analysis_start_time]
            rate_hz = len(spikes_in_window) / window_duration_s
            
            # Adjust v_thresh
            if rate_hz > self.ip_target_rate_max:
                self.v_thresh[i] += self.ip_v_thresh_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.v_thresh[i] -= self.ip_v_thresh_adjustment
                
            # Adjust tau_m
            if rate_hz > self.ip_target_rate_max:
                self.tau_m[i] -= self.ip_tau_m_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.tau_m[i] += self.ip_tau_m_adjustment

        # Clamp parameters to their bounds
        np.clip(self.v_thresh, self.ip_v_thresh_bounds[0], self.ip_v_thresh_bounds[1], out=self.v_thresh)
        np.clip(self.tau_m, self.ip_tau_m_bounds[0], self.ip_tau_m_bounds[1], out=self.tau_m)

    def apply_synaptic_scaling(self, target_sum=1.0):
        """
        Applies simple multiplicative scaling to incoming excitatory weights to
        keep the total input around a target value. Based on the reference
        validation script and documentation B.7.ii.
        """
        W_dense = self.W.toarray()
        
        # Calculate sum of incoming positive (excitatory) weights for each neuron
        incoming_exc_sums = np.sum(np.maximum(W_dense, 0), axis=0)
        
        # Avoid division by zero
        incoming_exc_sums[incoming_exc_sums < 1e-6] = 1.0
        
        # Calculate scaling factors needed to bring sum to target
        scale_factors = target_sum / incoming_exc_sums
        
        # Get a dense matrix of the excitatory weights only
        exc_W_dense = W_dense.copy()
        exc_W_dense[W_dense < 0] = 0
        
        # Apply scaling multiplicatively to the excitatory weights
        scaled_exc_W = exc_W_dense * scale_factors[np.newaxis, :]
        
        # Reconstruct the full weight matrix
        W_dense[W_dense > 0] = scaled_exc_W[W_dense > 0]
        
        self.W = csc_matrix(W_dense)
        self.W.prune()

    def _setup_device(self, device_preference):
        if device_preference == 'gpu':
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                print("Warning: GPU requested but not available. Falling back to CPU.")
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
        elif device_preference == 'cpu':
            self.device = torch.device("cpu")
            self.device_type = 'cpu'
            print("--- Substrate configured to run on CPU. ---")
        else: # auto
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
                print("--- Substrate configured to run on CPU. ---")]]></content>
    </file>
    <file>
      <path>core/text_utils.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import re, random, time
from collections import Counter

# Minimal stopword list; purely for compact summaries at the I/O boundary.
STOP = set(
    """
    the a an and or for with into of to from in on at by is are was were be been being
    it this that as if then than so thus such not no nor but over under up down out
    you your yours me my mine we our ours they their theirs he him his she her hers
    i am do does did done have has had will would can could should shall may might
    """.split()
)

def summarize_keywords(text: str, k: int = 4) -> str:
    """
    Extract a compact, lowercased keyword summary from recent text.
    Deterministic and lightweight; used only for composing human-readable
    context in UTD macros. Core remains void-native.
    """
    if not text:
        return ""
    words = [w.lower() for w in re.findall(r"[A-Za-z][A-Za-z0-9_+\-]*", text)]
    words = [w for w in words if w not in STOP and len(w) > 2]
    if not words:
        return ""
    top_words = [w for w, _ in Counter(words).most_common(k)]
    return ", ".join(top_words)

def tokenize_text(text: str):
    """Capture any sequence of non-whitespace characters."""
    return [w.lower() for w in re.findall(r"\S+", str(text))]

def update_ngrams(tokens, ng2, ng3):
    """Update streaming n-gram models (bigram/trigram)."""
    toks = [t for t in tokens if t]
    n = len(toks)
    for i in range(n - 1):
        a, b = toks[i], toks[i+1]
        d = ng2.setdefault(a, {})
        d[b] = d.get(b, 0) + 1
    for i in range(n - 2):
        key = (toks[i], toks[i+1])
        c = toks[i+2]
        d = ng3.setdefault(key, {})
        d[c] = d.get(c, 0) + 1

def generate_emergent_sentence(lexicon: dict, ng2: dict, ng3: dict, seed=None, seed_tokens: set = None):
    """Assemble a sentence from a lexicon and learned n-grams, optionally seeded from recent tokens."""
    if not lexicon:
        return ""
    
    # 1. Determine candidate pool for start word
    if seed_tokens:
        candidates = {k: v for k, v in lexicon.items() if k in seed_tokens}
        if candidates:
            items = list(candidates.items())
        else:
            items = list(lexicon.items())
    else:
        items = list(lexicon.items())
    if not items:
        return ""
    
    rnd = random.Random(seed if seed is not None else int(time.time() * 1000))
    
    # 2. Weighted start token draw from the candidate pool
    weights = [max(1, int(cnt)) for _, cnt in items]
    total = sum(weights)
    r = rnd.uniform(0, total)
    acc = 0.0
    start = items[0][0]
    for tok, cnt in items:
        acc += max(1, int(cnt))
        if acc >= r:
            start = tok
            break
    
    words = [start]
    # Markov walk using trigram then bigram
    while True:
        nxt = None
        if len(words) >= 2:
            key = (words[-2], words[-1])
            d = ng3.get(key)
            if d:
                total_c = sum(d.values())
                r = rnd.uniform(0, total_c)
                s = 0.0
                for tok, c in d.items():
                    s += c
                    if s >= r:
                        nxt = tok
                        break
        if nxt is None:
            d2 = ng2.get(words[-1], {})
            if d2:
                total_c = sum(d2.values())
                r = rnd.uniform(0, total_c)
                s = 0.0
                for tok, c in d2.items():
                    s += c
                    if s >= r:
                        nxt = tok
                        break
        if nxt is None:
            break
        words.append(nxt)
    
    sent = " ".join(words).strip()
    if sent:
        sent = sent[0].upper() + sent[1:]
        if not sent.endswith((".", "!", "?")):
            sent += "."
    return sent]]></content>
    </file>
    <file>
      <path>core/visualizer.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os

class Visualizer:
    def __init__(self, run_dir: str):
        self.run_dir = run_dir

    def dashboard(self, history):
        os.makedirs(self.run_dir, exist_ok=True)
        try:
            import matplotlib.pyplot as plt
        except Exception:
            # Matplotlib not available; skip rendering without crashing the runtime
            return None
        fig, axs = plt.subplots(2, 2, figsize=(12, 9))
        t = [h['t'] for h in history]

        axs[0,0].plot(t, [h['active_synapses'] for h in history])
        axs[0,0].set_title('UKG Sparsity Over Time')
        axs[0,0].set_xlabel('Tick')
        axs[0,0].set_ylabel('Active Synapses')

        axs[0,1].plot(t, [h['avg_weight'] for h in history], marker='o', linestyle='-')
        axs[0,1].set_title('Average Synaptic Weight Over Time')
        axs[0,1].set_xlabel('Tick')
        axs[0,1].set_ylabel('Average Weight')

        axs[1,0].plot(t, [h['cohesion_components'] for h in history])
        axs[1,0].set_title('UKG Cohesion (Component Count)')
        axs[1,0].set_xlabel('Tick')
        axs[1,0].set_ylabel('Components')

        axs[1,1].plot(t, [h['complexity_cycles'] for h in history])
        axs[1,1].set_title('UKG Complexity (Cycles)')
        axs[1,1].set_xlabel('Tick')
        axs[1,1].set_ylabel('Cycles')

        fig.suptitle('FUM Performance Dashboard', fontsize=14)
        fig.tight_layout()
        # Overlay Control URL (Load Engram) if available
        try:
            import json as _json
            ctrl_url = None
            try:
                with open(os.path.join(self.run_dir, 'control.json'), 'r', encoding='utf-8') as _fh:
                    _ctrl = _json.load(_fh)
                    if isinstance(_ctrl, dict):
                        ctrl_url = _ctrl.get('url')
            except Exception:
                ctrl_url = None
            if ctrl_url:
                try:
                    import matplotlib.pyplot as _plt
                    _plt.subplots_adjust(bottom=0.16)
                except Exception:
                    pass
                fig.text(0.01, 0.01, f'Controls: {ctrl_url} - Load Engram', fontsize=9, color='#8b949e')
        except Exception:
            pass

        path = os.path.join(self.run_dir, 'dashboard.png')
        fig.savefig(path, dpi=150)
        plt.close(fig)
        return path

    def graph(self, G, fname='connectome.png'):
        os.makedirs(self.run_dir, exist_ok=True)
        try:
            import matplotlib.pyplot as plt
            import networkx as nx
        except Exception:
            # Missing viz deps; skip without crashing
            return None
        fig = plt.figure(figsize=(8,8))
        pos = nx.spring_layout(G, seed=42, dim=2)
        nx.draw_networkx_nodes(G, pos, node_size=10)
        nx.draw_networkx_edges(G, pos, alpha=0.3)
        fig.suptitle('Foundational UKG Structure', fontsize=14)
        path = os.path.join(self.run_dir, fname)
        fig.savefig(path, dpi=150)
        plt.close(fig)
        return path
]]></content>
    </file>
    <file>
      <path>core/void_b1.py</path>
      <content><![CDATA[# void_b1.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Void-faithful, streaming B1 surrogate and Euler-rank estimate.

Design goals
- No dense NxN; use adjacency lists or the connectome's active-edge iterator
- O(E_active) for sparse; sampling to keep per-tick cost bounded
- Outputs:
  - void_b1: [0,1] scalar capturing cyclic structure density with smoothing
  - euler_rank: E_active - V_active + C_active (graph-level Betti-1 proxy)
  - triangles_per_edge: local triangle frequency around sampled active edges
  - active_node_ratio: V_active / N
  - active_edges_est: estimated or exact count of active undirected edges

Notes
- For SparseConnectome we rely on connectome._active_edge_iter() to enumerate
  active edges without scanning non-active neighbors.
- For dense Connectome this module only executes for small N where mask ops
  are acceptable; large-N runs auto-sparse in Nexus.
"""

from __future__ import annotations
import math
from typing import Iterable, List, Tuple, Dict, Any, Optional

import numpy as np
from .primitives.dsu import DSU as _DSU


def _count_intersection_sorted(a: np.ndarray, b: np.ndarray) -> int:
    """
    Count |a ∩ b| given two ascending-sorted int arrays.
    """
    i = j = 0
    na, nb = a.size, b.size
    cnt = 0
    while i < na and j < nb:
        ai = int(a[i]); bj = int(b[j])
        if ai == bj:
            cnt += 1
            i += 1; j += 1
        elif ai < bj:
            i += 1
        else:
            j += 1
    return cnt


def _alpha_from_half_life(half_life_ticks: int) -> float:
    hl = max(1, int(half_life_ticks))
    return 1.0 - math.exp(math.log(0.5) / float(hl))


class _Reservoir:
    """
    Fixed-size reservoir sampler for a stream of unknown-length items.
    Items are small tuples (i,j).
    """
    def __init__(self, k: int, rng: np.random.Generator):
        self.k = int(max(1, k))
        self.rng = rng
        self.buf: List[Tuple[int, int]] = []
        self.seen = 0

    def push(self, item: Tuple[int, int]):
        self.seen += 1
        if len(self.buf) < self.k:
            self.buf.append(item)
            return
        # Replace with probability k/seen
        if self.rng.random() < (float(self.k) / float(self.seen)):
            idx = int(self.rng.integers(0, self.k))
            self.buf[idx] = item

    def items(self) -> List[Tuple[int, int]]:
        return self.buf

    def count(self) -> int:
        return self.seen


class VoidB1Meter:
    """
    Streaming surrogate for B1 with Euler-rank estimate.

    - Maintains EMA of b1_raw to produce void_b1 in [0,1]
    - Computes euler_rank = E_active - V_active + C_active (cycles count)
    - Estimates triangles_per_edge over a bounded reservoir of active edges

    Parameters
    - sample_edges: maximum number of active edges sampled per tick
    - half_life_ticks: EMA half-life for void_b1 smoothing
    """
    def __init__(self, sample_edges: int = 4096, half_life_ticks: int = 50):
        self.sample_edges = int(max(32, sample_edges))
        self.alpha = _alpha_from_half_life(half_life_ticks)
        self._ema_b1: Optional[float] = None

    # ---------------- Sparse path (preferred) ----------------

    def _update_sparse(
        self,
        adj: List[np.ndarray],
        W: np.ndarray,
        threshold: float,
        rng: np.random.Generator,
        active_edge_iter: Iterable[Tuple[int, int]],
        N: int,
    ) -> Dict[str, Any]:
        """
        O(E_active) with bounded sampling for triangles.
        """
        N = int(N)
        W = np.asarray(W, dtype=np.float32)
        th = float(threshold)

        # Reservoir over active edges; avoid global scans by building DSU over active vertices only
        res = _Reservoir(self.sample_edges, rng)
        E_active = 0

        # Active-vertex DSU keyed by local contiguous ids (no O(N) scans)
        dsu = _DSU(0)
        idmap: Dict[int, int] = {}
        local_n = 0

        for (i, j) in active_edge_iter:
            i = int(i); j = int(j)
            # Active edge guaranteed by iterator contract
            E_active += 1
            ii = idmap.get(i)
            if ii is None:
                ii = local_n
                idmap[i] = ii
                dsu.grow_to(local_n + 1)
                local_n += 1
            jj = idmap.get(j)
            if jj is None:
                jj = local_n
                idmap[j] = jj
                dsu.grow_to(local_n + 1)
                local_n += 1
            dsu.union(ii, jj)
            res.push((i, j))

        V_active = int(local_n)
        if E_active == 0:
            C_active = N  # no active edges: consider each node isolated
        else:
            C_active = int(getattr(dsu, "components", dsu.count_sets()))

        # Triangles-per-edge over the reservoir
        tri = 0
        m = len(res.items())
        if m > 0:
            for (i, j) in res.items():
                ai = adj[i]; aj = adj[j]
                # Intersect active neighbors only: W[i]*W[k] > th and W[j]*W[k] > th
                # Fast path: build filtered lists then intersect
                if ai.size == 0 or aj.size == 0:
                    continue
                # Filter by active threshold
                ai_act = ai[(W[i] * W[ai]) > th]
                if ai_act.size == 0:
                    continue
                aj_act = aj[(W[j] * W[aj]) > th]
                if aj_act.size == 0:
                    continue
                tri += _count_intersection_sorted(ai_act, aj_act)

        triangles_per_edge = (float(tri) / float(m)) if m > 0 else 0.0

        # Cycles (Euler-rank for graphs)
        cycles = max(0, int(E_active - V_active + C_active))

        # Node activity ratio
        active_node_ratio = (float(V_active) / float(max(1, N)))

        return {
            "E_active": int(E_active),
            "V_active": int(V_active),
            "C_active": int(C_active),
            "cycles": int(cycles),
            "triangles_per_edge": float(triangles_per_edge),
            "active_node_ratio": float(active_node_ratio),
            "reservoir_seen": int(res.count()),
            "reservoir_used": int(m),
        }

    # ---------------- Dense path (small-N only) ----------------

    def _update_dense(
        self,
        A: np.ndarray,
        E: np.ndarray,
        W: np.ndarray,
        threshold: float,
        rng: np.random.Generator,
    ) -> Dict[str, Any]:
        """
        Small-N fallback using masks; cost is acceptable only for validation runs.
        """
        N = int(A.shape[0])
        th = float(threshold)

        # Active mask, undirected, upper triangle to avoid double counting
        mask = (E > th) & (A == 1)
        # Degrees and active vertices
        deg = mask.sum(axis=1).astype(np.int64)
        V_active = int((deg > 0).sum())

        # Active edge list (upper triangle)
        iu, ju = np.where(np.triu(mask, k=1))
        edges = np.stack([iu, ju], axis=1).astype(np.int32, copy=False)
        E_active = int(edges.shape[0])

        # DSU for active components (restricted to active vertices)
        parent = np.arange(N, dtype=np.int32)
        rank = np.zeros(N, dtype=np.int8)

        def find(x: int) -> int:
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a: int, b: int):
            ra, rb = find(a), find(b)
            if ra == rb:
                return
            if rank[ra] < rank[rb]:
                parent[ra] = rb
            elif rank[rb] < rank[ra]:
                parent[rb] = ra
            else:
                parent[rb] = ra
                rank[ra] = rank[ra] + 1

        for i, j in edges:
            union(int(i), int(j))

        if E_active == 0:
            C_active = N
        else:
            act_idx = np.nonzero(deg > 0)[0]
            roots = set(int(find(int(idx))) for idx in act_idx)
            C_active = len(roots)

        # Reservoir over edges
        k = min(self.sample_edges, E_active)
        triangles_per_edge = 0.0
        if k > 0:
            sel = rng.choice(E_active, size=k, replace=False)
            sel_edges = edges[sel]
            # Build neighbor lists once (sorted)
            # For small N, extracting sorted neighbor arrays is fine
            nbrs = [np.where(A[i] > 0)[0].astype(np.int32) for i in range(N)]
            # Intersect with active condition using threshold
            tri = 0
            for (i, j) in sel_edges:
                ai = nbrs[int(i)]; aj = nbrs[int(j)]
                if ai.size == 0 or aj.size == 0:
                    continue
                ai_act = ai[(W[int(i)] * W[ai]) > th]
                if ai_act.size == 0:
                    continue
                aj_act = aj[(W[int(j)] * W[aj]) > th]
                if aj_act.size == 0:
                    continue
                tri += _count_intersection_sorted(ai_act, aj_act)
            triangles_per_edge = float(tri) / float(k)

        cycles = max(0, int(E_active - V_active + C_active))
        active_node_ratio = float(V_active) / float(max(1, N))

        return {
            "E_active": int(E_active),
            "V_active": int(V_active),
            "C_active": int(C_active),
            "cycles": int(cycles),
            "triangles_per_edge": float(triangles_per_edge),
            "active_node_ratio": float(active_node_ratio),
            "reservoir_seen": int(E_active),
            "reservoir_used": int(k),
        }

    # ---------------- Public API ----------------

    def update(self, connectome) -> Dict[str, Any]:
        """
        Compute a void-faithful topology packet and return:
        {
          'void_b1': [0,1],
          'euler_rank': int,
          'cycles': int,
          'triangles_per_edge': float,
          'active_node_ratio': float,
          'active_edges_est': int
        }

        The method chooses sparse vs dense automatically.
        """
        rng = getattr(connectome, "rng", np.random.default_rng(0))

        if hasattr(connectome, "_active_edge_iter"):
            # Sparse path
            adj = getattr(connectome, "adj", None)
            if adj is None:
                raise RuntimeError("Sparse path requires 'adj' on connectome")
            W = np.asarray(connectome.W, dtype=np.float32)
            th = float(getattr(connectome, "threshold", 0.0))
            N = int(getattr(connectome, "N", W.shape[0]))
            pkt = self._update_sparse(
                adj=adj,
                W=W,
                threshold=th,
                rng=rng,
                active_edge_iter=getattr(connectome, "_active_edge_iter")(),
                N=N,
            )
        else:
            # Dense path
            A = np.asarray(connectome.A, dtype=np.int8)
            E = np.asarray(connectome.E, dtype=np.float32)
            W = np.asarray(connectome.W, dtype=np.float32)
            th = float(getattr(connectome, "threshold", 0.0))
            pkt = self._update_dense(A=A, E=E, W=W, threshold=th, rng=rng)

        # Compose a normalized void_b1 score with EMA smoothing.
        # Mix cycles density and triangles per edge; both emphasize local cyclic structure.
        E_act = max(1, int(pkt["E_active"]))
        cycles_density = float(pkt["cycles"]) / float(E_act)  # [0, +]
        # Heuristic normalization: triangles_per_edge is usually small (0..few)
        tri_norm = min(1.0, float(pkt["triangles_per_edge"]) / 4.0)

        b1_raw = 0.6 * cycles_density + 0.4 * tri_norm
        b1_raw = max(0.0, min(1.0, b1_raw))  # clamp to [0,1]

        if self._ema_b1 is None:
            self._ema_b1 = b1_raw
        else:
            a = self.alpha
            self._ema_b1 = (1.0 - a) * float(self._ema_b1) + a * float(b1_raw)

        out = {
            "void_b1": float(self._ema_b1),
            "euler_rank": int(pkt["cycles"]),  # Graph Euler-rank equals cycles for 1D complexes
            "cycles": int(pkt["cycles"]),
            "triangles_per_edge": float(pkt["triangles_per_edge"]),
            "active_node_ratio": float(pkt["active_node_ratio"]),
            "active_edges_est": int(pkt["E_active"]),
            "active_vertices_est": int(pkt["V_active"]),
            "active_components_est": int(pkt["C_active"]),
            "reservoir_seen": int(pkt["reservoir_seen"]),
            "reservoir_used": int(pkt["reservoir_used"]),
        }
        return out


# Convenience singleton for quick integration
_GLOBAL_B1_METER: Optional[VoidB1Meter] = None


def update_void_b1(connectome, sample_edges: int = 4096, half_life_ticks: int = 50) -> Dict[str, Any]:
    """
    Module-level helper to update and return the topology packet.
    Lazily initializes a process-local meter.
    """
    global _GLOBAL_B1_METER
    if _GLOBAL_B1_METER is None:
        _GLOBAL_B1_METER = VoidB1Meter(sample_edges=sample_edges, half_life_ticks=half_life_ticks)
    return _GLOBAL_B1_METER.update(connectome)]]></content>
    </file>
    <file>
      <path>core/void_dynamics_adapter.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

# Try to import user's libraries first
try:
    # Import only the elemental deltas from the user's equations.
    # We compose universal_void_dynamics locally to guarantee growth+decay are combined.
    from Void_Equations import delta_re_vgsp, delta_gdsp, get_universal_constants
    HAVE_EXTERNAL = True
except Exception:
    HAVE_EXTERNAL = False
    # Minimal fallback (keeps runtime alive if your file isn't on PYTHONPATH yet)
    ALPHA, BETA, F_REF, PHASE_SENS = 0.25, 0.1, 0.02, 0.5
    def get_universal_constants():
        return {'ALPHA': ALPHA, 'BETA': BETA, 'F_REF': F_REF, 'PHASE_SENS': PHASE_SENS}
    def delta_re_vgsp(W, t, alpha=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
        if alpha is None: alpha = ALPHA
        if f_ref is None: f_ref = F_REF
        if phase_sens is None: phase_sens = PHASE_SENS
        eff = alpha * domain_modulation
        noise = np.random.uniform(-0.02, 0.02, size=W.shape)
        base = eff * W * (1 - W) + noise
        if use_time_dynamics:
            phase = np.sin(2 * np.pi * f_ref * t)
            return base * (1 + phase_sens * phase)
        return base
    def delta_gdsp(W, t, beta=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
        if beta is None: beta = BETA
        if f_ref is None: f_ref = F_REF
        if phase_sens is None: phase_sens = PHASE_SENS
        eff = beta * domain_modulation
        base = -eff * W
        if use_time_dynamics:
            phase = np.sin(2 * np.pi * f_ref * t)
            return base * (1 + phase_sens * phase)
        return base

# Domain modulation
# Compose universal dynamics locally so growth+decay are always present (avoids saturation).
def universal_void_dynamics(W, t, domain_modulation=1.0, use_time_dynamics=True):
    re = delta_re_vgsp(W, t, use_time_dynamics=use_time_dynamics, domain_modulation=domain_modulation)
    gd = delta_gdsp(W, t, use_time_dynamics=use_time_dynamics, domain_modulation=domain_modulation)
    return re + gd

# Domain modulation
def get_domain_modulation(domain: str):
    # Try user's universal modulation
    try:
        from Void_Debt_Modulation import VoidDebtModulation
        mod = VoidDebtModulation().get_universal_domain_modulation(domain)
        return float(mod['domain_modulation'])
    except Exception:
        # Fallback to safe default
        targets = {
            'quantum': 0.15, 'standard_model': 0.22, 'dark_matter': 0.27,
            'biology_consciousness': 0.20, 'cosmogenesis': 0.84, 'higgs': 0.80
        }
        ALPHA = get_universal_constants()['ALPHA']
        BETA  = get_universal_constants()['BETA']
        void_debt_ratio = BETA/ALPHA if ALPHA != 0 else 0.4
        s = targets.get(domain, 0.25)
        return 1.0 + (s ** 2) / void_debt_ratio
]]></content>
    </file>
    <file>
      <path>electron-frontend/README.md</path>
      <content/>
    </file>
    <file>
      <path>frontend/DEV_PLANS.md</path>
      <content/>
    </file>
    <file>
      <path>frontend/README.md</path>
      <content/>
    </file>
    <file>
      <path>frontend/__init__.py</path>
      <content><![CDATA["""
FUM Runtime Frontend (modularized)

Modules:
- fs_utils: filesystem helpers (runs listing, JSON IO) - re-exported from utilities.fs_utils
- tail: tailing JSONL with byte offsets - re-exported from utilities.tail
- series: streaming metrics buffers and helpers - re-exported from models.series
- process_manager: launch/stop runtime process from the UI - re-exported from services.process_manager
- app: Dash app entrypoint (build_app, main)
"""

# Re-export modules at package root for stable imports: fum_rt.frontend.fs_utils, etc.
from .utilities import fs_utils as fs_utils
from .utilities import tail as tail
from .models import series as series
from .services import process_manager as process_manager

__all__ = ["fs_utils", "tail", "series", "process_manager"]]]></content>
    </file>
    <file>
      <path>frontend/__main__.py</path>
      <content><![CDATA[from __future__ import annotations

import os

from .app import build_app


def main() -> None:
    rr = os.getenv("RUNS_ROOT", "").strip() or os.path.abspath("runs")
    app = build_app(rr)
    host = os.getenv("DASH_HOST", "127.0.0.1")
    try:
        port = int(os.getenv("DASH_PORT", "8050"))
    except Exception:
        port = 8050
    # Avoid debug reloader to prevent duplicate callbacks
    app.run(host=host, port=port, debug=False)


if __name__ == "__main__":
    main()]]></content>
    </file>
    <file>
      <path>frontend/app.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

import os
import dash
from typing import List, Dict, Any

from dash import Dash, dcc, html

from fum_rt.frontend.utilities.fs_utils import list_runs, _list_files
from fum_rt.frontend.services.process_manager import ProcessManager
from fum_rt.frontend.utilities.profiles import get_default_profile
from fum_rt.frontend.styles import get_global_css

from fum_rt.frontend.components.workspace import workspace_card
from fum_rt.frontend.components.runtime_controls import runtime_controls_card
from fum_rt.frontend.components.feed import feed_card
from fum_rt.frontend.components.run_config import run_config_card
from fum_rt.frontend.components.charts import charts_card
from fum_rt.frontend.components.chat import chat_card
from fum_rt.frontend.components.perf import perf_card

from fum_rt.frontend.callbacks.workspace import register_workspace_callbacks
from fum_rt.frontend.callbacks.charts import register_chart_callbacks
from fum_rt.frontend.callbacks.runtime import register_runtime_callbacks
from fum_rt.frontend.callbacks.process import register_process_callbacks
from fum_rt.frontend.callbacks.feed import register_feed_callbacks
from fum_rt.frontend.callbacks.profile import register_profile_callbacks
from fum_rt.frontend.callbacks.logs import register_logs_callbacks
from fum_rt.frontend.callbacks.chat import register_chat_callbacks
from fum_rt.frontend.callbacks.engram import register_engram_callbacks
from fum_rt.frontend.callbacks.perf import register_perf_callbacks
from fum_rt.frontend.callbacks.interval import register_interval_callbacks
from fum_rt.frontend.callbacks.file_picker.registrars import (
    register_file_picker_static,
    register_file_picker_engram,
)


def build_app(runs_root: str) -> Dash:
    """
    Factory for the FUVDM Live Dashboard Dash app.
    - Assembles layout (cards/components)
    - Installs modular callbacks
    - Keeps ProcessManager scoped to the app instance
    """
    # UI throttling and safe startup:
    # - prevent_initial_callbacks avoids callback storms at load
    # - suppress_callback_exceptions allows lazy mounting of tabs/sections
    app = Dash(
        __name__,
        prevent_initial_callbacks=True,
        suppress_callback_exceptions=True,
    )
    app.title = "FUM Live Dashboard"

    # Global theme
    GLOBAL_CSS = get_global_css()
    app.index_string = app.index_string.replace("</head>", f"<style>{GLOBAL_CSS}</style></head>")

    # Workspace context
    runs = list_runs(runs_root)
    default_run = runs[0] if runs else ""

    # Paths (compute repository root from this file so structure works regardless of CWD)
    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    PROFILES_DIR = os.path.join(repo_root, "run_profiles")
    os.makedirs(PROFILES_DIR, exist_ok=True)

    # Services
    manager = ProcessManager(runs_root)
    default_profile = get_default_profile()

    # Profile discovery for initial options
    def list_profiles() -> List[str]:
        try:
            return sorted([os.path.join(PROFILES_DIR, f) for f in os.listdir(PROFILES_DIR) if f.endswith(".json")])
        except Exception:
            return []

    # Static dropdowns
    domain_options = [
        {"label": n, "value": n}
        for n in [
            "math_physics",
            "quantum",
            "standard_model",
            "dark_matter",
            "biology_consciousness",
            "cosmogenesis",
            "higgs",
        ]
    ]
    profile_options = [{"label": os.path.basename(p), "value": p} for p in list_profiles()]
    # Data feed files - bound and non-recursive to avoid heavy startup scans
    try:
        DATA_SCAN_MAX = int(os.getenv("DASH_DATA_SCAN_MAX", "300"))
    except Exception:
        DATA_SCAN_MAX = 300
    DATA_EXTS = [".txt", ".jsonl", ".json", ".csv", ".pdf"]
    data_dir = os.path.join(repo_root, "fum_rt", "data")
    os.makedirs(data_dir, exist_ok=True)
    paths = _list_files(data_dir, exts=DATA_EXTS, recursive=False)[: max(0, DATA_SCAN_MAX)]
    data_files_options = [{"label": p, "value": p} for p in paths]

    # Layout
    app.layout = html.Div(
        [
            html.H3("FUVDM Live Dashboard (experimental control)"),
            html.Div(
                [
                    # Left panel
                    html.Div(
                        [
                            workspace_card(runs_root, runs, default_run),
                            runtime_controls_card(default_profile),
                            perf_card(),
                            feed_card(data_files_options),
                        ],
                        style={"minWidth": "320px", "display": "grid", "gap": "16px"},
                    ),
                    # Right panel
                    html.Div(
                        [
                            run_config_card(default_profile, domain_options, profile_options),
                            charts_card(),
                            chat_card(),
                        ],
                        style={"minWidth": "400px", "display": "grid", "gap": "16px"},
                    ),
                ],
                className="grid",
            ),
            # Global UI poll - environment-tunable and disable-able
            # DASH_POLL_MS: >0 interval in ms (default 1200); <=0 disables polling
            dcc.Interval(
                id="poll",
                interval=max(250, int(os.getenv("DASH_POLL_MS", "1200")) if os.getenv("DASH_POLL_MS", "").strip() != "" else 1200),
                n_intervals=0,
                disabled=(int(os.getenv("DASH_POLL_MS", "1200")) if os.getenv("DASH_POLL_MS", "").strip() != "" else 1200) <= 0,
            ),
            dcc.Store(id="chat-state"),
            dcc.Store(id="ui-state"),
        ],
        style={"padding": "10px"},
    )

    # Callbacks (modular)
    # Diagnostic: print Dash/version and callback registration progress
    try:
        print(f"[dash] version={dash.__version__}")
    except Exception:
        pass

    register_workspace_callbacks(app, runs_root, manager)
    print(f"[callbacks] workspace registered; total={len(getattr(app, 'callback_map', {}))}")
    register_chart_callbacks(app)
    print(f"[callbacks] charts registered; total={len(getattr(app, 'callback_map', {}))}")
    register_runtime_callbacks(app, default_profile)
    print(f"[callbacks] runtime registered; total={len(getattr(app, 'callback_map', {}))}")
    register_feed_callbacks(app, manager, repo_root)
    print(f"[callbacks] feed registered; total={len(getattr(app, 'callback_map', {}))}")
    register_process_callbacks(app, runs_root, manager, default_profile)
    print(f"[callbacks] process registered; total={len(getattr(app, 'callback_map', {}))}")
    register_profile_callbacks(app, PROFILES_DIR, default_profile)
    print(f"[callbacks] profile registered; total={len(getattr(app, 'callback_map', {}))}")
    register_logs_callbacks(app, manager)
    print(f"[callbacks] logs registered; total={len(getattr(app, 'callback_map', {}))}")
    register_chat_callbacks(app)
    print(f"[callbacks] chat registered; total={len(getattr(app, 'callback_map', {}))}")
    register_engram_callbacks(app)
    print(f"[callbacks] engram registered; total={len(getattr(app, 'callback_map', {}))}")
    register_perf_callbacks(app)
    print(f"[callbacks] perf registered; total={len(getattr(app, 'callback_map', {}))}")
    register_interval_callbacks(app)
    print(f"[callbacks] interval registered; total={len(getattr(app, 'callback_map', {}))}")

    # File Picker integrations (bounded IO, no recursive scans):
    register_file_picker_static(app, prefix="feed-file", root=data_dir, exts=DATA_EXTS, target_id="feed-path")
    print(f"[callbacks] filepicker feed registered; total={len(getattr(app, 'callback_map', {}))}")
    register_file_picker_static(app, prefix="profile-file", root=PROFILES_DIR, exts=[".json"], target_id="profile-path")
    print(f"[callbacks] filepicker profile registered; total={len(getattr(app, 'callback_map', {}))}")
    register_file_picker_engram(app, prefix="engram-file", exts=[".h5", ".npz"], target_id="rc-load-engram-path", fallback_root=runs_root)
    print(f"[callbacks] filepicker engram registered; total={len(getattr(app, 'callback_map', {}))}")

    print(f"[callbacks] registration complete; total={len(getattr(app, 'callback_map', {}))}")
    return app]]></content>
    </file>
    <file>
      <path>frontend/callbacks/charts.py</path>
      <content><![CDATA[from __future__ import annotations

import plotly.graph_objs as go
from dash import Input, Output  # noqa: F401 (Dash binds these at runtime)
from fum_rt.frontend.controllers.charts_controller import compute_dashboard_figures
from fum_rt.frontend.models.series import SeriesState  # for type awareness in compute


def register_chart_callbacks(app):
    """
    Register figure update callbacks on the provided Dash app.
    Delegates business logic to controllers.compute_dashboard_figures
    so it can be reused outside Dash (e.g., batch exporters).
    """

    @app.callback(
        Output("fig-dashboard", "figure"),
        Output("fig-discovery", "figure"),
        Input("poll", "n_intervals"),
        Input("run-dir", "value"),
        Input("proc-status", "children"),
        Input("ui-state", "data"),
        prevent_initial_call=False,
    )
    def update_figs(_n, run_dir, proc_status, ui_state):
        if not run_dir:
            return go.Figure(), go.Figure()

        # Clear once when process (re)starts or resumes (avoid perpetual resets).
        try:
            last_ps = getattr(update_figs, "_last_proc_status", None)
            if isinstance(proc_status, str) and proc_status != last_ps:
                if ("Resumed." in proc_status) or ("Started." in proc_status):
                    setattr(update_figs, "_state", None)
            setattr(update_figs, "_last_proc_status", proc_status)
        except Exception:
            pass

        state = getattr(update_figs, "_state", None)
        ui = ui_state or {}
        fig1, fig2, new_state = compute_dashboard_figures(run_dir, state, ui)
        setattr(update_figs, "_state", new_state)
        return fig1, fig2]]></content>
    </file>
    <file>
      <path>frontend/callbacks/chat.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import json
from dash import Input, Output, State, no_update  # noqa: F401

from fum_rt.frontend.utilities.tail import tail_jsonl_bytes
from fum_rt.frontend.controllers.chat_controller import (
    items_from_utd_records,
    items_from_inbox_records,
    trim_items,
    render_chat_view,
)


def register_chat_callbacks(app):
    """
    Chat callbacks:
      - on_chat_send: append user text to chat_inbox.jsonl in the selected run
      - on_chat_update: stream UTD macro 'say' events + user inbox into a simple text view

    State shape stored in dcc.Store(id="chat-state"):
      {
        "run_dir": str,
        "utd_size": int,     # last read byte offset for utd_events.jsonl
        "inbox_size": int,   # last read byte offset for chat_inbox.jsonl
        "items": List[Dict]  # standardized chat items
      }
    """

    @app.callback(
        Output("chat-status", "children"),
        Output("chat-input", "value"),
        Input("chat-send", "n_clicks"),
        State("run-dir", "value"),
        State("chat-input", "value"),
        prevent_initial_call=True,
    )
    def on_chat_send(_n, run_dir, text):
        rd = (run_dir or "").strip()
        msg = (text or "").strip()
        if not rd:
            return "Select a run directory.", no_update
        if not msg:
            return "Type a message.", no_update
        try:
            inbox = os.path.join(rd, "chat_inbox.jsonl")
            os.makedirs(os.path.dirname(inbox), exist_ok=True)
            with open(inbox, "a", encoding="utf-8") as fh:
                fh.write(json.dumps({"type": "text", "msg": msg}, ensure_ascii=False) + "\n")
            return "Sent.", ""
        except Exception as e:
            return f"Error writing chat_inbox.jsonl: {e}", no_update

    @app.callback(
        Output("chat-view", "children"),
        Output("chat-state", "data"),
        Input("poll", "n_intervals"),
        Input("chat-filter", "value"),
        Input("ui-state", "data"),
        State("run-dir", "value"),
        State("chat-state", "data"),
        prevent_initial_call=False,
    )
    def on_chat_update(_n, filt, ui_state, run_dir, data):
        rd = (run_dir or "").strip()
        if not rd:
            return "", {"run_dir": "", "utd_size": 0, "inbox_size": 0, "items": []}

        state = data or {}
        items = list(state.get("items", []))
        last_run = state.get("run_dir")
        utd_size = int(state.get("utd_size", 0)) if isinstance(state.get("utd_size"), int) else 0
        inbox_size = int(state.get("inbox_size", 0)) if isinstance(state.get("inbox_size"), int) else 0

        if last_run != rd:
            # Reset when switching runs
            items = []
            utd_size = 0
            inbox_size = 0

        # UI-governed tailing (default OFF). If tail_chat is False, render from in-memory only.
        ui = ui_state or {}
        try:
            tail_enabled = bool(ui.get("tail_chat", False))
        except Exception:
            tail_enabled = False

        if not tail_enabled:
            items = trim_items(items, limit=200)
            view = render_chat_view(items, filt=(filt or "all"))
            return view, {
                "run_dir": rd,
                "utd_size": int(utd_size),
                "inbox_size": int(inbox_size),
                "items": items,
            }

        # Stream UTD macro events (UI explicitly enabled)
        utd_path = os.path.join(rd, "utd_events.jsonl")
        new_utd_recs, new_utd_size = tail_jsonl_bytes(utd_path, utd_size)
        items.extend(items_from_utd_records(new_utd_recs))

        # Stream user inbox (UI explicitly enabled)
        inbox_path = os.path.join(rd, "chat_inbox.jsonl")
        new_inbox_recs, new_inbox_size = tail_jsonl_bytes(inbox_path, inbox_size)
        items.extend(items_from_inbox_records(new_inbox_recs))

        items = trim_items(items, limit=200)
        view = render_chat_view(items, filt=(filt or "all"))

        return view, {
            "run_dir": rd,
            "utd_size": int(new_utd_size),
            "inbox_size": int(new_inbox_size),
            "items": items,
        }]]></content>
    </file>
    <file>
      <path>frontend/callbacks/engram.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from dash import Input, Output, State  # noqa: F401
from fum_rt.frontend.utilities.fs_utils import _list_files


def register_engram_callbacks(app):
    """
    Engram path helpers:
      - Populate dropdown options by scanning runs-root for .h5/.npz
      - Keep the free-text input in sync with dropdown selection
    """

    @app.callback(
        Output("rc-load-engram-path", "options"),
        Input("runs-root", "value"),
        Input("poll", "n_intervals"),
        Input("proc-status", "children"),
        State("run-dir", "value"),
        prevent_initial_call=False,
    )
    def on_runs_root_change(runs_root_dir, _n, _proc_status, run_dir):
        """
        Keep engram dropdown options fresh:
        - Rescan on runs-root changes
        - Rescan periodically (poll)
        - Rescan on process status transitions (Start/Resume)
        Also include current run_dir tree when it falls outside runs_root.
        """
        options = []
        seen = set()

        # 1) Scan runs_root recursively
        if runs_root_dir and os.path.isdir(runs_root_dir):
            try:
                engram_files = _list_files(runs_root_dir, exts=[".h5", ".npz"], recursive=True)
            except Exception:
                engram_files = []
            for rel_path in engram_files:
                full_path = os.path.join(runs_root_dir, rel_path)
                if full_path in seen:
                    continue
                seen.add(full_path)
                options.append({"label": rel_path.replace(os.path.sep, "/"), "value": full_path})

        # 2) Also scan active run_dir if provided and not already covered
        rd = (run_dir or "").strip()
        if rd and os.path.isdir(rd):
            try:
                rd_files = _list_files(rd, exts=[".h5", ".npz"], recursive=True)
            except Exception:
                rd_files = []
            # If rd lies under runs_root_dir, the above scan likely covered it; de-dup via 'seen'
            base = os.path.basename(rd.rstrip(os.path.sep))
            for rel_path in rd_files:
                full_path = os.path.join(rd, rel_path)
                if full_path in seen:
                    continue
                seen.add(full_path)
                label = f"{base}/{rel_path.replace(os.path.sep, '/')}"
                options.append({"label": label, "value": full_path})

        return options

    @app.callback(
        Output("rc-load-engram-input", "value", allow_duplicate=True),
        Input("rc-load-engram-path", "value"),
        prevent_initial_call=True,
    )
    def sync_engram_input_from_dropdown(val):
        return val]]></content>
    </file>
    <file>
      <path>frontend/callbacks/feed.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from dash import Input, Output, State, no_update  # noqa: F401
from fum_rt.frontend.utilities.pdf_utils import convert_pdf_to_text_file


def register_feed_callbacks(app, manager, repo_root: str):
    """
    Feed controls:
      - Start feeding a file into the managed process stdin at a given rate
      - Stop feeding
    Uses the same IDs as the inline version in fum_live to preserve behavior.
    """

    @app.callback(
        Output("send-status", "children", allow_duplicate=True),
        Input("feed-start", "n_clicks"),
        State("feed-path", "value"),
        State("feed-rate", "value"),
        prevent_initial_call=True,
    )
    def on_feed_start(_n, path, rate):
        p = (path or "").strip()
        if not p:
            return "Provide a feed path (relative to fum_rt/data or absolute)."
        chosen = p
        try:
            if (not os.path.isabs(chosen)) or (not os.path.exists(chosen)):
                data_dir = os.path.join(repo_root, "fum_rt", "data")
                cand = os.path.join(data_dir, p)
                if os.path.exists(cand):
                    chosen = cand
        except Exception:
            pass

        # If a PDF is selected, convert to text (best-effort with graceful fallback) before feeding
        status_prefix = ""
        try:
            if chosen.lower().endswith(".pdf"):
                out_dir = os.path.join(repo_root, "outputs", "pdf_text")
                os.makedirs(out_dir, exist_ok=True)
                txt_path, method = convert_pdf_to_text_file(chosen, out_dir)
                if txt_path:
                    status_prefix = f"Converted PDF via {method}; "
                    chosen = txt_path
                else:
                    return "PDF conversion failed (install PyMuPDF/pdfminer.six/PyPDF2 or pytesseract+pdf2image)."
        except Exception:
            return "PDF conversion failed."

        ok = manager.feed_file(chosen, float(rate or 20.0))
        return f"{status_prefix}Feeding from {chosen}." if ok else "Feed failed (check process running and path)."

    @app.callback(
        Output("send-status", "children", allow_duplicate=True),
        Input("feed-stop", "n_clicks"),
        prevent_initial_call=True,
    )
    def on_feed_stop(_n):
        manager.stop_feed()
        return "Feed stopped."]]></content>
    </file>
    <file>
      <path>frontend/callbacks/file_picker/__init__.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker callbacks package.

Exports:
- register_file_picker_static
- register_file_picker_engram

Implementation files:
- common.py      (shared callback logic: navigation, tree rendering, status, confirm)
- registrars.py  (thin wrappers: static and dynamic/engram root selection)
"""

from .registrars import (
    register_file_picker_static,
    register_file_picker_engram,
)

__all__ = ["register_file_picker_static", "register_file_picker_engram"]]]></content>
    </file>
    <file>
      <path>frontend/callbacks/file_picker/common.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker Common Callback Registrar (package-scoped)

Shared callback logic:
- Navigation (root/up/open-dir)
- Tree render (bounded IO; no recursion)
- Breadcrumbs
- Status computation (dir vs file)
- File click and Confirm

Design:
- Bounded IO (single-directory listings only)
- No scans in core/ or maps/ (enforced in controllers)
"""

import os
from typing import List

import dash
from dash import html, Input, Output, State, ALL, no_update

# Widgets
from fum_rt.frontend.components.widgets.file_picker import _modal_styles as _fp_modal_styles
from fum_rt.frontend.components.widgets.file_tree import dir_row as _w_dir_row, file_row as _w_file_row
from fum_rt.frontend.components.widgets.file_breadcrumbs import breadcrumbs as _w_breadcrumbs

# Controllers (bounded IO + guards)
from fum_rt.frontend.controllers.file_picker_controller import (
    clamp_to_root as _ctl_clamp,
    list_dir as _ctl_list_dir,
    next_children as _ctl_next_children,
)

# Controller helpers for ctx parsing and status formatting
from fum_rt.frontend.controllers.file_picker_ctx import get_trigger_id_obj as _get_ctx_obj
from fum_rt.frontend.controllers.file_picker_status import (
    file_status_text as _file_status_text,
    directory_status_text as _dir_status_text,
)


def register_file_picker_common(app, prefix: str, target_id: str, project_root: str) -> None:
    """
    Register navigation, list population, cancel, file click, and confirm callbacks
    for a file-picker instance. The 'open' callbacks (static/dynamic) are provided by registrars.

    Args:
      app: Dash app
      prefix: ID prefix for this instance
      target_id: Component id whose options/value will be updated on Confirm
      project_root: Absolute repository root for global clamp
    """
    mid = f"{prefix}-modal"
    root_store = f"{prefix}-root"
    cwd_store = f"{prefix}-cwd"
    exts_store = f"{prefix}-exts"
    dirs_dd = f"{prefix}-dirs"
    files_dd = f"{prefix}-files"
    cwd_label = f"{prefix}-cwd-label"
    status_div = f"{prefix}-status"
    up_btn = f"{prefix}-up-btn"
    root_btn = f"{prefix}-root-btn"
    open_dir_btn = f"{prefix}-open-dir-btn"
    cancel_btn = f"{prefix}-cancel-btn"
    confirm_btn = f"{prefix}-confirm-btn"
    sel_store = f"{prefix}-selected-path"
    sel_label = f"{prefix}-selected-label"
    dir_sel_store = f"{prefix}-dir-sel"
    file_sel_store = f"{prefix}-file-sel"
    sel_dir_store = f"{prefix}-selected-dir"
    tree_store = f"{prefix}-tree-store"
    last_action_store = f"{prefix}-last-action"

    def _clamp_to_project(path: str) -> str:
        return _ctl_clamp(path, project_root)

    # Cancel -> hide modal
    @app.callback(
        Output(mid, "style", allow_duplicate=True),
        Input(cancel_btn, "n_clicks"),
        prevent_initial_call=True,
    )
    def on_cancel(_n):
        style = _fp_modal_styles()
        # Keep 'display' as defined by component default (hidden)
        return style

    # Navigate to root
    @app.callback(
        Output(cwd_store, "data", allow_duplicate=True),
        Input(root_btn, "n_clicks"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def on_root(_n, root):
        r = (root or "").strip()
        return _clamp_to_project(os.path.abspath(r)) if r else no_update

    # Navigate up
    @app.callback(
        Output(cwd_store, "data", allow_duplicate=True),
        Input(up_btn, "n_clicks"),
        State(cwd_store, "data"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def on_up(_n, cwd, root):
        c = (cwd or "").strip()
        r = (root or "").strip()
        if not c or not r:
            return no_update
        parent = os.path.dirname(os.path.abspath(c))
        return _ctl_clamp(parent, r)

    # Open selected subfolder
    @app.callback(
        Output(cwd_store, "data", allow_duplicate=True),
        Input(open_dir_btn, "n_clicks"),
        State(dir_sel_store, "data"),
        State(cwd_store, "data"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def on_open_dir(_n, sel, cwd, root):
        s = (sel or "").strip()
        c = (cwd or "").strip()
        r = (root or "").strip()
        if not s or not c:
            return no_update
        cand = os.path.join(c, s)
        if os.path.isdir(cand):
            return _ctl_clamp(cand, r or cand)
        return no_update

    # Render a single tree (folders + files) with breadcrumbs; bounded IO (no scans here)
    @app.callback(
        Output(f"{prefix}-dirs-list", "children"),
        Output(cwd_label, "children"),
        Output(f"{prefix}-crumbs", "children"),
        Output(status_div, "children", allow_duplicate=True),
        Input(tree_store, "data"),
        Input(sel_dir_store, "data"),
        Input(exts_store, "data"),
        Input(file_sel_store, "data"),
        Input(last_action_store, "data"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def on_render(tree_data, selected_dir, exts, file_sel, last_action, root):
        # Resolve root and selected directory (clamped to project root)
        try:
            rabs_in = os.path.abspath((root or "").strip()) if root else None
        except Exception:
            rabs_in = None
        rabs = _clamp_to_project(rabs_in or project_root)
        s_in = (selected_dir or "").strip()
        s = os.path.abspath(s_in) if s_in else rabs
        s = _ctl_clamp(s, rabs)

        # Build tree UI from cached nodes only (no os.listdir here)
        nodes = {}
        try:
            if isinstance(tree_data, dict):
                nodes = dict(tree_data.get("nodes") or {})
                if rabs is None and tree_data.get("root"):
                    rabs = tree_data.get("root")
        except Exception:
            nodes = {}

        allow_all = not exts
        lowered_exts = [e.lower() for e in (exts or [])]

        tree_children = []

        def _build(path: str, depth: int):
            label = os.path.basename(path) or path
            node = nodes.get(path, {})
            expanded = bool(node.get("expanded"))
            is_sel = os.path.abspath(path) == s
            tree_children.append(_w_dir_row(prefix, path, label, depth, expanded, is_sel))
            if expanded:
                # Render subdirectories first
                for child in (node.get("subdirs") or []):
                    try:
                        ch = os.path.join(path, child)
                    except Exception:
                        continue
                    _build(ch, depth + 1)
                # Then render files belonging to this directory.
                # Use cached node files; if absent, do a bounded one-step listing with ext filter & dotfile hiding.
                files_all = node.get("files") or []
                if (not files_all) and os.path.isdir(path):
                    try:
                        _subdirs_tmp, files_tmp = _ctl_list_dir(path, exts=(exts or []), hide_dotfiles=True)
                        files_all = files_tmp or []
                    except Exception:
                        files_all = []
                for f in files_all:
                    try:
                        fpath = os.path.join(path, f)
                    except Exception:
                        continue
                    # node files are already ext-filtered; keep downstream filter for robustness
                    if allow_all or any(f.lower().endswith(e) for e in lowered_exts):
                        tree_children.append(_w_file_row(prefix, fpath, depth, (file_sel or "").strip() == fpath))

        start_root = rabs or s
        if start_root:
            if start_root not in nodes:
                nodes[start_root] = {"expanded": True, "subdirs": [], "files": []}
            _build(start_root, 0)

        cwd_label_text = f"cwd: {s}" if s and os.path.isdir(s) else "cwd: (missing)"

        # Breadcrumbs based on root and selected directory (widgetized)
        try:
            base = os.path.abspath((rabs or s) or "")
            sel = os.path.abspath(s or base)
            crumbs = _w_breadcrumbs(prefix, base, sel)
        except Exception:
            crumbs = []

        # Metadata statusbar - centralized with robust gating (only show file when it belongs to the selected dir)
        fsel = (file_sel or "").strip()
        try:
            s_abs = os.path.abspath(s) if s else ""
            fsel_abs = os.path.abspath(fsel) if fsel else ""
            fsel_dir_abs = os.path.abspath(os.path.dirname(fsel_abs)) if fsel_abs else ""
        except Exception:
            s_abs, fsel_abs, fsel_dir_abs = "", "", ""
        if (last_action == "file") and fsel_abs and os.path.isfile(fsel_abs) and (fsel_dir_abs == s_abs):
            status_text = _file_status_text(fsel_abs)
        else:
            status_text = _dir_status_text(s, exts=(exts or []), hide_dotfiles=True)
        return tree_children, cwd_label_text, crumbs, status_text

    # Tree toggle: expand/collapse one node; cache listing on expand (one directory at a time)
    @app.callback(
        Output(tree_store, "data", allow_duplicate=True),
        Output(sel_dir_store, "data", allow_duplicate=True),
        Output(file_sel_store, "data", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Output(status_div, "children", allow_duplicate=True),
        Input({"role": f"{prefix}-tree-dir", "path": ALL}, "n_clicks"),
        State(tree_store, "data"),
        State(root_store, "data"),
        State(exts_store, "data"),
        prevent_initial_call=True,
    )
    def on_tree_toggle(_clicks, tree_data, root, exts):
        ctx = dash.callback_context
        obj = _get_ctx_obj(ctx)
        if not isinstance(obj, dict) or obj.get("role") != f"{prefix}-tree-dir":
            return no_update, no_update, no_update, no_update
        target = (obj.get("path", "") or "").strip()
        if not target:
            return no_update, no_update, no_update, no_update

        r = (root or "").strip()
        rabs0 = os.path.abspath(r) if r else os.path.abspath(target)
        rabs = _clamp_to_project(rabs0)

        # Initialize tree structure if needed
        tree = tree_data if isinstance(tree_data, dict) else {}
        if not tree:
            tree = {"root": rabs, "nodes": {}}
        nodes = tree.get("nodes") or {}
        tree["root"] = tree.get("root") or rabs

        # Clamp and toggle
        p = _ctl_clamp(target, rabs)
        node = dict(nodes.get(p) or {})
        # Never collapse the root node to avoid "everything disappears" UX
        if os.path.abspath(p) == os.path.abspath(rabs):
            node["expanded"] = True
        else:
            toggled = not bool(node.get("expanded"))
            node["expanded"] = toggled
        if node.get("expanded") and (not node.get("subdirs") or not node.get("files")):
            # Bounded IO: list this node's direct children once on expansion (ext-filtered, dotfiles hidden)
            subdirs, files_all = _ctl_next_children(p, exts=(exts or []), hide_dotfiles=True)
            node["subdirs"] = subdirs or []
            node["files"] = files_all or []
        nodes[p] = node
        tree["nodes"] = nodes
        # Also clear any existing file selection on folder toggle so status shows folder metadata
        return tree, p, "", "nav", _dir_status_text(p, exts=(exts or []), hide_dotfiles=True)

    # Breadcrumb click -> select directory (clamped)
    @app.callback(
        Output(sel_dir_store, "data", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Input({"role": f"{prefix}-crumb", "path": ALL}, "n_clicks"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def on_crumb_click(_clicks, root):
        ctx = dash.callback_context
        obj = _get_ctx_obj(ctx)
        if not isinstance(obj, dict) or obj.get("role") != f"{prefix}-crumb":
            return no_update, no_update
        target = (obj.get("path", "") or "").strip()
        if not target:
            return no_update, no_update
        r = (root or "").strip()
        if r:
            return _ctl_clamp(target, r), "nav"
        return target, "nav"

    # Sync selected-dir when legacy cwd changes (Root/Up)
    @app.callback(
        Output(sel_dir_store, "data", allow_duplicate=True),
        Output(file_sel_store, "data", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Input(cwd_store, "data"),
        State(root_store, "data"),
        prevent_initial_call=True,
    )
    def sync_selected_dir(cwd, root):
        c = (cwd or "").strip()
        r = (root or "").strip()
        if not c:
            return no_update, no_update, no_update
        c = _clamp_to_project(c)
        if r:
            r = _clamp_to_project(r)
            return _ctl_clamp(c, r), "", "nav"
        return c, "", "nav"

    # Clear stale file selection when directory changes
    @app.callback(
        Output(file_sel_store, "data", allow_duplicate=True),
        Input(sel_dir_store, "data"),
        prevent_initial_call=True,
    )
    def _clear_file_sel_on_dir_change(_sel):
        return ""

    # Click handlers for files inside the tree
    @app.callback(
        Output(dir_sel_store, "data"),
        Output(file_sel_store, "data", allow_duplicate=True),
        Output(status_div, "children", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Output(sel_store, "data", allow_duplicate=True),
        Output(sel_label, "children", allow_duplicate=True),
        Input({"role": f"{prefix}-file", "path": ALL}, "n_clicks"),
        prevent_initial_call=True,
    )
    def on_explorer_click(_file_clicks):
        ctx = dash.callback_context

        # Parse triggering id; avoid strict n_clicks gating which can be None or string in some Dash versions
        obj = _get_ctx_obj(ctx)
        if not isinstance(obj, dict) or obj.get("role") != f"{prefix}-file":
            return no_update, no_update, no_update, no_update, no_update, no_update
        fpath = (obj.get("path", "") or "").strip()
        if not fpath:
            return no_update, no_update, no_update, no_update, no_update, no_update
        # Persist selection into stores so Confirm has a reliable fallback
        from os.path import basename
        return no_update, fpath, no_update, "file", fpath, basename(fpath)

    # Confirm selection -> set stores, hide modal, and update target value
    @app.callback(
        Output(sel_store, "data"),
        Output(sel_label, "children"),
        Output(status_div, "children", allow_duplicate=True),
        Output(mid, "style", allow_duplicate=True),
        Output(target_id, "options", allow_duplicate=True),
        Output(target_id, "value", allow_duplicate=True),
        Input(confirm_btn, "n_clicks"),
        State(file_sel_store, "data"),
        State(sel_dir_store, "data"),
        State(target_id, "options"),
        State(sel_store, "data"),
        prevent_initial_call=True,
    )
    def on_confirm(_n, file_sel, sel_dir, options, sel_data):
        # Primary source: file selected by clicking in explorer
        fsel = (file_sel or "").strip()
        # Fallback to last persisted selection (set on click) if primary missing
        if not fsel:
            fsel = (sel_data or "").strip()
        c = (sel_dir or "").strip()
        if not fsel:
            return no_update, no_update, "Select a file.", no_update, no_update, no_update
        path = fsel if os.path.isabs(fsel) else (os.path.abspath(os.path.join(c, fsel)) if c else os.path.abspath(fsel))
        label = os.path.basename(path)

        # Ensure Dropdown 'options' contains the selected path to avoid client-side validation blocking the update
        try:
            existing = options if isinstance(options, list) else []
        except Exception:
            existing = []
        try:
            vals = set()
            new_options = []
            for o in existing:
                if isinstance(o, dict):
                    v = o.get("value")
                    if isinstance(v, str) and v not in vals:
                        vals.add(v)
                        new_options.append(o)
            if path not in vals:
                new_options.append({"label": path, "value": path})
        except Exception:
            new_options = [{"label": path, "value": path}]

        status = f"Selected: {path}"
        style = _fp_modal_styles()  # hidden (default)
        return path, label, status, style, new_options, path]]></content>
    </file>
    <file>
      <path>frontend/callbacks/file_picker/registrars.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker Registrars (package-scoped)

- Static root registrar
- Dynamic (engram) root registrar

Both delegate common behavior to .common.register_file_picker_common
"""

import os
from typing import List

from dash import Output, Input, State  # type: ignore

from fum_rt.frontend.components.widgets.file_picker import _modal_styles as _fp_modal_styles
from fum_rt.frontend.controllers.file_picker_controller import init_tree as _ctl_init_tree
from .common import register_file_picker_common


def _project_root_default() -> str:
    """
    Resolve a safe project root for clamping.
    Prefers FUM_PROJECT_ROOT; falls back to repository root by relative path.
    """
    try:
        env = os.path.abspath((os.getenv("FUM_PROJECT_ROOT") or "").strip())
        if env and os.path.isdir(env):
            return env
    except Exception:
        pass
    # callbacks/file_picker/ -> callbacks/ -> frontend/ -> .. (repo root)
    return os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))


def register_file_picker_static(app, prefix: str, root: str, exts: List[str] | None, target_id: str) -> None:
    """
    Register a file-picker with a fixed root (bounded IO).
    """
    project_root = _project_root_default()

    mid = f"{prefix}-modal"
    root_store = f"{prefix}-root"
    cwd_store = f"{prefix}-cwd"
    exts_store = f"{prefix}-exts"
    status_div = f"{prefix}-status"
    open_btn = f"{prefix}-open-btn"

    # Open -> show modal and initialize stores
    @app.callback(
        Output(mid, "style", allow_duplicate=True),
        Output(root_store, "data"),
        Output(cwd_store, "data", allow_duplicate=True),
        Output(exts_store, "data"),
        Output(f"{prefix}-file-sel", "data", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Input(open_btn, "n_clicks"),
        prevent_initial_call=True,
    )
    def on_open(_n):
        r0 = os.path.abspath(root or "")
        r = r0 if os.path.isdir(r0) else project_root
        style = _fp_modal_styles()
        style["display"] = "flex"
        return style, r, r, list(exts or []), "", "nav"

    # Initialize tree and selection on open (static)
    @app.callback(
        Output(f"{prefix}-selected-dir", "data", allow_duplicate=True),
        Output(f"{prefix}-tree-store", "data", allow_duplicate=True),
        Output(status_div, "children", allow_duplicate=True),
        Input(open_btn, "n_clicks"),
        prevent_initial_call=True,
    )
    def on_open_init(_n):
        r0 = os.path.abspath(root or "")
        r = r0 if os.path.isdir(r0) else project_root
        tree = _ctl_init_tree(r, exts=(exts or []), hide_dotfiles=True)
        try:
            files0 = (tree.get("nodes", {}).get(r, {}) or {}).get("files") or []
            status_text = f"root: {r}  files: {len(files0)}"
        except Exception:
            status_text = f"root: {r}  files: 0"
        return r, tree, status_text

    register_file_picker_common(app, prefix, target_id, project_root)


def register_file_picker_engram(
    app,
    prefix: str,
    exts: List[str] | None,
    target_id: str,
    fallback_root: str,
) -> None:
    """
    Register a file-picker whose root = current run-dir if available, else runs-root, else fallback_root.
    """
    project_root = _project_root_default()

    mid = f"{prefix}-modal"
    root_store = f"{prefix}-root"
    cwd_store = f"{prefix}-cwd"
    exts_store = f"{prefix}-exts"
    status_div = f"{prefix}-status"
    open_btn = f"{prefix}-open-btn"

    # Open -> compute root dynamically from run-dir / runs-root
    @app.callback(
        Output(mid, "style", allow_duplicate=True),
        Output(root_store, "data"),
        Output(cwd_store, "data", allow_duplicate=True),
        Output(exts_store, "data"),
        Output(f"{prefix}-file-sel", "data", allow_duplicate=True),
        Output(f"{prefix}-last-action", "data", allow_duplicate=True),
        Input(open_btn, "n_clicks"),
        State("run-dir", "value"),
        State("runs-root", "value"),
        prevent_initial_call=True,
    )
    def on_open_dynamic(_n, run_dir, runs_root):
        candidates = []
        rd = (run_dir or "").strip()
        rr = (runs_root or "").strip()
        if rd and os.path.isdir(rd):
            candidates.append(rd)
        if rr and os.path.isdir(rr):
            candidates.append(rr)
        if fallback_root:
            candidates.append(fallback_root)
        # choose the first existing (clamped) directory
        for cand in candidates:
            if not cand:
                continue
            r_candidate = os.path.abspath(cand)
            if os.path.isdir(r_candidate):
                r = r_candidate
                style = _fp_modal_styles()
                style["display"] = "flex"
                return style, r, r, list(exts or []), "", "nav"
        # fallback: open at project root
        style = _fp_modal_styles()
        style["display"] = "flex"
        return style, project_root, project_root, list(exts or []), "", "nav"

    # Initialize tree and selection on open (dynamic)
    @app.callback(
        Output(f"{prefix}-selected-dir", "data", allow_duplicate=True),
        Output(f"{prefix}-tree-store", "data", allow_duplicate=True),
        Output(status_div, "children", allow_duplicate=True),
        Input(open_btn, "n_clicks"),
        State("run-dir", "value"),
        State("runs-root", "value"),
        prevent_initial_call=True,
    )
    def on_open_init_dynamic(_n, run_dir, runs_root):
        candidates = []
        rd = (run_dir or "").strip()
        rr = (runs_root or "").strip()
        if rd and os.path.isdir(rd):
            candidates.append(rd)
        if rr and os.path.isdir(rr):
            candidates.append(rr)
        if fallback_root:
            candidates.append(fallback_root)
        for cand in candidates:
            if not cand:
                continue
            r = os.path.abspath(cand)
            if os.path.isdir(r):
                tree = _ctl_init_tree(r, exts=(exts or []), hide_dotfiles=True)
                try:
                    files0 = (tree.get("nodes", {}).get(r, {}) or {}).get("files") or []
                    status_text = f"root: {r}  files: {len(files0)}"
                except Exception:
                    status_text = f"root: {r}  files: 0"
                return r, tree, status_text
        r = project_root
        tree = _ctl_init_tree(r, exts=(exts or []), hide_dotfiles=True)
        try:
            files0 = (tree.get("nodes", {}).get(r, {}) or {}).get("files") or []
            status_text = f"root: {r}  files: {len(files0)}"
        except Exception:
            status_text = f"root: {r}  files: 0"
        return r, tree, status_text

    register_file_picker_common(app, prefix, target_id, project_root)]]></content>
    </file>
    <file>
      <path>frontend/callbacks/interval.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import Any
from dash import Input, Output  # noqa: F401


def _int_or(v: Any, dv: int) -> int:
    try:
        return int(v)
    except Exception:
        return dv


def register_interval_callbacks(app):
    """
    Bind dcc.Interval(id="poll") cadence to UI Performance settings (ui-state).
    - update_ms <= 0 disables polling
    - update_ms > 0 sets the interval in milliseconds (min clamp = 100 ms)
    This replaces the env-only cadence for responsiveness control without restart.
    """
    @app.callback(
        Output("poll", "interval"),
        Output("poll", "disabled"),
        Input("ui-state", "data"),
        prevent_initial_call=False,
    )
    def on_ui_interval(ui_state: dict | None):
        ui = ui_state or {}
        update_ms = _int_or(ui.get("update_ms", 800), 800)
        if update_ms <= 0:
            # Disable polling entirely
            return 100, True  # Dash requires a valid interval even if disabled
        # Enforce a sane lower bound
        return int(max(100, update_ms)), False


__all__ = ["register_interval_callbacks"]]]></content>
    </file>
    <file>
      <path>frontend/callbacks/logs.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from dash import Input, Output  # noqa: F401


def register_logs_callbacks(app, manager):
    """
    Launcher log viewer:
      - Streams the tail of the last launcher log written by ProcessManager.
    IDs preserved to match existing layout.
    """

    @app.callback(
        Output("launch-log", "children"),
        Input("poll", "n_intervals"),
        Input("show-log", "n_clicks"),
        Input("ui-state", "data"),
        prevent_initial_call=False,
    )
    def update_launch_log(_n, _clicks, ui_state):
        """
        Tail the launcher log efficiently without rereading entire file each tick.
        - UI-governed: only tails when ui_state['tail_launch_log'] is True (default OFF).
        - Bounds bytes via DASH_LOG_TAIL_BYTES (env) while staying purely UI-controlled for enable/disable.
        - One-shot behavior on "Show Launcher Log" click: shows once, then reverts to UI toggle gating.
        """
        ui = ui_state or {}
        clicks = int(_clicks or 0)

        # One-shot gating: only tail on button press transitions, or when toggle is ON.
        try:
            last_clicks = int(getattr(update_launch_log, "_last_clicks", 0))
        except Exception:
            last_clicks = 0
        one_shot = clicks > last_clicks
        try:
            setattr(update_launch_log, "_last_clicks", clicks)
        except Exception:
            pass

        try:
            tail_enabled = bool(ui.get("tail_launch_log", False))
        except Exception:
            tail_enabled = False

        if not (tail_enabled or one_shot):
            return "Launcher log tail disabled (toggle in UI Performance or click Show Launcher Log)."

        try:
            path = getattr(manager, "launch_log", None)
            if not path or not os.path.exists(path):
                return "No launcher log yet."

            try:
                tail_bytes = int(os.getenv("DASH_LOG_TAIL_BYTES", "16384"))
            except Exception:
                tail_bytes = 16384

            st = os.stat(path)
            size = int(getattr(st, "st_size", 0))
            start = max(0, size - max(1024, tail_bytes))  # at least 1 KiB

            with open(path, "rb") as fh:
                fh.seek(start, os.SEEK_SET)
                data = fh.read()

            text = data.decode("utf-8", errors="ignore")
            # Keep a compact tail to avoid heavy DOM nodes
            return text[-4000:] if len(text) > 4000 else text
        except Exception as e:
            return f"Error reading launcher log: {e}"]]></content>
    </file>
    <file>
      <path>frontend/callbacks/perf.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import Any, Dict, List
from dash import Input, Output  # noqa: F401


def _is_on(v: Any) -> bool:
    try:
        return isinstance(v, list) and ("on" in v)
    except Exception:
        return False


def _int_or(v: Any, dv: int) -> int:
    try:
        i = int(v)
        return i
    except Exception:
        return dv


def _float_or(v: Any, dv: float) -> float:
    try:
        f = float(v)
        return f
    except Exception:
        return dv


def _str_or(v: Any, dv: str) -> str:
    try:
        s = str(v).strip()
        return s if s else dv
    except Exception:
        return dv


def register_perf_callbacks(app):
    """
    Mirror UI Performance controls into dcc.Store(id="ui-state").
    No environment variables are required; settings apply immediately.
    """
    @app.callback(
        Output("ui-state", "data"),
        Input("ui-charts-http", "value"),
        Input("ui-update-ms", "value"),
        Input("ui-points-cap", "value"),
        Input("ui-series-cap", "value"),
        Input("ui-status-url", "value"),
        Input("ui-status-timeout", "value"),
        Input("ui-tail-chat", "value"),
        Input("ui-tail-launch", "value"),
        prevent_initial_call=False,
    )
    def on_perf_update(
        charts_http_v: List[str],
        update_ms_v: Any,
        points_cap_v: Any,
        series_cap_v: Any,
        status_url_v: Any,
        status_timeout_v: Any,
        tail_chat_v: List[str],
        tail_launch_v: List[str],
    ):
        charts_http = _is_on(charts_http_v)
        tail_chat = _is_on(tail_chat_v)
        tail_launch_log = _is_on(tail_launch_v)
        update_ms = _int_or(update_ms_v, 800)
        points_cap = _int_or(points_cap_v, 1200)
        series_cap = _int_or(series_cap_v, 6)
        status_url = _str_or(status_url_v, "http://127.0.0.1:8787/status/snapshot")
        status_timeout = _float_or(status_timeout_v, 0.2)

        # Provide keys consumed by charts controller (maxp/decimate)
        ui: Dict[str, Any] = {
            "charts_http_only": bool(charts_http),
            "update_ms": int(max(100, update_ms)),
            "points_cap": int(max(50, points_cap)),
            "series_cap": int(max(1, series_cap)),
            "tail_chat": bool(tail_chat),
            "tail_launch_log": bool(tail_launch_log),
            "status_url": status_url,
            "status_timeout": float(max(0.05, status_timeout)),
            # Charts controller knobs (no env):
            "maxp": int(max(50, points_cap)),
            "decimate": int(max(0, points_cap)),  # treat as target plotted points (0=off if caller wants)
        }
        return ui


__all__ = ["register_perf_callbacks"]]]></content>
    </file>
    <file>
      <path>frontend/callbacks/process.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import dash
from typing import Any, Dict, Tuple
from dash import Input, Output, State, no_update  # noqa: F401 (bound at runtime)

from fum_rt.frontend.utilities.profiles import assemble_profile as _assemble_profile, safe_int as _safe_int, safe_float as _safe_float
from fum_rt.frontend.utilities.fs_utils import latest_checkpoint


def register_process_callbacks(app, runs_root: str, manager, default_profile: Dict[str, Any]):
    """
    Process lifecycle callbacks:
      - Start New Run
      - Resume Selected Run
      - Stop Managed Run
    Depends on:
      - manager: ProcessManager instance
      - runs_root: initial runs root (UI may update manager later)
      - default_profile: dict of defaults for assembling profiles
    """

    def _start_or_resume(profile: Dict[str, Any]) -> Tuple[str, Any]:
        ok, msg = manager.start(profile)
        if not ok:
            return f"Start failed:\n{msg}", no_update
        rd = profile.get("run_dir") or msg
        cmd_echo = " ".join(manager.last_cmd or [])
        status = (
            f"Started.\nrun_dir={rd}\n"
            f"checkpoint_every={profile.get('checkpoint_every')} keep={profile.get('checkpoint_keep')}\n"
            f"cmd: {cmd_echo}\n"
            f"launch_log: {manager.launch_log}"
        )
        return status, rd or no_update

    @app.callback(
        Output("proc-status", "children", allow_duplicate=True),
        Output("run-dir", "value", allow_duplicate=True),
        Input("start-run", "n_clicks"),
        State("runs-root", "value"),
        State("cfg-neurons", "value"),
        State("cfg-k", "value"),
        State("cfg-hz", "value"),
        State("cfg-domain", "value"),
        State("cfg-use-time-dynamics", "value"),
        State("cfg-sparse-mode", "value"),
        State("cfg-threshold", "value"),
        State("cfg-lambda-omega", "value"),
        State("cfg-candidates", "value"),
        State("cfg-walkers", "value"),
        State("cfg-hops", "value"),
        State("cfg-status-interval", "value"),
        State("cfg-bundle-size", "value"),
        State("cfg-prune-factor", "value"),
        State("cfg-stim-group-size", "value"),
        State("cfg-stim-amp", "value"),
        State("cfg-stim-decay", "value"),
        State("cfg-stim-max-symbols", "value"),
        State("cfg-speak-auto", "value"),
        State("cfg-speak-z", "value"),
        State("cfg-speak-hysteresis", "value"),
        State("cfg-speak-cooldown-ticks", "value"),
        State("cfg-speak-valence-thresh", "value"),
        State("cfg-b1-half-life-ticks", "value"),
        State("cfg-viz-every", "value"),
        State("cfg-log-every", "value"),
        State("cfg-checkpoint-every", "value"),
        State("cfg-checkpoint-keep", "value"),
        State("cfg-duration", "value"),
        State("rc-load-engram-path", "value"),
        State("rc-load-engram-input", "value"),
        prevent_initial_call=True,
    )
    def on_start_run(
        n_start,
        root,
        neurons,
        k,
        hz,
        domain,
        use_td,
        sparse_mode,
        threshold,
        lambda_omega,
        candidates,
        walkers,
        hops,
        status_interval,
        bundle_size,
        prune_factor,
        stim_group_size,
        stim_amp,
        stim_decay,
        stim_max_symbols,
        speak_auto,
        speak_z,
        speak_hyst,
        speak_cd,
        speak_val,
        b1_hl,
        viz_every,
        log_every,
        checkpoint_every,
        checkpoint_keep,
        duration,
        load_engram_path,
        load_engram_input,
    ):
        if not n_start:
            raise dash.exceptions.PreventUpdate
        if root:
            try:
                manager.set_runs_root(root)
            except Exception:
                pass

        profile = _assemble_profile(
            neurons,
            k,
            hz,
            domain,
            use_td,
            sparse_mode,
            threshold,
            lambda_omega,
            candidates,
            walkers,
            hops,
            status_interval,
            bundle_size,
            prune_factor,
            stim_group_size,
            stim_amp,
            stim_decay,
            stim_max_symbols,
            speak_auto,
            speak_z,
            speak_hyst,
            speak_cd,
            speak_val,
            b1_hl,
            viz_every,
            log_every,
            checkpoint_every,
            checkpoint_keep,
            duration,
            default_profile,
        )

        lep = (load_engram_input or load_engram_path)
        if lep:
            profile["load_engram"] = lep
            # Adopt the folder as run_dir so the whole bundle (events, lexicon, macro board) is reused.
            try:
                p = str(lep).strip()
                if p:
                    adopt_dir = p if os.path.isdir(p) else os.path.dirname(p)
                    if adopt_dir:
                        profile["run_dir"] = adopt_dir
            except Exception:
                pass

        return _start_or_resume(profile)

    @app.callback(
        Output("proc-status", "children", allow_duplicate=True),
        Output("run-dir", "value", allow_duplicate=True),
        Input("resume-run", "n_clicks"),
        State("runs-root", "value"),
        State("run-dir", "value"),
        State("cfg-neurons", "value"),
        State("cfg-k", "value"),
        State("cfg-hz", "value"),
        State("cfg-domain", "value"),
        State("cfg-use-time-dynamics", "value"),
        State("cfg-sparse-mode", "value"),
        State("cfg-threshold", "value"),
        State("cfg-lambda-omega", "value"),
        State("cfg-candidates", "value"),
        State("cfg-walkers", "value"),
        State("cfg-hops", "value"),
        State("cfg-status-interval", "value"),
        State("cfg-bundle-size", "value"),
        State("cfg-prune-factor", "value"),
        State("cfg-stim-group-size", "value"),
        State("cfg-stim-amp", "value"),
        State("cfg-stim-decay", "value"),
        State("cfg-stim-max-symbols", "value"),
        State("cfg-speak-auto", "value"),
        State("cfg-speak-z", "value"),
        State("cfg-speak-hysteresis", "value"),
        State("cfg-speak-cooldown-ticks", "value"),
        State("cfg-speak-valence-thresh", "value"),
        State("cfg-b1-half-life-ticks", "value"),
        State("cfg-viz-every", "value"),
        State("cfg-log-every", "value"),
        State("cfg-checkpoint-every", "value"),
        State("cfg-checkpoint-keep", "value"),
        State("cfg-duration", "value"),
        State("rc-load-engram-path", "value"),
        State("rc-load-engram-input", "value"),
        prevent_initial_call=True,
    )
    def on_resume_run(
        n_resume,
        root,
        run_dir,
        neurons,
        k,
        hz,
        domain,
        use_td,
        sparse_mode,
        threshold,
        lambda_omega,
        candidates,
        walkers,
        hops,
        status_interval,
        bundle_size,
        prune_factor,
        stim_group_size,
        stim_amp,
        stim_decay,
        stim_max_symbols,
        speak_auto,
        speak_z,
        speak_hyst,
        speak_cd,
        speak_val,
        b1_hl,
        viz_every,
        log_every,
        checkpoint_every,
        checkpoint_keep,
        duration,
        load_engram_path,
        load_engram_input,
    ):
        if not n_resume:
            raise dash.exceptions.PreventUpdate
        if root:
            try:
                manager.set_runs_root(root)
            except Exception:
                pass

        rd = (run_dir or "").strip()
        if not rd or not os.path.isdir(rd):
            return "Select an existing run directory to resume.", no_update

        profile = _assemble_profile(
            neurons,
            k,
            hz,
            domain,
            use_td,
            sparse_mode,
            threshold,
            lambda_omega,
            candidates,
            walkers,
            hops,
            status_interval,
            bundle_size,
            prune_factor,
            stim_group_size,
            stim_amp,
            stim_decay,
            stim_max_symbols,
            speak_auto,
            speak_z,
            speak_hyst,
            speak_cd,
            speak_val,
            b1_hl,
            viz_every,
            log_every,
            checkpoint_every,
            checkpoint_keep,
            duration,
            default_profile,
        )
        profile["run_dir"] = rd

        lep = (load_engram_input or load_engram_path)
        if not lep:
            lep = latest_checkpoint(rd)
        if lep:
            profile["load_engram"] = lep

        ok, msg = manager.start(profile)
        if not ok:
            return f"Resume failed:\n{msg}", no_update

        cmd_echo = " ".join(manager.last_cmd or [])
        status = (
            f"Resumed.\nrun_dir={rd}\n"
            f"load_engram={profile.get('load_engram','')}\n"
            f"cmd: {cmd_echo}\n"
            f"launch_log: {manager.launch_log}"
        )
        return status, rd

    @app.callback(
        Output("proc-status", "children", allow_duplicate=True),
        Input("stop-run", "n_clicks"),
        prevent_initial_call=True,
    )
    def on_stop_run(n_stop):
        if not n_stop:
            raise dash.exceptions.PreventUpdate
        ok, msg = manager.stop()
        return "Stopped." if ok else msg]]></content>
    </file>
    <file>
      <path>frontend/callbacks/profile.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import dash
from typing import Any, Dict, List
from dash import Input, Output, State  # noqa: F401

from fum_rt.frontend.utilities.fs_utils import read_json_file, write_json_file
from fum_rt.frontend.utilities.profiles import (
    assemble_profile as _assemble_profile,
    checklist_from_bool as _checklist_from_bool,
)


def register_profile_callbacks(app, profiles_dir: str, default_profile: Dict[str, Any]):
    """
    Profile management callbacks:
      - Save current UI config to a named profile under profiles_dir
      - Load a selected profile into the UI config controls

    IDs preserved to match existing layout (fum_live).
    """

    os.makedirs(profiles_dir, exist_ok=True)

    def _list_profiles() -> List[str]:
        try:
            return sorted(
                [
                    os.path.join(profiles_dir, f)
                    for f in os.listdir(profiles_dir)
                    if f.endswith(".json")
                ]
            )
        except Exception:
            return []

    @app.callback(
        Output("profile-path", "options"),
        Output("profile-save-status", "children"),
        Input("save-profile", "n_clicks"),
        State("profile-name", "value"),
        State("cfg-neurons", "value"),
        State("cfg-k", "value"),
        State("cfg-hz", "value"),
        State("cfg-domain", "value"),
        State("cfg-use-time-dynamics", "value"),
        State("cfg-sparse-mode", "value"),
        State("cfg-threshold", "value"),
        State("cfg-lambda-omega", "value"),
        State("cfg-candidates", "value"),
        State("cfg-walkers", "value"),
        State("cfg-hops", "value"),
        State("cfg-status-interval", "value"),
        State("cfg-bundle-size", "value"),
        State("cfg-prune-factor", "value"),
        State("cfg-stim-group-size", "value"),
        State("cfg-stim-amp", "value"),
        State("cfg-stim-decay", "value"),
        State("cfg-stim-max-symbols", "value"),
        State("cfg-speak-auto", "value"),
        State("cfg-speak-z", "value"),
        State("cfg-speak-hysteresis", "value"),
        State("cfg-speak-cooldown-ticks", "value"),
        State("cfg-speak-valence-thresh", "value"),
        State("cfg-b1-half-life-ticks", "value"),
        State("cfg-viz-every", "value"),
        State("cfg-log-every", "value"),
        State("cfg-checkpoint-every", "value"),
        State("cfg-checkpoint-keep", "value"),
        State("cfg-duration", "value"),
        prevent_initial_call=True,
    )
    def on_save_profile(
        _n,
        name,
        neurons,
        k,
        hz,
        domain,
        use_td,
        sparse_mode,
        threshold,
        lambda_omega,
        candidates,
        walkers,
        hops,
        status_interval,
        bundle_size,
        prune_factor,
        stim_group_size,
        stim_amp,
        stim_decay,
        stim_max_symbols,
        speak_auto,
        speak_z,
        speak_hyst,
        speak_cd,
        speak_val,
        b1_hl,
        viz_every,
        log_every,
        checkpoint_every,
        checkpoint_keep,
        duration,
    ):
        name = (name or "").strip()
        if not name:
            return [{"label": os.path.basename(p), "value": p} for p in _list_profiles()], "Provide a profile name."
        data = _assemble_profile(
            neurons,
            k,
            hz,
            domain,
            use_td,
            sparse_mode,
            threshold,
            lambda_omega,
            candidates,
            walkers,
            hops,
            status_interval,
            bundle_size,
            prune_factor,
            stim_group_size,
            stim_amp,
            stim_decay,
            stim_max_symbols,
            speak_auto,
            speak_z,
            speak_hyst,
            speak_cd,
            speak_val,
            b1_hl,
            viz_every,
            log_every,
            checkpoint_every,
            checkpoint_keep,
            duration,
            default_profile,
        )
        path = os.path.join(profiles_dir, f"{name}.json")
        ok = write_json_file(path, data)
        status = f"Saved profile to {path}" if ok else f"Error writing {path}"
        return [{"label": os.path.basename(p), "value": p} for p in _list_profiles()], status

    @app.callback(
        Output("cfg-neurons", "value"),
        Output("cfg-k", "value"),
        Output("cfg-hz", "value"),
        Output("cfg-domain", "value"),
        Output("cfg-use-time-dynamics", "value"),
        Output("cfg-sparse-mode", "value"),
        Output("cfg-threshold", "value"),
        Output("cfg-lambda-omega", "value"),
        Output("cfg-candidates", "value"),
        Output("cfg-walkers", "value"),
        Output("cfg-hops", "value"),
        Output("cfg-status-interval", "value"),
        Output("cfg-bundle-size", "value"),
        Output("cfg-prune-factor", "value"),
        Output("cfg-stim-group-size", "value"),
        Output("cfg-stim-amp", "value"),
        Output("cfg-stim-decay", "value"),
        Output("cfg-stim-max-symbols", "value"),
        Output("cfg-speak-auto", "value"),
        Output("cfg-speak-z", "value"),
        Output("cfg-speak-hysteresis", "value"),
        Output("cfg-speak-cooldown-ticks", "value"),
        Output("cfg-speak-valence-thresh", "value"),
        Output("cfg-b1-half-life-ticks", "value"),
        Output("cfg-viz-every", "value"),
        Output("cfg-log-every", "value"),
        Output("cfg-checkpoint-every", "value"),
        Output("cfg-checkpoint-keep", "value"),
        Output("cfg-duration", "value"),
        Output("profile-save-status", "children", allow_duplicate=True),
        Input("load-profile", "n_clicks"),
        Input("profile-path", "value"),
        Input("profile-file-selected-label", "children"),
        prevent_initial_call=True,
    )
    def on_load_profile(_n, path, _sel_label):
        if not path:
            raise dash.exceptions.PreventUpdate
        data = read_json_file(path) or {}

        def g(k, dv):
            v = data.get(k, dv)
            return v if v is not None else dv

        return (
            g("neurons", default_profile["neurons"]),
            g("k", default_profile["k"]),
            g("hz", default_profile["hz"]),
            g("domain", default_profile["domain"]),
            _checklist_from_bool(bool(g("use_time_dynamics", default_profile["use_time_dynamics"]))),
            _checklist_from_bool(bool(g("sparse_mode", default_profile["sparse_mode"]))),
            g("threshold", default_profile["threshold"]),
            g("lambda_omega", default_profile["lambda_omega"]),
            g("candidates", default_profile["candidates"]),
            g("walkers", default_profile["walkers"]),
            g("hops", default_profile["hops"]),
            g("status_interval", default_profile["status_interval"]),
            g("bundle_size", default_profile["bundle_size"]),
            g("prune_factor", default_profile["prune_factor"]),
            g("stim_group_size", default_profile["stim_group_size"]),
            g("stim_amp", default_profile["stim_amp"]),
            g("stim_decay", default_profile["stim_decay"]),
            g("stim_max_symbols", default_profile["stim_max_symbols"]),
            _checklist_from_bool(bool(g("speak_auto", default_profile["speak_auto"]))),
            g("speak_z", default_profile["speak_z"]),
            g("speak_hysteresis", default_profile["speak_hysteresis"]),
            g("speak_cooldown_ticks", default_profile["speak_cooldown_ticks"]),
            g("speak_valence_thresh", default_profile["speak_valence_thresh"]),
            g("b1_half_life_ticks", default_profile["b1_half_life_ticks"]),
            g("viz_every", default_profile["viz_every"]),
            g("log_every", default_profile["log_every"]),
            g("checkpoint_every", default_profile["checkpoint_every"]),
            g("checkpoint_keep", default_profile["checkpoint_keep"]),
            g("duration", default_profile["duration"]),
            f"Loaded profile: {path}",
        )]]></content>
    </file>
    <file>
      <path>frontend/callbacks/runtime.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from dash import Input, Output, State, no_update  # noqa: F401
from fum_rt.frontend.controllers.runtime_controller import (
    build_phase_update,
    update_phase_json,
    queue_load_engram,
    parse_engram_events_for_message,
)
from fum_rt.frontend.utilities.tail import tail_jsonl_bytes


def register_runtime_callbacks(app, default_profile):
    """
    Runtime-level controls:
      - Apply phase/runtime tuning -> phase.json
      - Queue load_engram into phase.json
      - Surface engram load events from events.jsonl
    """

    @app.callback(
        Output("phase-status", "children"),
        Input("apply-phase", "n_clicks"),
        State("run-dir", "value"),
        State("phase", "value"),
        State("rc-speak-z", "value"),
        State("rc-speak-hysteresis", "value"),
        State("rc-speak-cooldown", "value"),
        State("rc-speak-valence", "value"),
        State("rc-walkers", "value"),
        State("rc-hops", "value"),
        State("rc-bundle-size", "value"),
        State("rc-prune-factor", "value"),
        State("rc-threshold", "value"),
        State("rc-lambda-omega", "value"),
        State("rc-candidates", "value"),
        State("rc-novelty-idf-gain", "value"),
        prevent_initial_call=True,
    )
    def on_apply_phase(
        _n,
        run_dir,
        phase,
        s_z,
        s_h,
        s_cd,
        s_vt,
        c_w,
        c_h,
        c_b,
        c_pf,
        c_thr,
        c_lw,
        c_cand,
        s_idf,
    ):
        if not run_dir:
            return "Select a run directory."
        update = build_phase_update(
            default_profile,
            phase,
            s_z,
            s_h,
            s_cd,
            s_vt,
            c_w,
            c_h,
            c_b,
            c_pf,
            c_thr,
            c_lw,
            c_cand,
            s_idf,
        )
        ok = update_phase_json(run_dir, update)
        return "Applied" if ok else "Error writing phase.json"

    @app.callback(
        Output("phase-status", "children", allow_duplicate=True),
        Input("rc-load-engram-btn", "n_clicks"),
        State("run-dir", "value"),
        State("rc-load-engram-path", "value"),
        State("rc-load-engram-input", "value"),
        prevent_initial_call=True,
    )
    def on_load_engram_now(_n, run_dir, path, path_text):
        rd = (run_dir or "").strip()
        if not rd:
            return "Select a run directory."
        p = ((path or path_text) or "").strip()
        if not p:
            return "Enter engram path."
        ok, norm = queue_load_engram(rd, p)
        return f"Queued load engram: {norm}" if ok else "Error writing phase.json"

    @app.callback(
        Output("phase-status", "children", allow_duplicate=True),
        Input("poll", "n_intervals"),
        State("run-dir", "value"),
        prevent_initial_call=True,
    )
    def notify_engram_events(_n, run_dir):
        rd = (run_dir or "").strip()
        if not rd:
            return no_update

        # UI responsiveness guard:
        # - By default we avoid any file IO here (large events.jsonl can be 100s of MB).
        # - Opt-in tailing only when DASH_ENGRAM_EVENT_TAIL is explicitly enabled.
        try:
            _disable_io = str(os.getenv("DASH_DISABLE_FILE_IO", "1")).strip().lower() in ("1", "true", "yes", "on")
        except Exception:
            _disable_io = True
        try:
            _engram_tail_on = str(os.getenv("DASH_ENGRAM_EVENT_TAIL", "0")).strip().lower() in ("1", "true", "yes", "on")
        except Exception:
            _engram_tail_on = False
        if _disable_io and not _engram_tail_on:
            return no_update

        state = getattr(notify_engram_events, "_state", None)
        if state is None or state.get("run_dir") != rd:
            state = {"run_dir": rd, "events_size": 0}
            setattr(notify_engram_events, "_state", state)

        ev_path = os.path.join(rd, "events.jsonl")
        recs, new_size = tail_jsonl_bytes(ev_path, state["events_size"])
        state["events_size"] = new_size
        msg = parse_engram_events_for_message(recs)
        return msg if msg else no_update]]></content>
    </file>
    <file>
      <path>frontend/callbacks/workspace.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import List
from dash import Input, Output, State, no_update  # noqa: F401 (bound by Dash at runtime)
from fum_rt.frontend.utilities.fs_utils import list_runs


def register_workspace_callbacks(app, runs_root: str, manager):
    """
    Workspace-level callbacks:
      - Refresh run list (options + default selection)
      - Use current managed run
      - Use latest run under a root
    """

    @app.callback(
        Output("run-dir", "options", allow_duplicate=True),
        Output("run-dir", "value", allow_duplicate=True),
        Input("refresh-runs", "n_clicks"),
        State("runs-root", "value"),
        prevent_initial_call=True,
    )
    def on_refresh_runs(_n, root):
        root = root or runs_root
        opts = [{"label": p, "value": p} for p in list_runs(root)]
        val = opts[0]["value"] if opts else ""
        return opts, val

    @app.callback(
        Output("run-dir", "value", allow_duplicate=True),
        Input("use-current-run", "n_clicks"),
        prevent_initial_call=True,
    )
    def on_use_current(_n):
        return manager.current_run_dir or no_update

    @app.callback(
        Output("run-dir", "value", allow_duplicate=True),
        Input("use-latest-run", "n_clicks"),
        State("runs-root", "value"),
        prevent_initial_call=True,
    )
    def on_use_latest(_n, root):
        r = root or runs_root
        rs = list_runs(r)
        return rs[0] if rs else no_update

    # Sync runs-root dropdown -> text input and refresh run list
    @app.callback(
        Output("runs-root", "value"),
        Output("run-dir", "options", allow_duplicate=True),
        Output("run-dir", "value", allow_duplicate=True),
        Input("runs-root-select", "value"),
        prevent_initial_call=True,
    )
    def on_runs_root_select(val):
        r = (val or "").strip()
        if not r:
            return no_update, no_update, no_update
        opts = [{"label": p, "value": p} for p in list_runs(r)]
        v = opts[0]["value"] if opts else ""
        return r, opts, v]]></content>
    </file>
    <file>
      <path>frontend/components/charts.py</path>
      <content><![CDATA[from __future__ import annotations

from dash import html
from fum_rt.frontend.components.widgets.graph import graph


def charts_card():
    """
    Charts card composed from graph primitives (widgets).
    IDs preserved to match existing callbacks in fum_live.py.
    """
    return html.Div(
        [
            graph("fig-dashboard", height=420),
            graph("fig-discovery", height=320),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/chat.py</path>
      <content><![CDATA[from __future__ import annotations

from dash import html, dcc


def chat_card():
    """
    Chat panel with log view, filter, input, and send button.
    IDs preserved to match existing callbacks in fum_live.py.
    """
    return html.Div(
        [
            html.H4("Chat"),
            html.Pre(
                id="chat-view",
                style={
                    "height": "220px",
                    "overflowY": "auto",
                    "overflowX": "hidden",
                    "backgroundColor": "#0f141a",
                    "color": "#e0e6ee",
                    "padding": "8px",
                    "whiteSpace": "pre-wrap",
                    "wordBreak": "break-word",
                    "overflowWrap": "anywhere",
                    "hyphens": "none",
                    "border": "1px solid #1d2733",
                    "borderRadius": "8px",
                },
            ),
            html.Div(
                [
                    html.Label("Chat filter"),
                    dcc.RadioItems(
                        id="chat-filter",
                        options=[
                            {"label": "All Outputs", "value": "all"},
                            {"label": "'say' Macro Only", "value": "say"},
                            {"label": "Self-Speak (Spike-Gated)", "value": "spike"},
                        ],
                        value="all",
                        labelStyle={"display": "inline-block", "marginRight": "10px"},
                    ),
                ],
                className="tight",
            ),
            html.Div(
                [
                    dcc.Input(
                        id="chat-input",
                        type="text",
                        placeholder="Type a message and click Send",
                        style={"width": "80%"},
                    ),
                    html.Button("Send", id="chat-send", n_clicks=0),
                ],
                className="row tight",
            ),
            html.Pre(id="chat-status", style={"fontSize": "12px"}),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/feed.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import List, Dict
from dash import html, dcc
from fum_rt.frontend.components.widgets.file_picker import file_picker


def feed_card(data_files_options: List[Dict[str, str]]):
    """
    Feed stdin (optional) card.
    IDs preserved to match existing callbacks.
    """
    return html.Div(
        [
            html.H4("Feed stdin (optional)"),
            file_picker(prefix="feed-file", title="Select feed file", initial="", width="100%"),
            dcc.Dropdown(
                id="feed-path",
                options=data_files_options,
                placeholder="select feed file...",
                style={"width": "100%", "display": "none"},
            ),
            dcc.Input(
                id="feed-rate",
                type="number",
                value=20,
                step=1,
                style={"width": "120px", "marginTop": "6px"},
            ),
            html.Div(
                [
                    html.Button("Start Feed", id="feed-start", n_clicks=0, className="btn-ok"),
                    html.Button("Stop Feed", id="feed-stop", n_clicks=0, className="btn-danger"),
                ],
                className="row tight",
            ),
            html.Pre(id="send-status", style={"fontSize": "12px"}),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/layout.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from typing import Any, Dict, List

from dash import html, dcc

from fum_rt.frontend.components.workspace import workspace_card
from fum_rt.frontend.components.runtime_controls import runtime_controls_card
from fum_rt.frontend.components.feed import feed_card
from fum_rt.frontend.components.run_config import run_config_card
from fum_rt.frontend.components.charts import charts_card
from fum_rt.frontend.components.chat import chat_card

__all__ = ["build_layout"]


def build_layout(
    runs_root: str,
    runs: List[str],
    default_run: str,
    repo_root: str,  # kept for legacy signature; not used
    profiles_dir: str,  # kept for legacy signature; not used
    default_profile: Dict[str, Any],
    domain_options: List[Dict[str, str]],
    data_files_options: List[Dict[str, str]],
    profile_options: List[Dict[str, str]],
) -> html.Div:
    """
    Construct the full dashboard layout (no callbacks).
    Wrapper around modular cards to preserve legacy build_layout signature.
    IDs preserved to match existing callbacks in fum_live.
    """
    poll_ms_env = os.getenv("DASH_POLL_MS", "1200")
    try:
        poll_ms_val = int(poll_ms_env) if poll_ms_env.strip() != "" else 1200
    except ValueError:
        poll_ms_val = 1200
    poll_interval = max(250, poll_ms_val)
    poll_disabled = poll_ms_val <= 0

    return html.Div(
        [
            html.H3("FUM Live Dashboard (external control)"),
            html.Div(
                [
                    html.Div(
                        [
                            workspace_card(runs_root, runs, default_run),
                            runtime_controls_card(default_profile),
                            feed_card(data_files_options),
                        ],
                        style={"minWidth": "320px", "display": "grid", "gap": "16px"},
                    ),
                    html.Div(
                        [
                            run_config_card(default_profile, domain_options, profile_options),
                            charts_card(),
                            chat_card(),
                        ],
                        style={"minWidth": "400px", "display": "grid", "gap": "16px"},
                    ),
                ],
                className="grid",
            ),
            dcc.Interval(id="poll", interval=poll_interval, n_intervals=0, disabled=poll_disabled),
            dcc.Store(id="chat-state"),
            dcc.Store(id="ui-state"),
        ],
        style={"padding": "10px"},
    )
]]></content>
    </file>
    <file>
      <path>frontend/components/perf.py</path>
      <content><![CDATA[from __future__ import annotations

from dash import html, dcc


def perf_card():
    """
    UI Performance card (no environment variables required).
    Publishes settings into dcc.Store(id="ui-state") via callbacks/perf.register_perf_callbacks.
    """
    return html.Div(
        [
            html.H4("UI Performance"),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Charts: HTTP snapshot only"),
                            dcc.Checklist(
                                id="ui-charts-http",
                                options=[{"label": " On", "value": "on"}],
                                value=[],  # default OFF - falls back to bounded file tails if HTTP not ready
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Update interval (ms)"),
                            dcc.Input(id="ui-update-ms", type="number", value=800, min=250, step=50),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Points per series cap"),
                            dcc.Input(id="ui-points-cap", type="number", value=1200, min=100, step=50),
                        ]
                    ),
                ],
                className="row",
            ),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Series count cap"),
                            dcc.Input(id="ui-series-cap", type="number", value=6, min=1, step=1),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Status URL"),
                            dcc.Input(
                                id="ui-status-url",
                                type="text",
                                value="http://127.0.0.1:8787/status/snapshot",
                                style={"width": "100%"},
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Status timeout (s)"),
                            dcc.Input(id="ui-status-timeout", type="number", value=0.2, min=0.05, step=0.05),
                        ]
                    ),
                ],
                className="row",
            ),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Tail Chat file"),
                            dcc.Checklist(id="ui-tail-chat", options=[{"label": " On", "value": "on"}], value=[]),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Tail Launcher log"),
                            dcc.Checklist(id="ui-tail-launch", options=[{"label": " On", "value": "on"}], value=[]),
                        ]
                    ),
                ],
                className="row",
            ),
            html.Small(
                "Settings apply immediately; no env vars or restart required.",
                style={"color": "#8699ac"},
            ),
        ],
        className="card",
    )


__all__ = ["perf_card"]]]></content>
    </file>
    <file>
      <path>frontend/components/run_config.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import Dict, Any, List
from dash import html

# Modular sections (IDs preserved exactly as before)
from fum_rt.frontend.components.run_config import (
    section_core_params,
    section_modes,
    section_structure_traversal,
    section_stimulus,
    section_speak_b1,
    section_viz_logs_checkpoints,
    section_profile_io,
    section_process_actions,
)


def run_config_card(
    default_profile: Dict[str, Any],
    domain_options: List[Dict[str, str]],
    profile_options: List[Dict[str, str]],
):
    """
    Run configuration & process card (modular assembly).
    All element IDs preserved to match existing callbacks.
    """
    return html.Div(
        [
            html.H4("Run configuration & process"),
            # Sections assembled in a clear, readable order
            section_core_params(default_profile, domain_options),
            section_modes(default_profile),
            section_structure_traversal(default_profile),
            section_stimulus(default_profile),
            section_speak_b1(default_profile),
            section_viz_logs_checkpoints(default_profile),
            section_profile_io(profile_options),
            section_process_actions(),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/run_config/__init__.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Run Config components package.

Exports:
- run_config_card(): assembled card (IDs preserved)
- section_* helpers from .sections for fine-grained composition
"""

from typing import Dict, Any, List
from dash import html

from .sections import (
    section_core_params,
    section_modes,
    section_structure_traversal,
    section_stimulus,
    section_speak_b1,
    section_viz_logs_checkpoints,
    section_profile_io,
    section_process_actions,
)


def run_config_card(
    default_profile: Dict[str, Any],
    domain_options: List[Dict[str, str]],
    profile_options: List[Dict[str, str]],
):
    """
    Run configuration & process card (modular assembly).
    All element IDs preserved to match existing callbacks.
    """
    return html.Div(
        [
            html.H4("Run configuration & process"),
            section_core_params(default_profile, domain_options),
            section_modes(default_profile),
            section_structure_traversal(default_profile),
            section_stimulus(default_profile),
            section_speak_b1(default_profile),
            section_viz_logs_checkpoints(default_profile),
            section_profile_io(profile_options),
            section_process_actions(),
        ],
        className="card",
    )


__all__ = [
    "run_config_card",
    "section_core_params",
    "section_modes",
    "section_structure_traversal",
    "section_stimulus",
    "section_speak_b1",
    "section_viz_logs_checkpoints",
    "section_profile_io",
    "section_process_actions",
]]]></content>
    </file>
    <file>
      <path>frontend/components/run_config/sections.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Run Config modular sections.

Purpose:
- Break the large run_config_card into readable, maintainable sections
- Preserve ALL existing element IDs to avoid breaking callbacks
- No new callbacks introduced; purely structural/UX refactor

Sections:
- Core (neurons, k, hz, domain)
- Modes (use_time_dynamics, sparse_mode)
- Structure & Traversal (threshold, lambda_omega, candidates, walkers, hops, bundle_size, prune_factor, status_interval)
- Stimulus (stim_* fields)
- Speak / B1 (speak_* and b1_* fields)
- Viz / Logs / Checkpoints
- Profile I/O (profile-name, save/load, file picker + status)
- Process Actions (start/resume/stop + status + log)

Author: Justin K. Lietz
"""

from typing import Dict, Any, List
from dash import html, dcc

from fum_rt.frontend.components.widgets.file_picker import file_picker


def _num(id_: str, label: str, value: Any, step: float | int, min: Any | None = None, max: Any | None = None) -> html.Div:
    """Consistent number input with label. ID preserved."""
    return html.Div(
        [
            html.Label(label),
            dcc.Input(id=id_, type="number", value=value, step=step, min=min, max=max),
        ]
    )


def _toggle(id_: str, label: str, on: bool) -> html.Div:
    """Consistent On/Off checklist with label. ID preserved."""
    return html.Div(
        [
            html.Label(label),
            dcc.Checklist(id=id_, options=[{"label": " On", "value": "on"}], value=["on"] if on else []),
        ]
    )


def section_core_params(default_profile: Dict[str, Any], domain_options: List[Dict[str, str]]) -> html.Div:
    return html.Div(
        [
            html.H5("Core", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _num("cfg-neurons", "Neurons", default_profile["neurons"], step=1, min=1),
                    _num("cfg-k", "k", default_profile["k"], step=1, min=1),
                    _num("cfg-hz", "Hz", default_profile["hz"], step=1, min=1),
                    html.Div(
                        [
                            html.Label("Domain"),
                            dcc.Dropdown(id="cfg-domain", options=domain_options, value=default_profile["domain"]),
                        ]
                    ),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_modes(default_profile: Dict[str, Any]) -> html.Div:
    return html.Div(
        [
            html.H5("Modes", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _toggle("cfg-use-time-dynamics", "Use time dynamics", bool(default_profile["use_time_dynamics"])),
                    _toggle("cfg-sparse-mode", "Sparse mode", bool(default_profile["sparse_mode"])),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_structure_traversal(default_profile: Dict[str, Any]) -> html.Div:
    return html.Div(
        [
            html.H5("Structure & traversal", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _num("cfg-threshold", "Threshold", default_profile["threshold"], step=0.01, min=0),
                    _num("cfg-lambda-omega", "Lambda omega", default_profile["lambda_omega"], step=0.01, min=0),
                    _num("cfg-candidates", "Candidates", default_profile["candidates"], step=1, min=1),
                    _num("cfg-walkers", "Walkers", default_profile["walkers"], step=1, min=1),
                    _num("cfg-hops", "Hops", default_profile["hops"], step=1, min=1),
                    _num("cfg-bundle-size", "Bundle size", default_profile["bundle_size"], step=1, min=1),
                    _num("cfg-prune-factor", "Prune factor", default_profile["prune_factor"], step=0.01, min=0, max=1),
                    _num("cfg-status-interval", "Status interval", default_profile["status_interval"], step=1, min=1),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_stimulus(default_profile: Dict[str, Any]) -> html.Div:
    return html.Div(
        [
            html.H5("Stimulus", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _num("cfg-stim-group-size", "Group size", default_profile["stim_group_size"], step=1, min=1),
                    _num("cfg-stim-amp", "Amp", default_profile["stim_amp"], step=0.01, min=0),
                    _num("cfg-stim-decay", "Decay", default_profile["stim_decay"], step=0.01, min=0, max=1),
                    _num("cfg-stim-max-symbols", "Max symbols", default_profile["stim_max_symbols"], step=1, min=1),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_speak_b1(default_profile: Dict[str, Any]) -> html.Div:
    return html.Div(
        [
            html.H5("Speak / B1 spike detector", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _toggle("cfg-speak-auto", "Speak auto", bool(default_profile["speak_auto"])),
                    _num("cfg-speak-z", "Speak z", default_profile["speak_z"], step=0.1, min=0),
                    _num("cfg-speak-hysteresis", "Hysteresis", default_profile["speak_hysteresis"], step=0.1, min=0),
                    _num("cfg-speak-cooldown-ticks", "Cooldown (ticks)", default_profile["speak_cooldown_ticks"], step=1, min=1),
                    _num("cfg-speak-valence-thresh", "Valence thresh", default_profile["speak_valence_thresh"], step=0.01, min=0, max=1),
                    _num("cfg-b1-half-life-ticks", "B1 half-life (ticks)", default_profile["b1_half_life_ticks"], step=1, min=1),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_viz_logs_checkpoints(default_profile: Dict[str, Any]) -> html.Div:
    return html.Div(
        [
            html.H5("Viz / Logs / Checkpoints", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    _num("cfg-viz-every", "viz_every", default_profile["viz_every"], step=1, min=0),
                    _num("cfg-log-every", "log_every", default_profile["log_every"], step=1, min=1),
                    _num("cfg-checkpoint-every", "checkpoint_every", default_profile["checkpoint_every"], step=1, min=0),
                    _num("cfg-checkpoint-keep", "checkpoint_keep", default_profile["checkpoint_keep"], step=1, min=0),
                    _num("cfg-duration", "duration (s)", default_profile["duration"], step=1, min=0),
                ],
                className="row",
            ),
        ],
        className="section",
    )


def section_profile_io(profile_options: List[Dict[str, str]]) -> html.Div:
    """Save/Load profile, with file picker; includes profile-save-status."""
    return html.Div(
        [
            html.H5("Profile I/O", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    dcc.Input(id="profile-name", type="text", placeholder="profile name", style={"width": "200px"}),
                    html.Button("Save Profile", id="save-profile", n_clicks=0),
                    file_picker(prefix="profile-file", title="Select profile (.json)", initial="", width="50%"),
                    dcc.Dropdown(
                        id="profile-path",
                        options=profile_options,
                        placeholder="load profile",
                        style={"width": "50%", "display": "none"},
                    ),
                    html.Button("Load", id="load-profile", n_clicks=0),
                ],
                className="row tight",
            ),
            html.Pre(id="profile-save-status", style={"fontSize": "12px", "whiteSpace": "pre-wrap"}),
        ],
        className="section",
    )


def section_process_actions() -> html.Div:
    """Start/Resume/Stop + proc-status + launcher log."""
    return html.Div(
        [
            html.H5("Process actions", style={"margin": "6px 0 4px 0", "opacity": 0.9}),
            html.Div(
                [
                    html.Button("Start New Run", id="start-run", n_clicks=0, className="btn-ok"),
                    html.Button("Resume Selected Run", id="resume-run", n_clicks=0),
                    html.Button("Stop Managed Run", id="stop-run", n_clicks=0, className="btn-danger"),
                ],
                className="row tight",
            ),
            html.Pre(id="proc-status", style={"fontSize": "12px", "whiteSpace": "pre-wrap"}),
            html.Button("Show Launcher Log", id="show-log", n_clicks=0),
            html.Pre(id="launch-log", style={"fontSize": "12px", "maxHeight": "240px", "overflowY": "auto"}),
        ],
        className="section",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/runtime_controls.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import Dict, Any
from dash import html, dcc
from fum_rt.frontend.components.widgets.file_picker import file_picker


def runtime_controls_card(default_profile: Dict[str, Any]):
    """
    Runtime tuning + load-engram controls.
    IDs preserved to match existing callbacks in fum_live.
    """
    return html.Div(
        [
            html.H4("Runtime Controls"),
            html.Label("Phase"),
            dcc.Slider(
                id="phase",
                min=0,
                max=4,
                step=1,
                value=0,
                marks={i: str(i) for i in range(5)},
            ),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Speak z"),
                            dcc.Input(
                                id="rc-speak-z",
                                type="number",
                                value=default_profile["speak_z"],
                                step=0.1,
                                min=0,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Hysteresis"),
                            dcc.Input(
                                id="rc-speak-hysteresis",
                                type="number",
                                value=default_profile["speak_hysteresis"],
                                step=0.1,
                                min=0,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Cooldown (ticks)"),
                            dcc.Input(
                                id="rc-speak-cooldown",
                                type="number",
                                value=default_profile["speak_cooldown_ticks"],
                                step=1,
                                min=1,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Valence thresh"),
                            dcc.Input(
                                id="rc-speak-valence",
                                type="number",
                                value=default_profile["speak_valence_thresh"],
                                step=0.01,
                                min=0,
                                max=1,
                            ),
                        ]
                    ),
                ],
                className="row tight",
            ),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Walkers"),
                            dcc.Input(
                                id="rc-walkers",
                                type="number",
                                value=default_profile["walkers"],
                                step=1,
                                min=1,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Hops"),
                            dcc.Input(
                                id="rc-hops",
                                type="number",
                                value=default_profile["hops"],
                                step=1,
                                min=1,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Bundle size"),
                            dcc.Input(
                                id="rc-bundle-size",
                                type="number",
                                value=default_profile["bundle_size"],
                                step=1,
                                min=1,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Prune factor"),
                            dcc.Input(
                                id="rc-prune-factor",
                                type="number",
                                value=default_profile["prune_factor"],
                                step=0.01,
                                min=0,
                                max=1,
                            ),
                        ]
                    ),
                ],
                className="row tight",
            ),
            html.Div(
                [
                    html.Div(
                        [
                            html.Label("Threshold"),
                            dcc.Input(
                                id="rc-threshold",
                                type="number",
                                value=default_profile.get("threshold", 0.15),
                                step=0.01,
                                min=0,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Lambda omega"),
                            dcc.Input(
                                id="rc-lambda-omega",
                                type="number",
                                value=default_profile.get("lambda_omega", 0.10),
                                step=0.01,
                                min=0,
                            ),
                        ]
                    ),
                    html.Div(
                        [
                            html.Label("Candidates"),
                            dcc.Input(
                                id="rc-candidates",
                                type="number",
                                value=default_profile.get("candidates", 64),
                                step=1,
                                min=1,
                            ),
                        ]
                    ),
                ],
                className="row tight",
            ),
            html.Div(
                [
                    html.Label("SIE novelty IDF gain"),
                    dcc.Input(
                        id="rc-novelty-idf-gain",
                        type="number",
                        value=1.0,
                        step=0.05,
                        min=0,
                    ),
                ],
                className="tight",
            ),
            html.Div(
                [
                    html.Button(
                        "Apply Runtime Settings",
                        id="apply-phase",
                        n_clicks=0,
                        className="btn-ok",
                    )
                ],
                className="tight",
            ),
            html.Pre(id="phase-status", style={"fontSize": "12px"}),
            html.Label("Load Engram (runtime, into selected Run)"),
            file_picker(prefix="engram-file", title="Select engram (.h5/.npz)", initial="", width="100%"),
            dcc.Input(
                id="rc-load-engram-input",
                type="text",
                placeholder="path to .h5/.npz (abs or under runs)",
                style={"width": "100%", "marginTop": "4px"},
            ),
            dcc.Dropdown(
                id="rc-load-engram-path",
                placeholder="select from runs...",
                style={"width": "100%", "marginTop": "4px", "display": "none"},
            ),
            html.Button(
                "Load Engram Now",
                id="rc-load-engram-btn",
                n_clicks=0,
                className="tight",
            ),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/components/widgets/file_breadcrumbs.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Breadcrumbs widget for the File Picker component.

Pure UI builder. No IO, no state. Pattern-matching friendly IDs.
"""

from dash import html


def breadcrumbs(prefix: str, base: str | None, selected: str | None):
    """
    Build breadcrumbs from base (root) to selected directory.

    Args:
      prefix: file picker instance prefix
      base: absolute root directory
      selected: absolute selected directory (within base)
    """
    try:
        base = (base or "").strip()
        selected = (selected or "").strip()
        if not base:
            return []
        root_label = base.rstrip("/").split("/")[-1] or base
        out = [
            html.Button(
                root_label,
                id={"role": f"{prefix}-crumb", "path": base},
                n_clicks=0,
                style={"background": "transparent", "border": "none", "color": "#9ab8d1", "cursor": "pointer", "padding": "2px 4px"},
            )
        ]
        if selected and selected != base:
            # Render each path segment from base -> selected
            rel = selected[len(base) + 1 :] if selected.startswith(base + "/") else selected
            for part in [p for p in rel.split("/") if p]:
                base = f"{base}/{part}"
                out.append(html.Span("›", style={"opacity": 0.6, "padding": "0 2px"}))
                out.append(
                    html.Button(
                        part,
                        id={"role": f"{prefix}-crumb", "path": base},
                        n_clicks=0,
                        style={"background": "transparent", "border": "none", "color": "#9ab8d1", "cursor": "pointer", "padding": "2px 4px"},
                    )
                )
        return out
    except Exception:
        return []]]></content>
    </file>
    <file>
      <path>frontend/components/widgets/file_picker.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Reusable, themed file-picker component for Dash (no heavy scans, lazy-only).
- Compact display: shows selected basename + a "Choose file" button
- Modal overlay: navigable file tree rooted at a configured directory
- Bounded IO: lists one directory at a time; no recursive scans
- Integrates with existing callbacks via a target component id to receive the chosen path

Usage:
  In a component:
    from fum_rt.frontend.components.widgets.file_picker import file_picker

    # Place the compact picker UI where the dropdown used to be
    file_picker(prefix="feed", title="Select feed file", initial="", width="100%")

  In app initialization:
    from fum_rt.frontend.callbacks.file_picker import register_file_picker_instance
    register_file_picker_instance(
        app,
        prefix="feed",
        root=data_root_abs,
        exts=[".txt", ".jsonl", ".json", ".csv"],
        target_id="feed-path",  # the component id that should receive the chosen path (value property)
    )

IDs produced (all prefixed):
  - {prefix}-open-btn
  - {prefix}-modal (overlay container)
  - {prefix}-root (dcc.Store of absolute root)
  - {prefix}-cwd (dcc.Store of current working dir)
  - {prefix}-exts (dcc.Store of allowed extensions list or [])
  - {prefix}-dirs (dcc.Dropdown listing folders in cwd)
  - {prefix}-files (dcc.Dropdown listing files in cwd)
  - {prefix}-up-btn, {prefix}-root-btn
  - {prefix}-confirm-btn, {prefix}-cancel-btn
  - {prefix}-selected-label (compact label of chosen file)
  - {prefix}-selected-path (dcc.Store holding absolute path)
"""

from dash import html, dcc


def _modal_styles() -> dict:
    # Fixed viewport overlay that does not affect background layout.
    # Avoid 100vw/100vh which can introduce horizontal scrollbars and trigger reflow.
    # Use explicit edges for broad browser support (instead of inset).
    return {
        "position": "fixed",
        "top": 0,
        "right": 0,
        "bottom": 0,
        "left": 0,
        "backgroundColor": "rgba(10,14,19,0.66)",
        "display": "none",  # toggled by callbacks
        "alignItems": "center",
        "justifyContent": "center",
        "zIndex": 20000,  # above react-select menus (9999)
        "pointerEvents": "auto",
        "contain": "layout paint",  # isolate overlay layout/paint so background is unaffected
        "isolation": "isolate",     # create a new stacking context for safety
    }


def _panel_styles() -> dict:
    return {
        "position": "relative",
        "backgroundColor": "#0f141a",
        "color": "#cfd7e3",
        "border": "1px solid #233140",
        "borderRadius": "8px",
        "minWidth": "560px",
        "maxWidth": "86vw",
        "maxHeight": "86vh",
        "display": "grid",
        "gridTemplateRows": "auto auto 1fr auto",
        "gap": "8px",
        "padding": "12px",
        "boxShadow": "0 4px 16px rgba(0,0,0,0.4)",
        "overflow": "hidden",
        "zIndex": 20001,              # ensure panel stacks above overlay
        "pointerEvents": "auto",      # interactive; background remains inert
        "contain": "layout paint",    # isolate panel's layout/paint
        "overscrollBehavior": "contain",  # prevent body scroll chaining
    }


def _row_styles() -> dict:
    return {"display": "flex", "gap": "8px", "alignItems": "center", "justifyContent": "flex-start"}


def file_picker(prefix: str, title: str, initial: str = "", width: str = "100%") -> html.Div:
    """
    Build the compact file-picker UI with modal overlay.

    Args:
      prefix: unique prefix for component IDs
      title: title to show in the modal
      initial: initial selected path (optional)
      width: width style for compact strip

    Returns:
      html.Div node
    """
    return html.Div(
        [
            # Compact strip: selected filename + button
            html.Div(
                [
                    html.Span(
                        "Selected: ",
                        style={"opacity": 0.8, "marginRight": "6px", "fontSize": "13px"},
                    ),
                    html.Span(
                        initial or "(none)",
                        id=f"{prefix}-selected-label",
                        style={
                            "fontWeight": 500,
                            "fontSize": "13px",
                            "maxWidth": "60%",
                            "overflow": "hidden",
                            "textOverflow": "ellipsis",
                            "whiteSpace": "nowrap",
                            "display": "inline-block",
                            "verticalAlign": "middle",
                        },
                    ),
                    html.Button(
                        "Choose file",
                        id=f"{prefix}-open-btn",
                        n_clicks=0,
                        className="btn-ok",
                    ),
                ],
                style={
                    "display": "flex",
                    "alignItems": "center",
                    "width": width,
                    "gap": "8px",
                    "flexWrap": "wrap",
                    "justifyContent": "flex-start",
                    "background": "var(--panel2)",
                    "border": "1px solid var(--border)",
                    "borderRadius": "8px",
                    "padding": "6px 8px",
                    "minHeight": "36px"
                },
            ),
            # Persistent stores for this instance
            dcc.Store(id=f"{prefix}-root"),
            dcc.Store(id=f"{prefix}-cwd"),
            dcc.Store(id=f"{prefix}-exts"),
            dcc.Store(id=f"{prefix}-selected-path", data=initial),
            dcc.Store(id=f"{prefix}-file-sel"),
            dcc.Store(id=f"{prefix}-dir-sel"),
            dcc.Store(id=f"{prefix}-selected-dir"),
            dcc.Store(id=f"{prefix}-tree-store"),
            dcc.Store(id=f"{prefix}-last-action"),
            # Modal overlay
            html.Div(
                [
                    html.Div(
                        [
                            # Title bar
                            html.Div(
                                [
                                    html.H4(title, style={"margin": "0", "fontSize": "16px"}),
                                    html.Button("×", id=f"{prefix}-cancel-btn", n_clicks=0, title="Close"),
                                ],
                                style={"display": "flex", "justifyContent": "space-between", "alignItems": "center"},
                            ),
                            # Toolbar
                            html.Div(
                                [
                                    html.Button("Root", id=f"{prefix}-root-btn", n_clicks=0),
                                    html.Button("Up", id=f"{prefix}-up-btn", n_clicks=0),
                                    html.Div(id=f"{prefix}-cwd-label", style={"marginLeft": "10px", "opacity": 0.8}),
                                ],
                                style=_row_styles(),
                            ),
                            # Breadcrumbs
                            html.Div(
                                id=f"{prefix}-crumbs",
                                style={"display": "flex", "gap": "6px", "flexWrap": "wrap", "fontSize": "12px", "opacity": 0.9},
                            ),
                            # Body: single explorer (folders + files)
                            html.Div(
                                [
                                    html.Div(
                                        [
                                            html.Label("Explorer"),
                                            html.Div(
                                                id=f"{prefix}-dirs-list",
                                                style={
                                                    "border": "1px solid var(--border)",
                                                    "borderRadius": "8px",
                                                    "background": "var(--panel2)",
                                                    "maxHeight": "60vh",
                                                    "overflowY": "auto",
                                                    "padding": "4px",
                                                },
                                            ),
                                            dcc.Dropdown(
                                                id=f"{prefix}-dirs",
                                                options=[],
                                                placeholder="(no subfolders)",
                                                clearable=False,
                                                style={"display": "none"},
                                            ),
                                            html.Button("Open folder", id=f"{prefix}-open-dir-btn", n_clicks=0, style={"display": "none"}),
                                        ],
                                        style={"display": "grid", "gap": "6px"},
                                    ),
                                ],
                                style={
                                    "overflow": "auto",
                                    "maxHeight": "60vh",
                                },
                            ),
                            # Action row
                            html.Div(
                                [
                                    html.Button("Confirm", id=f"{prefix}-confirm-btn", n_clicks=0, className="btn-ok"),
                                    html.Div(id=f"{prefix}-status", style={"marginLeft": "12px", "opacity": 0.8, "fontSize": "12px"}),
                                ],
                                style=_row_styles(),
                            ),
                        ],
                        style=_panel_styles(),
                    )
                ],
                id=f"{prefix}-modal",
                style=_modal_styles(),
            ),
        ],
    )]]></content>
    </file>
    <file>
      <path>frontend/components/widgets/file_tree.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File tree row widgets for the File Picker component.

Pure UI builders (no IO, no state). IDs are pattern-matching friendly.
"""

from dash import html


def dir_row(prefix: str, path: str, name: str, depth: int, expanded: bool, is_selected: bool):
    """
    Build a single directory row with chevron and folder icon.

    Args:
      prefix: file picker instance prefix
      path: absolute directory path this row represents
      name: label to display
      depth: nesting depth for left indent
      expanded: whether the directory is expanded
      is_selected: whether this directory is the currently selected one
    """
    chevron = "▾" if expanded else "▸"
    icon = "📁"
    return html.Div(
        html.Button(
            [
                html.Span(chevron, style={"width": "1em"}),
                html.Span(icon, style={"width": "1.2em"}),
                html.Span(name, style={"overflow": "hidden", "textOverflow": "ellipsis"}),
            ],
            id={"role": f"{prefix}-tree-dir", "path": path},
            n_clicks=0,
            style={
                "display": "flex",
                "alignItems": "center",
                "gap": "6px",
                "padding": "4px 6px",
                "borderRadius": "6px",
                "cursor": "pointer",
                "width": "100%",
                "textAlign": "left",
                "background": "rgba(106,160,194,0.18)" if is_selected else "transparent",
                "border": "1px solid var(--border)" if is_selected else "1px solid rgba(35,49,64,0.0)",
                "marginLeft": f"{depth * 12}px",
            },
        )
    )


def file_row(prefix: str, file_path: str, depth: int, selected: bool):
    """
    Build a single file row with bullet and page icon.

    Args:
      prefix: file picker instance prefix
      file_path: absolute file path this row represents
      depth: nesting depth (aligned as child of its directory)
      selected: whether this file is currently selected
    """
    fname = file_path.split("/")[-1]
    return html.Div(
        html.Button(
            [
                html.Span("•", style={"width": "1em", "opacity": 0.7}),
                html.Span("📄", style={"width": "1.2em"}),
                html.Span(fname, style={"overflow": "hidden", "textOverflow": "ellipsis"}),
            ],
            id={"role": f"{prefix}-file", "path": file_path},
            n_clicks=0,
            style={
                "display": "flex",
                "alignItems": "center",
                "gap": "6px",
                "padding": "4px 6px",
                "borderRadius": "6px",
                "cursor": "pointer",
                "width": "100%",
                "textAlign": "left",
                "background": "rgba(106,160,194,0.18)" if selected else "transparent",
                "border": "1px solid var(--border)" if selected else "1px solid rgba(35,49,64,0.0)",
                "marginLeft": f"{(depth + 1) * 12}px",
            },
        )
    )]]></content>
    </file>
    <file>
      <path>frontend/components/widgets/graph.py</path>
      <content><![CDATA[from __future__ import annotations

from dash import dcc


def graph(id: str, height: int = 420, width: str = "100%"):
    """
    Primitive graph widget (lego block).
    Used by higher-level components to compose chart cards.
    """
    return dcc.Graph(id=id, style={"height": f"{int(height)}px", "width": width})]]></content>
    </file>
    <file>
      <path>frontend/components/workspace.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from typing import List
from dash import html, dcc


def _runs_root_options(runs_root: str):
    """
    Build candidate runs-root dropdown options:
    - current runs_root
    - ./runs
    - ./runs.bak
    Only include those that exist (abs or relative).
    """
    cand = []
    for p in [runs_root, "runs", "runs.bak"]:
        p = (p or "").strip()
        if not p:
            continue
        # Normalize to absolute if relative exists
        abs_p = p if os.path.isabs(p) else os.path.abspath(p)
        try:
            if os.path.isdir(abs_p) and abs_p not in cand:
                cand.append(abs_p)
        except Exception:
            continue
    if not cand and runs_root:
        cand.append(runs_root)
    return [{"label": s, "value": s} for s in cand]


def workspace_card(runs_root: str, runs: List[str], default_run: str):
    """
    Workspace card with runs root controls and run selector.
    IDs preserved to match existing callbacks.

    Changes:
    - Restores a Runs Root dropdown (runs-root-select) alongside the text input.
      A small sync callback in workspace callbacks should update runs-root when the
      dropdown changes.
    """
    return html.Div(
        [
            html.H4("Workspace"),
            html.Label("Runs root (select)"),
            dcc.Dropdown(
                id="runs-root-select",
                options=_runs_root_options(runs_root),
                value=runs_root,
                placeholder="Select runs root...",
                style={"width": "100%"},
            ),
            html.Label("Runs root (edit)"),
            dcc.Input(id="runs-root", type="text", value=runs_root, style={"width": "100%"}),
            html.Div(
                [
                    html.Button("Refresh Runs", id="refresh-runs", n_clicks=0),
                    html.Button("Use Current Run", id="use-current-run", n_clicks=0),
                    html.Button("Use Latest Run", id="use-latest-run", n_clicks=0),
                ],
                className="row tight",
            ),
            html.Label("Run directory"),
            dcc.Dropdown(
                id="run-dir",
                options=[{"label": p, "value": p} for p in runs],
                value=default_run,
            ),
        ],
        className="card",
    )]]></content>
    </file>
    <file>
      <path>frontend/controllers/__init__.py</path>
      <content><![CDATA["""
Controllers for reusable business logic extracted from Dash callbacks.
"""

from .runtime_controller import (
    build_phase_update,
    update_phase_json,
    queue_load_engram,
    parse_engram_events_for_message,
)

__all__ = [
    "build_phase_update",
    "update_phase_json",
    "queue_load_engram",
    "parse_engram_events_for_message",
]]]></content>
    </file>
    <file>
      <path>frontend/controllers/charts_controller.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import math
from typing import Tuple, Optional
import plotly.graph_objs as go

from fum_rt.frontend.utilities.tail import tail_jsonl_bytes
from fum_rt.frontend.models.series import SeriesState, append_event, append_say, ffill, extract_tick
from fum_rt.frontend.services.status_client import get_status_snapshot as _get_status


def compute_dashboard_figures(run_dir: str, state: Optional[SeriesState], ui: Optional[dict] = None) -> Tuple[go.Figure, go.Figure, SeriesState]:
    """
    Pure controller for figure construction.
    - Stateless inputs: run_dir, prior state (SeriesState or None)
    - UI dict: optional caps and toggles from in-app controls (no env needed)
    - Returns: (fig_dashboard, fig_discovery, new_state)
    """
    if state is None or getattr(state, "run_dir", None) != run_dir:
        state = SeriesState(run_dir)
    ui = ui or {}

    # Prefer HTTP status snapshot; fallback to cheap file tails if unavailable.
    # Detect truncation/rotation or run restart remains supported via tick/time regression.
    prev_es = getattr(state, "events_size", 0)
    prev_us = getattr(state, "utd_size", 0)
    new_events, esize = [], prev_es
    new_utd, usize = [], prev_us

    snap = None
    try:
        url = None
        timeout_s = None
        try:
            _u = ui.get("status_url") if isinstance(ui, dict) else None
            if isinstance(_u, str) and _u:
                url = _u
        except Exception:
            pass
        try:
            _ts = ui.get("status_timeout") if isinstance(ui, dict) else None
            if _ts is not None:
                timeout_s = float(_ts)
        except Exception:
            pass
        snap = _get_status(url, timeout_s)
    except Exception:
        snap = None

    # Honor UI preference: use HTTP snapshot only when requested; otherwise allow bounded file tails.
    # Default: False (allow file tails) so graphs populate even before ui-state is ready.
    charts_http_only = False
    try:
        charts_http_only = bool(ui.get("charts_http_only", False))
    except Exception:
        charts_http_only = False

    if isinstance(snap, dict) and snap:
        # Use snapshot directly as an "event-like" record; append_event() reads from the dict.
        new_events = [snap]
    elif not charts_http_only:
        # Fallback: tail events.jsonl and utd_events.jsonl incrementally (bounded by last offsets).
        try:
            epath = os.path.join(run_dir, "events.jsonl")
            if os.path.exists(epath):
                new_events, esize = tail_jsonl_bytes(epath, prev_es)
        except Exception:
            new_events, esize = [], prev_es
        try:
            upath = os.path.join(run_dir, "utd_events.jsonl")
            if os.path.exists(upath):
                new_utd, usize = tail_jsonl_bytes(upath, prev_us)
        except Exception:
            new_utd, usize = [], prev_us
    else:
        # Keep empty tails; rely on HTTP to populate when available.
        new_events, new_utd = [], []
        esize, usize = prev_es, prev_us

    # Reset conditions:
    truncated = (prev_es > 0 and esize < prev_es) or (prev_us > 0 and usize < prev_us)

    # 1) Explicit run restart marker from backend (logged at nexus start)
    restart_marker = False
    try:
        for rec in new_events:
            name = str(
                rec.get("event_type")
                or rec.get("event")
                or rec.get("message")
                or rec.get("msg")
                or rec.get("name")
                or ""
            ).lower()
            if name in ("nexus_started", "engram_loaded"):
                restart_marker = True
                break
    except Exception:
        pass

    # 2) Tick regression (incoming ticks lower than last known)
    tick_regression = False
    try:
        last_t = state.t[-1] if state.t else None
        if last_t is not None:
            min_new_t = None
            for rec in new_events:
                tv = extract_tick(rec)
                if tv is None:
                    continue
                tv = int(tv)
                if min_new_t is None or tv < min_new_t:
                    min_new_t = tv
            if min_new_t is not None and min_new_t < last_t:
                tick_regression = True
    except Exception:
        pass

    if truncated or restart_marker or tick_regression:
        # Clear buffers while keeping run_dir; prevents graph overlay on resume/restart.
        # Also advance offsets to current EOF so we don't re-ingest old tail data.
        try:
            state.__post_init__()  # re-init SeriesState buffers and counters
            state.events_size = esize
            state.utd_size = usize
            new_events = []
            new_utd = []
        except Exception:
            pass

    # Apply tails after optional reset
    for rec in new_events:
        append_event(state, rec)
    for rec in new_utd:
        append_say(state, rec)
    state.events_size = esize
    state.utd_size = usize

    # Bound buffers (UI-tunable; no env)
    try:
        MAXP = int(ui.get("maxp", 1200))
    except Exception:
        MAXP = 1200
    if len(state.t) > MAXP:
        state.t = state.t[-MAXP:]
        state.active = state.active[-MAXP:]
        state.avgw = state.avgw[-MAXP:]
        state.coh = state.coh[-MAXP:]
        state.comp = state.comp[-MAXP:]
        state.b1z = state.b1z[-MAXP:]
        state.val = state.val[-MAXP:]
        state.val2 = state.val2[-MAXP:]
        state.entro = state.entro[-MAXP:]
    # Speak tick overlay window (fixed UI-default)
    MAX_SAY = 800
    if len(state.speak_ticks) > MAX_SAY:
        state.speak_ticks = state.speak_ticks[-MAX_SAY:]

    # Forward-fill holes
    t = state.t
    active = ffill(state.active)
    avgw = ffill(state.avgw)
    coh = ffill(state.coh)
    comp = ffill(state.comp)
    b1z = ffill(state.b1z)
    val = ffill(state.val)
    val2 = ffill(state.val2)
    entro = ffill(state.entro)

    # Optional decimation to bound plotting work (UI-only). Applied after MAXP slicing.
    # Env:
    #   DASH_DECIMATE_TO = 0 (off) or N (target max plotted points per series)
    try:
        DEC_TO = int(ui.get("decimate", 600))
    except Exception:
        DEC_TO = 600
    if DEC_TO > 0 and len(t) > DEC_TO:
        stride = max(1, int(math.ceil(len(t) / float(DEC_TO))))
        def _dec(seq):
            return seq[::stride] if stride > 1 else seq
        t = t[::stride]
        active = _dec(active)
        avgw = _dec(avgw)
        coh = _dec(coh)
        comp = _dec(comp)
        b1z = _dec(b1z)
        val = _dec(val)
        val2 = _dec(val2)
        entro = _dec(entro)

    # Palette (env-overridable)
    def _env_color(k: str, default: str) -> str:
        try:
            v = os.getenv(f"DASH_COLOR_{k.upper()}", "").strip()
            return v if v else default
        except Exception:
            return default

    C = {
        "synapses": _env_color("synapses", "#7aa2c7"),
        "avgw": _env_color("avgw", "#9ab8d1"),
        "valence": _env_color("valence", "#c39b70"),
        "valence2": _env_color("valence2", "#8bb995"),
        "components": _env_color("components", "#b68484"),
        "cycles": _env_color("cycles", "#a08878"),
        "b1z": _env_color("b1z", "#76b0a7"),
        "entropy": _env_color("entropy", "#a495c7"),
        "speak_line": _env_color("speak_line", "rgba(120,180,120,0.45)"),
    }

    # fig1
    fig1 = go.Figure()
    # Enforce series_cap (UI control; default 6). Prioritize essential series first.
    try:
        SERIES_CAP = int(ui.get("series_cap", 6))
    except Exception:
        SERIES_CAP = 6
    SERIES_CAP = max(1, min(8, SERIES_CAP))

    add_count = 0
    def _add_if(cond: bool, fn) -> None:
        nonlocal add_count
        if add_count >= SERIES_CAP:
            return
        if cond:
            fn()
            add_count += 1

    # Priority order: Active, Cycles, AvgW, B1z, Components, Valence, Valence2, Entropy
    _add_if(True, lambda: fig1.add_trace(
        go.Scattergl(x=t, y=active, name="Active synapses", line=dict(width=1, color=C["synapses"]))
    ))
    _add_if(True, lambda: fig1.add_trace(
        go.Scattergl(x=t, y=comp, name="Cycles", yaxis="y4", line=dict(width=1, color=C["cycles"]))
    ))
    _add_if(True, lambda: fig1.add_trace(
        go.Scattergl(x=t, y=avgw, name="Avg W", yaxis="y2", line=dict(width=1, color=C["avgw"]))
    ))
    _add_if(True, lambda: fig1.add_trace(
        go.Scattergl(x=t, y=b1z, name="B1 z", yaxis="y5", line=dict(width=1, color=C["b1z"]))
    ))
    _add_if(True, lambda: fig1.add_trace(
        go.Scattergl(x=t, y=coh, name="Components", yaxis="y3", line=dict(width=1, color=C["components"]))
    ))
    _add_if(any(v is not None for v in val), lambda: fig1.add_trace(
        go.Scattergl(x=t, y=val, name="SIE valence", yaxis="y2", line=dict(width=1, dash="dot", color=C["valence"]))
    ))
    _add_if(any(v is not None for v in val2), lambda: fig1.add_trace(
        go.Scattergl(x=t, y=val2, name="SIE v2 valence", yaxis="y2", line=dict(width=1, dash="dash", color=C["valence2"]))
    ))
    _add_if(any(v is not None for v in entro), lambda: fig1.add_trace(
        go.Scattergl(x=t, y=entro, name="Connectome entropy", yaxis="y6", line=dict(width=1, color=C["entropy"]))
    ))
    fig1.update_layout(
        title=f"Dashboard - {os.path.basename(run_dir)}",
        paper_bgcolor="#10151c",
        plot_bgcolor="#0f141a",
        font=dict(color="#cfd7e3"),
        xaxis=dict(domain=[0.05, 0.95], title="Tick", gridcolor="#233140", zerolinecolor="#233140"),
        yaxis=dict(title="Active synapses", side="left", gridcolor="#233140", zerolinecolor="#233140"),
        yaxis2=dict(overlaying="y", side="right", title="Avg W / Valence", showgrid=False, zeroline=False),
        yaxis3=dict(overlaying="y", side="left", position=0.02, showticklabels=False, showgrid=False, zeroline=False),
        yaxis4=dict(overlaying="y", side="right", position=0.98, showticklabels=False, showgrid=False, zeroline=False),
        yaxis5=dict(overlaying="y", side="right", position=0.96, showticklabels=False, showgrid=False, zeroline=False),
        yaxis6=dict(overlaying="y", side="left", position=0.04, showticklabels=False, showgrid=False, zeroline=False),
        legend=dict(orientation="h", bgcolor="rgba(0,0,0,0)"),
        margin=dict(l=40, r=20, t=40, b=40),
    )

    # fig2
    fig2 = go.Figure()
    fig2.add_trace(go.Scattergl(x=t, y=comp, name="Cycle hits", line=dict(width=1, color=C["cycles"])))
    for tk in state.speak_ticks[-200:]:
        fig2.add_vline(x=tk, line_width=1, line_dash="dash", line_color=C["speak_line"])
    fig2.add_trace(go.Scattergl(x=t, y=b1z, name="B1 z", yaxis="y2", line=dict(width=1, color=C["b1z"])))
    fig2.update_layout(
        title="Cycle Hits & B1 z",
        paper_bgcolor="#10151c",
        plot_bgcolor="#0f141a",
        font=dict(color="#cfd7e3"),
        xaxis=dict(title="Tick", gridcolor="#233140", zerolinecolor="#233140"),
        yaxis=dict(title="Cycle hits", gridcolor="#233140", zerolinecolor="#233140"),
        yaxis2=dict(overlaying="y", side="right", title="B1 z", showgrid=False, zeroline=False),
        legend=dict(orientation="h", bgcolor="rgba(0,0,0,0)"),
        margin=dict(l=40, r=20, t=40, b=40),
    )

    return fig1, fig2, state]]></content>
    </file>
    <file>
      <path>frontend/controllers/chat_controller.py</path>
      <content><![CDATA[from __future__ import annotations

from typing import Any, Dict, Iterable, List, Optional


def parse_utd_macro_record(rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Convert a single UTD macro record into a standardized chat item dict.
    Expected rec schema (best-effort):
      { "type": "macro", "macro": "...", "args": {...}, "why": {...} }
    """
    if not isinstance(rec, dict):
        return None
    if rec.get("type") != "macro":
        return None

    macro_name = rec.get("macro", "unknown")
    args = rec.get("args", {}) or {}
    if macro_name == "say":
        text = args.get("text", "")
    else:
        text = f"macro: {macro_name}"
        if args:
            try:
                arg_str = ", ".join(f"{k}={v}" for k, v in args.items())
                text += f" ({arg_str})"
            except Exception:
                text += f" (args: {args})"

    # Extract why.t and spike-like markers if present
    why = rec.get("why") or {}
    t = None
    try:
        t = int((why or {}).get("t"))
    except Exception:
        t = None

    spike = False
    if isinstance(why, dict):
        try:
            speak_ok = why.get("speak_ok")
            spike = bool(speak_ok) or bool((why or {}).get("spike"))
        except Exception:
            spike = False

    return {"kind": "model", "text": str(text), "t": t, "spike": bool(spike), "macro": macro_name}


def items_from_utd_records(records: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for rec in (records or []):
        try:
            item = parse_utd_macro_record(rec)
            if item:
                out.append(item)
        except Exception:
            # ignore malformed lines
            pass
    return out


def items_from_inbox_records(records: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Convert chat_inbox.jsonl records into standardized chat items.
    Supported entries (best-effort):
      - {"type": "text", "msg": "..."}  or {"type": "text", "text": "..."}
    """
    out: List[Dict[str, Any]] = []
    for rec in (records or []):
        try:
            if not isinstance(rec, dict):
                continue
            mtype = (rec.get("type") or "").lower()
            if mtype == "text":
                msg = rec.get("msg") or rec.get("text") or ""
                if msg:
                    out.append({"kind": "user", "text": str(msg), "t": None, "spike": False})
        except Exception:
            pass
    return out


def trim_items(items: List[Dict[str, Any]], limit: int = 200) -> List[Dict[str, Any]]:
    if not items:
        return []
    if len(items) <= limit:
        return items
    return items[-limit:]


def render_chat_view(items: List[Dict[str, Any]], filt: str = "all") -> str:
    """
    Render a plain-text view of chat items based on filter:
      - "all": show all
      - "say": only model items with macro == "say"
      - "spike": only model items with spike==True and macro == "say"
    """
    f = (filt or "all").lower()
    lines: List[str] = []

    for it in (items or []):
        if f == "say":
            if it.get("kind") != "model" or it.get("macro") != "say":
                continue
        elif f == "spike":
            if it.get("kind") != "model" or not it.get("spike", False) or it.get("macro") != "say":
                continue

        text = it.get("text") or ""
        if it.get("kind") == "user":
            lines.append(f"You: {text}")
        else:
            t = it.get("t")
            if t is not None:
                lines.append(f"[t={t}] {text}")
            else:
                lines.append(f"{text}")

    return "\n".join(lines)]]></content>
    </file>
    <file>
      <path>frontend/controllers/file_picker_controller.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker Controller Utilities

Purpose:
- Provide bounded, reusable filesystem helpers for the Dash file-picker
- Centralize path clamping, directory listing, and initial tree construction
- Ensure safety (no traversal beyond root), performance (scandir), and filtering (exts, dotfiles)

Design constraints:
- Sparse-first: single-directory listings only (no recursion)
- No schedulers
- No scans in core/ or maps/ (caller controls roots; we only walk the provided path)
- Contracts: Pure functions with deterministic outputs for given inputs
"""

import os
from typing import List, Tuple, Dict, Any


def clamp_to_root(path: str, root: str) -> str:
    """
    Clamp an arbitrary path to the provided root. If 'path' escapes 'root', return the root.

    Args:
        path: Any filesystem path (relative or absolute)
        root: Root boundary (absolute or relative, coerced to absolute internally)

    Returns:
        Absolute path inside 'root' (or the root itself when 'path' escapes)
    """
    try:
        root_abs = os.path.abspath(root)
        path_abs = os.path.abspath(path)
        common = os.path.commonpath([root_abs, path_abs])
        return path_abs if common == root_abs else root_abs
    except Exception:
        return os.path.abspath(root)


def is_within_root(path: str, root: str) -> bool:
    """
    Check if 'path' is within 'root' boundary.

    Returns:
        True if path is inside root, False otherwise (including exceptions).
    """
    try:
        root_abs = os.path.abspath(root)
        path_abs = os.path.abspath(path)
        return os.path.commonpath([root_abs, path_abs]) == root_abs
    except Exception:
        return False


def list_dir(path: str, exts: List[str] | None = None, hide_dotfiles: bool = True) -> Tuple[List[str], List[str]]:
    """
    List a single directory (bounded IO).

    Args:
        path: Directory to list (absolute or relative)
        exts: Optional list of allowed file extensions (e.g., [".json", ".csv"]); when None or empty, allow all
        hide_dotfiles: When True, hide entries whose name starts with '.'

    Returns:
        (subdirs, files) - both sorted, names only (no absolute paths)

    Policy:
        - No scans in 'core/' or 'maps/' at any depth. If the resolved path contains either
          restricted segment, short-circuit and return empty results. This enforces the global
          guard while keeping IO strictly bounded.
    """
    subdirs: List[str] = []
    files: List[str] = []
    try:
        pabs = os.path.abspath(path)
        # Enforce "no scans in core/ or maps/" at any depth
        parts = os.path.normpath(pabs).split(os.sep)
        if "core" in parts or "maps" in parts:
            return [], []

        lower_exts = [e.lower() for e in (exts or [])]
        allow_all = not lower_exts
        with os.scandir(pabs) as it:
            for entry in it:
                name = entry.name
                if hide_dotfiles and name.startswith("."):
                    continue
                try:
                    if entry.is_dir(follow_symlinks=False):
                        # Don't even offer restricted dirs for expansion
                        if name not in ("core", "maps"):
                            subdirs.append(name)
                    else:
                        if allow_all or any(name.lower().endswith(e) for e in lower_exts):
                            files.append(name)
                except Exception:
                    # Skip entries that cause IO/stat errors
                    continue
    except Exception:
        return [], []
    subdirs.sort()
    files.sort()
    return subdirs, files


def init_tree(root: str, exts: List[str] | None = None, hide_dotfiles: bool = True) -> Dict[str, Any]:
    """
    Construct the initial tree structure for a file-picker rooted at 'root'.

    Structure:
        {
          "root": <abs_root>,
          "nodes": {
             <abs_root>: {
                "expanded": True,
                "subdirs": [<names>],
                "files": [<names>]
             }
          }
        }

    Notes:
      - Bounded IO: lists only the root directory once for initialization
      - No recursion; deeper nodes are discovered on user toggle via list_dir()
    """
    rabs = os.path.abspath(root)
    subdirs, files = list_dir(rabs, exts=exts, hide_dotfiles=hide_dotfiles)
    return {
        "root": rabs,
        "nodes": {
            rabs: {
                "expanded": True,
                "subdirs": subdirs or [],
                "files": files or [],
            }
        },
    }


def next_children(path: str, exts: List[str] | None = None, hide_dotfiles: bool = True) -> Tuple[List[str], List[str]]:
    """
    One-step discovery for a directory node. Intended for use when the user expands a folder.
    """
    pabs = os.path.abspath(path)
    return list_dir(pabs, exts=exts, hide_dotfiles=hide_dotfiles)]]></content>
    </file>
    <file>
      <path>frontend/controllers/file_picker_ctx.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker Dash Context Helpers

Atomic helper(s) to robustly parse Dash callback_context for pattern-matched components.
"""

from typing import Optional, Dict, Any


def get_trigger_id_obj(ctx) -> Optional[Dict[str, Any]]:
    """
    Return the pattern-matched dict id for the triggering component if available.
    Works on newer Dash (ctx.triggered_id) and older (JSON prop_id).
    """
    try:
        tid = getattr(ctx, "triggered_id", None)
        if isinstance(tid, dict):
            return tid
    except Exception:
        pass
    try:
        import json  # local import to keep dependency surface minimal
        if getattr(ctx, "triggered", None):
            tid_s = ctx.triggered[0]["prop_id"].rsplit(".", 1)[0]
            return json.loads(tid_s)
    except Exception:
        return None
    return None]]></content>
    </file>
    <file>
      <path>frontend/controllers/file_picker_status.py</path>
      <content><![CDATA[from __future__ import annotations

"""
File Picker Status/Formatting Controller (atomic helpers)

Purpose:
- Pure helpers used by Dash callbacks and widgets to compute status strings and sizes
- Keep IO bounded and consistent with global guards via controller.list_dir()

Contracts:
- No recursion. Single-directory listings only.
- No scans in core/ or maps/ are enforced by file_picker_controller.list_dir
- Deterministic outputs; exceptions are contained and produce safe fallbacks.
"""

import os
from typing import List

from fum_rt.frontend.controllers.file_picker_controller import (
    list_dir as _ctl_list_dir,
)


def human_size(n: int | float) -> str:
    """
    Convert a byte count to a human-readable string.
    """
    try:
        n = int(n)
    except Exception:
        return "0 B"
    units = [("TB", 1024**4), ("GB", 1024**3), ("MB", 1024**2), ("KB", 1024), ("B", 1)]
    for label, factor in units:
        if n >= factor:
            if factor == 1:
                return f"{n} B"
            return f"{n / factor:.2f} {label}"
    return "0 B"


def sum_filesizes(dir_path: str, filenames: List[str] | None) -> int:
    """
    Sum sizes of the given filenames inside dir_path (non-recursive).
    """
    total = 0
    for name in (filenames or []):
        try:
            fp = os.path.join(dir_path, name)
            if os.path.isfile(fp):
                total += os.path.getsize(fp)
        except Exception:
            continue
    return total


def file_status_text(file_path: str) -> str:
    """
    Build status text for a single file path.
    """
    base = os.path.basename(file_path or "")
    try:
        size_b = os.path.getsize(file_path) if os.path.isfile(file_path) else 0
        return f"File: {base} - Size: {human_size(size_b)}"
    except Exception:
        return f"File: {base} - Size: unknown"


def directory_status_text(dir_path: str, exts: List[str] | None = None, hide_dotfiles: bool = True) -> str:
    """
    Build status text for a directory: counts and aggregate size of visible files.

    IO is strictly bounded to a single directory via controller.list_dir.
    """
    try:
        if not (dir_path and os.path.isdir(dir_path)):
            return "Contains: 0 folders; 0 files; Total Size: 0 B"
        subdirs, files = _ctl_list_dir(dir_path, exts=exts or [], hide_dotfiles=hide_dotfiles)
        folders_n = len(subdirs or [])
        files_n = len(files or [])
        total_b = sum_filesizes(dir_path, files or [])
        return f"Contains: {folders_n} folders; {files_n} files; Total Size: {human_size(total_b)}"
    except Exception:
        return "Contains: 0 folders; 0 files; Total Size: 0 B"]]></content>
    </file>
    <file>
      <path>frontend/controllers/runtime_controller.py</path>
      <content><![CDATA[from __future__ import annotations

import os
from typing import Any, Dict, Iterable, Optional

from fum_rt.frontend.utilities.profiles import safe_int as _safe_int, safe_float as _safe_float
from fum_rt.frontend.utilities.fs_utils import read_json_file, write_json_file, latest_checkpoint


def build_phase_update(
    default_profile: Dict[str, Any],
    phase,
    s_z,
    s_h,
    s_cd,
    s_vt,
    c_w,
    c_h,
    c_b,
    c_pf,
    c_thr,
    c_lw,
    c_cand,
    s_idf,
) -> Dict[str, Any]:
    """
    Build a dict of runtime/phase settings, coercing types with defaults.
    This normalizes UI inputs into a consistent structure.
    """
    return {
        "phase": int(_safe_int(phase, 0)),
        "speak": {
            "speak_z": float(_safe_float(s_z, default_profile["speak_z"])),
            "speak_hysteresis": float(_safe_float(s_h, default_profile["speak_hysteresis"])),
            "speak_cooldown_ticks": int(_safe_int(s_cd, default_profile["speak_cooldown_ticks"])),
            "speak_valence_thresh": float(_safe_float(s_vt, default_profile["speak_valence_thresh"])),
        },
        "connectome": {
            "walkers": int(_safe_int(c_w, default_profile["walkers"])),
            "hops": int(_safe_int(c_h, default_profile["hops"])),
            "bundle_size": int(_safe_int(c_b, default_profile["bundle_size"])),
            "prune_factor": float(_safe_float(c_pf, default_profile["prune_factor"])),
            "threshold": float(_safe_float(c_thr, default_profile.get("threshold", 0.15))),
            "lambda_omega": float(_safe_float(c_lw, default_profile.get("lambda_omega", 0.10))),
            "candidates": int(_safe_int(c_cand, default_profile.get("candidates", 64))),
        },
        "sie": {"novelty_idf_gain": float(_safe_float(s_idf, 1.0))},
    }


def _merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    """
    Shallow-merge with one level of deep-merge for nested dicts 'speak' and 'connectome'.
    Preserves keys like 'load_engram' in existing phase.json while applying updates.
    """
    out = dict(a or {})
    for k, v in (b or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            nv = dict(out[k])
            nv.update(v)
            out[k] = nv
        else:
            out[k] = v
    return out


def update_phase_json(run_dir: str, update: Dict[str, Any]) -> bool:
    """
    Merge-update run_dir/phase.json with the provided update dict.
    Ensures directory exists; returns True on successful write.
    """
    try:
        os.makedirs(run_dir, exist_ok=True)
        p = os.path.join(run_dir, "phase.json")
        current = read_json_file(p) or {}
        if not isinstance(current, dict):
            current = {}
        merged = _merge(current, update)
        return write_json_file(p, merged)
    except Exception:
        return False


def queue_load_engram(run_dir: str, path: str) -> tuple[bool, str]:
    """
    Add/overwrite 'load_engram' in phase.json, preserving other keys.
    Normalizes directories to the latest checkpoint file to prevent loader errors.
    Returns (ok, normalized_path) so the UI can echo the actual target.
    """
    try:
        os.makedirs(run_dir, exist_ok=True)
        p = os.path.join(run_dir, "phase.json")
        obj = read_json_file(p) or {}
        if not isinstance(obj, dict):
            obj = {}
        # Normalize provided path:
        target = (path or "").strip()
        if not target:
            return False, ""
        # If a directory was provided, resolve to latest checkpoint inside it
        if os.path.isdir(target):
            lp = latest_checkpoint(target)
            if not lp:
                return False, ""
            target = lp
        else:
            # If file doesn't exist, try resolving relative to run_dir
            if not os.path.exists(target):
                cand = os.path.join(run_dir, target)
                if os.path.isdir(cand):
                    lp = latest_checkpoint(cand)
                    if not lp:
                        return False, ""
                    target = lp
                elif os.path.exists(cand):
                    target = cand
                else:
                    return False, ""
        obj["load_engram"] = target
        ok = write_json_file(p, obj)
        return (True, target) if ok else (False, "")
    except Exception:
        return False, ""


def parse_engram_events_for_message(records: Iterable[Dict[str, Any]]) -> Optional[str]:
    """
    Scan a set of event dicts and derive a user-facing message summarizing an engram load result.
    Returns the last relevant message if multiple are present.
    """
    msg: Optional[str] = None
    for rec in (records or []):
        if not isinstance(rec, dict):
            continue
        name = str(
            rec.get("event_type")
            or rec.get("event")
            or rec.get("message")
            or rec.get("msg")
            or rec.get("name")
            or ""
        ).lower()
        extra = rec.get("extra") or rec.get("meta") or {}
        if name == "engram_loaded":
            engram_path = extra.get("path") or extra.get("engram") or rec.get("path")
            msg = f"Engram loaded: {engram_path}" if engram_path else "Engram loaded."
        elif name == "engram_load_error":
            err = extra.get("err") or extra.get("error") or rec.get("error")
            engram_path = extra.get("path") or extra.get("engram") or rec.get("path")
            if err and engram_path:
                msg = f"Engram load error: {err} ({engram_path})"
            elif err:
                msg = f"Engram load error: {err}"
            else:
                msg = "Engram load error."
    return msg]]></content>
    </file>
    <file>
      <path>frontend/debug_ui.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import time
from typing import List, Dict, Any

from dash import Dash, html, dcc
from dash.dependencies import Input, Output, State  # type: ignore
import plotly.graph_objects as go

# Reuse existing lightweight filesystem utilities (no heavy scans)
from fum_rt.frontend.utilities.fs_utils import list_runs
from fum_rt.frontend.controllers.charts_controller import compute_dashboard_figures as _compute_dashboard_figures
from fum_rt.frontend.models.series import SeriesState as _SeriesState
# Optional: reuse controller to hook components incrementally
from fum_rt.frontend.controllers.charts_controller import compute_dashboard_figures as _compute_dashboard_figures
from fum_rt.frontend.models.series import SeriesState as _SeriesState


def _abs(path: str) -> str:
    try:
        return path if os.path.isabs(path) else os.path.abspath(path)
    except Exception:
        return path


def _file_stat(path: str) -> Dict[str, Any]:
    """
    Cheap file stat: no full read, only exists/size/mtime.
    """
    try:
        if not os.path.exists(path):
            return {"exists": False, "size": 0, "mtime": 0.0}
        return {
            "exists": True,
            "size": os.path.getsize(path) if os.path.isfile(path) else 0,
            "mtime": os.path.getmtime(path),
        }
    except Exception:
        return {"exists": False, "size": 0, "mtime": 0.0}


def _dir_stat(path: str) -> Dict[str, Any]:
    """
    Cheap dir stat: exists, dir mtime, count of subdirs (first-level).
    Avoids walking recursively; bounded and cheap.
    """
    try:
        if not os.path.isdir(path):
            return {"exists": False, "mtime": 0.0, "dirs": 0}
        try:
            mt = os.path.getmtime(path)
        except Exception:
            mt = 0.0
        try:
            cnt = sum(1 for n in os.listdir(path) if os.path.isdir(os.path.join(path, n)))
        except Exception:
            cnt = 0
        return {"exists": True, "mtime": mt, "dirs": cnt}
    except Exception:
        return {"exists": False, "mtime": 0.0, "dirs": 0}


def _runs_options(root: str) -> List[Dict[str, str]]:
    """
    Options for run-dir dropdown. Uses list_runs(root) which returns absolute paths,
    sorted by mtime desc (cheap).
    """
    try:
        runs = list_runs(root)
    except Exception:
        runs = []
    return [{"label": os.path.basename(p.rstrip(os.path.sep)) or p, "value": p} for p in runs]


def build_debug_app(runs_root: str) -> Dash:
    """
    Minimal, surgical debug UI:
    - No charts, no complex callbacks
    - Only polls a selected run_dir and reports file sizes/mtimes
    - Designed to isolate UI overhead vs backend/logging issues
    """
    app = Dash(
        __name__,
        prevent_initial_callbacks=True,
        suppress_callback_exceptions=True,
    )
    app.title = "FUM Debug UI"

    rr_abs = _abs(runs_root) or _abs("runs")
    runs_opts = _runs_options(rr_abs)
    default_run = runs_opts[0]["value"] if runs_opts else ""

    # Layout: runs root input + refresh button, run-dir dropdown, poll interval, component toggles, stats and graphs
    app.layout = html.Div(
        [
            html.H3("FUM Debug UI - Minimal"),
            html.Div(
                [
                    html.Label("Runs root"),
                    dcc.Input(id="dbg-runs-root", type="text", value=rr_abs, style={"width": "100%"}),
                    html.Button("Refresh", id="dbg-refresh", n_clicks=0),
                ],
                style={"display": "grid", "gap": "8px", "maxWidth": "640px"},
            ),
            html.Div(
                [
                    html.Label("Run directory"),
                    dcc.Dropdown(
                        id="dbg-run-dir",
                        options=runs_opts,
                        value=default_run,
                        style={"minWidth": "360px"},
                    ),
                ],
                style={"display": "grid", "gap": "8px", "maxWidth": "640px", "marginTop": "8px"},
            ),
            html.Div(
                [
                    html.Label("Polling"),
                    dcc.Checklist(
                        id="dbg-poll-enabled",
                        options=[{"label": "Enable polling", "value": "on"}],
                        value=["on"],
                        inline=True,
                    ),
                    dcc.Slider(
                        id="dbg-poll-ms",
                        min=250,
                        max=5000,
                        step=250,
                        value=1000,
                        tooltip={"always_visible": False},
                    ),
                ],
                style={"display": "grid", "gap": "8px", "maxWidth": "640px", "marginTop": "8px"},
            ),
            html.Div(
                [
                    html.Label("Components"),
                    dcc.Checklist(
                        id="dbg-components",
                        options=[
                            {"label": "Tiny chart: file sizes", "value": "file_sizes"},
                            {"label": "Controller charts (dashboard + discovery)", "value": "controller_charts"},
                        ],
                        value=["file_sizes"],
                        inline=False,
                    ),
                ],
                style={"display": "grid", "gap": "8px", "maxWidth": "960px", "marginTop": "8px"},
            ),
            html.Div(
                [
                    html.Label("Controller settings"),
                    html.Div(
                        [
                            html.Div(
                                [
                                    html.Label("Status URL"),
                                    dcc.Input(id="dbg-ctrl-status-url", type="text", value="http://127.0.0.1:8787/status/snapshot", style={"width": "100%"}),
                                ],
                                style={"minWidth": "280px"},
                            ),
                            html.Div(
                                [
                                    html.Label("Status timeout (s)"),
                                    dcc.Input(id="dbg-ctrl-status-timeout", type="number", value=0.2, min=0.05, step=0.05),
                                ]
                            ),
                            html.Div(
                                [
                                    html.Label("Points cap (maxp)"),
                                    dcc.Input(id="dbg-ctrl-points-cap", type="number", value=800, min=50, step=50),
                                ]
                            ),
                            html.Div(
                                [
                                    html.Label("Series cap"),
                                    dcc.Input(id="dbg-ctrl-series-cap", type="number", value=4, min=1, step=1),
                                ]
                            ),
                            html.Div(
                                [
                                    html.Label("Decimate to N points (0=off)"),
                                    dcc.Input(id="dbg-ctrl-decimate", type="number", value=400, min=0, step=50),
                                ]
                            ),
                        ],
                        className="row",
                    ),
                ],
                style={"display": "grid", "gap": "8px", "maxWidth": "960px", "marginTop": "8px"},
            ),
            # Graphs (rendered empty unless enabled via dbg-components)
            html.Div(
                [
                    dcc.Graph(id="dbg-fig", figure=go.Figure(), style={"minHeight": "220px"}),
                    dcc.Graph(id="dbg-fig-dashboard", figure=go.Figure(), style={"minHeight": "360px"}),
                    dcc.Graph(id="dbg-fig-discovery", figure=go.Figure(), style={"minHeight": "300px"}),
                ],
                style={"display": "grid", "gap": "16px", "maxWidth": "1200px"},
            ),
            dcc.Interval(id="dbg-interval", interval=1000, n_intervals=0, disabled=False),
            html.Hr(),
            html.Pre(id="dbg-stats", children="Waiting for data..."),
            dcc.Store(id="dbg-state"),
        ],
        style={"padding": "10px", "fontFamily": "sans-serif"},
    )

    # Callbacks
    @app.callback(
        Output("dbg-run-dir", "options", allow_duplicate=True),
        Output("dbg-run-dir", "value", allow_duplicate=True),
        Input("dbg-refresh", "n_clicks"),
        State("dbg-runs-root", "value"),
        prevent_initial_call=True,
    )
    def on_refresh_runs(_n, root):
        r = (_abs(root or "")).strip()
        if not r:
            raise Exception("Runs root is empty")
        opts = _runs_options(r)
        val = opts[0]["value"] if opts else ""
        return opts, val

    @app.callback(
        Output("dbg-interval", "interval", allow_duplicate=True),
        Output("dbg-interval", "disabled", allow_duplicate=True),
        Input("dbg-poll-ms", "value"),
        Input("dbg-poll-enabled", "value"),
        prevent_initial_call=True,
    )
    def on_poll_config(ms, enabled_vals):
        try:
            interval = int(ms) if ms else 1000
        except Exception:
            interval = 1000
        disabled = ("on" not in (enabled_vals or []))
        return max(250, interval), disabled

    @app.callback(
        Output("dbg-stats", "children", allow_duplicate=True),
        Input("dbg-interval", "n_intervals"),
        State("dbg-run-dir", "value"),
        State("dbg-runs-root", "value"),
        prevent_initial_call=False,
    )
    def on_tick(_n, rd, rr):
        t0 = time.perf_counter()

        rr_abs_local = _abs((rr or "").strip())
        rd_local = (rd or "").strip()
        if rd_local and not os.path.isabs(rd_local):
            # Normalize run_dir against runs_root (defensive)
            rd_abs = _abs(os.path.join(rr_abs_local, rd_local))
        else:
            rd_abs = rd_local

        # Resolve key file stats in the selected run
        events_path = os.path.join(rd_abs, "events.jsonl") if rd_abs else ""
        utd_path = os.path.join(rd_abs, "utd_events.jsonl") if rd_abs else ""
        phase_path = os.path.join(rd_abs, "phase.json") if rd_abs else ""

        rr_stat = _dir_stat(rr_abs_local) if rr_abs_local else {"exists": False, "mtime": 0.0, "dirs": 0}
        rd_exists = os.path.isdir(rd_abs) if rd_abs else False
        events_stat = _file_stat(events_path) if rd_abs else {"exists": False, "size": 0, "mtime": 0.0}
        utd_stat = _file_stat(utd_path) if rd_abs else {"exists": False, "size": 0, "mtime": 0.0}
        phase_stat = _file_stat(phase_path) if rd_abs else {"exists": False, "size": 0, "mtime": 0.0}

        # Also surface a cheap "top N" run dirs snapshot for verification (labels only)
        try:
            runs_list = list_runs(rr_abs_local)[:6]
        except Exception:
            runs_list = []

        dt_ms = int((time.perf_counter() - t0) * 1000.0)
        now_s = time.strftime("%Y-%m-%d %H:%M:%S")

        # Produce a compact, human-readable diagnostics block (no heavy formatting)
        lines = []
        lines.append(f"[{now_s}] server_callback_ms={dt_ms}")
        lines.append(f"runs_root_abs={rr_abs_local}")
        lines.append(f"runs_root_exists={rr_stat.get('exists')} subdirs={rr_stat.get('dirs')} mtime={rr_stat.get('mtime'):.3f}")
        lines.append(f"run_dir={rd_abs}")
        lines.append(f"run_dir_exists={rd_exists}")
        lines.append(f"events.jsonl exists={events_stat['exists']} size={events_stat['size']} mtime={events_stat['mtime']:.3f}")
        lines.append(f"utd_events.jsonl exists={utd_stat['exists']} size={utd_stat['size']} mtime={utd_stat['mtime']:.3f}")
        lines.append(f"phase.json exists={phase_stat['exists']} size={phase_stat['size']} mtime={phase_stat['mtime']:.3f}")
        lines.append("runs_head:")
        for p in runs_list:
            lines.append(f"  - {p}")
        try:
            _c_ms = int(getattr(on_controller_figs, "_last_ms", -1))
        except Exception:
            _c_ms = -1
        lines.append(f"controller_dt_ms={_c_ms}")
        return "\n".join(lines)

    # Minimal figure: file size over time (isolates Plotly/dash overhead)
    @app.callback(
        Output("dbg-fig", "figure", allow_duplicate=True),
        Input("dbg-interval", "n_intervals"),
        State("dbg-components", "value"),
        State("dbg-run-dir", "value"),
        State("dbg-runs-root", "value"),
        prevent_initial_call=False,
    )
    def on_fig(_n, comps, rd, rr):
        comps = comps or []
        if "file_sizes" not in comps:
            return go.Figure()

        # Resolve absolute run_dir
        rr_abs_local = _abs((rr or "").strip())
        rd_local = (rd or "").strip()
        if rd_local and not os.path.isabs(rd_local):
            rd_abs = _abs(os.path.join(rr_abs_local, rd_local))
        else:
            rd_abs = rd_local

        events_path = os.path.join(rd_abs, "events.jsonl") if rd_abs else ""
        utd_path = os.path.join(rd_abs, "utd_events.jsonl") if rd_abs else ""

        e_stat = _file_stat(events_path) if rd_abs else {"exists": False, "size": 0}
        u_stat = _file_stat(utd_path) if rd_abs else {"exists": False, "size": 0}

        series = getattr(on_fig, "_series", {"t": [], "events": [], "utd": []})
        t = series["t"]; ev = series["events"]; ut = series["utd"]
        t.append(len(t) + 1)
        ev.append(int(e_stat.get("size", 0)))
        ut.append(int(u_stat.get("size", 0)))

        # Keep small ring buffer to bound memory/CPU
        if len(t) > 256:
            t[:] = t[-256:]; ev[:] = ev[-256:]; ut[:] = ut[-256:]
        setattr(on_fig, "_series", series)

        fig = go.Figure()
        if ev:
            fig.add_trace(go.Scatter(x=t, y=[x / 1e6 for x in ev], name="events.jsonl MB", mode="lines"))
        if ut:
            fig.add_trace(go.Scatter(x=t, y=[x / 1e6 for x in ut], name="utd_events.jsonl MB", mode="lines"))

        fig.update_layout(
            template="plotly_dark",
            margin=dict(l=30, r=10, t=30, b=30),
            xaxis_title="Tick",
            yaxis_title="Size (MB)",
            legend=dict(orientation="h"),
        )
        return fig

    # Controller charts (reuse production controller with small, bounded UI knobs)
    @app.callback(
        Output("dbg-fig-dashboard", "figure", allow_duplicate=True),
        Output("dbg-fig-discovery", "figure", allow_duplicate=True),
        Input("dbg-interval", "n_intervals"),
        State("dbg-components", "value"),
        State("dbg-run-dir", "value"),
        State("dbg-runs-root", "value"),
        State("dbg-ctrl-status-url", "value"),
        State("dbg-ctrl-status-timeout", "value"),
        State("dbg-ctrl-points-cap", "value"),
        State("dbg-ctrl-series-cap", "value"),
        State("dbg-ctrl-decimate", "value"),
        prevent_initial_call=False,
    )
    def on_controller_figs(_n, comps, rd, rr, status_url, status_timeout, points_cap, series_cap, decimate_to):
        comps = comps or []
        if "controller_charts" not in comps:
            return go.Figure(), go.Figure()

        # Resolve absolute run_dir
        rr_abs_local = _abs((rr or "").strip())
        rd_local = (rd or "").strip()
        if rd_local and not os.path.isabs(rd_local):
            rd_abs = _abs(os.path.join(rr_abs_local, rd_local))
        else:
            rd_abs = rd_local

        # Build UI dict for controller
        try:
            ui = {
                "status_url": str(status_url or "http://127.0.0.1:8787/status/snapshot"),
                "status_timeout": float(status_timeout if status_timeout is not None else 0.2),
                "maxp": int(points_cap if points_cap is not None else 800),
                "series_cap": int(series_cap if series_cap is not None else 4),
                "decimate": int(decimate_to if decimate_to is not None else 400),
            }
        except Exception:
            ui = {
                "status_url": "http://127.0.0.1:8787/status/snapshot",
                "status_timeout": 0.2,
                "maxp": 800,
                "series_cap": 4,
                "decimate": 400,
            }

        # Persist SeriesState between ticks (per-run)
        state = getattr(on_controller_figs, "_state", None)
        if not isinstance(state, _SeriesState) or getattr(state, "run_dir", None) != rd_abs:
            state = _SeriesState(rd_abs or "")
        t0 = time.perf_counter()
        fig1, fig2, new_state = _compute_dashboard_figures(rd_abs or "", state, ui)
        try:
            setattr(on_controller_figs, "_last_ms", int((time.perf_counter() - t0) * 1000.0))
        except Exception:
            pass
        setattr(on_controller_figs, "_state", new_state)
        return fig1, fig2

    return app


if __name__ == "__main__":
    # Environment-driven defaults for manual testing:
    # RUNS_ROOT: override runs root; default to ./runs
    # DASH_HOST / DASH_PORT: debug server bind (default 127.0.0.1:8060)
    rr = os.getenv("RUNS_ROOT", "").strip() or _abs("runs")
    app = build_debug_app(rr)
    host = os.getenv("DASH_HOST", "127.0.0.1")
    try:
        port = int(os.getenv("DASH_PORT", "8060"))
    except Exception:
        port = 8060
    # No debug reloader to avoid duplicate callbacks; UI is already minimal
    app.run(host=host, port=port, debug=False)]]></content>
    </file>
    <file>
      <path>frontend/models/series.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import math
from typing import Any, Dict, List
from dataclasses import dataclass


class StreamingZEMA:
    """
    Streaming z-score of deltas (ZEMA) with exponential moving averages.
    Matches legacy dashboard behavior exactly.
    """
    def __init__(self, half_life_ticks: int = 120):
        self.alpha = 1.0 - math.exp(math.log(0.5) / float(max(1, int(half_life_ticks))))
        self.mu = 0.0
        self.var = 1e-6
        self.prev = None

    def update(self, v: float) -> float:
        v = float(v)
        if self.prev is None:
            self.prev = v
            return 0.0
        d = v - self.prev
        self.prev = v
        a = self.alpha
        self.mu = (1.0 - a) * self.mu + a * d
        diff = d - self.mu
        self.var = (1.0 - a) * self.var + a * (diff * diff)
        sigma = (self.var if self.var > 1e-24 else 1e-24) ** 0.5
        return float(diff / sigma)


@dataclass
class SeriesState:
    """
    Rolling buffers for timeseries in the dashboard (unchanged semantics).
    """
    run_dir: str

    def __post_init__(self):
        self.events_path = os.path.join(self.run_dir, "events.jsonl")
        self.utd_path = os.path.join(self.run_dir, "utd_events.jsonl")
        self.events_size = 0
        self.utd_size = 0
        self.b1_ema = StreamingZEMA(half_life_ticks=120)
        self.t: List[int] = []
        self.active: List[float | None] = []
        self.avgw: List[float | None] = []
        self.coh: List[float | None] = []
        self.comp: List[float | None] = []
        self.b1z: List[float | None] = []
        self.val: List[float | None] = []
        self.val2: List[float | None] = []
        self.entro: List[float | None] = []
        self.speak_ticks: List[int] = []


def extract_tick(rec: Dict[str, Any]) -> int | None:
    for k in ("t", "tick"):
        if k in rec:
            try:
                return int(rec[k])
            except Exception:
                pass
    ex = rec.get("extra", {})
    for k in ("t", "tick"):
        if k in ex:
            try:
                return int(ex[k])
            except Exception:
                pass
    return None


def append_event(ss: SeriesState, rec: Dict[str, Any]):
    t = extract_tick(rec)
    if t is None:
        return
    ex = rec.get("extra", rec)
    ss.t.append(int(t))
    ss.active.append(ex.get("active_synapses"))
    ss.avgw.append(ex.get("avg_weight"))
    ss.coh.append(ex.get("cohesion_components"))
    cc = ex.get("complexity_cycles")
    ss.comp.append(cc)
    bz = ex.get("b1_z")
    if bz is None:
        v = 0.0 if cc is None else float(cc)
        bz = ss.b1_ema.update(v)
    ss.b1z.append(float(bz))
    # Robust SIE valence extraction with fallbacks (handles various field names and ranges)
    val = ex.get("sie_valence_01")
    if val is None:
        for k in ("sie_valence", "valence"):
            v = ex.get(k)
            if v is not None:
                try:
                    fv = float(v)
                    # Normalize [-1,1] -> [0,1] if appropriate
                    val = (fv + 1.0) / 2.0 if -1.001 <= fv <= 1.001 else fv
                except Exception:
                    val = v
                break
        if val is None:
            sie = ex.get("sie") or {}
            if isinstance(sie, dict):
                if "valence_01" in sie:
                    val = sie.get("valence_01")
                elif "valence" in sie:
                    try:
                        fv = float(sie.get("valence"))
                        val = (fv + 1.0) / 2.0 if -1.001 <= fv <= 1.001 else fv
                    except Exception:
                        val = sie.get("valence")
    ss.val.append(val)
    val2 = ex.get("sie_v2_valence_01")
    if val2 is None:
        for k in ("sie_v2_valence", "sie_v2"):
            v = ex.get(k)
            if v is not None:
                try:
                    fv = float(v)
                    val2 = (fv + 1.0) / 2.0 if -1.001 <= fv <= 1.001 else fv
                except Exception:
                    val2 = v
                break
        if val2 is None:
            sie2 = ex.get("sie_v2") or {}
            if isinstance(sie2, dict):
                if "valence_01" in sie2:
                    val2 = sie2.get("valence_01")
                elif "valence" in sie2:
                    try:
                        fv = float(sie2.get("valence"))
                        val2 = (fv + 1.0) / 2.0 if -1.001 <= fv <= 1.001 else fv
                    except Exception:
                        val2 = sie2.get("valence")
    ss.val2.append(val2)
    ss.entro.append(ex.get("connectome_entropy"))


def append_say(ss: SeriesState, rec: Dict[str, Any]):
    name = (rec.get("macro") or rec.get("name") or rec.get("kind") or "").lower()
    if name != "say":
        return
    t = rec.get("t") or rec.get("tick") or rec.get("meta", {}).get("t") or rec.get("meta", {}).get("tick")
    if t is None:
        return
    try:
        ss.speak_ticks.append(int(t))
    except Exception:
        pass


def ffill(arr: List[Any]) -> List[float | None]:
    out: List[float | None] = []
    last: float | None = None
    for x in arr:
        if x is None:
            out.append(last)
        else:
            try:
                v = float(x)
            except Exception:
                v = last
            out.append(v)
            last = v
    return out]]></content>
    </file>
    <file>
      <path>frontend/plugins/fum_visualizer_v1/fum_visualizer.py</path>
      <content><![CDATA[# visualizer.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.
"""
import matplotlib.pyplot as plt
import numpy as np
import imageio
from tqdm import tqdm
import plotly.graph_objects as go
import scipy.sparse
import os
from Void_Equations import delta_re_vgsp, delta_gdsp # Import the void dynamics

def void_driven_layout(W, iterations=50, dim=3):
    """
    Computes a node layout using void dynamics for attraction and repulsion.
    This creates a more organic, biologically plausible layout than random placement.
    Handles both dense and sparse matrices.

    Improvements:
    - O(E) attraction accumulation using numpy.add.at (no NxN tensor)
    - Adaptive, sub-sampled repulsion for large N to avoid O(N^2) blowups
    - Step clipping for stability
    """
    num_nodes = W.shape[0]
    pos = np.random.rand(num_nodes, dim)

    # Extract edges and weights from sparse or dense adjacency
    if scipy.sparse.issparse(W):
        rows, cols = W.nonzero()
        weights = W.data
    else:
        rows, cols = np.where(W > 0)
        weights = W[rows, cols]

    rows = np.asarray(rows, dtype=np.int64)
    cols = np.asarray(cols, dtype=np.int64)
    weights = np.asarray(weights, dtype=float)

    for t_idx in range(iterations):
        # Repulsion (adaptive) - base global void pressure
        if num_nodes <= 800:
            delta = pos[:, np.newaxis, :] - pos[np.newaxis, :, :]
            distance = np.linalg.norm(delta, axis=-1) + 1e-9
            repulsive_force = (1.0 / distance) * (1.0 - (1.0 / distance))
            repulsion = np.sum(delta * (repulsive_force / distance)[..., np.newaxis], axis=1)
        else:
            # Subsample for approximate repulsion, re-scale to keep magnitude consistent
            P = min(400, num_nodes)
            idx = np.random.choice(num_nodes, P, replace=False)
            delta_s = pos[:, np.newaxis, :] - pos[idx][np.newaxis, :, :]
            distance_s = np.linalg.norm(delta_s, axis=-1) + 1e-9
            repulsive_force_s = (1.0 / distance_s) * (1.0 - (1.0 / distance_s))
            repulsion = np.sum(delta_s * (repulsive_force_s / distance_s)[..., np.newaxis], axis=1) * (num_nodes / P)

        # Attraction/Repulsion along existing edges (i -> j), governed by Void Dynamics
        attraction = np.zeros_like(pos)
        vec_ij = pos[cols] - pos[rows]  # direction from i to j
        signs = np.sign(weights).astype(float)  # excitatory(+)/inhibitory(-)

        # Time-dynamic void equations
        try:
            gdsp = delta_gdsp(weights, t_idx, use_time_dynamics=True)  # structural pull/closure
        except Exception:
            gdsp = np.array([delta_gdsp(w, t_idx, use_time_dynamics=True) for w in weights], dtype=float)
        try:
            re_vgsp = delta_re_vgsp(np.abs(weights), t_idx, use_time_dynamics=True)  # resonance modulation
        except Exception:
            re_vgsp = np.array([delta_re_vgsp(abs(w), t_idx, use_time_dynamics=True) for w in weights], dtype=float)

        # Normalize absolute synaptic strength and combine with void signals
        abs_w = np.abs(weights).astype(float)
        if abs_w.size and abs_w.max() > 0:
            abs_w = abs_w / (abs_w.max() + 1e-12)

        # Effective magnitude: strong edges + dynamic modulation
        fmag = abs_w * (1.0 + np.abs(gdsp)) * (1.0 + 0.5 * np.abs(re_vgsp))

        # Direction: excitatory pulls together (+), inhibitory pushes apart (-)
        vec_dir = vec_ij * signs[:, np.newaxis]
        forces = vec_dir * fmag[:, np.newaxis]
        np.add.at(attraction, rows, forces)

        # Global repulsion modulation from RE-VGSP (prevents collapse, encourages differentiation)
        repulsion *= (1.0 + 0.1 * float(np.mean(np.abs(re_vgsp)))) if re_vgsp.size else 1.0

        # Update positions with clipping + annealed step size
        update = repulsion + attraction
        norms = np.linalg.norm(update, axis=1) + 1e-12
        cap = np.percentile(norms, 95)
        scale = np.minimum(1.0, cap / norms)
        step = 0.03 * (0.95 ** t_idx)  # anneal for stability
        pos += step * (update * scale[:, np.newaxis])

    # Normalize positions to [0, 1]^dim
    pos -= pos.min(axis=0)
    span = pos.max(axis=0) - pos.min(axis=0) + 1e-9
    pos /= span

    return {i: pos[i] for i in range(num_nodes)}

def void_traverse_graph(W, iterations=100):
    """
    Traverse the entire connectome via the Void Equations and accumulate per-node potentials.
    Returns a dict with:
      - 'node_potential': ndarray (N,) accumulated resonance/closure magnitude per node
      - 'edge_flux': ndarray (M,) final per-edge flux (|delta_re| + |delta_gd|)*|w|
      - 'rows','cols': edge index mapping for 'edge_flux'
    This can be consumed by the SelfImprovementEngine to guide learning.
    """
    import numpy as _np
    import scipy.sparse as _sp

    N = int(W.shape[0])
    if _sp.issparse(W):
        rows, cols = W.nonzero()
        weights = _np.asarray(W.data, dtype=float)
    else:
        Wd = _np.asarray(W)
        rows, cols = _np.where(Wd != 0)
        weights = _np.asarray(Wd[rows, cols], dtype=float)

    node_potential = _np.zeros(N, dtype=float)
    edge_flux = _np.zeros_like(weights, dtype=float)

    for t in range(int(iterations)):
        try:
            re = delta_re_vgsp(_np.abs(weights), t, use_time_dynamics=True)
        except Exception:
            re = _np.array([delta_re_vgsp(abs(w), t, use_time_dynamics=True) for w in weights], dtype=float)
        try:
            gd = delta_gdsp(weights, t, use_time_dynamics=True)
        except Exception:
            gd = _np.array([delta_gdsp(w, t, use_time_dynamics=True) for w in weights], dtype=float)

        flux = (_np.abs(re) + _np.abs(gd)) * _np.maximum(1e-12, _np.abs(weights))
        edge_flux = flux  # keep last (or could accumulate with +=)

        # Deposit flux to target nodes (incoming) and a small portion to source
        _np.add.at(node_potential, cols, flux)
        _np.add.at(node_potential, rows, 0.25 * flux)

    return {
        "node_potential": node_potential,
        "edge_flux": edge_flux,
        "rows": rows,
        "cols": cols,
    }

def plot_network_graph(W, t, title: str, save_path: str, pos: dict, threshold=0.0, node_strength_mode: str = "inout"):
    """
    Creates a 2D visual representation of the FUM's connectome using a pre-calculated layout.
    Renders all connections above a minimal threshold, with transparency based on weight.
    """
    if scipy.sparse.issparse(W):
        W_dense = W.toarray()
    else:
        W_dense = W

    if hasattr(W_dense, 'cpu'):
        W_dense = W_dense.cpu().numpy()
        
    num_nodes = W_dense.shape[0]
    if not pos:
        pos = {i: np.random.rand(2) for i in range(num_nodes)}

    plt.figure(figsize=(12, 12))
    ax = plt.gca()
    
    node_coords = np.array(list(pos.values()))
    # --- Node coloring by synaptic strength (dark blue = strong, light blue = weak) ---
    absW = np.abs(W_dense)
    in_strength = absW.sum(axis=0)
    out_strength = absW.sum(axis=1)
    mode = str(node_strength_mode).lower()
    if mode == "in":
        strengths = in_strength
    elif mode == "out":
        strengths = out_strength
    else:
        strengths = in_strength + out_strength  # default: total in+out strength
    strengths = np.asarray(strengths, dtype=float)
    cmap = plt.get_cmap('Blues')  # light->dark; high (strong) appears darker
    vmin = float(np.min(strengths))
    vmax = float(np.max(strengths)) if np.max(strengths) > vmin else vmin + 1e-9
    norm = plt.Normalize(vmin=vmin, vmax=vmax)
    node_colors = cmap(norm(strengths))
    ax.scatter(node_coords[:, 0], node_coords[:, 1], s=150, color=node_colors, alpha=0.9, zorder=2, edgecolors='white', linewidth=0.5)

    # Correctly handle sparse or dense matrix for edges
    if scipy.sparse.issparse(W):
        rows, cols = W.nonzero()
        weights = W.data
    else:
        rows, cols = np.where(W_dense > threshold)
        weights = W_dense[rows, cols]
        
    valid_weights = weights[weights > 0]
    if len(valid_weights) > 0:
        edge_norm = plt.Normalize(vmin=valid_weights.min(), vmax=valid_weights.max())
    else:
        edge_norm = plt.Normalize(vmin=0, vmax=1) # Fallback

    for i, j, weight in zip(rows, cols, weights):
        if weight <= threshold:
             continue
        edge_color = cmap(edge_norm(weight))
        alpha = max(0.1, min(1.0, weight*2))
        linewidth = max(0.5, weight*3)
        ax.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], color=edge_color, alpha=alpha, linewidth=linewidth, zorder=1)
    
    plt.title(title, fontsize=20, color='white')
    plt.axis('off')
    plt.tight_layout()
    fig = plt.gcf()
    fig.set_facecolor('black')
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='black')
    plt.close()
    print(f"Saved network graph to '{save_path}'")

def plot_spike_raster(spike_times: list, title: str, save_path: str):
    """
    Creates and saves a spike raster plot.
    """
    plt.figure(figsize=(12, 8), facecolor='black')
    ax = plt.gca()
    ax.set_facecolor('black')
    
    plt.eventplot(spike_times, colors='cyan', linelengths=0.75)
    
    plt.title(title, fontsize=20, color='white')
    plt.xlabel("Time (Global Steps)", fontsize=14, color='white')
    plt.ylabel("Computational Unit (CU) ID", fontsize=14, color='white')
    
    ax.spines['bottom'].set_color('white')
    ax.spines['top'].set_color('white') 
    ax.spines['right'].set_color('white')
    ax.spines['left'].set_color('white')
    ax.tick_params(axis='x', colors='white')
    ax.tick_params(axis='y', colors='white')
    
    plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray', alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, facecolor='black')
    plt.close()
    print(f"Saved spike raster plot to '{save_path}'")

def create_maze_animation(maze_layout: np.ndarray, goal_pos: tuple, path: list, title: str, save_path: str):
    """
    Creates an animated GIF of the agent's path through a maze.
    This function is for specific experiments and is not part of the core FUM.
    """
    if not path:
        print("Warning: Cannot create animation for an empty path.")
        return
        
    frames = []
    for i in tqdm(range(len(path)), desc="Generating Animation Frames"):
        fig, ax = plt.subplots(figsize=(8, 6), facecolor='black')
        ax.set_facecolor('black')
        
        ax.imshow(maze_layout, cmap='Greys', interpolation='nearest')
        
        goal_patch = plt.Rectangle((goal_pos[1] - 0.5, goal_pos[0] - 0.5), 1, 1, color='gold', alpha=0.9)
        ax.add_patch(goal_patch)

        if i > 0:
            path_arr = np.array(path[:i+1])
            ax.plot(path_arr[:, 1], path_arr[:, 0], color='cyan', linewidth=2.5, alpha=0.7)

        agent_patch = plt.Circle((path[i][1], path[i][0]), radius=0.3, color='red')
        ax.add_patch(agent_patch)

        ax.set_title(f"{title}\nStep: {i+1}/{len(path)}", fontsize=16, color='white')
        ax.set_xticks([])
        ax.set_yticks([])
        
        fig.canvas.draw()
        frame = np.array(fig.canvas.renderer.buffer_rgba())
        frames.append(frame)
        plt.close(fig)

    imageio.mimsave(save_path, frames, fps=max(10, len(path)//10))
    print(f"Saved animation to '{save_path}'")

def plot_3d_network_graph(
    W,
    t,
    title: str,
    save_path: str,
    pos: dict,
    threshold=0.0,
    edge_bins: int = 4,
    edge_style: str = "gray",          # "gray" for classic brain look, "blues" for weight-binned blues
    save_png: bool = True,             # also write a static PNG via kaleido
    camera: dict | None = None,        # optional plotly camera dict
    node_size: int = 7,
    node_opacity: float = 0.9
):
    """
    Creates an interactive 3D visual representation of the FUM's connectome using Plotly.
    Upgrades:
    - Consolidates edges into a small number of traces (binned by weight) for faster rendering.
    - Optional classic brain styling: light-blue nodes with soft gray edges.
    - Saves BOTH an interactive HTML and a high-res PNG (requires 'kaleido').
    """
    # Ensure a dense array for node metrics only; edges are drawn from original W
    if scipy.sparse.issparse(W):
        W_dense = W.toarray()
    else:
        W_dense = W

    if hasattr(W_dense, 'cpu'):
        W_dense = W_dense.cpu().numpy()

    num_nodes = W_dense.shape[0]
    if not pos:
        pos = {i: np.random.rand(3) for i in range(num_nodes)}

    # --- Nodes (colored by incoming strength) ---
    node_coords = np.array(list(pos.values()))
    # Node color encodes synaptic strength (dark=strong, light=weak)
    absW = np.abs(W_dense)
    in_strength = absW.sum(axis=0)
    out_strength = absW.sum(axis=1)
    strengths = (in_strength + out_strength).astype(float)
    vmin = float(np.min(strengths))
    vmax = float(np.max(strengths)) if np.max(strengths) > vmin else vmin + 1e-9
    node_trace = go.Scatter3d(
        x=node_coords[:, 0], y=node_coords[:, 1], z=node_coords[:, 2],
        mode='markers',
        marker=dict(
            size=int(node_size),
            color=strengths,
            colorscale='Blues',
            cmin=vmin,
            cmax=vmax,
            showscale=True,
            colorbar=dict(title='Synapse Strength'),
            opacity=float(node_opacity),
        ),
        hoverinfo='text',
        text=[f'Neuron {i}<br>Syn Strength: {strengths[i]:.2f}' for i in range(num_nodes)],
    )

    # --- Edges: build in a handful of weight bins for speed ---
    if scipy.sparse.issparse(W):
        rows, cols = W.nonzero()
        weights = np.asarray(W.data, dtype=float)
    else:
        rows, cols = np.where(W_dense > threshold)
        weights = np.asarray(W_dense[rows, cols], dtype=float)

    # Filter by threshold
    mask = weights > float(threshold)
    rows, cols, weights = rows[mask], cols[mask], weights[mask]

    edge_traces = []
    if len(weights) > 0:
        # Compute bin edges (quantiles); fall back to single-bin if all weights equal
        try:
            qs = np.linspace(0.0, 1.0, num=max(2, edge_bins + 1))
            bin_edges = np.quantile(weights, qs)
            # Ensure strictly increasing (handle degenerate distributions)
            bin_edges = np.unique(bin_edges)
        except Exception:
            bin_edges = np.array([weights.min(), weights.max()])

        # If degenerate, just one bin
        if bin_edges.size <= 1:
            bin_edges = np.array([float(weights.min()), float(weights.max() + 1e-9)])

        # Edge color palette
        if str(edge_style).lower() == "gray":
            # Classic brain style: soft gray edges with increasing opacity
            palette = [
                'rgba(170, 170, 170, 0.22)',
                'rgba(170, 170, 170, 0.30)',
                'rgba(170, 170, 170, 0.40)',
                'rgba(170, 170, 170, 0.52)',
                'rgba(170, 170, 170, 0.65)',
            ]
        else:
            # Pastel blues (light to brighter)
            palette = [
                'rgba(135, 206, 235, 0.35)',  # light skyblue
                'rgba(100, 149, 237, 0.5)',   # cornflower
                'rgba(65, 105, 225, 0.65)',   # royalblue
                'rgba(30, 144, 255, 0.8)',    # dodgerblue
                'rgba(0, 191, 255, 0.9)',     # deepskyblue
            ]

        # Build one trace per bin with None-separated line segments
        for b in range(bin_edges.size - 1):
            lo, hi = bin_edges[b], bin_edges[b + 1]
            # Include the upper edge for the last bin
            in_bin = (weights >= lo) & (weights <= hi if b == bin_edges.size - 2 else weights < hi)
            if not np.any(in_bin):
                continue
            r_b = rows[in_bin]
            c_b = cols[in_bin]

            edge_x, edge_y, edge_z = [], [], []
            for i, j in zip(r_b, c_b):
                edge_x.extend([pos[i][0], pos[j][0], None])
                edge_y.extend([pos[i][1], pos[j][1], None])
                edge_z.extend([pos[i][2], pos[j][2], None])

            color = palette[min(b, len(palette) - 1)]
            width = max(1.0, 1.0 + 0.6 * (b + 1))
            edge_traces.append(
                go.Scatter3d(
                    x=edge_x, y=edge_y, z=edge_z,
                    mode='lines',
                    line=dict(color=color, width=width),
                    hoverinfo='none',
                    showlegend=False,
                )
            )

    # --- Compose figure ---
    fig = go.Figure(data=edge_traces + [node_trace])
    fig.update_layout(
        title=dict(text=title, font=dict(size=20, color='white')),
        showlegend=False,
        scene=dict(
            xaxis=dict(showbackground=False, showticklabels=False, title=''),
            yaxis=dict(showbackground=False, showticklabels=False, title=''),
            zaxis=dict(showbackground=False, showticklabels=False, title=''),
            bgcolor='black',
        ),
        margin=dict(l=0, r=0, b=0, t=40),
        paper_bgcolor='black',
        font_color='white',
    )
    # Set a pleasant default camera if none provided
    if camera is None:
        camera = dict(eye=dict(x=1.6, y=1.6, z=0.9))
    fig.update_layout(scene_camera=camera)

    # Save as interactive HTML
    html_save_path = save_path.replace('.png', '.html')
    fig.write_html(html_save_path)
    print(f"Saved interactive 3D network graph to '{html_save_path}'")

    # Optionally save a high-res PNG using kaleido
    if save_png and save_path.lower().endswith(".png"):
        try:
            # Width/height chosen for publication-quality output
            fig.write_image(save_path, format="png", width=1920, height=1200, scale=2)
            print(f"Saved static 3D network graph PNG to '{save_path}'")
        except Exception as e:
            print(f"[WARN] Could not save PNG to '{save_path}'. Install 'kaleido' to enable static image export. Error: {e}")

def export_connectome_json(W, pos, path, threshold=0.0):
    """
    Export adjacency and node positions to a JSON schema for front-end visualization.

    - Handles scipy.sparse and dense matrices
    - Accepts pos as a dict {node_id: [x,y,(z)?]} or ndarray shape (N, D)
    - Applies a weight threshold (exclusive) to filter edges

    JSON schema (FUM.connectome.v1):
    {
      "schema": "FUM.connectome.v1",
      "directed": true,
      "num_nodes": N,
      "num_edges": M,
      "threshold": float,
      "nodes": [{ "id": int, "pos": [x,y,(z)?] }, ...],
      "edges": [{ "source": int, "target": int, "weight": float }, ...]
    }
    """
    import json
    import numpy as _np
    import scipy.sparse as _sp

    num_nodes = int(W.shape[0])

    # Build nodes with positions
    nodes = []
    if isinstance(pos, dict):
        for i in range(num_nodes):
            coords = pos.get(i)
            if coords is None:
                coords = _np.random.rand(2)
            coords = _np.asarray(coords).tolist()
            nodes.append({"id": i, "pos": coords})
    else:
        pos_arr = _np.asarray(pos)
        for i in range(num_nodes):
            coords = pos_arr[i].tolist()
            nodes.append({"id": i, "pos": coords})

    # Extract edges
    if _sp.issparse(W):
        rows, cols = W.nonzero()
        weights = W.data
    else:
        W_dense = _np.asarray(W)
        rows, cols = _np.where(W_dense > threshold)
        weights = W_dense[rows, cols]

    edges = []
    for i, j, w in zip(rows, cols, weights):
        if w <= threshold:
            continue
        edges.append({"source": int(i), "target": int(j), "weight": float(w)})

    payload = {
        "schema": "FUM.connectome.v1",
        "directed": True,
        "num_nodes": num_nodes,
        "num_edges": len(edges),
        "threshold": float(threshold),
        "nodes": nodes,
        "edges": edges,
    }

    with open(path, "w") as f:
        json.dump(payload, f)

    print(f"Saved connectome JSON to '{path}'")


def export_connectome_npz(W, path):
    """
    Persist the connectome to NPZ (CSR) for exact reconstruction later.
    """
    import scipy.sparse as _sp
    from scipy.sparse import csr_matrix as _csr

    W_csr = W if _sp.issparse(W) else _csr(W)
    _sp.save_npz(path, W_csr)
    print(f"Saved connectome NPZ to '{path}'")]]></content>
    </file>
    <file>
      <path>frontend/services/process_manager.py</path>
      <content><![CDATA[import os
import sys
import time
import threading
import subprocess
from typing import Any, Dict, List, Tuple


class ProcessManager:
    """
    Extracted from the dashboard without behavior changes.
    Launches python -m fum_rt.run_nexus with a profile, manages stdin feeding, and exposes launcher log.
    """

    def __init__(self, runs_root: str):
        # Persist UI-configured runs root and normalize to absolute path
        self.runs_root = runs_root
        self.runs_root_abs = os.path.abspath(runs_root)
        # Ensure runs_root exists and store repo root for module resolution
        try:
            os.makedirs(self.runs_root_abs, exist_ok=True)
        except Exception:
            pass
        # Resolve repository root (directory that contains the 'fum_rt' package)
        # so 'python -m fum_rt.run_nexus' works even when runs_root is outside repo.
        # services -> frontend -> fum_rt -> REPO_ROOT
        self.repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        # Launch logging (so you can inspect startup failures)
        self.launch_log = os.path.join(self.runs_root_abs, "launcher_last.log")
        self._logf = None
        self.last_cmd: List[str] | None = None

        self.proc: subprocess.Popen | None = None
        self.proc_lock = threading.Lock()
        self.current_run_dir: str | None = None
        self._stdin_lock = threading.Lock()
        self._feed_thread: threading.Thread | None = None
        self._feed_stop = threading.Event()
        # Instrumentation for run-dir detection
        self.last_detect_ms: float = 0.0
        self.last_detect_method: str = "init"
        self.last_cwd: str | None = None

    def set_runs_root(self, root: str):
        """Update runs root to match UI selection and rotate launch log path."""
        try:
            self.runs_root = root
            self.runs_root_abs = os.path.abspath(root)
            os.makedirs(self.runs_root_abs, exist_ok=True)
        except Exception:
            pass
        try:
            self.launch_log = os.path.join(self.runs_root_abs, "launcher_last.log")
        except Exception:
            pass

    def _build_cmd(self, profile: Dict[str, Any]) -> List[str]:
        py = sys.executable or "python"
        cmd = [py, "-m", "fum_rt.run_nexus"]

        def add(flag: str, val: Any, cast=str):
            if val is None:
                return
            cmd.extend([flag, cast(val)])

        # basic
        add("--neurons", profile.get("neurons"), str)
        add("--k", profile.get("k"), str)
        add("--hz", profile.get("hz"), str)
        add("--domain", profile.get("domain"), str)
        if profile.get("use_time_dynamics", True):
            cmd.append("--use-time-dynamics")
        else:
            cmd.append("--no-time-dynamics")
        # sparse / structure
        if profile.get("sparse_mode", False):
            cmd.append("--sparse-mode")
        add("--threshold", profile.get("threshold"), str)
        add("--lambda-omega", profile.get("lambda_omega"), str)
        add("--candidates", profile.get("candidates"), str)
        add("--walkers", profile.get("walkers"), str)
        add("--hops", profile.get("hops"), str)
        add("--status-interval", profile.get("status_interval"), str)
        add("--bundle-size", profile.get("bundle_size"), str)
        add("--prune-factor", profile.get("prune_factor"), str)
        # stim
        add("--stim-group-size", profile.get("stim_group_size"), str)
        add("--stim-amp", profile.get("stim_amp"), str)
        add("--stim-decay", profile.get("stim_decay"), str)
        add("--stim-max-symbols", profile.get("stim_max_symbols"), str)
        # speak
        if profile.get("speak_auto", True):
            cmd.append("--speak-auto")
        else:
            cmd.append("--no-speak-auto")
        add("--speak-z", profile.get("speak_z"), str)
        add("--speak-hysteresis", profile.get("speak_hysteresis"), str)
        add("--speak-cooldown-ticks", profile.get("speak_cooldown_ticks"), str)
        add("--speak-valence-thresh", profile.get("speak_valence_thresh"), str)
        add("--b1-half-life-ticks", profile.get("b1_half_life_ticks"), str)
        # viz/log
        add("--viz-every", profile.get("viz_every"), str)
        add("--log-every", profile.get("log_every"), str)
        # checkpoints
        add("--checkpoint-every", profile.get("checkpoint_every"), str)
        add("--checkpoint-keep", profile.get("checkpoint_keep"), str)
        add("--duration", profile.get("duration"), str)
        # explicit run dir (resume)
        add("--run-dir", profile.get("run_dir"), str)
        # optional: load existing engram (folder or file path; runtime normalizes)
        if profile.get("load_engram"):
            cmd.extend(["--load-engram", str(profile["load_engram"])])
        return cmd

    def start(self, profile: Dict[str, Any]) -> Tuple[bool, str]:
        with self.proc_lock:
            if self.proc and self.proc.poll() is None:
                return False, "Already running"

            # Normalize runs root and ensure it exists
            rr = getattr(self, "runs_root_abs", None) or os.path.abspath(self.runs_root)
            try:
                os.makedirs(rr, exist_ok=True)
            except Exception:
                pass
            before = set(os.listdir(rr)) if os.path.exists(rr) else set()
            detection_t0 = time.time()

            # Ensure explicit run_dir honors UI-selected runs_root on fresh starts.
            # If the profile does not specify run_dir (i.e., Start New Run without adoption),
            # synthesize runs_root/<timestamp> to avoid defaulting to 'runs/<ts>' regardless of UI choice.
            if not profile.get("run_dir"):
                try:
                    ts = time.strftime('%Y%m%d_%H%M%S')
                    profile["run_dir"] = os.path.join(rr, ts)
                except Exception:
                    pass

            cmd = self._build_cmd(profile)
            self.last_cmd = cmd[:]

            # Prepare environment so 'python -m fum_rt.run_nexus' resolves even if GUI was launched elsewhere
            env = os.environ.copy()
            try:
                repo_root = self.repo_root
            except Exception:
                repo_root = os.path.dirname(os.path.abspath(__file__))
            env["PYTHONPATH"] = f"{repo_root}:{env.get('PYTHONPATH','')}"
            env.setdefault("PYTHONUNBUFFERED", "1")

            # open launch log so we can surface failures
            try:
                if self._logf:
                    try:
                        self._logf.close()
                    except Exception:
                        pass
                self._logf = open(self.launch_log, "wb")
            except Exception:
                self._logf = None

            # Run from parent dir of runs_root so runtime writes to rr = <parent>/runs
            cwd_dir = os.path.dirname(rr)
            try:
                self.proc = subprocess.Popen(
                    cmd,
                    stdin=subprocess.PIPE,
                    stdout=self._logf or subprocess.DEVNULL,
                    stderr=self._logf or subprocess.DEVNULL,
                    cwd=cwd_dir,
                    env=env
                )
            except Exception as e:
                self.proc = None
                return False, f"Failed to start: {e}"

            # If the process died immediately, surface the log
            time.sleep(0.5)
            if self.proc and self.proc.poll() is not None:
                try:
                    if self._logf:
                        self._logf.flush()
                        self._logf.close()
                        self._logf = None
                    with open(self.launch_log, "rb") as fh:
                        tail = fh.read()[-4096:]
                    return False, f"Process exited during start.\nCommand: {' '.join(cmd)}\nLog({self.launch_log}):\n{tail.decode('utf-8','ignore')}"
                except Exception:
                    return False, f"Process exited during start.\nCommand: {' '.join(cmd)}\nNo launch log available."

            # Resolve run dir with instrumentation
            run_dir = None
            detect_method = None
            specified = profile.get("run_dir")
            if specified:
                run_dir = str(specified)
                detect_method = "explicit"
            else:
                # Detect new run dir (robust loop)
                for _ in range(20):  # ~5s total
                    try:
                        after = set(os.listdir(rr)) if os.path.exists(rr) else set()
                        new_dirs = list(after - before)
                        if new_dirs:
                            run_dir = max(
                                (os.path.join(rr, d) for d in new_dirs),
                                key=lambda p: os.path.getmtime(p)
                            )
                            detect_method = "create_watch"
                            break
                    except Exception:
                        pass
                    time.sleep(0.25)

                if not run_dir:
                    # fallback: latest by mtime under runs_root
                    runs = sorted(
                        [os.path.join(rr, d) for d in os.listdir(rr) if os.path.isdir(os.path.join(rr, d))],
                        key=lambda p: os.path.getmtime(p),
                        reverse=True
                    ) if os.path.exists(rr) else []
                    run_dir = runs[0] if runs else None
                    detect_method = "fallback_latest" if run_dir else "none"

            # Record detection diagnostics and surface to launcher log
            try:
                self.last_detect_ms = float((time.time() - detection_t0) * 1000.0)
            except Exception:
                self.last_detect_ms = 0.0
            self.last_detect_method = detect_method or "unknown"
            try:
                self.last_cwd = cwd_dir
            except Exception:
                self.last_cwd = None
            try:
                if self._logf:
                    line = f"[UI] run_dir_detected method={self.last_detect_method} ms={int(self.last_detect_ms)} rd={run_dir or ''} rr={rr} cwd={cwd_dir}\n"
                    self._logf.write(line.encode("utf-8", "ignore"))
                    self._logf.flush()
            except Exception:
                pass

            self.current_run_dir = run_dir
            return True, run_dir or ""

    def stop(self) -> Tuple[bool, str]:
        with self.proc_lock:
            if not self.proc:
                return False, "Not running"
            try:
                self.proc.terminate()
                try:
                    self.proc.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    self.proc.kill()
            except Exception as e:
                return False, f"Stop error: {e}"
            finally:
                self.proc = None
                self.current_run_dir = None
                try:
                    if self._logf:
                        self._logf.close()
                        self._logf = None
                except Exception:
                    pass
            return True, "Stopped"

    def send_line(self, text: str) -> bool:
        with self.proc_lock:
            if not self.proc or self.proc.stdin is None:
                return False
            try:
                with self._stdin_lock:
                    self.proc.stdin.write((text.rstrip("\n") + "\n").encode("utf-8"))
                    self.proc.stdin.flush()
                return True
            except Exception:
                return False

    def feed_file(self, path: str, rate_lps: float = 20.0):
        if not os.path.exists(path):
            return False
        if not self.proc or self.proc.stdin is None:
            return False
        if self._feed_thread and self._feed_thread.is_alive():
            return False
        self._feed_stop.clear()

        def _runner():
            try:
                with open(path, "r", encoding="utf-8") as fh:
                    for line in fh:
                        if self._feed_stop.is_set():
                            break
                        ok = self.send_line(line)
                        if not ok:
                            break
                        time.sleep(1.0 / max(1e-3, rate_lps))
            except Exception:
                pass

        self._feed_thread = threading.Thread(target=_runner, daemon=True)
        self._feed_thread.start()
        return True

    def stop_feed(self):
        self._feed_stop.set()]]></content>
    </file>
    <file>
      <path>frontend/services/status_client.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Lightweight HTTP status client (frontend-only; zero file IO).

- Purpose: fetch the latest runtime status snapshot from the in-process HTTP endpoint
  to drive live dashboard charts without scanning or tailing JSONL files.
- Endpoint (served by runtime.helpers.status_http.maybe_start_status_http):
  GET /status  -> 200 JSON (latest nx._emit_last_metrics) or 204 when not ready
  GET /health  -> 200 {"ok": true}

Environment
- STATUS_HTTP_URL (default "http://127.0.0.1:8787/status")
- STATUS_HTTP_TIMEOUT_MS (default 200)

Contracts
- Returns a Python dict on HTTP 200 with valid JSON; None on 204 or any error.
- No retries, no background threads; caller controls cadence (e.g., via dcc.Interval).
"""

import os
import json
from typing import Any, Optional
import urllib.request
import urllib.error


def get_status_snapshot(url: Optional[str] = None, timeout_s: Optional[float] = None) -> Optional[dict[str, Any]]:
    """
    Fetch the latest status snapshot.

    Args:
        url: Optional override for status URL (e.g., "http://127.0.0.1:8787/status/snapshot").
             If None, use env STATUS_HTTP_URL or default "http://127.0.0.1:8787/status".
        timeout_s: Optional override for request timeout in seconds. If None, use env
                   STATUS_HTTP_TIMEOUT_MS (default 0.2s).

    Returns:
        dict[str, Any] on success (HTTP 200 JSON)
        None on 204 or any error/parse failure
    """
    # URL
    if url is None:
        try:
            url = os.getenv("STATUS_HTTP_URL", "http://127.0.0.1:8787/status").strip() or "http://127.0.0.1:8787/status"
        except Exception:
            url = "http://127.0.0.1:8787/status"
    # Prefer /status/snapshot if caller passed base /status
    try:
        if url.endswith("/status"):
            url = url + "/snapshot"
    except Exception:
        pass

    # Timeout
    if timeout_s is None:
        try:
            tms = int(os.getenv("STATUS_HTTP_TIMEOUT_MS", "200"))
        except Exception:
            tms = 200
        timeout_s = max(0.05, float(tms) / 1000.0)
    else:
        try:
            timeout_s = max(0.05, float(timeout_s))
        except Exception:
            timeout_s = 0.2

    try:
        req = urllib.request.Request(url, headers={"Accept": "application/json"})
        with urllib.request.urlopen(req, timeout=timeout_s) as resp:  # nosec B310 (local loopback by default)
            code = getattr(resp, "status", 200)
            if code == 204:
                return None
            data = resp.read()
            if not data:
                return None
            try:
                return json.loads(data.decode("utf-8", "ignore"))
            except Exception:
                return None
    except (urllib.error.URLError, urllib.error.HTTPError, TimeoutError, OSError):
        return None
    except Exception:
        return None


__all__ = ["get_status_snapshot"]]]></content>
    </file>
    <file>
      <path>frontend/styles/README.md</path>
      <content><![CDATA[# FUM Frontend Styles (Modular)

This package provides a modular CSS system for the Dash UI. Instead of a single monolithic CSS string, styles are layered and composed at runtime in a stable order.

Layering contract:
1) base: variables, resets, typography, form controls, scrollbars (no layout)
2) layout: grids, cards, responsive rules, utilities (no component overrides)
3) components: dcc.Dropdown/react-select, rc-slider, and component-specific rules

Exported API:
- from fum_rt.frontend.styles import get_global_css
- from fum_rt.frontend.styles import get_base_css, get_layout_css, get_components_css

## Usage

Inject the CSS into Dash index_string (already wired in app.py):

```python
from fum_rt.frontend.styles import get_global_css

GLOBAL_CSS = get_global_css()
app.index_string = app.index_string.replace("</head>", f"<style>{GLOBAL_CSS}</style></head>")
```

If legacy imports are still present elsewhere:
```python
# Backward compatible shim
from fum_rt.frontend.styles.theme import get_global_css
```
The shim forwards to the modular aggregator.

## Why modular?

- Separation of concerns:
  - base is safe, foundational, and broadly applicable
  - layout organizes structure and responsiveness
  - components encapsulate third‑party widget tweaks (react-select, rc-slider)
- Lower coupling: view-specific changes can be added without touching core layers
- Fewer regressions: targeted updates reduce incidental style bleed

## Files

- base.py
  - get_base_css(): CSS variables, resets, typography, form controls, scrollbars
- layout.py
  - get_layout_css(): grid/card/row utilities, responsive rules, minor layout helpers (e.g., .tight)
- components.py
  - get_components_css(): dropdown (react-select) and rc-slider styling, layering fixes (z-index), clipping fixes

- __init__.py
  - get_global_css(): concatenates base → layout → components in a fail-soft manner

- theme.py (deprecated shim)
  - Keeps old import path working by delegating to styles.get_global_css

## Adding view-specific styles

For a new view or composite component, there are two options:

1) Extend components.py (small and generic additions)
   - Ideal for adding minor UI polish or tweaking existing component rules.

2) Create a new module and opt-in at composition time
   - Create a new file (e.g., view_run_config.py) next to existing layers with:
     ```python
     def get_run_config_view_css() -> str:
         return '''
         /* styles specific to the run-config view */
         .run-config-advanced {
             display: grid;
             grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
             gap: 12px;
         }
         '''
     ```
   - Then import and append in __init__.py as a new layer after components:
     ```python
     from .view_run_config import get_run_config_view_css
     ...
     layers = [get_base_css, get_layout_css, get_components_css, get_run_config_view_css]
     ```
   - Keep the rule of thumb: more specific layers later in the list.

Note: If you expect multiple view-specific files, consider a subpackage (e.g., styles/views/) with a local __init__.py to assemble view layers, then include that combined view CSS at the end of the main list.

## Conventions and constraints

- No global scans, no schedulers: All CSS is static strings concatenated at build/start time.
- Respect existing className contracts in layout (e.g., className="grid").
- Keep base.css free of layout or component specifics to minimize interactions.
- Layout should not override third‑party components; that belongs in components.py.
- Use z-index sparingly; menu portals (react-select) need to sit above cards. This is handled in components.py.

## Responsive decisions (summary)

- The grid defaults to minmax(300px, 360px) 1fr for two-column layouts, with breakpoints:
  - ≤1200px: left column narrows slightly
  - ≤900px: stacks to a single column
- Cards allow overflow: visible for dropdowns and menus (prevents clipping).
- Inputs and long text are clamped with max-width: 100% to prevent layout shifts.

## Duplicate rule sanity

- By design, base/layout/components do not duplicate selectors with conflicting declarations.
- If you add view-specific rules, keep selector specificity scoped (e.g., prefix with a container class for the view) to avoid broad overrides.
- In case of an intentional override, place it later in the layer order.

## Example: switching from legacy

Before:
```python
from fum_rt.frontend.styles.theme import get_global_css
```

After (preferred):
```python
from fum_rt.frontend.styles import get_global_css
```

Legacy imports continue to function via the shim.

## Governance

- Any proposal to add a new layer must justify why existing layers cannot host the rules.
- Maintain the base → layout → components → views ordering to preserve determinism.
- Target changes to the minimal layer that owns the concern.

Author: Justin K. Lietz]]></content>
    </file>
    <file>
      <path>frontend/styles/__init__.py</path>
      <content><![CDATA["""
FUM Frontend Styles (modular)

Exports:
- get_global_css(): concatenates base → layout → components CSS in a safe order
- get_base_css, get_layout_css, get_components_css: individual layers (advanced usage)

Layering contract:
1) base: variables, resets, typography, form controls, scrollbars (no layout)
2) layout: grids, cards, responsive rules, utilities (no component overrides)
3) components: dcc.Dropdown/react-select, rc-slider, and component-specific rules

Author: Justin K. Lietz
"""

from __future__ import annotations

from typing import Callable, List

# Individual layers
try:
    from .base import get_base_css
except Exception:  # pragma: no cover
    def get_base_css() -> str:  # type: ignore[no-redef]
        return ""

try:
    from .layout import get_layout_css
except Exception:  # pragma: no cover
    def get_layout_css() -> str:  # type: ignore[no-redef]
        return ""

try:
    from .components import get_components_css
except Exception:  # pragma: no cover
    def get_components_css() -> str:  # type: ignore[no-redef]
        return ""


def get_global_css() -> str:
    """
    Return the full CSS string for injection into Dash index.
    Concatenates in stable order: base → layout → components.
    Robust to partial availability (missing modules return empty string).
    """
    layers: List[Callable[[], str]] = [get_base_css, get_layout_css, get_components_css]
    parts: List[str] = []
    for layer in layers:
        try:
            css = layer()
            if css and isinstance(css, str):
                parts.append(css)
        except Exception:
            # Fail-soft: do not crash the app if one layer errors
            continue
    return "\n".join(parts)


__all__ = [
    "get_global_css",
    "get_base_css",
    "get_layout_css",
    "get_components_css",
]]]></content>
    </file>
    <file>
      <path>frontend/styles/base.py</path>
      <content><![CDATA["""
Base CSS for FUM Live Dashboard.

Modularized: foundational variables, typography, resets, form controls, scrollbars.
Include this first; layer layout- and component-specific CSS on top.

Author: Justin K. Lietz
"""

from __future__ import annotations

def get_base_css() -> str:
    """
    Return core CSS variables and base elements without layout or component rules.
    Safe to include standalone; other style modules should be appended after this.
    """
    return """
    :root{
      --bg:#0b0f14; --panel:#10151c; --panel2:#0e141a; --text:#cfd7e3; --muted:#8699ac;
      --accent:#6aa0c2; --ok:#3a8f5c; --danger:#b3565c; --border:#1d2733; --grid:#233140;
      --plot:#0f141a; --paper:#10151c;
    }
    *{box-sizing:border-box}
    html,body{height:100%}
    html{scrollbar-gutter: stable both-edges}
    body{background:var(--bg);color:var(--text);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Ubuntu,Cantarell,Helvetica Neue,Arial,Noto Sans,sans-serif;line-height:1.35;margin:0}
    h1,h2,h3,h4{color:var(--text);font-weight:600;margin:0 0 8px 0}
    p{margin:0 0 8px 0}
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}

    label{font-size:12px;color:var(--muted);margin-bottom:4px;display:block}

    button{cursor:pointer;border-radius:8px;border:1px solid var(--border);padding:6px 10px;background:var(--panel2);color:var(--text)}
    button:hover{filter:brightness(1.05)}
    .btn-ok{background:var(--ok);color:#fff;border:none}
    .btn-danger{background:var(--danger);color:#fff;border:none}

    pre{background:#0a0e13;border:1px solid var(--border);border-radius:8px;padding:8px;white-space:pre-wrap;color:#dfe7f1}
    input[type="text"],input[type="number"],textarea,select{
        width:100%;background:var(--panel2);color:var(--text);border:1px solid var(--border);
        border-radius:8px;padding:6px 8px;outline:none;
    }
    input::placeholder,textarea::placeholder{color:var(--muted)}
    input:focus,textarea:focus,select:focus{border-color:var(--accent);box-shadow:0 0 0 3px rgba(106,160,194,0.15)}

    /* Scrollbars (WebKit) */
    *::-webkit-scrollbar{width:10px;height:10px}
    *::-webkit-scrollbar-track{background:var(--panel)}
    *::-webkit-scrollbar-thumb{
        background:#1d2733;
        border-radius:8px;
        border:1px solid var(--grid);
    }
    *::-webkit-scrollbar-thumb:hover{background:#273445}

    /* Scrollbars (Firefox) */
    *{scrollbar-color:#1d2733 var(--panel2);scrollbar-width:thin}
    """]]></content>
    </file>
    <file>
      <path>frontend/styles/components.py</path>
      <content><![CDATA["""
Component CSS for FUM Live Dashboard.

Modularized: dcc.Dropdown (react-select), rc-slider, and component-level utilities.
Include after styles.layout.get_layout_css().

Author: Justin K. Lietz
"""

from __future__ import annotations


def get_components_css() -> str:
    """
    Return component-specific rules for dropdowns, sliders, etc.
    """
    return """
    /* dcc.Dropdown (react-select) */
    .Select-control{
      background:var(--panel2)!important;
      border:1px solid var(--border)!important;
      color:var(--text)!important;
      border-radius:8px;
      min-height: 34px;
    }
    .Select--single>.Select-control .Select-value{
      color:var(--text)!important;
    }
    .Select-menu-outer{
      background:var(--panel2)!important;
      border:1px solid var(--border)!important;
      color:var(--text)!important;
      z-index: 9999 !important; /* ensure above neighbors */
    }
    .Select-option{
      background:var(--panel2)!important;
      color:var(--text)!important;
    }
    .Select-option.is-focused{background:#121a22!important}
    .Select-option.is-selected{background:#17222c!important}
    .VirtualizedSelectFocusedOption{background:#121a22!important}

    /* Prevent clipping of dropdown menus by container overflow */
    .dash-dropdown, .Select, .Select-menu-outer{
      overflow: visible !important;
    }

    /* Ensure focus rings render above adjacent inputs */
    .Select-control{ z-index: 2 }

    /* rc-slider */
    .rc-slider{padding:8px 0; z-index: 1}
    .rc-slider-rail{background:#0d1218}
    .rc-slider-track{background:var(--accent)}
    .rc-slider-dot{border-color:#233140;background:#10151c}
    .rc-slider-handle{border:1px solid var(--border);background:var(--panel2)}
    """]]></content>
    </file>
    <file>
      <path>frontend/styles/layout.py</path>
      <content><![CDATA["""
Layout CSS for FUM Live Dashboard.

Modularized: grid system, card container, responsive rules, utilities.
Include after styles.base.get_base_css().

Author: Justin K. Lietz
"""

from __future__ import annotations

def get_layout_css() -> str:
    """
    Return responsive layout rules (no component styling).
    """
    return """
    /* Layout hardening and responsive behavior */

    /* Constrain the app container to avoid full-width stretch on huge monitors */
    #react-entry-point, #_dash-app-content, #_dash-app-content > div{
      max-width: 1600px;
      margin: 0 auto;
      width: 100%;
    }

    /* Improve base grid: use a flexible left column and maintain gap consistency */
    .grid{
      display: grid;
      grid-template-columns: minmax(300px, 360px) 1fr;
      gap: 16px;
      align-items: start;
    }

    /* On mid-sized screens, ease the left column width to reduce crowding */
    @media (max-width: 1200px){
      .grid{
        grid-template-columns: minmax(260px, 320px) 1fr;
        gap: 14px;
      }
    }

    /* On narrow screens, stack to a single column to prevent overlap */
    @media (max-width: 900px){
      .grid{
        grid-template-columns: 1fr;
        gap: 12px;
      }
    }

    /* Cards: allow inner components to overflow (e.g., dropdown menus), add column flow */
    .card{
      position: relative;
      overflow: visible; /* prevent dropdown clipping */
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    /* Utility rows/columns for consistent spacing */
    .row{ display: flex; gap: 8px; flex-wrap: wrap }
    .row > div{ flex: 1; min-width: 140px }
    @media (max-width: 900px){
      .row{ gap: 6px }
      .row > div{ min-width: 120px }
    }

    /* Prevent horizontal scrolling from wide pre/code blocks or long text inputs */
    .card pre, .card code, .card textarea{
      max-width: 100%;
      overflow: auto;
    }

    /* Buttons spacing in cards */
    .card .btn-row{
      display: flex;
      gap: 8px;
      flex-wrap: wrap;
    }

    /* Guard against accidental absolute children overlap by default */
    .card > *{
      position: relative;
    }

    /* Optional helper: grid variant that auto-fits equal-width cards */
    .grid-auto{
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 16px;
      align-items: start;
    }

    .tight{ margin-top: 6px }

    /* Tweak focus ring thickness on small screens to reduce visual 'chunkiness' */
    @media (max-width: 900px){
      input:focus, textarea:focus, select:focus{
        box-shadow: 0 0 0 2px rgba(106,160,194,0.15);
      }
    }
    """]]></content>
    </file>
    <file>
      <path>frontend/styles/theme.py</path>
      <content><![CDATA["""
Deprecated shim for FUM Frontend Styles.

Old path (legacy):
  from fum_rt.frontend.styles.theme import get_global_css

New canonical path:
  from fum_rt.frontend.styles import get_global_css

This module delegates to the modular styles package to preserve backward compatibility.
"""

from __future__ import annotations

# Prefer aggregator in styles/__init__.py
try:
    from . import get_global_css as _get_global_css  # type: ignore[attr-defined]
except Exception:
    # Fail-soft: compute by concatenating available layers
    def _safe(layer):
        try:
            return layer()
        except Exception:
            return ""

    def _get_global_css() -> str:
        try:
            from .base import get_base_css  # type: ignore
        except Exception:
            def get_base_css() -> str:  # type: ignore[no-redef]
                return ""
        try:
            from .layout import get_layout_css  # type: ignore
        except Exception:
            def get_layout_css() -> str:  # type: ignore[no-redef]
                return ""
        try:
            from .components import get_components_css  # type: ignore
        except Exception:
            def get_components_css() -> str:  # type: ignore[no-redef]
                return ""
        return "\n".join([_safe(get_base_css), _safe(get_layout_css), _safe(get_components_css)])


def get_global_css() -> str:
    """
    Return full CSS string for injection.
    Prefer importing from fum_rt.frontend.styles instead of this legacy module.
    """
    return _get_global_css()


__all__ = ["get_global_css"]]]></content>
    </file>
    <file>
      <path>frontend/utilities/fs_utils.py</path>
      <content><![CDATA[import os
import json
from typing import Any, List


def list_runs(root: str) -> List[str]:
    """
    List run directories under root, sorted by mtime desc.
    Mirrors logic from the legacy dashboard for identical behavior.
    """
    if not os.path.exists(root):
        return []
    items = []
    for name in os.listdir(root):
        path = os.path.join(root, name)
        if os.path.isdir(path):
            try:
                mt = os.path.getmtime(path)
            except Exception:
                mt = 0.0
            items.append((mt, path))
    items.sort(key=lambda x: x[0], reverse=True)
    return [p for _, p in items]


def read_json_file(path: str) -> Any:
    try:
        with open(path, "r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        return None


def write_json_file(path: str, data: Any) -> bool:
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as fh:
            json.dump(data, fh, indent=2)
        return True
    except Exception:
        return False


def _list_files(path: str, exts: List[str] | None, recursive: bool = False) -> List[str]:
    """
    List files under a path, optionally filtered by extensions and recursively.
    Excludes common compressed archive extensions to mirror prior UI behavior.
    Returns relative paths when recursive=True, otherwise basenames.
    """
    if not os.path.isdir(path):
        return []

    found: List[str] = []
    compressed_exts = {".zip", ".gz", ".bz2", ".xz", ".rar", ".7z"}
    try:
        if not recursive:
            return [
                f
                for f in os.listdir(path)
                if (exts is None or any(f.lower().endswith(e) for e in exts))
                and not any(f.lower().endswith(c) for c in compressed_exts)
            ]

        for root, _, files in os.walk(path):
            for f in files:
                if (exts is None or any(f.lower().endswith(e) for e in exts)) and not any(
                    f.lower().endswith(c) for c in compressed_exts
                ):
                    # store relative path from the initial scan path
                    rel_path = os.path.relpath(os.path.join(root, f), path)
                    found.append(rel_path)
        return found
    except Exception:
        return []


def latest_checkpoint(run_dir: str) -> str | None:
    """
    Return the absolute path to the latest checkpoint (.h5 or .npz) in a run directory,
    determined by numeric step parsed from filenames like state_000123.h5/npz.
    """
    try:
        files = []
        for fn in os.listdir(run_dir):
            if fn.startswith("state_") and (fn.endswith(".h5") or fn.endswith(".npz")):
                ext = ".h5" if fn.endswith(".h5") else ".npz"
                step_str = fn[6:-len(ext)]
                try:
                    s = int(step_str)
                    files.append((s, os.path.join(run_dir, fn)))
                except Exception:
                    pass
        if files:
            files.sort(key=lambda x: x[0], reverse=True)
            return files[0][1]
    except Exception:
        return None
    return None]]></content>
    </file>
    <file>
      <path>frontend/utilities/pdf_utils.py</path>
      <content><![CDATA[from __future__ import annotations

"""
PDF -> Text utilities (bounded, best-effort, dependency-optional)

Design:
- Try light, pure extraction first (no OCR) using available libs in this order:
  1) PyMuPDF (fitz)
  2) pdfminer.six
  3) PyPDF2
- If those produce too little text, optionally fallback to OCR if pytesseract + pdf2image are available.
- Write result to an output .txt path and return it along with the method used.
- Bounded IO: operate only on the provided file, no recursion. No scans in core/ or maps/.

Returns:
- (out_path, method) when successful, where method in {"pymupdf", "pdfminer", "pypdf2", "ocr"}
- (None, "") if conversion not possible or failed.

Notes:
- OCR requires poppler (for pdf2image) and Tesseract installed; function degrades gracefully if missing.
- Keep memory and compute usage modest; OCR dpi kept moderate (e.g., 200).
"""

import os
from typing import Tuple, Optional


_MIN_TEXT_CHARS = 64  # minimal payload to consider extraction successful


def _write_text(out_path: str, text: str) -> bool:
    try:
        os.makedirs(os.path.dirname(out_path), exist_ok=True)
        with open(out_path, "w", encoding="utf-8") as fh:
            fh.write(text)
        return True
    except Exception:
        return False


def _try_pymupdf(pdf_path: str) -> Optional[str]:
    try:
        import fitz  # PyMuPDF
    except Exception:
        return None
    try:
        text_parts = []
        with fitz.open(pdf_path) as doc:
            for page in doc:
                text_parts.append(page.get_text() or "")
        text = "\n".join(text_parts)
        return text
    except Exception:
        return None


def _try_pdfminer(pdf_path: str) -> Optional[str]:
    try:
        from pdfminer.high_level import extract_text
    except Exception:
        return None
    try:
        return extract_text(pdf_path) or ""
    except Exception:
        return None


def _try_pypdf2(pdf_path: str) -> Optional[str]:
    try:
        from PyPDF2 import PdfReader
    except Exception:
        return None
    try:
        reader = PdfReader(pdf_path)
        text_parts = []
        for page in reader.pages:
            try:
                text_parts.append(page.extract_text() or "")
            except Exception:
                continue
        return "\n".join(text_parts)
    except Exception:
        return None


def _try_ocr(pdf_path: str, dpi: int = 200) -> Optional[str]:
    """
    OCR fallback using pdf2image + pytesseract, if both available and system supports them.
    """
    try:
        from pdf2image import convert_from_path
        import pytesseract
    except Exception:
        return None
    try:
        images = convert_from_path(pdf_path, dpi=dpi)
        text_parts = []
        for im in images:
            try:
                text_parts.append(pytesseract.image_to_string(im) or "")
            except Exception:
                continue
        return "\n".join(text_parts)
    except Exception:
        return None


def convert_pdf_to_text_file(pdf_path: str, out_dir: str) -> Tuple[Optional[str], str]:
    """
    Convert a PDF to a UTF-8 .txt file.

    Args:
        pdf_path: absolute or relative path to the .pdf file
        out_dir: directory to write the .txt sidecar into

    Returns:
        (out_txt_path, method) on success; (None, "") on failure
    """
    try:
        base = os.path.splitext(os.path.basename(pdf_path))[0] or "document"
        out_txt = os.path.join(out_dir, f"{base}.txt")
    except Exception:
        return None, ""

    # 1) PyMuPDF
    text = _try_pymupdf(pdf_path)
    if text and len(text.strip()) >= _MIN_TEXT_CHARS:
        return (out_txt, "pymupdf") if _write_text(out_txt, text) else (None, "")

    # 2) pdfminer
    text = _try_pdfminer(pdf_path)
    if text and len(text.strip()) >= _MIN_TEXT_CHARS:
        return (out_txt, "pdfminer") if _write_text(out_txt, text) else (None, "")

    # 3) PyPDF2
    text = _try_pypdf2(pdf_path)
    if text and len(text.strip()) >= _MIN_TEXT_CHARS:
        return (out_txt, "pypdf2") if _write_text(out_txt, text) else (None, "")

    # 4) OCR fallback
    text = _try_ocr(pdf_path, dpi=200)
    if text and len(text.strip()) >= _MIN_TEXT_CHARS:
        return (out_txt, "ocr") if _write_text(out_txt, text) else (None, "")

    return None, ""]]></content>
    </file>
    <file>
      <path>frontend/utilities/profiles.py</path>
      <content><![CDATA["""
Profile utilities for FUM Live Dashboard.
Centralizes default profile defaults and assembly helpers to avoid duplication.
"""

from typing import Any, Dict, List


def safe_int(x, default=None):
    try:
        return int(x)
    except Exception:
        return default


def safe_float(x, default=None):
    try:
        return float(x)
    except Exception:
        return default


def bool_from_checklist(val) -> bool:
    """
    Dash checklist returns a list of selected values.
    We treat presence of 'on' as True for single-toggle checklists.
    """
    if isinstance(val, list):
        return "on" in val
    return bool(val)


def checklist_from_bool(b: bool) -> List[str]:
    """Inverse of bool_from_checklist for initial Dash values."""
    return ["on"] if bool(b) else []


def get_default_profile() -> Dict[str, Any]:
    """
    Default profile used by the dashboard UI for initial values.
    Mirrors the legacy dashboard defaults exactly.
    """
    return {
        "neurons": 1000,
        "k": 12,
        "hz": 10,
        "domain": "math_physics",
        "use_time_dynamics": True,
        "sparse_mode": False,
        "threshold": 0.15,
        "lambda_omega": 0.10,
        "candidates": 64,
        "walkers": 256,
        "hops": 3,
        "bundle_size": 3,
        "prune_factor": 0.10,
        "status_interval": 1,
        "viz_every": 0,
        "log_every": 1,
        "speak_auto": True,
        "speak_z": 3.0,
        "speak_hysteresis": 0.5,
        "speak_cooldown_ticks": 10,
        "speak_valence_thresh": 0.55,
        "b1_half_life_ticks": 50,
        "stim_group_size": 8,
        "stim_amp": 0.08,
        "stim_decay": 0.92,
        "stim_max_symbols": 128,
        "checkpoint_every": 60,
        "checkpoint_keep": 5,
        "duration": None,
    }


def assemble_profile(
    neurons,
    k,
    hz,
    domain,
    use_td,
    sparse_mode,
    threshold,
    lambda_omega,
    candidates,
    walkers,
    hops,
    status_interval,
    bundle_size,
    prune_factor,
    stim_group_size,
    stim_amp,
    stim_decay,
    stim_max_symbols,
    speak_auto,
    speak_z,
    speak_hyst,
    speak_cd,
    speak_val,
    b1_hl,
    viz_every,
    log_every,
    checkpoint_every,
    checkpoint_keep,
    duration,
    default_profile: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Build a normalized runtime profile dict from UI inputs with robust typing.
    All conversions mirror the legacy dashboard behavior to avoid surprises.
    """
    return {
        "neurons": int(safe_int(neurons, default_profile["neurons"])),
        "k": int(safe_int(k, default_profile["k"])),
        "hz": int(safe_int(hz, default_profile["hz"])),
        "domain": str(domain or default_profile["domain"]),
        "use_time_dynamics": bool_from_checklist(use_td) if use_td is not None else default_profile["use_time_dynamics"],
        "sparse_mode": bool_from_checklist(sparse_mode) if sparse_mode is not None else default_profile["sparse_mode"],
        "threshold": float(safe_float(threshold, default_profile["threshold"])),
        "lambda_omega": float(safe_float(lambda_omega, default_profile["lambda_omega"])),
        "candidates": int(safe_int(candidates, default_profile["candidates"])),
        "walkers": int(safe_int(walkers, default_profile["walkers"])),
        "hops": int(safe_int(hops, default_profile["hops"])),
        "status_interval": int(safe_int(status_interval, default_profile["status_interval"])),
        "bundle_size": int(safe_int(bundle_size, default_profile["bundle_size"])),
        "prune_factor": float(safe_float(prune_factor, default_profile["prune_factor"])),
        "stim_group_size": int(safe_int(stim_group_size, default_profile["stim_group_size"])),
        "stim_amp": float(safe_float(stim_amp, default_profile["stim_amp"])),
        "stim_decay": float(safe_float(stim_decay, default_profile["stim_decay"])),
        "stim_max_symbols": int(safe_int(stim_max_symbols, default_profile["stim_max_symbols"])),
        "speak_auto": bool_from_checklist(speak_auto) if speak_auto is not None else default_profile["speak_auto"],
        "speak_z": float(safe_float(speak_z, default_profile["speak_z"])),
        "speak_hysteresis": float(safe_float(speak_hyst, default_profile["speak_hysteresis"])),
        "speak_cooldown_ticks": int(safe_int(speak_cd, default_profile["speak_cooldown_ticks"])),
        "speak_valence_thresh": float(safe_float(speak_val, default_profile["speak_valence_thresh"])),
        "b1_half_life_ticks": int(safe_int(b1_hl, default_profile["b1_half_life_ticks"])),
        "viz_every": int(safe_int(viz_every, default_profile["viz_every"])),
        "log_every": int(safe_int(log_every, default_profile["log_every"])),
        "checkpoint_every": int(safe_int(checkpoint_every, default_profile["checkpoint_every"])),
        "checkpoint_keep": int(safe_int(checkpoint_keep, default_profile["checkpoint_keep"])),
        "duration": None if duration in (None, "", "None") else int(safe_int(duration, 0)),
    }]]></content>
    </file>
    <file>
      <path>frontend/utilities/tail.py</path>
      <content><![CDATA[import os
import json
from typing import Any, Tuple, List


def _parse_jsonl_line(line: str) -> Any:
    try:
        return json.loads(line)
    except Exception:
        return None


def _env_int(name: str, default: int) -> int:
    try:
        v = int(os.environ.get(name, str(default)))
        return v
    except Exception:
        return default


def tail_jsonl_bytes(path: str, last_size: int) -> Tuple[List[Any], int]:
    """
    Tail a JSONL file by byte offset with bounded IO and parse work.

    Inputs:
    - path: file path to JSONL
    - last_size: previous file size (bytes) to resume from

    Returns:
    - (records, new_size)
      records: list of parsed JSON objects appended since last_size (possibly truncated to most recent window)
      new_size: new file size to store for the next call

    Environment (all optional):
    - FUM_UI_TAIL_CAP_BYTES         (default 1_048_576)  - initial/rotation cap window
    - FUM_UI_TAIL_MAX_DELTA_BYTES   (default 131_072)    - max bytes read per tick even if more appended
    - FUM_UI_TAIL_MAX_LINES         (default 600)        - max lines parsed per tick from the new chunk

    Notes:
    - These bounds are UI-only to avoid lag on very large or fast-growing files; older appended
      records may be skipped when the per-tick delta exceeds caps. Core runtime is unaffected.
    """
    if not os.path.exists(path):
        return [], 0
    try:
        size = os.path.getsize(path)
    except Exception:
        return [], last_size

    cap = _env_int("FUM_UI_TAIL_CAP_BYTES", 1_048_576)
    max_delta = _env_int("FUM_UI_TAIL_MAX_DELTA_BYTES", 131_072)
    max_lines = _env_int("FUM_UI_TAIL_MAX_LINES", 600)

    # Establish start offset
    start = last_size if 0 <= last_size <= size else 0

    # Initial read or file rotation: only read the tail cap window
    if (last_size <= 0 or last_size > size) and size > cap:
        start = max(0, size - cap)

    # Always bound per-tick delta to avoid huge reads when many bytes were appended
    delta = size - start
    if delta <= 0:
        return [], size
    if max_delta > 0 and delta > max_delta:
        start = size - max_delta
        delta = max_delta

    try:
        with open(path, "rb") as f:
            f.seek(start)
            data = f.read(delta)
        text = data.decode("utf-8", errors="ignore")
    except Exception:
        return [], size

    # Split once; then optionally keep only the last K lines to bound JSON parsing work
    lines = text.splitlines()
    if max_lines > 0 and len(lines) > max_lines:
        lines = lines[-max_lines:]

    recs: List[Any] = []
    for s in lines:
        s = s.strip()
        if not s:
            continue
        obj = _parse_jsonl_line(s)
        if obj is not None:
            recs.append(obj)
    return recs, size

def tail_jsonl_bytes_config(path: str, last_size: int, cap: int, max_delta: int, max_lines: int) -> Tuple[List[Any], int]:
    """
    Tail a JSONL file by byte offset using explicit per-call caps (no env vars).

    Args:
        path: JSONL file path
        last_size: previous file size in bytes (seek start)
        cap: initial/rotation cap (bytes)
        max_delta: maximum bytes to read this call
        max_lines: maximum lines to parse from the read chunk

    Returns:
        (records, new_size)
    """
    if not os.path.exists(path):
        return [], 0

    # Sanitize inputs
    try:
        cap = int(cap)
    except Exception:
        cap = 1_048_576
    try:
        max_delta = int(max_delta)
    except Exception:
        max_delta = 131_072
    try:
        max_lines = int(max_lines)
    except Exception:
        max_lines = 600
    cap = max(1024, cap)
    max_delta = max(4096, max_delta)
    max_lines = max(1, max_lines)

    try:
        size = os.path.getsize(path)
    except Exception:
        return [], last_size

    start = last_size if 0 <= last_size <= size else 0

    # Initial read or file rotation: read only tail cap window
    if (last_size <= 0 or last_size > size) and size > cap:
        start = max(0, size - cap)

    delta = size - start
    if delta <= 0:
        return [], size
    if max_delta > 0 and delta > max_delta:
        start = size - max_delta
        delta = max_delta

    try:
        with open(path, "rb") as f:
            f.seek(start)
            data = f.read(delta)
        text = data.decode("utf-8", errors="ignore")
    except Exception:
        return [], size

    lines = text.splitlines()
    if max_lines > 0 and len(lines) > max_lines:
        lines = lines[-max_lines:]

    recs: List[Any] = []
    for s in lines:
        s = s.strip()
        if not s:
            continue
        obj = _parse_jsonl_line(s)
        if obj is not None:
            recs.append(obj)
    return recs, size]]></content>
    </file>
    <file>
      <path>io/README.md</path>
      <content/>
    </file>
    <file>
      <path>io/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>io/actuators/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>io/actuators/macros.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""


from __future__ import annotations
import json, os, threading, time
from typing import Any, Dict, Iterable, Optional
from fum_rt.io.logging.rolling_jsonl import RollingJsonlWriter
try:
    # Prefer zip spooler when available
    from fum_rt.io.logging.rolling_jsonl import RollingZipJsonlWriter  # type: ignore
except Exception:
    RollingZipJsonlWriter = None  # type: ignore

class MacroEmitter:
    """
    Thread-safe NDJSON macro emitter.
    Schema per event:
      {
        "type": "macro",
        "macro": <lowercase name>,
        "args": {
          "text": <flattened, human-readable line for classifiers>,
          "why": { ... metrics/context ... },
          ... macro-specific fields (optional) ...
        },
        "score": <float, optional>
      }
    """
    def __init__(self, path: str, why_provider=None):
        # Output path to NDJSON, e.g., runs/<ts>/utd_events.jsonl
        self.path = path or ""
        self.lock = threading.Lock()
        # why_provider: callable returning a dict with context (t, phase, etc.)
        self.why_provider = why_provider or (lambda: {"t": int(time.time() * 1000), "phase": 0})
        # ensure directory exists
        os.makedirs(os.path.dirname(os.path.abspath(self.path)), exist_ok=True)
        # Prefer zip-spooled writer (bounded disk pressure); fallback to rolling JSONL
        use_zip = True
        try:
            use_zip = str(os.getenv("FUM_ZIP_SPOOL", "1")).strip().lower() in ("1", "true", "yes", "on", "y")
        except Exception:
            use_zip = True
        try:
            if use_zip and (RollingZipJsonlWriter is not None):  # type: ignore
                self._writer = RollingZipJsonlWriter(self.path)  # type: ignore
            else:
                self._writer = RollingJsonlWriter(self.path)
        except Exception:
            self._writer = RollingJsonlWriter(self.path)

    def _emit(self, macro: str, text: str, score: Optional[float] = None, **kwargs: Any):
        evt = {
            "type": "macro",
            "macro": str(macro).lower(),
            "args": {
                "text": str(text),
                "why": (self.why_provider() or {}),
            }
        }
        # attach any extra fields into args (vars, edges, etc.)
        for k, v in kwargs.items():
            evt["args"][k] = v
        if score is not None:
            try:
                evt["score"] = float(score)
            except Exception:
                pass
        line = json.dumps(evt, ensure_ascii=False)
        with self.lock:
            self._writer.write_line(line)

    # ---- basic channels ----
    def say(self, text: str, score: Optional[float] = None, **kw: Any):
        self._emit("say", text, score=score, **kw)

    def status(self, text: str, score: Optional[float] = None, **kw: Any):
        self._emit("status", text, score=score, **kw)

    def think(self, text: str, **kw: Any):
        self._emit("think", text, **kw)

    # ---- reasoning macros (flatten to readable text) ----
    def vars(self, mapping: Dict[str, str], **kw: Any):
        # VARS: N=neural; G=global_access; ...
        flat = "VARS: " + "; ".join(f"{k}={v}" for k, v in mapping.items())
        self._emit("vars", flat, vars=mapping, **kw)

    def edges(self, edges: Iterable[str], **kw: Any):
        # EDGES: N->G; G->B; E->B?
        flat = "EDGES: " + "; ".join(edges)
        self._emit("edges", flat, edges=list(edges), **kw)

    def assumptions(self, items: Iterable[str], **kw: Any):
        flat = "ASSUMPTIONS: " + "; ".join(items)
        self._emit("assumptions", flat, assumptions=list(items), **kw)

    def derivation(self, sentence: str, **kw: Any):
        """
        Expect at least one inference marker to trip 'chain' classifier:
        e.g., 'If A and B, therefore C.'
        """
        flat = "DERIVATION: " + sentence
        self._emit("derivation", flat, **kw)

    def target(self, text: str, **kw: Any):
        """
        Include 'do(' when applicable so 'intervention' classifier fires.
        e.g., 'TARGET: P(Y|do(X))'
        """
        flat = "TARGET: " + text
        self._emit("target", flat, **kw)

    def prediction_delta(self, text: str, **kw: Any):
        flat = "PREDICTION-DELTA: " + text
        self._emit("prediction-delta", flat, **kw)

    def transfer(self, text: str, **kw: Any):
        flat = "TRANSFER: " + text
        self._emit("transfer", flat, **kw)

    def equation(self, text: str, **kw: Any):
        """
        Encourage SEM/SCM form to trip 'equation' classifier:
        e.g., 'EQUATION: Y = β X + U_Y'
        """
        flat = "EQUATION: " + text
        self._emit("equation", flat, **kw)]]></content>
    </file>
    <file>
      <path>io/actuators/motor_control.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

# Motor control actuator interface]]></content>
    </file>
    <file>
      <path>io/actuators/symbols.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/actuators/thoughts.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

import json
import os
import threading
import time
from typing import Any, Dict, Iterable, Optional


class ThoughtEmitter:
    """
    Introspection Ledger (emit-only).
    Thread-safe NDJSON writer for typed "thought events" that are never ingested back.

    Event shape (one JSON per line):
    {
      "type": "thought",
      "why": { ... context from why_provider ... },
      "kind": "<observation|motif|hypothesis|test|derivation|revision|plan>",
      ... kind-specific fields ...
    }
    """

    def __init__(self, path: str, why: Optional[callable] = None):
        """
        Args:
            path: Output NDJSON path (e.g., runs/<ts>/thoughts.ndjson)
            why:  Callable returning a dict of read-only context (t, phase, b1_z, etc.)
        """
        self.path = path
        self._why = why or (lambda: {"t": int(time.time() * 1000), "phase": 0})
        self._lock = threading.Lock()
        # Ensure parent directory exists
        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)

    # -------------- core --------------

    def _emit(self, payload: Dict[str, Any]) -> None:
        evt = {"type": "thought", "why": (self._why() or {})}
        evt.update(payload or {})
        line = json.dumps(evt, ensure_ascii=False)
        with self._lock, open(self.path, "a", encoding="utf-8") as f:
            f.write(line + "\n")

    # -------------- typed helpers --------------

    def observation(self, key: str, value: Any, **kw: Any) -> None:
        self._emit({"kind": "observation", "key": key, "value": value, **kw})

    def motif(self, motif_id: str, nodes: Optional[Iterable[Any]] = None, **kw: Any) -> None:
        self._emit({"kind": "motif", "motif_id": motif_id, "nodes": list(nodes or []), **kw})

    def hypothesis(
        self,
        hid: str,
        claim: str,
        status: str = "tentative",
        conf: Optional[float] = None,
        **kw: Any,
    ) -> None:
        self._emit({"kind": "hypothesis", "id": hid, "claim": claim, "status": status, "conf": conf, **kw})

    def test(self, kind: str, result: bool, vars: Optional[Dict[str, Any]] = None, **kw: Any) -> None:
        self._emit({"kind": "test", "test_kind": kind, "result": bool(result), "vars": dict(vars or {}), **kw})

    def derivation(
        self,
        premises: Iterable[str],
        therefore: str,
        conf: Optional[float] = None,
        **kw: Any,
    ) -> None:
        self._emit({"kind": "derivation", "premises": list(premises or []), "therefore": therefore, "conf": conf, **kw})

    def revision(self, hyp: str, new_status: str, because: Optional[Iterable[str]] = None, **kw: Any) -> None:
        self._emit({"kind": "revision", "hyp": hyp, "new_status": new_status, "because": list(because or []), **kw})

    def plan(self, act: str, vars: Optional[Dict[str, Any]] = None, rationale: Optional[str] = None, **kw: Any) -> None:
        self._emit({"kind": "plan", "act": act, "vars": dict(vars or {}), "rationale": rationale, **kw})]]></content>
    </file>
    <file>
      <path>io/actuators/visualize.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/actuators/vocalizer.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/cognition/composer.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Cognition - speech composer (Phase 3 move-only).

Behavior-preserving extraction of Nexus._compose_say_text:
- Prefer emergent sentence generation from streaming n-grams.
- Fallback to phrase templates with context formatting.
- Final fallback to keyword summary.
"""

from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple

# Use absolute import to avoid relative package ambiguity
from fum_rt.core import text_utils


def compose_say_text(
    metrics: Dict[str, Any],
    step: int,
    lexicon: Dict[str, int],
    ng2: Dict[str, Dict[str, int]],
    ng3: Dict[Tuple[str, str], Dict[str, int]],
    recent_text: Iterable[str],
    templates: Optional[Sequence[str]] = None,
    seed_tokens: Optional[Set[str]] = None,
) -> str:
    """
    Compose a short sentence using emergent language or templates.

    Parameters:
        metrics: last tick metrics dict
        step: current tick number (used as seed)
        lexicon/ng2/ng3: emergent language state
        recent_text: iterable of recent inbound text strings
        templates: optional sequence of phrase templates with named fields
        seed_tokens: optional set of tokens influencing emergent generation

    Returns:
        A composed sentence string (non-empty). On failure, returns "".
    """
    try:
        # 1) Fully emergent sentence generation
        sent = text_utils.generate_emergent_sentence(
            lexicon=lexicon,
            ng2=ng2,
            ng3=ng3,
            seed=int(step),
            seed_tokens=seed_tokens,
        )
        if sent:
            return sent

        # 2) Template-based composition if n-grams are not mature
        #    Preserve original keyword summary behavior
        summary = text_utils.summarize_keywords(" ".join(str(s) for s in recent_text), k=6)
        words = [w.strip() for w in summary.split(",") if w.strip()]
        top1 = words[0] if len(words) > 0 else "frontier"
        top2 = words[1] if len(words) > 1 else "structure"

        ctx = {
            "keywords": (summary or "salient loop detected"),
            "top1": top1,
            "top2": top2,
            "vt_entropy": float(metrics.get("vt_entropy", 0.0)),
            "vt_coverage": float(metrics.get("vt_coverage", 0.0)),
            "b1_z": float(metrics.get("b1_z", 0.0)),
            "connectome_entropy": float(metrics.get("connectome_entropy", 0.0)),
            "valence": float(metrics.get("sie_v2_valence_01", 0.0)),
        }

        tpls: List[str] = list(templates or [])
        if tpls:
            tpl = tpls[int(step) % len(tpls)]
            try:
                return tpl.format(**ctx)
            except Exception:
                # fall through to final summary fallback
                pass

        # 3) Final fallback to keyword summary
        return (summary or "").strip() or "."
    except Exception:
        return ""]]></content>
    </file>
    <file>
      <path>io/cognition/speaker.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Cognition - speaker gating and scoring (Phase 3 move-only).

Behavior-preserving helpers extracted from Nexus.run():
- Gating decision based on B1 spike and valence threshold.
- Novelty-IDF computation and emission score calculation.

No logging or IO; pure functions only.
"""

from typing import Callable, Dict, Iterable, Optional, Tuple


def should_speak(valence_v2: float, spike: bool, valence_thresh: float) -> Tuple[bool, Optional[str]]:
    """
    Decide whether to speak this tick.

    Mirrors Nexus policy:
    - Must have a topology spike (b1_spike == True).
    - Valence must be >= threshold.
    - Only logs suppression for low_valence; absence of spike is silent.

    Returns:
        (can_speak, reason)
        reason is "low_valence" when valence is below threshold, else None.
    """
    if not spike:
        return False, None
    if valence_v2 >= float(valence_thresh):
        return True, None
    return False, "low_valence"


def novelty_and_score(
    speech: str,
    lexicon: Dict[str, int],
    doc_count: int,
    tokenizer: Callable[[str], Iterable[str]],
    composer_k: float,
    valence_v2: float,
) -> Tuple[float, float]:
    """
    Compute composer-local novelty IDF factor and output score.

    Equivalent to inline Nexus logic:
      novelty_idf = IDF(emitted_tokens; lexicon, doc_count)
      score_out = valence_v2 * (novelty_idf ** composer_k)

    Robust to errors: falls back to (1.0, valence_v2) if anything fails.
    """
    novelty_idf = 1.0
    try:
        try:
            from fum_rt.io.lexicon.idf import compute_idf_scale as _compute_idf_scale
        except Exception:
            _compute_idf_scale = None

        tokens = []
        try:
            tokens = list(set(tokenizer(speech)))
        except Exception:
            tokens = []

        if _compute_idf_scale is not None:
            novelty_idf = float(
                _compute_idf_scale(
                    tokens,
                    dict(lexicon or {}),
                    int(doc_count or 0),
                    default=1.0,
                    min_scale=0.5,
                    max_scale=2.0,
                )
            )
    except Exception:
        novelty_idf = 1.0

    try:
        k = float(composer_k)
    except Exception:
        k = 0.0

    try:
        val = float(valence_v2)
    except Exception:
        val = 0.0

    try:
        score_out = float(val * (novelty_idf ** k))
    except Exception:
        score_out = float(val)

    return novelty_idf, score_out


__all__ = ["should_speak", "novelty_and_score"]]]></content>
    </file>
    <file>
      <path>io/cognition/stimulus.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Cognition - stimulus mapping (Phase 3 move-only).

Deterministic, stateless symbol→group mapping used by Nexus ingestion.
Behavior preserved: identical arithmetic hash and iteration order.
"""

from typing import Dict, List, Optional


def symbols_to_indices(
    text: str,
    stim_group_size: int,
    stim_max_symbols: int,
    N: int,
    reverse_map: Optional[Dict[int, str]] = None,
) -> List[int]:
    """
    Deterministic mapping from input symbols to neuron indices.

    Parameters:
        text: source string to stimulate
        stim_group_size: number of neurons per unique symbol
        stim_max_symbols: max number of unique symbols to map per call
        N: total neuron count (bounds the index space)
        reverse_map: optional dict to populate with index->symbol for first-claiming symbols

    Returns:
        List of neuron indices (may contain duplicates if group_size overlaps across symbols).
    """
    try:
        g = int(max(1, int(stim_group_size)))
        max_syms = int(max(1, int(stim_max_symbols)))
        N_int = int(N)
        out: List[int] = []
        seen = set()
        for ch in str(text):
            if ch in seen:
                continue
            seen.add(ch)
            code = ord(ch)
            base = (code * 1315423911) % N_int
            for j in range(g):
                idx = int((base + j * 2654435761) % N_int)
                out.append(idx)
                if isinstance(reverse_map, dict) and idx not in reverse_map:
                    reverse_map[idx] = ch
            if len(seen) >= max_syms:
                break
        return out
    except Exception:
        return []]]></content>
    </file>
    <file>
      <path>io/lexicon/idf.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
IDF-based novelty scaling helpers for lexicon-driven runtime.

Design constraints:
- Pure functions, no side-effects. Safe to import anywhere.
- Stable under empty inputs: returns default scale (1.0) to preserve behavior.
- Bounded output: clamp to [min_scale, max_scale] for predictable gating.
- Mirrors Nexus expectations: caller multiplies by novelty_idf_gain and may clamp again.
"""

from typing import Iterable, Mapping
import math

def _safe_log(x: float) -> float:
    try:
        return math.log(x) if x > 0 else 0.0
    except Exception:
        return 0.0

def idf(df: int, doc_count: int) -> float:
    """
    Compute standard IDF with +1 smoothing:
        idf = 1 + ln( (doc_count + 1) / (df + 1) )
    Returns >= 0.0; equals 1.0 when df ≈ doc_count.
    """
    try:
        dc = max(0, int(doc_count))
        dfv = max(0, int(df))
        return 1.0 + _safe_log((dc + 1.0) / (dfv + 1.0))
    except Exception:
        return 1.0

def compute_idf_scale(tokens: Iterable[str], lexicon: Mapping[str, int], doc_count: int, default: float = 1.0, min_scale: float = 0.5, max_scale: float = 2.0) -> float:
    """
    Compute a bounded novelty scale from token set and a DF-style lexicon.

    Parameters:
    - tokens: Iterable of tokens observed this tick
    - lexicon: Mapping token -> document frequency (DF)
    - doc_count: Total number of documents/messages observed so far
    - default: Fallback scale when inputs are empty or invalid
    - min_scale, max_scale: Output clamp bounds

    Returns:
    - Scale in [min_scale, max_scale], or default if insufficient information
    """
    try:
        if tokens is None:
            return float(default)
        toks = {str(t).lower() for t in tokens if str(t).strip()}
        if not toks:
            return float(default)
        if not isinstance(lexicon, Mapping) or len(lexicon) == 0:
            return float(default)
        dc = max(0, int(doc_count))
        if dc <= 0:
            return float(default)
        # Compute mean IDF across unique tokens for a smooth, low-variance scale
        s = 0.0
        n = 0
        for w in toks:
            dfv = int(lexicon.get(w, 0))
            s += idf(dfv, dc)
            n += 1
        if n == 0:
            return float(default)
        mean_idf = s / float(n)
        # Bound per runtime expectations
        if mean_idf < float(min_scale):
            return float(min_scale)
        if mean_idf > float(max_scale):
            return float(max_scale)
        return float(mean_idf)
    except Exception:
        return float(default)

__all__ = ["idf", "compute_idf_scale"]]]></content>
    </file>
    <file>
      <path>io/lexicon/store.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Lexicon and phrase-bank utilities extracted from Nexus.

Goals:
- Preserve existing behavior exactly (paths, merge order, formats).
- Pure functions where possible; no logging side-effects.
- Robust to malformed files; fail-soft with safe defaults.
"""

import os
import json
from typing import Dict, Tuple, List, Mapping

# ---- Phrase templates -------------------------------------------------------

def load_phrase_templates(run_dir: str) -> List[str]:
    """
    Load phrase templates for the 'say' macro using the same precedence and shape handling
    Nexus used inline:
      1) runs/<ts>/macro_board.json under key: say.templates or say.phrases
      2) runs/<ts>/phrase_bank.json under key: say or templates (list)
      3) fallback: package file fum_rt/io/lexicon/phrase_bank_min.json under key: say or templates (list)
    Returns an ordered list of strings. Any missing/invalid sources are ignored.
    """
    templates: List[str] = []

    # 1) Per-run macro board metadata
    try:
        mb_path = os.path.join(run_dir, "macro_board.json")
        if os.path.exists(mb_path):
            with open(mb_path, "r", encoding="utf-8") as fh:
                _mb = json.load(fh)
            if isinstance(_mb, dict):
                _say_meta = _mb.get("say") or {}
                if isinstance(_say_meta, dict):
                    _tpls = _say_meta.get("templates") or _say_meta.get("phrases") or []
                    if isinstance(_tpls, list):
                        templates.extend([str(x) for x in _tpls if isinstance(x, (str,))])
    except Exception:
        pass

    # 2) Per-run phrase bank
    try:
        pb_run = os.path.join(run_dir, "phrase_bank.json")
        if os.path.exists(pb_run):
            with open(pb_run, "r", encoding="utf-8") as fh:
                obj = json.load(fh)
            if isinstance(obj, dict):
                _say = obj.get("say") or obj.get("templates") or []
                if isinstance(_say, list):
                    templates.extend([str(x) for x in _say if isinstance(x, (str,))])
            return templates
    except Exception:
        pass

    # 3) Fallback package phrase bank (minimum)
    try:
        pkg_dir = os.path.dirname(__file__)
        pb_pkg = os.path.join(pkg_dir, "phrase_bank_min.json")
        if os.path.exists(pb_pkg):
            with open(pb_pkg, "r", encoding="utf-8") as fh:
                obj = json.load(fh)
            if isinstance(obj, dict):
                _say = obj.get("say") or obj.get("templates") or []
                if isinstance(_say, list):
                    templates.extend([str(x) for x in _say if isinstance(x, (str,))])
    except Exception:
        pass

    return templates


# ---- Lexicon I/O ------------------------------------------------------------

def load_lexicon(run_dir: str) -> Tuple[Dict[str, int], int]:
    """
    Load DF-style lexicon and document count from runs/<ts>/lexicon.json.

    Supports:
      - {"tokens":[{"token":..., "count":...}], "doc_count": int}
      - {"doc_count": int, "word": count, ...}
      - Or a bare mapping without doc_count (defaults to 0)

    Returns: (lexicon_lowercased, doc_count_int)
    """
    path = os.path.join(run_dir, "lexicon.json")
    lex: Dict[str, int] = {}
    doc_count = 0
    try:
        if not os.path.exists(path):
            return lex, doc_count
        with open(path, "r", encoding="utf-8") as fh:
            obj = json.load(fh)
        if not isinstance(obj, dict):
            return lex, doc_count

        # document count metadata
        try:
            dc = obj.get("doc_count", obj.get("documents", obj.get("docs", 0)))
            if dc is not None:
                doc_count = int(dc)
        except Exception:
            pass

        if "tokens" in obj and isinstance(obj["tokens"], list):
            for ent in obj["tokens"]:
                try:
                    lex[str(ent["token"]).lower()] = int(ent["count"])
                except Exception:
                    pass
        else:
            for k, v in obj.items():
                if str(k) in ("doc_count", "documents", "docs"):
                    continue
                try:
                    lex[str(k).lower()] = int(v)
                except Exception:
                    pass
    except Exception:
        # fail-soft: empty
        pass
    return lex, doc_count


def save_lexicon(run_dir: str, lexicon: Mapping[str, int], doc_count: int) -> None:
    """
    Persist lexicon to runs/<ts>/lexicon.json using the same normalized format Nexus used:
      {
        "doc_count": int,
        "tokens": [{"token": word, "count": n}, ...]  // sorted by (-count, token)
      }
    """
    try:
        toks = [
            {"token": str(k), "count": int(v)}
            for k, v in sorted(
                ((str(k), int(v)) for k, v in (lexicon or {}).items()),
                key=lambda kv: (-int(kv[1]), kv[0]),
            )
        ]
        obj = {"doc_count": int(max(0, int(doc_count))), "tokens": toks}
        path = os.path.join(run_dir, "lexicon.json")
        with open(path, "w", encoding="utf-8") as fh:
            json.dump(obj, fh, ensure_ascii=False, indent=2)
    except Exception:
        # fail-soft
        pass


__all__ = ["load_phrase_templates", "load_lexicon", "save_lexicon"]]]></content>
    </file>
    <file>
      <path>io/logging/rolling_jsonl.py</path>
      <content><![CDATA["""
Rolling JSONL writer with bounded main file and archival segments.

- Maintains a capped "active" JSONL file (e.g., events.jsonl, utd_events.jsonl).
- When the active file exceeds the configured size or line cap, the oldest lines
  are streamed into an archive segment and the active file is rewritten to keep
  only the newest tail (rolling buffer).
- Archive segments live under: <run_dir>/archived/<YYYYMMDD_HHMMSS>/<base_name>
  Example: runs/<ts>/archived/20250815_120828/events.jsonl
- When the current archive segment exceeds its cap, a new timestamped segment
  directory is created and subsequent archival lines are appended there.

Configuration (env):
- For events.jsonl (category="EVENTS"):
    FUM_EVENTS_MAX_MB                  (default: 256)
    FUM_EVENTS_MAX_LINES               (default: unset; bytes cap used)
    FUM_EVENTS_ARCHIVE_SEGMENT_MB      (default: 512)
    FUM_EVENTS_ARCHIVE_SEGMENT_LINES   (default: unset; bytes cap used)
- For utd_events.jsonl (category="UTD"):
    FUM_UTD_MAX_MB                     (default: 256)
    FUM_UTD_MAX_LINES                  (default: unset; bytes cap used)
    FUM_UTD_ARCHIVE_SEGMENT_MB         (default: 512)
    FUM_UTD_ARCHIVE_SEGMENT_LINES      (default: unset; bytes cap used)
- Global:
    FUM_LOG_ROLL_CHECK_EVERY           (default: 200)  # enforce cadence (per write)

Notes:
- Uses a cross-process advisory lock via <base_path>.lock to serialize trimming with writers.
- Writers should not hold persistent file handles; always append per call (MacroEmitter, UTD updated).
"""

from __future__ import annotations

import io
import os
import time
import threading
from typing import Optional, Tuple

try:
    import fcntl as _fcntl
except Exception:  # non-posix fallback (no-op locks)
    _fcntl = None


def _now_ts() -> str:
    return time.strftime("%Y%m%d_%H%M%S", time.localtime())


def _ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)


def _is_ts_dir(name: str) -> bool:
    # YYYYMMDD_HHMMSS
    if len(name) != 15:
        return False
    d, u = name.split("_", 1) if "_" in name else ("", "")
    return d.isdigit() and u.isdigit() and len(d) == 8 and len(u) == 6


class RollingJsonlWriter:
    """
    Append-only JSONL writer with rolling buffer and archival segments.

    Usage:
        w = RollingJsonlWriter("/path/to/events.jsonl")
        w.write_line('{"msg":"hello"}')
    """

    def __init__(
        self,
        base_path: str,
        *,
        max_main_bytes: Optional[int] = None,
        max_main_lines: Optional[int] = None,
        archive_dir: Optional[str] = None,
        archive_segment_max_bytes: Optional[int] = None,
        archive_segment_max_lines: Optional[int] = None,
        check_every: Optional[int] = None,
    ) -> None:
        self.base_path = os.path.abspath(base_path)
        _ensure_dir(os.path.dirname(self.base_path))
        self.lock_path = self.base_path + ".lock"
        self._local_lock = threading.Lock()

        base_name = os.path.basename(self.base_path).lower()
        if base_name == "events.jsonl":
            cat = "EVENTS"
        elif "utd" in base_name:
            cat = "UTD"
        else:
            cat = "LOG"

        # Defaults via env (prefer bytes caps unless specific line caps are set)
        def _env_int(name: str, default: Optional[int]) -> Optional[int]:
            v = os.environ.get(name, None)
            if v is None or str(v).strip() == "":
                return default
            try:
                return int(v)
            except Exception:
                return default

        if max_main_bytes is None:
            if cat == "EVENTS":
                max_main_bytes = _env_int("FUM_EVENTS_MAX_MB", 256)
            elif cat == "UTD":
                max_main_bytes = _env_int("FUM_UTD_MAX_MB", 256)
            else:
                max_main_bytes = _env_int("FUM_LOG_MAX_MB", 128)
            max_main_bytes = int(max_main_bytes) * 1024 * 1024 if max_main_bytes else None

        if max_main_lines is None:
            if cat == "EVENTS":
                max_main_lines = _env_int("FUM_EVENTS_MAX_LINES", None)
            elif cat == "UTD":
                max_main_lines = _env_int("FUM_UTD_MAX_LINES", None)
            else:
                max_main_lines = _env_int("FUM_LOG_MAX_LINES", None)

        if archive_dir is None:
            archive_dir = os.path.join(os.path.dirname(self.base_path), "archived")
        self.archive_dir = archive_dir

        if archive_segment_max_bytes is None:
            if cat == "EVENTS":
                archive_segment_max_bytes = _env_int("FUM_EVENTS_ARCHIVE_SEGMENT_MB", 512)
            elif cat == "UTD":
                archive_segment_max_bytes = _env_int("FUM_UTD_ARCHIVE_SEGMENT_MB", 512)
            else:
                archive_segment_max_bytes = _env_int("FUM_LOG_ARCHIVE_SEGMENT_MB", 256)
            archive_segment_max_bytes = (
                int(archive_segment_max_bytes) * 1024 * 1024 if archive_segment_max_bytes else None
            )

        if archive_segment_max_lines is None:
            if cat == "EVENTS":
                archive_segment_max_lines = _env_int("FUM_EVENTS_ARCHIVE_SEGMENT_LINES", None)
            elif cat == "UTD":
                archive_segment_max_lines = _env_int("FUM_UTD_ARCHIVE_SEGMENT_LINES", None)
            else:
                archive_segment_max_lines = _env_int("FUM_LOG_ARCHIVE_SEGMENT_LINES", None)

        self.max_main_bytes = max_main_bytes
        self.max_main_lines = max_main_lines
        self.archive_segment_max_bytes = archive_segment_max_bytes
        self.archive_segment_max_lines = archive_segment_max_lines

        if check_every is None:
            check_every = _env_int("FUM_LOG_ROLL_CHECK_EVERY", 200) or 200
        self._check_every = int(check_every)
        self._ops = 0

    # ------------- public -------------
    def write_line(self, line: str) -> None:
        data = (line.rstrip("\n") + "\n").encode("utf-8", errors="ignore")
        with self._local_lock:
            with self._acquire_lock():
                # append
                with open(self.base_path, "ab") as fh:
                    fh.write(data)
                self._ops += 1
                if (self._ops % self._check_every) == 0:
                    self._enforce()

    # ------------- internals -------------
    def _acquire_lock(self):
        class _Locker:
            def __init__(self, p: str) -> None:
                self.p = p
                self.fh = None

            def __enter__(self):
                _ensure_dir(os.path.dirname(self.p))
                self.fh = open(self.p, "a+")
                if _fcntl is not None:
                    _fcntl.flock(self.fh.fileno(), _fcntl.LOCK_EX)
                return self

            def __exit__(self, exc_type, exc, tb):
                try:
                    if _fcntl is not None and self.fh is not None:
                        _fcntl.flock(self.fh.fileno(), _fcntl.LOCK_UN)
                finally:
                    try:
                        if self.fh:
                            self.fh.close()
                    except Exception:
                        pass

        return _Locker(self.lock_path)

    def _enforce(self) -> None:
        """Trim oldest lines to keep main file under configured cap and move trimmed lines to archive."""
        try:
            size = os.path.getsize(self.base_path)
        except Exception:
            return

        # Prefer bytes cap unless a line cap is explicitly configured
        if self.max_main_lines and self.max_main_lines > 0:
            self._enforce_by_lines(self.max_main_lines)
        elif self.max_main_bytes and size > self.max_main_bytes:
            to_remove = size - self.max_main_bytes
            self._trim_oldest_bytes_to_archive(to_remove)

    def _enforce_by_lines(self, keep_last_lines: int) -> None:
        try:
            # Count lines
            total = 0
            with open(self.base_path, "rb") as fh:
                for _ in fh:
                    total += 1
            if total <= keep_last_lines:
                return
            to_remove = total - keep_last_lines

            # Stream: first 'to_remove' lines -> archive; remainder -> temp; then replace
            self._stream_archive_and_tail(to_remove_lines=to_remove)
        except Exception:
            return

    def _trim_oldest_bytes_to_archive(self, remove_bytes: int) -> None:
        if remove_bytes <= 0:
            return
        # Best-effort: move enough whole lines to cover remove_bytes
        moved = 0
        try:
            seg_fh, _ = self._open_archive_for_append()
            try:
                tmp_path = self.base_path + ".tmp"
                with open(self.base_path, "rb") as src, open(tmp_path, "wb") as dst:
                    for line in src:
                        if moved < remove_bytes:
                            # ensure we rotate segment if needed
                            self._seg_write(seg_fh, line)
                            moved += len(line)
                        else:
                            dst.write(line)
                # Replace atomically
                os.replace(tmp_path, self.base_path)
            finally:
                try:
                    if seg_fh:
                        seg_fh.close()
                except Exception:
                    pass
        except Exception:
            return

    def _stream_archive_and_tail(self, to_remove_lines: int) -> None:
        if to_remove_lines <= 0:
            return
        removed = 0
        try:
            seg_fh, _ = self._open_archive_for_append()
            try:
                tmp_path = self.base_path + ".tmp"
                with open(self.base_path, "rb") as src, open(tmp_path, "wb") as dst:
                    for line in src:
                        if removed < to_remove_lines:
                            self._seg_write(seg_fh, line)
                            removed += 1
                        else:
                            dst.write(line)
                os.replace(tmp_path, self.base_path)
            finally:
                try:
                    if seg_fh:
                        seg_fh.close()
                except Exception:
                    pass
        except Exception:
            return

    # ----- archive segment helpers -----
    def _open_archive_for_append(self) -> Tuple[io.BufferedWriter, str]:
        """
        Return a file handle opened for appending to the current archive segment and the segment dir.
        Creates archive dir/segment as needed.
        """
        _ensure_dir(self.archive_dir)
        # Select latest timestamp directory or create new
        try:
            dirs = [d for d in os.listdir(self.archive_dir) if _is_ts_dir(d)]
        except Exception:
            dirs = []
        if dirs:
            dirs.sort()
            seg_dir = os.path.join(self.archive_dir, dirs[-1])
        else:
            seg_dir = os.path.join(self.archive_dir, _now_ts())
            _ensure_dir(seg_dir)

        # Archive filename mirrors base filename inside the segment directory
        arch_file = os.path.join(seg_dir, os.path.basename(self.base_path))
        _ensure_dir(os.path.dirname(arch_file))

        # If current segment overflows max, rotate to a new segment directory
        if self._segment_full(arch_file):
            seg_dir = os.path.join(self.archive_dir, _now_ts())
            _ensure_dir(seg_dir)
            arch_file = os.path.join(seg_dir, os.path.basename(self.base_path))

        fh = open(arch_file, "ab")
        return fh, seg_dir

    def _segment_full(self, arch_file: str) -> bool:
        try:
            size = os.path.getsize(arch_file)
        except Exception:
            size = 0
        # bytes-based check first
        if self.archive_segment_max_bytes and self.archive_segment_max_bytes > 0:
            if size >= self.archive_segment_max_bytes:
                return True
        # optional lines-based check
        if self.archive_segment_max_lines and self.archive_segment_max_lines > 0:
            try:
                cnt = 0
                with open(arch_file, "rb") as fh:
                    for _ in fh:
                        cnt += 1
                if cnt >= self.archive_segment_max_lines:
                    return True
            except Exception:
                return False
        return False

    def _seg_write(self, seg_fh: io.BufferedWriter, line: bytes) -> None:
        # Append line to current segment, rotate if segment crosses cap
        try:
            seg_fh.write(line)
            seg_fh.flush()
        except Exception:
            return
        # If segment now full, open a new one for subsequent writes
        try:
            if self._segment_full(seg_fh.name):  # type: ignore[attr-defined]
                seg_fh.close()
                new_fh, _ = self._open_archive_for_append()
                seg_fh.__dict__.update(new_fh.__dict__)  # swap internals (best-effort)
        except Exception:
            pass


# ---------- Logging handler integration ----------

import logging


class RollingJsonlHandler(logging.Handler):
    """
    logging.Handler that writes formatted JSON lines to a bounded rolling JSONL file.
    """
    def __init__(self, path: str) -> None:
        super().__init__(level=logging.INFO)
        self._writer = RollingJsonlWriter(path)
    def emit(self, record: logging.LogRecord) -> None:
        try:
            msg = self.format(record)
            self._writer.write_line(msg)
        except Exception:
            # Avoid crashing logging subsystem
            pass

class RollingZipJsonlHandler(logging.Handler):
    """
    logging.Handler that writes formatted JSON lines to a zip-spooled JSONL buffer.
    The buffer is truncated when it exceeds the configured threshold and compressed
    into a .zip archive adjacent to the buffer file (e.g., events.jsonl -> events.zip).
    """
    def __init__(self, path: str) -> None:
        super().__init__(level=logging.INFO)
        # Prefer zip spooler for bounded disk pressure
        self._writer = RollingZipJsonlWriter(path)  # type: ignore[name-defined]
    def emit(self, record: logging.LogRecord) -> None:
        try:
            msg = self.format(record)
            self._writer.write_line(msg)
        except Exception:
            # Avoid crashing logging subsystem
            pass

__all__ = ["RollingJsonlWriter", "RollingJsonlHandler", "RollingZipJsonlWriter", "RollingZipJsonlHandler"]
# ---------- Zip spool writer (optional) ----------
# Lightweight, void-faithful spooler that compresses the active JSONL buffer into a growing .zip
# once it exceeds a bounded threshold, then truncates the buffer. Keeps a tiny in-process ring
# for quick peeks. Uses RollingJsonlWriter's advisory lock to coordinate with other writers.
import zipfile as _zipfile  # stdlib

class RollingZipJsonlWriter:
    """
    Zip spooler for JSONL:
    - Appends lines to a small active buffer file (base_path)
    - When buffer exceeds max_buffer_bytes, compresses it into <base_name>.zip (append mode)
      under the same directory and truncates the buffer to zero
    - Tracks coarse stats for UI/status reporting (entries, sizes)
    - Thread-safe; coordinates with other processes via RollingJsonlWriter's lock
    """

    def __init__(
        self,
        base_path: str,
        *,
        max_buffer_bytes: int | None = None,
        ring_bytes: int | None = None,
        zip_path: str | None = None,
    ) -> None:
        self.base_path = os.path.abspath(base_path)
        _ensure_dir(os.path.dirname(self.base_path))
        # Defaults (env-overridable)
        try:
            if max_buffer_bytes is None:
                max_buffer_bytes = int(os.getenv("FUM_ZIP_BUFFER_BYTES", "1048576"))  # 1 MiB
        except Exception:
            max_buffer_bytes = 1_048_576
        try:
            if ring_bytes is None:
                ring_bytes = int(os.getenv("FUM_ZIP_RING_BYTES", "65536"))  # 64 KiB
        except Exception:
            ring_bytes = 65_536
        self.max_buffer_bytes = int(max(32 * 1024, max_buffer_bytes or 1_048_576))
        self._ring_cap = int(max(1024, ring_bytes or 65_536))
        self._ring = bytearray()
        # Zip path next to base_path (events.jsonl -> events.zip)
        if not zip_path:
            stem = os.path.splitext(os.path.basename(self.base_path))[0]
            zip_path = os.path.join(os.path.dirname(self.base_path), f"{stem}.zip")
        self.zip_path = os.path.abspath(zip_path)

        # Use RollingJsonlWriter with huge caps to avoid its archival path interfering
        # (we rely on the zip spool rotation below)
        self._writer = RollingJsonlWriter(
            self.base_path,
            max_main_bytes=10**12,              # effectively disable
            max_main_lines=None,
            archive_dir=os.path.join(os.path.dirname(self.base_path), "archived"),
            archive_segment_max_bytes=None,
            archive_segment_max_lines=None,
            check_every=2_147_483_647,          # effectively disable
        )
        self._zip_entries_cache: int | None = None
        self._local_lock = threading.Lock()

    def write_line(self, line: str) -> None:
        # Append line (delegates to rolling writer for atomic append)
        self._writer.write_line(line)
        # Update in-process ring
        try:
            data = (line.rstrip("\n") + "\n").encode("utf-8", errors="ignore")
            self._ring.extend(data)
            if len(self._ring) > self._ring_cap:
                # keep last _ring_cap bytes
                self._ring[:] = self._ring[-self._ring_cap:]
        except Exception:
            pass

        # Rotate to zip if buffer exceeds threshold (guarded by advisory lock)
        try:
            with self._writer._acquire_lock():  # type: ignore[attr-defined]
                try:
                    size = os.path.getsize(self.base_path)
                except Exception:
                    size = 0
                if size >= self.max_buffer_bytes:
                    # Read buffer
                    try:
                        with open(self.base_path, "rb") as fh:
                            buf = fh.read()
                    except Exception:
                        buf = b""
                    if buf:
                        arcname = f"{os.path.basename(self.base_path)}.{_now_ts()}.jsonl"
                        try:
                            with _zipfile.ZipFile(self.zip_path, mode="a", compression=_zipfile.ZIP_DEFLATED) as zf:
                                zf.writestr(arcname, buf)
                                # Seed entries cache if unknown
                                try:
                                    if self._zip_entries_cache is None:
                                        self._zip_entries_cache = 0
                                    self._zip_entries_cache += 1
                                except Exception:
                                    pass
                        except Exception:
                            # best-effort: do not abort rotation
                            pass
                        # Truncate buffer
                        try:
                            with open(self.base_path, "wb") as fh2:
                                fh2.write(b"")
                        except Exception:
                            pass
        except Exception:
            # best-effort; avoid throwing on contentions
            pass

    def stats(self) -> dict:
        """
        Return coarse spool statistics for status reporting:
          - buffer_bytes: size of active JSONL buffer
          - zip_bytes: size of the zip archive file
          - zip_entries: count of archived segments (approximate)
          - ring_bytes: size of in-process ring buffer
        """
        try:
            buffer_bytes = os.path.getsize(self.base_path)
        except Exception:
            buffer_bytes = 0
        try:
            zip_bytes = os.path.getsize(self.zip_path)
        except Exception:
            zip_bytes = 0
        # If entries unknown, estimate by inspecting the zip once
        if self._zip_entries_cache is None:
            try:
                if os.path.exists(self.zip_path):
                    with _zipfile.ZipFile(self.zip_path, mode="r") as zf:
                        self._zip_entries_cache = len(zf.namelist())
                else:
                    self._zip_entries_cache = 0
            except Exception:
                self._zip_entries_cache = 0
        return {
            "buffer_bytes": int(buffer_bytes),
            "zip_bytes": int(zip_bytes),
            "zip_entries": int(self._zip_entries_cache or 0),
            "ring_bytes": int(len(self._ring)),
        }

# expose in module exports
try:
    __all__.append("RollingZipJsonlWriter")  # type: ignore[name-defined]
except Exception:
    __all__ = ["RollingJsonlWriter", "RollingJsonlHandler", "RollingZipJsonlWriter"]]]></content>
    </file>
    <file>
      <path>io/maps_ring.py</path>
      <content><![CDATA["""
Compatibility shim for visualization ring buffer.

Deprecated: import from 'fum_rt.io.visualization.maps_ring' instead.

Kept for transitional period to avoid breaking existing imports:
    from fum_rt.io.maps_ring import MapsRing, MapsFrame
"""

from __future__ import annotations

from fum_rt.io.visualization.maps_ring import MapsRing, MapsFrame

__all__ = ["MapsRing", "MapsFrame"]]]></content>
    </file>
    <file>
      <path>io/sensors/__init__.py</path>
      <content/>
    </file>
    <file>
      <path>io/sensors/auditory.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/sensors/somatosensory.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/sensors/symbols.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/sensors/vision.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>io/utd.py</path>
      <content><![CDATA[import sys, json, os
from fum_rt.io.logging.rolling_jsonl import RollingJsonlWriter
try:
    # Prefer zip spooler when available
    from fum_rt.io.logging.rolling_jsonl import RollingZipJsonlWriter  # type: ignore
except Exception:
    RollingZipJsonlWriter = None  # type: ignore

class UTD:
    """Universal Transduction Decoder.
    Emits opportunistic outputs (stdout + file sink) and supports a simple macro board.

    API
    - emit_text(payload: dict, score: float=1.0)
    - register_macro(name: str, meta: dict | None=None) -> bool
    - list_macros() -> list[str]
    - emit_macro(name: str, args: dict | None=None, score: float=1.0)
    """
    def __init__(self, run_dir: str):
        self.run_dir = run_dir
        os.makedirs(self.run_dir, exist_ok=True)
        self.path = os.path.join(self.run_dir, 'utd_events.jsonl')
        # Prefer zip-spooled writer to bound disk pressure; fallback to rolling JSONL
        use_zip = True
        try:
            # Allow explicit opt-out via env
            use_zip = str(os.getenv("FUM_ZIP_SPOOL", "1")).strip().lower() in ("1", "true", "yes", "on", "y")
        except Exception:
            use_zip = True
        try:
            if use_zip and RollingZipJsonlWriter is not None:  # type: ignore
                self._writer = RollingZipJsonlWriter(self.path)  # type: ignore
            else:
                self._writer = RollingJsonlWriter(self.path)
        except Exception:
            # Safe fallback
            self._writer = RollingJsonlWriter(self.path)
        # Macro registry and on-disk macro board for persistence
        self._macro_registry = {}
        self._macro_board_path = os.path.join(self.run_dir, 'macro_board.json')
        # Eager-load persisted macro board if present
        try:
            if os.path.exists(self._macro_board_path):
                with open(self._macro_board_path, 'r', encoding='utf-8') as fh:
                    reg = json.load(fh)
                    if isinstance(reg, dict):
                        for name, meta in reg.items():
                            self._macro_registry[str(name)] = meta if isinstance(meta, dict) else {}
        except Exception:
            # do not fail runtime if file is corrupt
            pass

    def register_macro(self, name: str, meta: dict | None=None) -> bool:
        """Register a macro key with optional metadata; idempotent. Persists to macro_board.json."""
        try:
            self._macro_registry[name] = meta or {}
            try:
                self._persist_macro_board()
            except Exception:
                pass
            return True
        except Exception:
            return False

    def list_macros(self):
        """List available macro keys."""
        try:
            return sorted(self._macro_registry.keys())
        except Exception:
            return []

    def _persist_macro_board(self):
        """Write macro registry to run_dir/macro_board.json."""
        try:
            with open(self._macro_board_path, 'w', encoding='utf-8') as fh:
                json.dump(self._macro_registry, fh, ensure_ascii=False, indent=2)
        except Exception:
            pass

    def emit_text(self, payload: dict, score: float=1.0):
        rec = {'type': 'text', 'payload': payload, 'score': float(score)}
        print("[UTD] text:", payload, f"(score={score:.3f})")
        try:
            line = json.dumps(rec, ensure_ascii=False)
            self._writer.write_line(line)
        except Exception:
            # keep stdout emission even if file writing fails
            pass

    def emit_macro(self, name: str, args: dict | None=None, score: float=1.0):
        """
        Emit a macro event. If the macro key is not registered, auto-register it
        (and persist to macro_board.json) to avoid breaking the runtime.
        """
        if name not in self._macro_registry:
            # go through register path so persistence occurs
            self.register_macro(name, {})
        rec = {'type': 'macro', 'macro': name, 'args': (args or {}), 'score': float(score)}
        print(f"[UTD] macro:{name}", (args or {}), f"(score={score:.3f})")
        try:
            line = json.dumps(rec, ensure_ascii=False)
            self._writer.write_line(line)
        except Exception:
            # keep stdout emission even if file writing fails
            pass

    def close(self):
        try:
            try:
                self._persist_macro_board()
            except Exception:
                pass
            # RollingJsonlWriter does not keep a persistent file handle; nothing to close.
        except Exception:
            pass
]]></content>
    </file>
    <file>
      <path>io/ute.py</path>
      <content><![CDATA[
import sys, time, queue, threading, os, json

class UTE:
    """Universal Temporal Encoder.
    Feeds inbound messages into a queue the Nexus can poll every tick.
    Sources implemented: stdin (lines) and synthetic 'tick' generator.
    """
    def __init__(self, use_stdin=True, inbox_path=None):
        self.q = queue.Queue(maxsize=1024)
        self._stop = threading.Event()
        self.use_stdin = use_stdin
        self._threads = []
        # Optional run-local chat inbox (JSONL), e.g. runs/<ts>/chat_inbox.jsonl
        self.inbox_path = inbox_path
        self._inbox_size = 0

    def start(self):
        if self.use_stdin:
            t = threading.Thread(target=self._stdin_reader, daemon=True)
            t.start()
            self._threads.append(t)
        # Optional chat inbox tailer
        if self.inbox_path:
            t3 = threading.Thread(target=self._inbox_reader, daemon=True)
            t3.start()
            self._threads.append(t3)
        # Always run a synthetic ticker as a heartbeat
        t2 = threading.Thread(target=self._ticker, daemon=True)
        t2.start()
        self._threads.append(t2)

    def stop(self):
        self._stop.set()

    def _stdin_reader(self):
        for line in sys.stdin:
            if self._stop.is_set(): break
            line = line.strip()
            if line:
                self.q.put({'type': 'text', 'msg': line})

    def _inbox_reader(self):
        # Tail a JSONL chat inbox file (appended by dashboard/chat UI)
        while not self._stop.is_set():
            try:
                path = self.inbox_path
                if not path or not os.path.exists(path):
                    time.sleep(0.5)
                    continue
                size = os.path.getsize(path)
                # handle truncation/rotation
                if size < self._inbox_size:
                    self._inbox_size = 0
                if size == self._inbox_size:
                    time.sleep(0.5)
                    continue
                with open(path, "rb") as f:
                    f.seek(self._inbox_size)
                    data = f.read(size - self._inbox_size)
                self._inbox_size = size
                text = data.decode("utf-8", errors="ignore")
                for line in text.splitlines():
                    s = line.strip()
                    if not s:
                        continue
                    try:
                        rec = json.loads(s)
                    except Exception:
                        rec = {"type": "text", "msg": s}
                    if isinstance(rec, dict):
                        if rec.get("type") == "text" and "msg" in rec:
                            self.q.put({"type": "text", "msg": str(rec.get("msg"))})
                        else:
                            # Allow passthrough of structured events if provided
                            self.q.put(rec)
            except Exception:
                # Keep runtime alive on any error
                time.sleep(0.5)

    def _ticker(self):
        # 1 Hz ticker (used as heartbeat input)
        while not self._stop.is_set():
            self.q.put({'type':'tick', 'msg':'tick'})
            time.sleep(1.0)

    def poll(self, max_items=32):
        out = []
        while len(out) < max_items:
            try:
                out.append(self.q.get_nowait())
            except queue.Empty:
                break
        return out
]]></content>
    </file>
    <file>
      <path>io/visualization/__init__.py</path>
      <content><![CDATA["""
Visualization transport primitives.

- maps_ring: bounded, drop-oldest ring buffer for maps/frame payloads
- websocket_server: bounded WebSocket forwarder for maps frames
"""
from .maps_ring import MapsRing, MapsFrame  # re-export
from .websocket_server import MapsWebSocketServer  # re-export]]></content>
    </file>
    <file>
      <path>io/visualization/maps_ring.py</path>
      <content><![CDATA["""
Maps frames ring buffer (drop-oldest, thread-safe, void-faithful).

Canonical location: fum_rt.io.visualization.maps_ring

Purpose
- Provide a tiny, bounded ring for maps frames (header+payload) with drop-oldest semantics.
- Decouples producers (telemetry/core engine) from consumers (UI/websocket) without scans.
- O(1) amortized operations; no full-buffer copies; copies only payload bytes as provided.

Contract
- Frame header schema is producer-defined; commonly:
  {topic, ver?, tick, n, shape, channels, dtype, endianness, stats, ...}
- Payload is a bytes-like buffer; typically planar blocks (e.g., Float32 LE: heat|exc|inh).

Usage
- nx._maps_ring = MapsRing(capacity=int(os.getenv("MAPS_RING", 3)))
- Producer: nx._maps_ring.push(tick, header, payload)
- Consumer: ring.latest(), ring.drain(max_items)

Security / Backpressure
- Always drops the oldest on overflow.
- Readers can choose to read only latest() to avoid client backlog.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional
import threading


@dataclass(frozen=True)
class MapsFrame:
    tick: int
    header: Dict[str, Any]
    payload: bytes
    seq: int  # monotonically increasing sequence id


class MapsRing:
    """
    Bounded, thread-safe ring buffer for maps frames.

    - push(): appends a frame, dropping the oldest when at capacity.
    - latest(): returns the newest frame or None.
    - drain(max_items): returns up to max_items frames from oldest to newest.
      Use latest() if you only need the most recent frame to minimize bandwidth.
    """

    __slots__ = ("capacity", "_lock", "_buf", "_seq", "_drop_count")

    def __init__(self, capacity: int = 3) -> None:
        self.capacity = max(1, int(capacity))
        self._lock = threading.Lock()
        self._buf: List[MapsFrame] = []
        self._seq = 0
        self._drop_count = 0

    def push(self, tick: int, header: Dict[str, Any], payload: bytes) -> int:
        """
        Append a frame; drop oldest on overflow.
        Returns the sequence id assigned to the inserted frame.
        """
        if not isinstance(payload, (bytes, bytearray, memoryview)):
            # Normalize to bytes once (producers should pass bytes already)
            try:
                payload = bytes(payload)  # type: ignore[assignment]
            except Exception:
                payload = b""
        with self._lock:
            self._seq += 1
            f = MapsFrame(tick=int(tick), header=dict(header or {}), payload=bytes(payload), seq=self._seq)
            if len(self._buf) >= self.capacity:
                self._buf.pop(0)
                self._drop_count += 1
            self._buf.append(f)
            return f.seq

    def latest(self) -> Optional[MapsFrame]:
        with self._lock:
            if not self._buf:
                return None
            return self._buf[-1]

    def drain(self, max_items: Optional[int] = None) -> List[MapsFrame]:
        """
        Return up to max_items frames in order (oldest..newest).
        Does not mutate the ring (non-destructive view); consumers should track seq.
        """
        with self._lock:
            if not self._buf:
                return []
            if max_items is None or max_items <= 0:
                return list(self._buf)
            return list(self._buf[-int(max_items):])

    def size(self) -> int:
        with self._lock:
            return len(self._buf)

    def dropped(self) -> int:
        """
        Returns the number of frames dropped due to overflow since creation.
        """
        with self._lock:
            return int(self._drop_count)

    def __len__(self) -> int:
        return self.size()


__all__ = ["MapsRing", "MapsFrame"]]]></content>
    </file>
    <file>
      <path>io/visualization/websocket_server.py</path>
      <content><![CDATA["""
Maps frames WebSocket forwarder (bounded, drop-oldest, void-faithful).

Canonical location: fum_rt.io.visualization.websocket_server

Purpose
- Serve UI consumers with the latest maps/frame payload from a bounded ring.
- Backpressure-safe: each client receives only the newest frame; old frames are dropped.
- Local-first: defaults to 127.0.0.1 binding; configurable via args/env.

Dependencies
- Optional: 'websockets' Python package (asyncio-based). If unavailable, this module is inert.

Env (defaults shown)
- MAPS_FPS=10                # >0 = limit; 0 = off; <0 = unlimited (tests/bench); sends at most this many frames per second when >0
- WS_MAX_CONN=2              # maximum concurrent WebSocket clients
- WS_ALLOW_ORIGIN=           # comma-separated origins; if empty, all origins allowed

Transport format
- Two-message sequence per frame:
  1) Text frame: JSON dump of header dict (augmented with dtype/ver/quant/etc. by producer)
  2) Binary frame: raw payload bytes (u8 or f32 LE as dictated by header['dtype'])

Notes
- This module does not mutate frames; it forwards exactly what producers pushed to the ring.
- For RGB visualization, typical mapping is RGB = [exc, heat, inh] client-side.
"""

from __future__ import annotations

import asyncio
import json
import os
import threading
from typing import Any, Dict, Optional, Set, Callable

try:
    import websockets  # type: ignore
    from websockets.server import WebSocketServerProtocol  # type: ignore
except Exception:  # pragma: no cover
    websockets = None
    WebSocketServerProtocol = object  # type: ignore

from fum_rt.io.visualization.maps_ring import MapsRing, MapsFrame


class MapsWebSocketServer:
    """
    Bounded WebSocket forwarder for maps frames.

    Usage:
      ring = MapsRing(capacity=3)
      ws = MapsWebSocketServer(ring, host="127.0.0.1", port=8888)
      ws.start()
      ...
      ws.stop()
    """

    __slots__ = (
        "ring",
        "host",
        "port",
        "max_conn",
        "allow_origins",
        "fps",
        "_running",
        "_thread",
        "_clients",
        "_loop",
        "_server",
        "_last_seq_sent",
        "_on_error",
    )

    def __init__(
        self,
        ring: MapsRing,
        host: str = "127.0.0.1",
        port: int = 8765,
        *,
        max_conn: Optional[int] = None,
        allow_origins: Optional[str] = None,
        fps: Optional[float] = None,
        on_error: Optional[Callable[[str], None]] = None,
    ) -> None:
        self.ring = ring
        self.host = str(host)
        self.port = int(port)
        try:
            self.max_conn = int(max_conn if max_conn is not None else os.getenv("WS_MAX_CONN", "2"))
        except Exception:
            self.max_conn = 2
        # Comma-separated origins string or None for any
        self.allow_origins = str(allow_origins) if allow_origins is not None else os.getenv("WS_ALLOW_ORIGIN", "")
        try:
            self.fps = float(fps if fps is not None else os.getenv("MAPS_FPS", "10"))
        except Exception:
            self.fps = 10.0
        self._running = False
        self._thread: Optional[threading.Thread] = None
        self._clients: Set[WebSocketServerProtocol] = set()
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._server = None
        self._last_seq_sent: int = 0
        self._on_error = on_error

    # ---- Public API ----

    def start(self) -> None:
        """
        Start the WebSocket server in a background thread. No-op if 'websockets' is missing.
        """
        if websockets is None:
            self._report_error("websocket_server_start_failed: websockets package not installed")
            return
        if self._running:
            return
        self._running = True
        self._thread = threading.Thread(target=self._run_loop, name="maps_ws_server", daemon=True)
        self._thread.start()

    def stop(self) -> None:
        """
        Stop the server and wait for the background thread to exit.
        """
        if not self._running:
            return
        self._running = False
        try:
            if self._loop is not None:
                asyncio.run_coroutine_threadsafe(self._shutdown_async(), self._loop).result(timeout=2.0)
        except Exception:
            pass
        try:
            if self._thread is not None:
                self._thread.join(timeout=2.0)
        except Exception:
            pass
        self._thread = None

    # ---- Internal ----

    def _report_error(self, msg: str) -> None:
        try:
            if self._on_error:
                self._on_error(msg)
            else:
                print("[maps_ws] " + msg, flush=True)
        except Exception:
            pass

    def _run_loop(self) -> None:
        try:
            loop = asyncio.new_event_loop()
            self._loop = loop
            asyncio.set_event_loop(loop)
            loop.run_until_complete(self._start_async())
            loop.run_forever()
        except Exception as e:
            self._report_error(f"websocket_server_loop_error: {e}")
        finally:
            try:
                if self._server is not None:
                    loop = self._loop or asyncio.get_event_loop()
                    loop.run_until_complete(self._shutdown_async())
            except Exception:
                pass

    async def _start_async(self) -> None:
        assert websockets is not None
        origins = None
        if self.allow_origins:
            try:
                origins = [o.strip() for o in self.allow_origins.split(",") if o.strip()]
                if not origins:
                    origins = None
            except Exception:
                origins = None

        self._server = await websockets.serve(  # type: ignore
            self._ws_handler,
            self.host,
            self.port,
            max_size=2**20,  # 1 MiB per message should suffice for control
            max_queue=1,     # backpressure: queue at most one message per client
            ping_interval=20,
            ping_timeout=20,
            origins=origins,
        )

        # Broadcaster loop
        asyncio.create_task(self._broadcast_loop())

    async def _shutdown_async(self) -> None:
        try:
            # Close all clients
            for ws in list(self._clients):
                try:
                    await ws.close()
                except Exception:
                    pass
            self._clients.clear()
        except Exception:
            pass
        try:
            if self._server is not None:
                self._server.close()
                await self._server.wait_closed()
        except Exception:
            pass
        try:
            loop = asyncio.get_event_loop()
            loop.stop()
        except Exception:
            pass

    async def _ws_handler(self, websocket: WebSocketServerProtocol, path: str) -> None:  # type: ignore[override]
        # Enforce max connections
        try:
            if len(self._clients) >= max(1, self.max_conn):
                await websocket.close(code=1013, reason="server_overload")  # Try again later
                return
        except Exception:
            pass

        self._clients.add(websocket)
        try:
            # Initial latest send to prime client
            await self._send_latest(websocket)
            # Then just keep the connection alive until client disconnects; no per-client loop needed
            # since broadcast loop handles sending updates to all clients.
            await websocket.wait_closed()
        except Exception:
            pass
        finally:
            try:
                self._clients.remove(websocket)
            except Exception:
                pass

    async def _broadcast_loop(self) -> None:
        # Send at most one frame per fps interval; drop-oldest by only ever sending the latest frame
        try:
            fps = float(self.fps)
        except Exception:
            fps = 10.0
        if fps < 0:
            interval = 0.0  # unlimited
        elif fps == 0:
            interval = None  # disabled
        else:
            interval = 1.0 / max(0.001, fps)

        while self._running:
            try:
                if interval is None:
                    # Emission disabled: avoid spin
                    await asyncio.sleep(0.1)
                    continue
                if not self._clients:
                    # Avoid unnecessary ring access when there are no clients
                    await asyncio.sleep(interval if interval > 0 else 0.1)
                    continue
                fr = self.ring.latest()
                if fr is not None and fr.seq != self._last_seq_sent:
                    # Broadcast header (text) then payload (binary)
                    await self._broadcast_frame(fr)
                    self._last_seq_sent = fr.seq
                # For unlimited (interval==0), yield to event loop without sleeping
                if interval > 0:
                    await asyncio.sleep(interval)
                else:
                    await asyncio.sleep(0.0)
            except Exception:
                # Keep server resilient
                await asyncio.sleep(0.05)

    async def _broadcast_frame(self, fr: MapsFrame) -> None:
        dead: Set[WebSocketServerProtocol] = set()
        # Serialize header once
        try:
            hdr_text = json.dumps(fr.header, separators=(",", ":"), ensure_ascii=False)
        except Exception:
            # Fallback minimal header
            hdr_text = json.dumps({"topic": "maps/frame", "tick": int(fr.tick)}, separators=(",", ":"))
        for ws in list(self._clients):
            try:
                await ws.send(hdr_text)     # text frame
                await ws.send(fr.payload)   # binary frame
            except Exception:
                dead.add(ws)
        # Purge disconnected clients
        for ws in dead:
            try:
                self._clients.remove(ws)
            except Exception:
                pass

    async def _send_latest(self, ws: WebSocketServerProtocol) -> None:
        fr = self.ring.latest()
        if fr is None:
            return
        try:
            hdr_text = json.dumps(fr.header, separators=(",", ":"), ensure_ascii=False)
        except Exception:
            hdr_text = json.dumps({"topic": "maps/frame", "tick": int(fr.tick)}, separators=(",", ":"))
        try:
            await ws.send(hdr_text)
            await ws.send(fr.payload)
        except Exception:
            pass


__all__ = ["MapsWebSocketServer"]]]></content>
    </file>
    <file>
      <path>nexus.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

# NOTE: This module acts as a thin façade over the runtime and core layers.
# Behavior is preserved; external imports remain stable. Core loop has moved to [loop.run_loop()](fum_rt/runtime/loop.py:40).
import time, os, sys

# Deprecation notice (documentation-only):
# - Inline orchestrator logic inside Nexus is deprecated; equivalent functionality lives in runtime/*.
# - External integrations continue to import Nexus and make_parser from this module (no change required).
# - No functional changes: IDF remains composer/telemetry-only; SIE/ADC/connectome unaffected.
# - Event-driven metrics are enabled by default (telemetry-only); disable via ENABLE_EVENT_METRICS=0.
# - Void cold scouts are enabled by default (telemetry-only); disable via ENABLE_COLD_SCOUTS=0.
from collections import deque
from .utils.logging_setup import get_logger
from .io.ute import UTE
from .io.utd import UTD
# emitters moved to runtime.emitters.initialize_emitters
from .core import text_utils
from .core.metrics import StreamingZEMA
from .core.visualizer import Visualizer
from .core.void_dynamics_adapter import get_domain_modulation
from .core.fum_sie import SelfImprovementEngine
from .core.bus import AnnounceBus
from .core.adc import ADC
# Modularized lexicon/phrase store (behavior-preserving)
from .io.lexicon.store import (
    load_phrase_templates as _lxn_load_phrases,
    load_lexicon as _lxn_load,
    save_lexicon as _lxn_save,
)
from .runtime.telemetry import macro_why_base as _telemetry_why_base
# Event-driven metrics seam (feature-flagged; pure core + adapter)
from .core.proprioception.events import EventDrivenMetrics as _EvtMetrics
from .runtime.emitters import initialize_emitters as _init_emitters
from .runtime.helpers import register_macro_board as _reg_macro_board, maybe_load_engram as _maybe_load_engram, derive_start_step as _derive_start_step
from .runtime.loop import run_loop as _run_loop
# Cognition seams (Phase 3 move-only; behavior-preserving)
from .io.cognition.stimulus import symbols_to_indices as _stim_symbols_to_indices
from .io.cognition.composer import compose_say_text as _compose_say_text_impl
# Core signals seam (B1 detector apply)
try:
    from .core.control_server import ControlServer  # optional UI
except Exception:
    ControlServer = None
class Nexus:
    def __init__(self, run_dir: str, N:int=1000, k:int=12, hz:int=10,
                 domain:str='biology_consciousness', use_time_dynamics:bool=True,
                 viz_every:int=10, log_every:int=1, checkpoint_every:int=0, seed:int=0,
                 sparse_mode:bool=False, threshold:float=0.15, lambda_omega:float=0.1,
                 candidates:int=64, walkers:int=256, hops:int=3, status_interval:int=1,
                 bundle_size:int=3, prune_factor:float=0.10,
                 speak_auto:bool=True, speak_z:float=1.0, speak_hysteresis:float=1.0,
                 speak_cooldown_ticks:int=10, speak_valence_thresh:float=0.01,
                 b1_half_life_ticks:int=50,
                 bus_capacity:int=65536, bus_drain:int=2048,
                 r_attach:float=0.25, ttl_init:int=120, split_patience:int=6,
                 stim_group_size:int=4, stim_amp:float=0.05, stim_decay:float=0.90, stim_max_symbols:int=64,
                 checkpoint_format:str="h5", checkpoint_keep:int=5, load_engram_path:str=None,
                 start_control_server:bool=False, emergent_macros:bool=False):
        self.run_dir = run_dir
        self.N = N
        self.k = k
        self.hz = hz
        self.dt = 1.0 / max(1, hz)
        self.domain = domain
        self.use_time_dynamics = use_time_dynamics
        self.viz_every = viz_every
        self.log_every = log_every
        self.checkpoint_every = checkpoint_every
        self.seed = seed
        self.emergent_macros = bool(emergent_macros)

        os.makedirs(self.run_dir, exist_ok=True)
        self.logger = get_logger("nexus", os.path.join(self.run_dir, "events.jsonl"))
        inbox_path = os.path.join(self.run_dir, "chat_inbox.jsonl")
        self.ute = UTE(use_stdin=True, inbox_path=inbox_path)
        self.utd = UTD(self.run_dir)
        # Macro/Thought emitters (delegated)
        self.emitter, self.thoughts = _init_emitters(self.utd, self.run_dir, why_provider=lambda: self._emit_why())
        # Start local control server only when requested (default: off)
        self._control_server = None
        if bool(start_control_server) and ControlServer is not None:
            try:
                self._control_server = ControlServer(self.run_dir)
                try:
                    self.logger.info("control_server_started", extra={"extra": {"url": getattr(self._control_server, "url", "")}})
                except Exception:
                    pass
            except Exception:
                self._control_server = None
        # Macro board: minimal defaults + optional JSON registry (delegated)
        try:
            _reg_macro_board(self.utd, self.run_dir)
        except Exception:
            pass

        # Phrase templates for 'say' macro and persistent lexicon (for richer sentences)
        self._phrase_templates = []
        try:
            self._phrase_templates = list(_lxn_load_phrases(self.run_dir) or [])
        except Exception:
            # Fail-soft: keep empty; store mirrors legacy behavior
            pass
        # 3) Persistent lexicon (word -> count), learned from inbound text and emissions
        try:
            self._lexicon_path = os.path.join(self.run_dir, 'lexicon.json')
            lx, dc = _lxn_load(self.run_dir)
            self._lexicon = dict(lx or {})
            self._doc_count = int(dc or 0)
        except Exception:
            # Fail-soft: empty lexicon
            self._lexicon = {}
            self._doc_count = 0
            pass

        # N-gram stores for emergent sentence composition (learned from inputs/outputs)
        self._ng2 = {}  # bigram: w1 -> {w2: count}
        self._ng3 = {}  # trigram: (w1,w2) -> {w3: count}

        # Sparse-first backend policy (void-faithful, no scans):
        # Runtime uses SparseConnectome by default. Dense is validation-only via FORCE_DENSE=1.
        use_dense = str(os.getenv("FORCE_DENSE", "0")).strip().lower() in ("1", "true", "yes", "on", "y", "t")

        # Keep 'sparse_mode' arg for compatibility but ignore it; log once.
        if sparse_mode:
            try:
                self.logger.info("deprecated_arg_sparse_mode_ignored", extra={"extra": {"arg": bool(sparse_mode)}})
            except Exception:
                pass

        # Select implementation with dynamic import to avoid accidental dense usage
        try:
            if use_dense:
                from fum_rt.core.connectome import Connectome as _Conn  # validation-only
            else:
                from fum_rt.core.sparse_connectome import SparseConnectome as _Conn
        except Exception:
            # Fallback to sparse on any import failure
            from fum_rt.core.sparse_connectome import SparseConnectome as _Conn

        # Instantiate connectome (both backends accept the same constructor args here)
        try:
            self.connectome = _Conn(
                N=self.N, k=self.k, seed=self.seed,
                threshold=threshold, lambda_omega=lambda_omega,
                candidates=candidates, traversal_walkers=walkers, traversal_hops=hops,
                bundle_size=bundle_size, prune_factor=prune_factor
            )
            if use_dense:
                try:
                    self.logger.info("backend_dense_forced", extra={"extra": {"reason": "FORCE_DENSE"}})
                except Exception:
                    pass
        except Exception:
            # Ensure we have a working sparse connectome if constructor failed
            from fum_rt.core.sparse_connectome import SparseConnectome as _SConn
            self.connectome = _SConn(
                N=self.N, k=self.k, seed=self.seed,
                threshold=threshold, lambda_omega=lambda_omega,
                candidates=candidates, traversal_walkers=walkers, traversal_hops=hops,
                bundle_size=bundle_size, prune_factor=prune_factor
            )
        # Load engram if provided (after backend selection)
        # Defer engram loading until after ADC is initialized to avoid spurious errors/logs.
        # The actual load (with logging) happens below after ADC is constructed.
        self.vis = Visualizer(run_dir=self.run_dir)
        # Status emission cadence for UTD
        self.status_every = max(1, int(status_interval))
        # Self-Improvement Engine (Rule 3): produces signed total_reward and legacy valence_01
        self.sie = SelfImprovementEngine(self.N)
        # Engram persistence config
        self.checkpoint_every = int(checkpoint_every)
        self.checkpoint_format = str(checkpoint_format).lower()
        self.checkpoint_keep = int(max(0, checkpoint_keep))
        # Text stimulus wiring for symbol→group activation
        self.stim_group_size = int(max(1, stim_group_size))
        self.stim_amp = float(stim_amp)
        self.stim_max_symbols = int(max(1, stim_max_symbols))
        try:
            if hasattr(self.connectome, "_stim_decay"):
                self.connectome._stim_decay = float(stim_decay)
        except Exception:
            pass

        # Self-speak configuration and topology spike detector (tick-based)
        self.speak_auto = bool(speak_auto)
        self.speak_valence_thresh = float(speak_valence_thresh)
        # Persist half-life for void_b1 meter to keep UX consistent with detector
        # Allow environment overrides to reduce inertia without changing CLI args
        try:
            import os as _os_b1
            _b1_hl_env = _os_b1.getenv("B1_HALF_LIFE_TICKS", None)
            if _b1_hl_env is not None:
                b1_half_life_ticks = int(_b1_hl_env)
        except Exception:
            pass
        try:
            import os as _os_hys
            _b1_hys_env = _os_hys.getenv("B1_HYSTERESIS", None)
            if _b1_hys_env is not None:
                speak_hysteresis = float(_b1_hys_env)
        except Exception:
            pass
        self.b1_half_life_ticks = int(max(1, b1_half_life_ticks))
        self.b1_detector = StreamingZEMA(
            half_life_ticks=self.b1_half_life_ticks,
            z_spike=float(speak_z),
            hysteresis=float(speak_hysteresis),
            min_interval_ticks=int(max(1, speak_cooldown_ticks)),
        )
        # Optional event-driven metrics aggregator (disabled by default; parity preserved)
        self._evt_metrics = None
        try:
            if str(os.getenv("ENABLE_EVENT_METRICS", "0")).lower() in ("1", "true", "yes", "on"):
                self._evt_metrics = _EvtMetrics(
                    z_half_life_ticks=self.b1_half_life_ticks,
                    z_spike=float(speak_z),
                    hysteresis=float(speak_hysteresis),
                    seed=int(self.seed),
                )
        except Exception:
            self._evt_metrics = None
        # External control plane: phase file and cache (void-faithful: gates only)
        self.phase_file = os.path.join(self.run_dir, "phase.json")
        self._phase = {"phase": 0}
        self._phase_mtime = None
        # Novelty rarity gain (tunable via phase.json under "sie": {"novelty_idf_gain": ...})
        self.novelty_idf_gain = 1.0

        # Announcement bus + ADC (void-walker observations -> incremental map)
        self.bus = AnnounceBus(capacity=int(max(1, bus_capacity)))
        self.bus_drain = int(max(1, bus_drain))
        self.adc = ADC(r_attach=float(r_attach), ttl_init=int(ttl_init), split_patience=int(split_patience))
        # Attach bus to connectome so walkers can publish Observation events
        try:
            self.connectome.bus = self.bus
        except Exception:
            pass

        # If an engram path was provided earlier and ADC is now available, reload including ADC
        try:
            _maybe_load_engram(self, load_engram_path)
        except Exception:
            pass
        # Derive starting step to continue numbering after resume and avoid retention deleting new snapshots
        try:
            self.start_step = int(_derive_start_step(self, load_engram_path))
            try:
                self.logger.info("resume_step", extra={"extra": {"start_step": int(self.start_step)}})
            except Exception:
                pass
        except Exception:
            self.start_step = 0
        self.dom_mod = float(get_domain_modulation(self.domain))
        self.history = []
        # Emitter context (read-only snapshot for why providers)
        self._emit_step = 0
        self._emit_last_metrics = {}
        self._macros_smoke_done = False
        self._thoughts_smoke_done = False
        # Rolling buffer of recent inbound text for composing human-friendly “say” content
        self.recent_text = deque(maxlen=256)
        # Track vt_entropy over time for SIE TD proxy (void-native signal)
        self._prev_vt_entropy = None
        self._last_vt_entropy = None

    def _symbols_to_indices(self, text, reverse_map=None):
        """
        Deterministic, stateless symbol→group mapping.

        Delegates to io.cognition.stimulus.symbols_to_indices (behavior-preserving).
        """
        try:
            return _stim_symbols_to_indices(
                str(text),
                int(getattr(self, "stim_group_size", 4)),
                int(getattr(self, "stim_max_symbols", 64)),
                int(self.N),
                reverse_map=reverse_map,
            )
        except Exception:
            return []

    def _update_lexicon_and_ngrams(self, text: str):
        try:
            if not hasattr(self, "_lexicon"): self._lexicon = {}
            toks = text_utils.tokenize_text(text)
            # Document-frequency semantics: increment once per message per token
            for w in set(toks):
                self._lexicon[w] = int(self._lexicon.get(w, 0)) + 1
            # Update streaming n-grams for emergent composition
            text_utils.update_ngrams(toks, self._ng2, self._ng3)
        except Exception: pass

    def _save_lexicon(self):
        try:
            _lxn_save(self.run_dir, getattr(self, "_lexicon", {}) or {}, int(getattr(self, "_doc_count", 0)))
        except Exception:
            pass

    def _compose_say_text(self, metrics: dict, step: int, seed_tokens: set = None) -> str:
        """
        Compose a short sentence using emergent language or templates.

        Delegates to io.cognition.composer.compose_say_text (behavior-preserving).
        """
        try:
            return _compose_say_text_impl(
                metrics or {},
                int(step),
                getattr(self, "_lexicon", {}) or {},
                getattr(self, "_ng2", {}) or {},
                getattr(self, "_ng3", {}) or {},
                self.recent_text,
                templates=list(getattr(self, "_phrase_templates", []) or []),
                seed_tokens=seed_tokens,
            ) or ""
        except Exception:
            return ""

    def _emit_why(self):
        """
        Provide context for MacroEmitter / ThoughtEmitter from the last computed metrics.
        Read-only; never mutates model state.
        """
        try:
            m = getattr(self, "_emit_last_metrics", {}) or {}
            step = int(getattr(self, "_emit_step", 0))
            return _telemetry_why_base(self, m, step)
        except Exception:
            try:
                return {"t": int(getattr(self, "_emit_step", 0)), "phase": int(getattr(self, "_phase", {}).get("phase", 0))}
            except Exception:
                return {"t": 0, "phase": 0}

        # --- Phase control plane (file-driven) ---------------------------------
    def _default_phase_profiles(self):
        from .runtime.phase import default_phase_profiles as _default_phase_profiles
        return _default_phase_profiles()

    def _apply_phase_profile(self, prof: dict):
        from .runtime.phase import apply_phase_profile as _apply_phase_profile_impl
        return _apply_phase_profile_impl(self, prof)
    def _poll_control(self):
        from .runtime.phase import poll_control as _poll_control_impl
        return _poll_control_impl(self)
    
    def run(self, duration_s:int=None):
        self.ute.start()
        self.logger.info("nexus_started", extra={"extra": {"N": self.N, "k": self.k, "hz": self.hz, "domain": self.domain, "dom_mod": self.dom_mod}})
        try:
            self.logger.info("checkpoint_config", extra={"extra": {"every": int(getattr(self, "checkpoint_every", 0)), "keep": int(getattr(self, "checkpoint_keep", 0)), "format": str(getattr(self, "checkpoint_format", ""))}})
        except Exception:
            pass
        t0 = time.time()
        step0 = int(getattr(self, "start_step", 0))
        try:
            _ = _run_loop(self, t0, step0, duration_s)
        except Exception as e:
            try:
                self.logger.info("nexus_fatal", extra={"extra": {"err": str(e)}})
            except Exception:
                try:
                    print("[nexus] fatal", str(e), file=sys.stderr, flush=True)
                except Exception:
                    pass
        finally:
            self.utd.close()
            try:
                if getattr(self, "_control_server", None):
                    self._control_server.stop()
            except Exception:
                pass

def make_parser():
    from .cli.args import make_parser as _mp
    return _mp()

__all__ = ["Nexus", "make_parser"]
]]></content>
    </file>
    <file>
      <path>physics/README.md</path>
      <content/>
    </file>
    <file>
      <path>physics/__init__.py</path>
      <content><![CDATA["""
Physics solvers for the FUM scalar EFT (finite-tube modes and condensation).

References:
- [derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1)
- [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:125-193)
- [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134)
"""

from .cylinder_modes import compute_kappas, mode_functions
from .condense_tube import (
    ModeEntry,
    compute_modes_for_R,
    build_quartic_diagonal,
    find_condensate_diagonal,
    mass_matrix_diagonal,
    energy_scan,
)

__all__ = [
    "compute_kappas",
    "mode_functions",
    "ModeEntry",
    "compute_modes_for_R",
    "build_quartic_diagonal",
    "find_condensate_diagonal",
    "mass_matrix_diagonal",
    "energy_scan",
]]]></content>
    </file>
    <file>
      <path>physics/memory_steering/memory_steering.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Memory-driven steering on graphs: rigorous mapping to the FUM derivations + dimensionless implementation.

How this maps to your derivations (clickable refs):
- Fast φ-sector (propagation + mass gap): the continuum equation and invariants are already derived
  in [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:121-128), with vacuum
  v = 1 - β/α and excitation mass m_eff² = α - β. The kinetic normalization c² = 2 J a² comes
  from the discrete action in [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:121-128).
  This module does not alter those results. φ governs propagation/modes; “memory” M biases routing.

- Steering law (geometric optics/ray limit): documented and derived in
  [derivation/memory_steering.md](derivation/memory_steering.md:1) from Voxtrium’s note
  [derivation/voxtrium/voxtrium_message.txt](derivation/voxtrium/voxtrium_message.txt:1).
  Define an index n(x,t) = exp[η M(x,t)]; then rays bend toward memory gradients:
      r'' = ∇_⊥ ln n = η ∇_⊥ M.
  Here r'' is the curvature of the path, ∇_⊥ is the transverse gradient, and η is a coupling.

- Memory dynamics (slow field): the minimal causal PDE
      ∂_t M = γ R - δ M + κ ∇² M,
  where R is a usage/co-activation rate (e.g., STDP proxy), γ is write gain, δ decay, κ consolidation/spread.
  This produces stored structure that later steers dynamics via n=exp(η M).

- Dimensionless groups (scaling, not units) with chosen rulers L, T, M0, R0:
      Θ = η M0,    D_a = γ R0 T / M0,    Λ = δ T,    Γ = κ T / L².
  In [derivation/fum_voxtrium_mapping.md](derivation/fum_voxtrium_mapping.md:44-80) the φ-sector uses (a, τ);
  you can set L=a, T=τ for a shared ruler so this steering layer aligns with the φ units map.

- Predictions (used for the tests in utils):
  • Junction choice:     P(A) ≈ σ(Θ Δm)   (logistic in Θ Δm at a fork)
  • Curvature scaling:   κ_path ∝ Θ |∇_⊥ m|
  • Stability band:      robust memory when D_a ≳ Λ; Γ too low → brittle; Γ too high → washed out
  See [derivation/memory_steering.md](derivation/memory_steering.md:1) for the full statement.

Graph discretization used here (orthogonal to φ):
- We represent M on nodes (vector m), and use the unnormalized graph Laplacian L = D - A to discretize ∇².
- Memory PDE (Euler step):  m ← m + dt ( γ r - δ m - κ L m ), where r is an independently measured usage proxy.
- Steering at node i toward neighbor j is modeled by a softmax over neighbor memory:
      P(i→j) ∝ exp(Θ m_j).
  At a two-branch junction this reduces to the logistic P(A)=σ(Θ Δm), matching the prediction.

What this module provides:
- build_graph_laplacian(A): compute L = D - A (undirected).
- update_memory(m, r, L, gamma, delta, kappa, dt): Euler step for the memory PDE (slow M-dynamics).
- transition_probs(i, neighbors, m, theta): softmax steering P(i→j) ∝ exp(Θ m_j).
- transition_probs_temp(i, neighbors, m, theta, temperature=1.0): temperatured softmax (default T=1).
- sample_next_neighbor(...): sample a neighbor according to transition_probs.
- sample_next_neighbor_heading(i, neighbors, m, theta, pos, heading, heading_bias=2.0, temperature=1.0, rng=None):
  heading-aware sampler for graphs with coordinates pos[N,d]; score_j = Θ m_j + heading_bias cos∠(heading, step_ij), softmax at T.
- compute_dimensionless_groups(eta, M0, gamma, R0, T, delta, kappa, L_scale): (Θ, D_a, Λ, Γ).
- y_junction_adjacency(...), collect_junction_choices(...): helpers to generate the logistic junction dataset.

Use with the experiments runner:
- See [fum_rt/utils/memory_steering_experiments.py](fum_rt/utils/memory_steering_experiments.py:1), which generates three
  datasets/plots for the predictions above (junction logistic, curvature scaling, stability band).

Author: Justin K. Lietz
Date: 2025-08-09
"""

from __future__ import annotations

from typing import Iterable, List, Optional, Sequence, Tuple

import numpy as np


def build_graph_laplacian(A: np.ndarray) -> np.ndarray:
    """
    Build the unnormalized graph Laplacian L = D - A (continuum analogue of -∇²).
    This is the standard discrete operator used in the memory PDE ∂_t m = γ r - δ m - κ L m,
    mapping directly to the ∇² term in [derivation/memory_steering.md](derivation/memory_steering.md:1).

    Args:
        A: np.ndarray (N x N). Nonzero → edge; diagonal should be zero. Ensure symmetry for undirected graphs.

    Returns:
        L: np.ndarray (N x N) Laplacian.

    Notes:
        - L = D - A is the unnormalized Laplacian (Dirichlet energy), which converges to -∇² under mesh refinement.
        - Self-loops are ignored (diagonal set to 0 in degree).
    """
    A = np.asarray(A)
    # Ensure zero diagonal in degree calculation
    deg = np.sum((A != 0) & (~np.eye(A.shape[0], dtype=bool)), axis=1).astype(np.float64)
    D = np.diag(deg)
    L = D - (A != 0).astype(np.float64)
    return L


def update_memory(
    m: np.ndarray,
    r: np.ndarray,
    L: np.ndarray,
    gamma: float,
    delta: float,
    kappa: float,
    dt: float,
) -> np.ndarray:
    """
    One explicit Euler step for the slow memory PDE (write-decay-spread),
        ∂_t m = γ r - δ m - κ L m,
    which is the graph-discretized form of ∂_t M = γ R - δ M + κ ∇² M in
    [derivation/memory_steering.md](derivation/memory_steering.md:1).

    Args:
        m: np.ndarray (N,). Memory field (dimensionless m = M/M0 if normalized to M0).
        r: np.ndarray (N,). Independent usage/co-activation proxy (dimensionless ρ = R/R0 if normalized to R0).
        L: np.ndarray (N x N). Graph Laplacian L = D - A.
        gamma, delta, kappa: PDE coefficients (map to D_a, Λ, Γ via compute_dimensionless_groups).
        dt: time step.

    Returns:
        m_next: updated memory field.

    Stability note:
        Explicit Euler requires dt small enough relative to (delta, kappa·λ_max(L)) for stability.
    """
    m = np.asarray(m, dtype=np.float64)
    r = np.asarray(r, dtype=np.float64)
    dm = gamma * r - delta * m - kappa * (L @ m)
    return m + dt * dm


def transition_probs(
    i: int,
    neighbors: Sequence[int],
    m: np.ndarray,
    theta: float,
) -> np.ndarray:
    """
    Softmax steering probabilities from node i toward its neighbors based on memory values:
        P(i→j) ∝ exp(Θ m_j),   Θ = η M0.
    At a 2-branch fork with memories (m_A, m_B) this reduces to the logistic
        P(A) = σ(Θ (m_A - m_B)),
    matching the prediction P(A) ≈ σ(Θ Δm) in [derivation/memory_steering.md](derivation/memory_steering.md:1).

    Args:
        i: current node index (unused; included for symmetry/extension).
        neighbors: iterable of neighbor node indices of i.
        m: np.ndarray (N,). Memory field (dimensionless).
        theta: dimensionless Θ (steering strength).

    Returns:
        probs: np.ndarray (len(neighbors),) summing to 1.0

    Notes:
        - Numerically stable softmax using max-subtraction.
        - If neighbors is empty, returns an empty array.
    """
    neigh = np.asarray(list(neighbors), dtype=int)
    if neigh.size == 0:
        return np.empty((0,), dtype=np.float64)

    z = theta * m[neigh]
    z = z - np.max(z)
    exps = np.exp(z)
    s = exps.sum()
    # Guard division by zero in pathological cases
    if s <= 0.0 or not np.isfinite(s):
        # fallback: uniform
        return np.ones_like(exps) / exps.size
    return exps / s


def transition_probs_temp(
    i: int,
    neighbors: Sequence[int],
    m: np.ndarray,
    theta: float,
    temperature: float = 1.0,
) -> np.ndarray:
    """
    Temperatured softmax steering probabilities:
        P(i→j) ∝ exp((Θ m_j) / T) with T = temperature.

    Notes:
    - T → 0 narrows to argmax; T → ∞ flattens to uniform.
    - Numerically stabilized with max-subtraction.
    - Keeps the original transition_probs() unchanged for backward compatibility.

    Args:
        i: current node (unused; placeholder for symmetry/extension).
        neighbors: iterable of neighbor node indices of i.
        m: memory field (dimensionless).
        theta: Θ (steering strength).
        temperature: softmax temperature T (dimensionless), default 1.0.

    Returns:
        probs over neighbors (sums to 1), or empty if neighbors empty.
    """
    neigh = np.asarray(list(neighbors), dtype=int)
    if neigh.size == 0:
        return np.empty((0,), dtype=np.float64)

    T = float(temperature) if np.isfinite(temperature) and temperature > 0 else 1.0
    z = (theta * m[neigh]) / T
    z = z - np.max(z)
    exps = np.exp(z)
    s = exps.sum()
    if s <= 0.0 or not np.isfinite(s):
        return np.ones_like(exps) / exps.size
    return exps / s


def sample_next_neighbor_heading(
    i: int,
    neighbors: Sequence[int],
    m: np.ndarray,
    theta: float,
    pos: np.ndarray,
    heading: np.ndarray,
    heading_bias: float = 2.0,
    temperature: float = 1.0,
    rng: Optional[np.random.Generator] = None,
) -> Optional[int]:
    """
    Heading-aware neighbor sampler for graphs with coordinates.

    Score for each neighbor j:
        score_j = Θ m_j + heading_bias * cos(∠(heading, step_ij))
    with step_ij = pos[j] - pos[i] and softmax at temperature T.

    This approximates the ray-limit routing r'' ∝ Θ ∇_⊥ m with an inertial heading term,
    reducing grid-quantization artifacts seen with purely memory-driven argmax hopping.

    Args:
        i: current node index.
        neighbors: iterable of neighbor indices of i.
        m: memory field (dimensionless).
        theta: Θ (steering strength).
        pos: positions array of shape (N, d) giving coordinates for nodes.
        heading: current unit heading vector in R^d (will be renormalized defensively).
        heading_bias: ξ, weight of the heading alignment term.
        temperature: softmax temperature T.
        rng: optional numpy Generator.

    Returns:
        neighbor index sampled according to temperatured, heading-aware softmax; or None if no neighbors.

    Requirements:
        - pos must provide geometric coordinates for all nodes; otherwise, use transition_probs[_temp] instead.
    """
    neigh = np.asarray(list(neighbors), dtype=int)
    if neigh.size == 0:
        return None

    pos = np.asarray(pos, dtype=np.float64)
    if pos.ndim != 2 or i < 0 or i >= pos.shape[0]:
        # Fallback to temperatured memory-only softmax if no usable geometry
        p = transition_probs_temp(i, neigh, m, theta, temperature=temperature)
        if rng is None: rng = np.random.default_rng()
        idx = int(rng.choice(neigh.size, p=p))
        return int(neigh[idx])

    h = np.asarray(heading, dtype=np.float64)
    hn = np.linalg.norm(h)
    h = h / hn if (hn > 0 and np.isfinite(hn)) else np.zeros_like(pos[0])

    scores = []
    pi = pos[i]
    for j in neigh:
        # direction from i to j
        v = np.asarray(pos[j], dtype=np.float64) - pi
        nv = float(np.linalg.norm(v))
        if nv <= 0.0 or not np.isfinite(nv):
            cosang = 0.0
        else:
            u = v / nv
            cosang = float(np.clip(np.dot(u, h), -1.0, 1.0))
        scores.append(theta * float(m[j]) + float(heading_bias) * cosang)

    T = float(temperature) if np.isfinite(temperature) and temperature > 0 else 1.0
    z = np.asarray(scores, dtype=np.float64) / T
    z -= np.max(z)
    exps = np.exp(z)
    s = exps.sum()
    if s <= 0.0 or not np.isfinite(s):
        p = np.ones_like(exps) / exps.size
    else:
        p = exps / s

    if rng is None:
        rng = np.random.default_rng()
    idx = int(rng.choice(neigh.size, p=p))
    return int(neigh[idx])


def sample_next_neighbor(
    i: int,
    neighbors: Sequence[int],
    m: np.ndarray,
    theta: float,
    rng: Optional[np.random.Generator] = None,
) -> Optional[int]:
    """
    Sample the next neighbor for a hop from node i using the softmax steering distribution.
    This is the discrete analogue of “rays bend toward ∇M” via n=exp(η M) (see derivation).

    Args:
        i: current node index.
        neighbors: neighbor indices of i.
        m: memory field (dimensionless).
        theta: steering strength Θ.
        rng: optional numpy Generator; if None, uses default.

    Returns:
        neighbor index or None if no neighbors.
    """
    neigh = np.asarray(list(neighbors), dtype=int)
    if neigh.size == 0:
        return None
    p = transition_probs(i, neigh, m, theta)
    if rng is None:
        rng = np.random.default_rng()
    idx = int(rng.choice(neigh.size, p=p))
    return int(neigh[idx])


def compute_dimensionless_groups(
    eta: float,
    M0: float,
    gamma: float,
    R0: float,
    T: float,
    delta: float,
    kappa: float,
    L_scale: float,
) -> Tuple[float, float, float, float]:
    """
    Compute the four dimensionless groups (Θ, D_a, Λ, Γ) that control the steering+memory dynamics.

    Definitions (see [derivation/memory_steering.md](derivation/memory_steering.md:1)):
        Θ = η M0,   D_a = γ R0 T / M0,   Λ = δ T,   Γ = κ T / L².

    Args:
        eta, M0: coupling and memory scale (produce Θ).
        gamma, R0, T, delta, kappa: PDE parameters and scales (produce D_a, Λ, Γ).
        L_scale: spatial length scale L (use a for φ-map alignment).

    Returns:
        (Theta, D_a, Lambda, Gamma)
    """
    Theta = eta * float(M0)
    Da = (gamma * R0 * T) / float(M0) if M0 != 0 else np.inf
    Lam = delta * T
    Gam = (kappa * T) / (L_scale ** 2) if L_scale != 0 else np.inf
    return (float(Theta), float(Da), float(Lam), float(Gam))


def y_junction_adjacency(
    len_in: int = 5,
    len_a: int = 5,
    len_b: int = 5,
) -> Tuple[np.ndarray, int, int, int]:
    """
    Construct a simple undirected Y-junction adjacency (for P(A)=σ(Θ Δm) tests).

    Topology:
      chain_in (0 ... len_in-1) feeds into a junction node J,
      which then splits into branch A (JA_1 ... JA_len_a)
      and branch B (JB_1 ... JB_len_b).

    Returns:
        A: adjacency (N x N) dense binary
        j: junction node index
        a0: first node on branch A
        b0: first node on branch B
    """
    # index layout: in: 0..len_in-1, J: len_in, A: len_in+1..+len_a, B: subsequent
    J = len_in
    a_start = J + 1
    b_start = J + 1 + len_a
    N = len_in + 1 + len_a + len_b
    A = np.zeros((N, N), dtype=np.int8)

    # inbound chain
    for t in range(1, len_in):
        A[t - 1, t] = 1
        A[t, t - 1] = 1
    # connect inbound tail to junction
    if len_in > 0:
        A[len_in - 1, J] = 1
        A[J, len_in - 1] = 1

    # branch A
    last = J
    for k in range(len_a):
        n = a_start + k
        A[last, n] = 1
        A[n, last] = 1
        last = n

    # branch B
    last = J
    for k in range(len_b):
        n = b_start + k
        A[last, n] = 1
        A[n, last] = 1
        last = n

    return A, J, a_start, b_start


def collect_junction_choices(
    A: np.ndarray,
    m: np.ndarray,
    J: int,
    a_next: int,
    b_next: int,
    theta: float,
    trials: int = 1000,
    rng: Optional[np.random.Generator] = None,
) -> Tuple[int, int]:
    """
    Collect Bernoulli choices at a Y-junction under softmax steering to empirically test
    P(A) ≈ σ(Θ Δm). This function is used by the experiment runner to produce the logistic
    collapse plot and fit.

    Args:
        A: adjacency (dense binary)
        m: memory field (dimensionless)
        J: junction node index
        a_next: first node on branch A
        b_next: first node on branch B
        theta: Θ
        trials: number of samples
        rng: optional RNG

    Returns:
        (count_A, count_B)
    """
    if rng is None:
        rng = np.random.default_rng()
    # neighbors of junction (exclude inbound if present by user’s choice; here include all)
    neighbors = np.where(A[J] != 0)[0]
    # restrict to branches if explicitly provided
    neighbors = [n for n in neighbors if n in (a_next, b_next)]
    if len(neighbors) != 2:
        # Fallback: use two highest-m_j neighbors if not a clean Y
        neigh = np.where(A[J] != 0)[0]
        if neigh.size < 2:
            return (0, 0)
        order = np.argsort(m[neigh])[::-1]
        neighbors = [int(neigh[order[0]]), int(neigh[order[1]])]

    counts = {neighbors[0]: 0, neighbors[1]: 0}
    for _ in range(int(trials)):
        p = transition_probs(J, neighbors, m, theta)
        idx = int(rng.choice(2, p=p))
        counts[neighbors[idx]] += 1
    # Map to (A,B) order if possible
    ca = counts.get(a_next, 0)
    cb = counts.get(b_next, 0)
    return (int(ca), int(cb))]]></content>
    </file>
    <file>
      <path>physics/memory_steering/memory_steering_experiments.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Memory-Driven Steering (experiments): rigorous mapping to your derivations + three falsifiable tests

What this file does (experiments layer)
- Generates datasets (and prints CSV) to test the three predictions introduced in
  [derivation/memory_steering.md](derivation/memory_steering.md:1) from the Voxtrium note
  [derivation/voxtrium/voxtrium_message.txt](derivation/voxtrium/voxtrium_message.txt:1):
  1) Junction logistic collapse:  P(A) ≈ σ(Θ Δm)
  2) Curvature scaling:           ⟨κ_path⟩ ∝ Θ |∇m|
  3) Stability band:              robust memory for D_a ≳ Λ with intermediate Γ

How this maps to your φ‑EFT derivations (orthogonal layer)
- The fast φ‑sector continuum equation and invariants are already derived in
  [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:121-128):
      □φ + α φ² - (α - β) φ = 0,   v = 1 - β/α,   m_eff² = α - β.
- The kinetic normalization c² = 2 J a² is rigorously obtained from a discrete action in
  [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:121-128).
- The memory‑steering layer (M) is slow and biases routing only; it does not modify φ propagation,
  the vacuum/mass results, nor the on‑site invariant Q_FUM from [derivation/symmetry_analysis.md](derivation/symmetry_analysis.md:141-148).

Dimensionless groups used implicitly in the tests (see [derivation/memory_steering.md](derivation/memory_steering.md:1))
- Θ = η M0        (steering strength)
- D_a = γ R0 T / M0,   Λ = δ T,   Γ = κ T / L²
- We choose simple graph‑native rulers (L, T, M0, R0) inside each test to demonstrate collapse
  and leave the physical alignment to φ’s (a, τ) to [derivation/fum_voxtrium_mapping.md](derivation/fum_voxtrium_mapping.md:44-80).

Outputs (printed to stdout when run)
- Junction logistic:            “Theta*Delta_m, P(A)”
- Curvature scaling:            “Theta*|grad m|, mean(kappa_path)”
- Stability band:               “D_a, Lambda, Gamma, Retention, Fidelity_w, Fidelity_end, Fidelity_shuffle_end, Fidelity_edge_end, AUC_end, SNR_end”

Usage
- python3 fum_rt/utils/memory_steering_experiments.py  > outputs/memory_steering_results.csv
- The plotting helper (separate) converts the combined CSV into figures saved in outputs/.
"""

from __future__ import annotations

import math
import sys
from dataclasses import dataclass
from typing import Iterable, List, Optional, Sequence, Tuple

import numpy as np

# Steering primitives (robust import: module or script)
try:
    from fum_rt.core.memory_steering import (
        build_graph_laplacian,
        update_memory,
        transition_probs,
        sample_next_neighbor,
        compute_dimensionless_groups,
        y_junction_adjacency,
        collect_junction_choices,
    )
except Exception as e1:
    # Second-chance import: add repo root to sys.path if running as a script
    try:
        import os as _os, sys as _sys
        _repo_root = _os.path.abspath(_os.path.join(_os.path.dirname(__file__), _os.pardir, _os.pardir))
        if _repo_root not in _sys.path:
            _sys.path.insert(0, _repo_root)
        from fum_rt.core.memory_steering import (
            build_graph_laplacian,
            update_memory,
            transition_probs,
            sample_next_neighbor,
            compute_dimensionless_groups,
            y_junction_adjacency,
            collect_junction_choices,
        )
    except Exception as e2:
        print("[warn] falling back to local copies of simple helpers:", e2, file=sys.stderr)

        def build_graph_laplacian(A: np.ndarray) -> np.ndarray:
            A = np.asarray(A)
            deg = np.sum((A != 0) & (~np.eye(A.shape[0], dtype=bool)), axis=1).astype(np.float64)
            return np.diag(deg) - (A != 0).astype(np.float64)

        def transition_probs(i: int, neighbors: Sequence[int], m: np.ndarray, theta: float) -> np.ndarray:
            neigh = np.asarray(list(neighbors), dtype=int)
            if neigh.size == 0:
                return np.empty((0,), dtype=np.float64)
            z = theta * m[neigh]
            z = z - np.max(z)
            exps = np.exp(z)
            s = exps.sum()
            if s <= 0.0 or not np.isfinite(s):
                return np.ones_like(exps) / exps.size
            return exps / s

        def sample_next_neighbor(
            i: int, neighbors: Sequence[int], m: np.ndarray, theta: float, rng: Optional[np.random.Generator] = None
        ) -> Optional[int]:
            neigh = np.asarray(list(neighbors), dtype=int)
            if neigh.size == 0:
                return None
            p = transition_probs(i, neigh, m, theta)
            if rng is None:
                rng = np.random.default_rng()
            idx = int(rng.choice(neigh.size, p=p))
            return int(neigh[idx])

        def y_junction_adjacency(len_in: int = 5, len_a: int = 5, len_b: int = 5) -> Tuple[np.ndarray, int, int, int]:
            J = len_in
            a_start = J + 1
            b_start = J + 1 + len_a
            N = len_in + 1 + len_a + len_b
            A = np.zeros((N, N), dtype=np.int8)
            for t in range(1, len_in):
                A[t - 1, t] = 1
                A[t, t - 1] = 1
            if len_in > 0:
                A[len_in - 1, J] = 1
                A[J, len_in - 1] = 1
            last = J
            for k in range(len_a):
                n = a_start + k
                A[last, n] = 1
                A[n, last] = 1
                last = n
            last = J
            for k in range(len_b):
                n = b_start + k
                A[last, n] = 1
                A[n, last] = 1
                last = n
            return A, J, a_start, b_start

        def collect_junction_choices(
            A: np.ndarray,
            m: np.ndarray,
            J: int,
            a_next: int,
            b_next: int,
            theta: float,
            trials: int = 1000,
            rng: Optional[np.random.Generator] = None,
        ) -> Tuple[int, int]:
            if rng is None:
                rng = np.random.default_rng()
            neighbors = np.where(A[J] != 0)[0]
            neighbors = [n for n in neighbors if n in (a_next, b_next)]
            if len(neighbors) != 2:
                neigh = np.where(A[J] != 0)[0]
                if neigh.size < 2:
                    return (0, 0)
                order = np.argsort(m[neigh])[::-1]
                neighbors = [int(neigh[order[0]]), int(neigh[order[1]])]
            counts = {neighbors[0]: 0, neighbors[1]: 0}
            for _ in range(int(trials)):
                p = transition_probs(J, neighbors, m, theta)
                idx = int(rng.choice(2, p=p))
                counts[neighbors[idx]] += 1
            ca = counts.get(a_next, 0)
            cb = counts.get(b_next, 0)
            return (int(ca), int(cb))

        def update_memory(m, r, L, gamma, delta, kappa, dt):
            m = np.asarray(m, dtype=np.float64)
            r = np.asarray(r, dtype=np.float64)
            return m + dt * (gamma * r - delta * m - kappa * (L @ m))

        def compute_dimensionless_groups(eta, M0, gamma, R0, T, delta, kappa, L_scale):
            Theta = eta * float(M0)
            Da = (gamma * R0 * T) / float(M0) if M0 != 0 else np.inf
            Lam = delta * T
            Gam = (kappa * T) / (L_scale ** 2) if L_scale != 0 else np.inf
            return (float(Theta), float(Da), float(Lam), float(Gam))


# ---------------------------
# Utilities for grid graphs
# ---------------------------

def grid_adjacency(nx: int, ny: int) -> np.ndarray:
    """4-neighbor undirected grid adjacency (no wrap). Nodes indexed row-major: i = y*nx + x."""
    N = nx * ny
    A = np.zeros((N, N), dtype=np.int8)
    for y in range(ny):
        for x in range(nx):
            i = y * nx + x
            if x + 1 < nx:
                j = y * nx + (x + 1)
                A[i, j] = 1
                A[j, i] = 1
            if y + 1 < ny:
                j = (y + 1) * nx + x
                A[i, j] = 1
                A[j, i] = 1
    return A


def grid_neighbors(nx: int, ny: int, i: int) -> List[int]:
    y, x = divmod(i, nx)
    out = []
    if x > 0:
        out.append(i - 1)
    if x + 1 < nx:
        out.append(i + 1)
    if y > 0:
        out.append(i - nx)
    if y + 1 < ny:
        out.append(i + nx)
    return out


# ---------------------------
# 1) Junction logistic collapse
# ---------------------------

def run_junction_logistic(theta: float = 2.0, delta_m_values: Sequence[float] = None, trials: int = 2000) -> Tuple[np.ndarray, np.ndarray]:
    """
    Junction logistic collapse: P(A) ≈ σ(Θ Δm)

    Why this maps to the derivation:
    - In [derivation/memory_steering.md](derivation/memory_steering.md:1) the steering index is n=exp(η M).
      At a fork, the two outgoing neighbors (A,B) inherit memory values (m_A, m_B). The softmax routing
      P(i→j) ∝ exp(Θ m_j) reduces to a binary logistic:
          P(A) = σ(Θ (m_A - m_B)) = σ(Θ Δm).
      Hence plotting P(A) vs Θ Δm should overlay across graph sizes/speeds, demonstrating a
      dimensionless collapse (Θ is the only slope).

    Args:
        theta: Θ (dimensionless steering strength)
        delta_m_values: sweep of Δm values in m‑units (dimensionless)
        trials: Bernoulli samples for P(A) estimation

    Returns:
        x: array of Θ Δm (abscissa of the collapse)
        pA: measured P(A)
    """
    if delta_m_values is None:
        delta_m_values = np.linspace(-2.0, 2.0, 17)  # symmetric sweep in m-units
    A, J, a0, b0 = y_junction_adjacency(5, 5, 5)
    N = A.shape[0]
    rng = np.random.default_rng(123)
    m = np.zeros(N, dtype=np.float64)
    xvals, pvals = [], []
    for d in delta_m_values:
        m[:] = 0.0
        m[a0] = +0.5 * d
        m[b0] = -0.5 * d
        ca, cb = collect_junction_choices(A, m, J, a0, b0, theta=theta, trials=trials, rng=rng)
        pA = ca / max(1, (ca + cb))
        xvals.append(theta * d)
        pvals.append(pA)
    return np.asarray(xvals), np.asarray(pvals)


# ---------------------------
# 2) Curvature scaling
# ---------------------------

def polyline_curvature(pts: np.ndarray) -> np.ndarray:
    """
    Discrete curvature estimate along a polyline:

    - We approximate the continuous curvature κ by the local turning angle Δθ and mean edge length ℓ:
          κ ≈ 2 sin(Δθ/2) / ℓ
      (endpoints are set to 0). This delivers a robust, grid‑agnostic estimator of path bending.

    - In the derivation [derivation/memory_steering.md](derivation/memory_steering.md:1), rays obey r'' = ∇_⊥ ln n = Θ ∇_⊥ m
      (with n=exp(Θ m)). The magnitude of r'' along a path is proportional to |∇m| with a slope ∝ Θ. This function
      yields the ⟨κ_path⟩ metric used in the curvature scaling test ⟨κ_path⟩ ∝ Θ |∇m|.
    """
    n = pts.shape[0]
    if n < 3:
        return np.zeros(n, dtype=np.float64)
    kappa = np.zeros(n, dtype=np.float64)
    for i in range(1, n - 1):
        p0 = pts[i - 1]
        p1 = pts[i]
        p2 = pts[i + 1]
        v1 = p1 - p0
        v2 = p2 - p1
        # normalize
        n1 = np.linalg.norm(v1)
        n2 = np.linalg.norm(v2)
        if n1 == 0 or n2 == 0:
            kappa[i] = 0.0
            continue
        v1n = v1 / n1
        v2n = v2 / n2
        cosang = np.clip(np.dot(v1n, v2n), -1.0, 1.0)
        dtheta = math.acos(cosang)
        ell = 0.5 * (n1 + n2)
        if ell == 0:
            kappa[i] = 0.0
        else:
            kappa[i] = 2.0 * math.sin(0.5 * dtheta) / ell
    return kappa

def polyline_curvature_signed(pts: np.ndarray) -> np.ndarray:
    """
    Discrete signed curvature estimate along a polyline.

    - Uses the same magnitude estimator as polyline_curvature:
          |κ| ≈ 2 sin(Δθ/2) / ℓ
      but multiplies by the sign sgn = sign( (v1 × v2)_z ) where v1 = p1-p0, v2 = p2-p1.
      In 2D, (v1 × v2)_z = v1_x v2_y - v1_y v2_x.

    - This returns the signed bending, suitable for falsification via gradient/Θ sign flips:
          ⟨κ_signed⟩ ∝ Θ (∇m · n_⊥)
    """
    n = pts.shape[0]
    if n < 3:
        return np.zeros(n, dtype=np.float64)
    kappa = np.zeros(n, dtype=np.float64)
    for i in range(1, n - 1):
        p0 = pts[i - 1]
        p1 = pts[i]
        p2 = pts[i + 1]
        v1 = p1 - p0
        v2 = p2 - p1
        n1 = np.linalg.norm(v1)
        n2 = np.linalg.norm(v2)
        if n1 == 0 or n2 == 0:
            kappa[i] = 0.0
            continue
        v1n = v1 / n1
        v2n = v2 / n2
        # turning angle
        cosang = float(np.clip(np.dot(v1n, v2n), -1.0, 1.0))
        dtheta = math.acos(cosang)
        ell = 0.5 * (n1 + n2)
        if ell == 0:
            kmag = 0.0
        else:
            kmag = 2.0 * math.sin(0.5 * dtheta) / ell
        # orientation sign from 2D cross product z-component
        cross_z = float(v1n[0] * v2n[1] - v1n[1] * v2n[0])
        sgn = 0.0
        if cross_z > 0:
            sgn = +1.0
        elif cross_z < 0:
            sgn = -1.0
        kappa[i] = sgn * kmag
    return kappa

def run_curvature_scaling(
    nx: int = 21,
    ny: int = 21,
    theta_values: Sequence[float] = (1.0, 2.0, 3.0),
    pulses: int = 50,
    heading_bias: float = 2.0,
    temperature: float = 0.3,
    mode: str = "graph",
    dt: float = 0.2,
    nsteps: int = 80,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Curvature scaling: ⟨κ_path⟩ ∝ Θ |∇m|

    Protocol and mapping:
    - Create a smooth, constant gradient in m across the grid: m(y) increases linearly with y.
      This fixes |∇m| uniformly (up to grid effects).
    - Two implementations:
      (graph) 8‑neighbor with heading inertia (score_j = Θ m_j + ξ cos(∠(h, step_j))).
      (ray)   Continuous 2‑D “ray” stepper: ẋ = ĥ, ḣ = Π_⊥(Θ ∇m), with ĥ renormalized each step.
    - Return pairs (X = Θ |∇m|, Y = ⟨κ_path⟩). The derivation predicts linear scaling.

    Args:
        nx, ny: grid size for graph mode (also sets the m-gradient scale).
        theta_values: Θ values to test.
        pulses: number of paths to average per Θ (seeds).
        heading_bias: ξ in the graph sampler’s heading term.
        mode: "graph" (default) or "ray".
        dt:   step size for the ray integrator.
        nsteps: number of steps for the ray integrator.

    Returns:
        X: array of Θ |∇m|
        Y: array of mean curvature per path
    """
    # Memory gradient: m(y) = y/(ny-1) ∈ [0,1], so |∇m| = 1/(ny-1)
    grad_mag = 1.0 / max(1, ny - 1)

    rng = np.random.default_rng(7)
    X_all, Y_all = [], []

    if mode == "graph":
        # Build discrete m on the grid
        m = np.zeros(nx * ny, dtype=np.float64)
        for y in range(ny):
            m[y * nx:(y + 1) * nx] = (y / max(1, ny - 1))

        # 8-neighbor helper (no wrap)
        def neighbors8(i: int) -> List[int]:
            y, x = divmod(i, nx)
            out = []
            for dy in (-1, 0, 1):
                for dx in (-1, 0, 1):
                    if dx == 0 and dy == 0:
                        continue
                    xx, yy = x + dx, y + dy
                    if 0 <= xx < nx and 0 <= yy < ny:
                        out.append(yy * nx + xx)
            return out

        def dir_unit(i: int, j: int) -> np.ndarray:
            yi, xi = divmod(i, nx)
            yj, xj = divmod(j, nx)
            v = np.array([xj - xi, yj - yi], dtype=np.float64)
            n = np.linalg.norm(v)
            return v / n if n > 0 else np.zeros(2, dtype=np.float64)

        # Sources along a central row; initial heading along +x so ∇m is transverse
        src_y = ny // 2
        src_nodes = [src_y * nx + x for x in range(1, nx - 1)]  # avoid borders
        for theta in theta_values:
            for s in rng.choice(src_nodes, size=min(pulses, len(src_nodes)), replace=False):
                path_nodes = [s]
                last = -1
                cur = s
                h = np.array([1.0, 0.0], dtype=np.float64)  # initial heading (+x)
                steps = nx // 2
                for _ in range(steps):
                    neigh = neighbors8(cur)
                    if len(neigh) == 0:
                        break
                    # Heading-aware softmax: score = Θ m_j + heading_bias * cos(∠(h, step))
                    scores = []
                    for j in neigh:
                        u = dir_unit(cur, j)
                        cosang = float(np.clip(np.dot(u, h), -1.0, 1.0))
                        scores.append(theta * m[j] + heading_bias * cosang)
                    z = np.asarray(scores, dtype=np.float64) / max(temperature, 1e-6)
                    z -= np.max(z)
                    p = np.exp(z)
                    ssum = p.sum()
                    if not np.isfinite(ssum) or ssum <= 0:
                        p = np.ones_like(p) / len(p)
                    else:
                        p /= ssum
                    idx = int(rng.choice(len(neigh), p=p))
                    nxt = int(neigh[idx])
                    if nxt == cur:
                        break
                    last, cur = cur, nxt
                    h = dir_unit(last, cur)
                    path_nodes.append(cur)

                # Compute curvature along the polyline
                pts = np.array([[n % nx, n // nx] for n in path_nodes], dtype=np.float64)
                if pts.shape[0] >= 3:
                    kappa = polyline_curvature(pts)
                    if kappa.size > 0:
                        X_all.append(theta * grad_mag)
                        Y_all.append(float(np.mean(kappa)))

    else:
        # Continuous ray integrator in a domain of size (nx, ny)
        g = np.array([0.0, grad_mag], dtype=np.float64)  # ∇m constant and vertical
        # Seeds: choose start positions along mid-height, avoid borders
        y0 = (ny - 1) * 0.5
        xs = rng.uniform(1.0, nx - 2.0, size=pulses)
        for theta in theta_values:
            for x0 in xs:
                # Initialize position and heading
                x = np.array([x0, y0], dtype=np.float64)
                h = np.array([1.0, 0.0], dtype=np.float64)
                pts = [x.copy()]
                for _ in range(int(nsteps)):
                    # ḣ = Π_⊥(Θ ∇m) = Θ(∇m - (∇m·h) h)
                    dv = theta * (g - np.dot(g, h) * h)
                    h = h + dt * dv
                    nrm = float(np.linalg.norm(h))
                    if nrm == 0 or not np.isfinite(nrm):
                        break
                    h = h / nrm
                    # ẋ = ĥ (unit speed)
                    x = x + dt * h
                    pts.append(x.copy())
                pts = np.asarray(pts, dtype=np.float64)
                if pts.shape[0] >= 3:
                    kappa = polyline_curvature(pts)
                    if kappa.size > 0:
                        X_all.append(theta * grad_mag)
                        Y_all.append(float(np.mean(kappa)))

    return np.asarray(X_all), np.asarray(Y_all)
# ---------------------------
# 2b) Curvature: calibration and signed-test helpers
# ---------------------------

def calibrate_curvature_on_arcs(R_values=(20.0, 40.0, 80.0), n_points=200, noise=0.0, out_png="outputs/curvature_calibration.png"):
    """
    Synthetic calibration: draw circular arcs of radius R and verify the polyline_curvature
    estimator returns kappa ≈ 1/R (±20%).

    Args:
        R_values: iterable radii to test
        n_points: samples per arc
        noise: optional Gaussian jitter to add to points
        out_png: path to save the calibration plot

    Returns:
        results: list of (R, kappa_mean, kappa_std, frac_error)
    """
    import os
    os.makedirs(os.path.dirname(out_png), exist_ok=True)
    import matplotlib.pyplot as plt
    res = []
    fig, ax = plt.subplots(figsize=(6,4))
    for R in R_values:
        theta = np.linspace(0.0, np.pi/3.0, n_points)  # 60-degree arc
        x = R * np.cos(theta)
        y = R * np.sin(theta)
        pts = np.stack([x, y], axis=1)
        if noise > 0.0:
            pts = pts + np.random.default_rng(123).normal(scale=noise, size=pts.shape)
        kappa = polyline_curvature(pts)
        if kappa.size == 0:
            kappa_mean, kappa_std = np.nan, np.nan
        else:
            kappa_mean = float(np.mean(kappa[1:-1]))  # ignore endpoints
            kappa_std = float(np.std(kappa[1:-1]))
        target = 1.0/float(R)
        frac_err = (kappa_mean/target - 1.0) if (target>0 and np.isfinite(target) and np.isfinite(kappa_mean)) else np.nan
        res.append((float(R), kappa_mean, kappa_std, frac_err))
        ax.errorbar([1.0/R], [kappa_mean], yerr=[kappa_std], fmt="o", label=f"R={R:g}")
    ax.axline((0,0),(1,1), color="#d62728", linestyle="--", label="ideal: kappa=1/R")
    ax.set_xlabel("1/R (ideal)")
    ax.set_ylabel("estimated kappa")
    ax.set_title("Curvature estimator calibration on circular arcs")
    ax.legend(loc="upper left", fontsize=8)
    fig.tight_layout()
    fig.savefig(out_png, dpi=160)
    return res


def run_curvature_scaling_signed(
    nx: int = 41,
    ny: int = 41,
    x_values: Optional[Sequence[float]] = None,   # desired X = Theta*|grad m|
    pulses_per_x: int = 64,
    dt: float = 0.10,
    nsteps: int = 600,
    signed_check_mids: int = 3,
    rng: Optional[np.random.Generator] = None,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Curvature scaling with ray-limit integrator + signed falsification and SE estimates.

    - Domain: continuous rays in a box of size (nx, ny), with constant gradient in m along +y.
    - For each target X = Theta*|grad m|, pick Theta = X/|grad m| and average path curvature over many seeds.
    - Signed test: for ~3 midpoints of X, repeat runs with (1) gradient flipped (|∇m|→-|∇m|) and (2) Theta→-Theta.

    Returns:
        X_all: array of X = Theta*|grad m|
        Y_mean: mean kappa per X/sign bucket
        Y_se: standard error per X/sign bucket
        sign_id: integer tags {0: baseline, 1: flip_grad, 2: flip_theta}
    """
    if rng is None:
        rng = np.random.default_rng(77)

    # Gradient magnitude based on grid
    grad_mag = 1.0 / max(1, ny - 1)
    if x_values is None:
        # default: 10 log-spaced values in [0.02, 0.45]
        x_values = np.geomspace(0.02, 0.45, 10)
    x_values = np.asarray(x_values, dtype=float)

    # Seeds: choose start x positions along mid-height, avoid borders
    y0 = (ny - 1) * 0.5
    xs_all = rng.uniform(1.0, nx - 2.0, size=pulses_per_x)

    def ray_batch(theta_val: float, grad_sign: float = 1.0) -> Tuple[float, float]:
        # Constant gradient vector (vertical), allow sign flip for falsification
        g = np.array([0.0, grad_sign * grad_mag], dtype=np.float64)
        kappas = []
        for x0 in xs_all:
            x = np.array([x0, y0], dtype=np.float64)
            h = np.array([1.0, 0.0], dtype=np.float64)  # initial heading +x
            pts = [x.copy()]
            for _ in range(int(nsteps)):
                dv = theta_val * (g - np.dot(g, h) * h)     # h' = Theta (g - (g·h)h)
                h = h + dt * dv
                nrm = float(np.linalg.norm(h))
                if nrm == 0 or not np.isfinite(nrm):
                    break
                h = h / nrm
                x = x + dt * h
                pts.append(x.copy())
            pts = np.asarray(pts, dtype=np.float64)
            if pts.shape[0] >= 3:
                kappa = polyline_curvature(pts)
                if kappa.size > 0:
                    kappas.append(float(np.mean(kappa)))
        if len(kappas) == 0:
            return (np.nan, np.nan)
        arr = np.asarray(kappas, dtype=float)
        return (float(np.mean(arr)), float(np.std(arr)/np.sqrt(max(1, arr.size))))

    # Baseline (sign_id=0)
    X_base, Y_mean_base, Y_se_base, sign_base = [], [], [], []
    for X in x_values:
        theta = X / max(grad_mag, 1e-12)
        mu, se = ray_batch(theta, grad_sign=+1.0)
        X_base.append(X); Y_mean_base.append(mu); Y_se_base.append(se); sign_base.append(0)

    # Signed falsification on ~3 midpoints
    mid_idx = np.linspace(0, len(x_values)-1, signed_check_mids, dtype=int)
    X_flip, Y_mean_flip, Y_se_flip, sign_flip = [], [], [], []
    for idx in mid_idx:
        Xmid = float(x_values[idx])
        theta_mid = Xmid / max(grad_mag, 1e-12)

        # flip gradient (sign_id=1)
        mu1, se1 = ray_batch(theta_mid, grad_sign=-1.0)
        X_flip.append(Xmid); Y_mean_flip.append(mu1); Y_se_flip.append(se1); sign_flip.append(1)

        # flip Theta (sign_id=2)
        mu2, se2 = ray_batch(-theta_mid, grad_sign=+1.0)
        X_flip.append(Xmid); Y_mean_flip.append(mu2); Y_se_flip.append(se2); sign_flip.append(2)

    # Concatenate
    X_all = np.asarray(list(X_base) + list(X_flip), dtype=float)
    Y_mean = np.asarray(list(Y_mean_base) + list(Y_mean_flip), dtype=float)
    Y_se = np.asarray(list(Y_se_base) + list(Y_se_flip), dtype=float)
    sign_id = np.asarray(list(sign_base) + list(sign_flip), dtype=int)
    return X_all, Y_mean, Y_se, sign_id


# ---------------------------
# 3) Stability band
# ---------------------------

def run_stability_band(
    nx: int = 21,
    ny: int = 21,
    T_write: float = 5.0,
    T_decay: float = 5.0,
    dt: float = 0.1,
    gamma_values: Sequence[float] = (0.5,),           # used only if da_values is None
    delta_values: Sequence[float] = (0.05, 0.1, 0.2),
    kappa_values: Sequence[float] = (0.2, 0.5, 1.0),
    da_values: Optional[Sequence[float]] = None,       # if provided, dose-controlled write using scale_R
    gamma_fixed: float = 1.0,
    dose_model: str = "scale_R",
    topk_frac: float = 0.05,
    cfl_limit: float = 0.9,
) -> List[Tuple[float, float, float, float, float, float, float, float, float, float, float, float]]:
    """
    Stability band in (D_a, Λ, Γ) with dose control and discriminative metrics.

    PDE: ∂_t m = γ r - δ m - κ L m
    Dimensionless: D_a = γ R_0 T / M_0, Λ = δ T, Γ = κ T / L²

    Protocol (two‑phase):
      - Write (duration T_write): r = R_amp * R_mask, evolve → m_w
      - Decay (duration T_decay): r = 0, evolve → m_end

    Dose control (when da_values is provided):
      Enforce ∫_0^{T_write} γ R_0 dt = D_a M_0 via R_amp = (D_a*M0)/(γ_fixed*T_write).
      We set γ = gamma_fixed during both phases.

    Metrics:
      - Retention         = ||m_end|| / ||m_w||                (||·|| = mean |·|)
      - Fidelity_w        = corr(m_w, R_mask)
      - Fidelity_end      = corr(m_end, R_mask)
      - Fidelity_shuffle  = corr(m_end, shuffle(R_mask))
      - Fidelity_edge     = corr(L m_end, L R_mask)
      - AUC_end           = ROC AUC for score=m_end vs mask
      - SNR_end           = (μ_in - μ_out) / σ_out
      - AUPRC_topk        = truncated AP using top k=floor(topk_frac*N) predictions
      - BPER              = band‑pass energy ratio = ||L_norm m_end|| / ||m_end||

    Returns rows:
      (D_a, Λ, Γ, Ret, Fid_w, Fid_end, Fid_shuffle, Fid_edge, AUC_end, SNR_end, AUPRC_topk, BPER)

    Notes:
      - L is the combinatorial Laplacian; L_norm = I - D^{-1/2} A D^{-1/2}
      - We clamp κ by a CFL condition: dt * κ * λ_max(L) ≤ cfl_limit with λ_max(L) ≈ 2 * deg_max
    """
    N = nx * ny
    A = grid_adjacency(nx, ny)
    L = build_graph_laplacian(A)

    # Degree-normalized Laplacian for BPER
    deg = np.sum((A != 0) & (~np.eye(A.shape[0], dtype=bool)), axis=1).astype(np.float64)
    with np.errstate(divide="ignore"):
        dinv2 = 1.0 / np.sqrt(np.maximum(deg, 1e-12))
    Dinv2 = np.diag(dinv2)
    L_norm = np.eye(N, dtype=np.float64) - (Dinv2 @ (A != 0).astype(np.float64) @ Dinv2)

    # Localized usage R_mask: small central disk
    R_mask = np.zeros(N, dtype=np.float64)
    cx, cy = (nx - 1) / 2.0, (ny - 1) / 2.0
    for y in range(ny):
        for x in range(nx):
            r2 = (x - cx) ** 2 + (y - cy) ** 2
            if r2 <= (min(nx, ny) * 0.15) ** 2:
                R_mask[y * nx + x] = 1.0

    # Scales for dimensionless groups (simple choice)
    L_scale = 1.0
    M0 = 1.0
    R0 = 1.0
    rng = np.random.default_rng(12345)

    # Helpers
    def pearson_corr(a: np.ndarray, b: np.ndarray) -> float:
        a = np.asarray(a, float).ravel()
        b = np.asarray(b, float).ravel()
        am = a.mean(); bm = b.mean()
        av = a - am; bv = b - bm
        num = float(np.dot(av, bv))
        den = float(np.linalg.norm(av) * np.linalg.norm(bv))
        if den == 0.0 or not np.isfinite(den):
            return float("nan")
        return num / den

    def average_ranks(x: np.ndarray) -> np.ndarray:
        x = np.asarray(x, float)
        order = np.argsort(x, kind="mergesort")
        ranks = np.empty_like(order, dtype=float)
        ranks[order] = np.arange(1, x.size + 1, dtype=float)
        i = 0
        while i < x.size:
            j = i + 1
            while j < x.size and x[order[j]] == x[order[i]]:
                j += 1
            if j - i > 1:
                avg = 0.5 * (i + 1 + j)
                ranks[order[i:j]] = avg
            i = j
        return ranks

    def auc_binary(scores: np.ndarray, labels: np.ndarray) -> float:
        scores = np.asarray(scores, float).ravel()
        labels = (np.asarray(labels).ravel() > 0.0).astype(int)
        n_pos = int(labels.sum()); n_neg = int(labels.size - n_pos)
        if n_pos == 0 or n_neg == 0:
            return float("nan")
        r = average_ranks(scores)
        R_pos = float(r[labels == 1].sum())
        U = R_pos - n_pos * (n_pos + 1) / 2.0
        return max(0.0, min(1.0, U / (n_pos * n_neg)))

    def average_precision_topk(scores: np.ndarray, labels: np.ndarray, topk: int) -> float:
        scores = np.asarray(scores, float).ravel()
        labels = (np.asarray(labels).ravel() > 0.0).astype(int)
        n_pos = int(labels.sum())
        if n_pos == 0 or topk <= 0:
            return float("nan")
        order = np.argsort(scores)[::-1]
        order = order[:min(topk, scores.size)]
        tp = 0
        ap_sum = 0.0
        for i, idx in enumerate(order, start=1):
            if labels[idx] == 1:
                tp += 1
                ap_sum += tp / i  # precision at this positive
        return float(ap_sum / max(1, n_pos))

    # CFL estimate for κ
    deg_max = int(np.max(deg)) if deg.size else 0
    lam_max = 2.0 * float(deg_max)  # rough bound for combinatorial Laplacian
    kappa_cfl = cfl_limit / max(1e-12, dt * lam_max)

    rows: List[Tuple[float, float, float, float, float, float, float, float, float, float, float, float]] = []

    if da_values is not None and len(da_values) > 0 and dose_model == "scale_R":
        # Dose-controlled path: use gamma_fixed and scale R amplitude to hit desired D_a
        for da_target in da_values:
            gamma = float(gamma_fixed)
            for delta in delta_values:
                for kappa in kappa_values:
                    kappa_eff = min(float(kappa), float(kappa_cfl))
                    # Write phase with amplitude scaling
                    R_amp = (da_target * M0) / max(1e-12, gamma * T_write)
                    m = np.zeros(N, dtype=np.float64)
                    steps_w = int(math.ceil(T_write / dt))
                    for _ in range(steps_w):
                        m = update_memory(m, R_amp * R_mask, L, gamma=gamma, delta=delta, kappa=kappa_eff, dt=dt)
                    m_w = m.copy()
                    # Decay
                    steps_d = int(math.ceil(T_decay / dt))
                    zero_R = np.zeros_like(R_mask)
                    for _ in range(steps_d):
                        m = update_memory(m, zero_R, L, gamma=gamma, delta=delta, kappa=kappa_eff, dt=dt)
                    m_end = m
                    # Metrics
                    denom = float(np.mean(np.abs(m_w))) if np.any(m_w != 0) else 1.0
                    Ret = float(np.mean(np.abs(m_end))) / max(denom, 1e-9)
                    Fid_w = pearson_corr(m_w, R_mask)
                    Fid_e = pearson_corr(m_end, R_mask)
                    # Controls
                    R_shuf = rng.permutation(R_mask)
                    Fid_s = pearson_corr(m_end, R_shuf)
                    LR = L @ R_mask
                    L_end = L @ m_end
                    Fid_edge = pearson_corr(L_end, LR)
                    mask_in = (R_mask > 0.0)
                    scores = m_end
                    auc = auc_binary(scores, mask_in.astype(int))
                    if np.any(~mask_in):
                        mu_in = float(np.mean(scores[mask_in])) if np.any(mask_in) else float("nan")
                        mu_out = float(np.mean(scores[~mask_in]))
                        sd_out = float(np.std(scores[~mask_in])) + 1e-9
                        snr = (mu_in - mu_out) / sd_out
                    else:
                        snr = float("nan")
                    # AUPRC top-k and BPER
                    k_top = max(1, int(round(topk_frac * N)))
                    ap_k = average_precision_topk(scores, mask_in.astype(int), k_top)
                    bper = float(np.linalg.norm(L_norm @ m_end) / max(1e-12, np.linalg.norm(m_end)))

                    # Dimensionless groups (record the target D_a explicitly)
                    Da = float(da_target)
                    Lam = float(delta * T_decay)
                    Gam = float((kappa_eff * T_write) / (L_scale ** 2))

                    rows.append((Da, Lam, Gam, Ret, Fid_w, Fid_e, Fid_s, Fid_edge, float(auc), float(snr), float(ap_k), float(bper)))
    else:
        # Legacy path (no explicit dose control): iterate gamma_values with unit-amplitude R
        for gamma in gamma_values:
            for delta in delta_values:
                for kappa in kappa_values:
                    kappa_eff = min(float(kappa), float(kappa_cfl))
                    # Write with unit amplitude
                    m = np.zeros(N, dtype=np.float64)
                    steps_w = int(math.ceil(T_write / dt))
                    for _ in range(steps_w):
                        m = update_memory(m, R_mask, L, gamma=float(gamma), delta=delta, kappa=kappa_eff, dt=dt)
                    m_w = m.copy()
                    # Decay
                    steps_d = int(math.ceil(T_decay / dt))
                    zero_R = np.zeros_like(R_mask)
                    for _ in range(steps_d):
                        m = update_memory(m, zero_R, L, gamma=float(gamma), delta=delta, kappa=kappa_eff, dt=dt)
                    m_end = m
                    # Metrics
                    denom = float(np.mean(np.abs(m_w))) if np.any(m_w != 0) else 1.0
                    Ret = float(np.mean(np.abs(m_end))) / max(denom, 1e-9)
                    Fid_w = pearson_corr(m_w, R_mask)
                    Fid_e = pearson_corr(m_end, R_mask)
                    R_shuf = rng.permutation(R_mask)
                    Fid_s = pearson_corr(m_end, R_shuf)
                    LR = L @ R_mask
                    L_end = L @ m_end
                    Fid_edge = pearson_corr(L_end, LR)
                    mask_in = (R_mask > 0.0)
                    scores = m_end
                    auc = auc_binary(scores, mask_in.astype(int))
                    if np.any(~mask_in):
                        mu_in = float(np.mean(scores[mask_in])) if np.any(mask_in) else float("nan")
                        mu_out = float(np.mean(scores[~mask_in]))
                        sd_out = float(np.std(scores[~mask_in])) + 1e-9
                        snr = (mu_in - mu_out) / sd_out
                    else:
                        snr = float("nan")
                    k_top = max(1, int(round(topk_frac * N)))
                    ap_k = average_precision_topk(scores, mask_in.astype(int), k_top)
                    bper = float(np.linalg.norm(L_norm @ m_end) / max(1e-12, np.linalg.norm(m_end)))

                    # Dimensionless groups from gamma, delta, kappa
                    Theta, Da, Lam_w, Gam = compute_dimensionless_groups(
                        eta=1.0, M0=M0, gamma=float(gamma), R0=R0, T=T_write, delta=delta, kappa=kappa_eff, L_scale=L_scale
                    )
                    Lam = float(delta * T_decay)
                    rows.append((float(Da), Lam, float(Gam), Ret, Fid_w, Fid_e, Fid_s, Fid_edge, float(auc), float(snr), float(ap_k), float(bper)))
    return rows


# ---------------------------
# Entry point
# ---------------------------

def main():
    # 1) Junction logistic
    theta = 2.0
    delta_m = np.linspace(-2.0, 2.0, 17)
    X, P = run_junction_logistic(theta=theta, delta_m_values=delta_m, trials=2000)
    print("# Junction logistic (CSV): Theta*Delta_m, P(A)")
    for x, p in zip(X, P):
        print(f"{x:.6f},{p:.6f}")

    # 2) Curvature scaling (unsigned overview; small-bend regime)
    Xc, Yc = run_curvature_scaling(
        nx=21, ny=21,
        theta_values=(0.5, 1.0, 2.0, 3.0, 4.0),
        pulses=160, mode="ray", dt=0.10, nsteps=200
    )
    print("\n# Curvature scaling (CSV): Theta*|grad m|, mean(kappa_path)")
    for x, y in zip(Xc, Yc):
        print(f"{x:.6f},{y:.8f}")

    # 2b) Curvature: calibration unit test + signed falsification (12 X values)
    cal_res = calibrate_curvature_on_arcs(
        R_values=(20.0, 40.0, 80.0), n_points=200, noise=0.0, out_png="outputs/curvature_calibration.png"
    )
    print("\n# Curvature calibration test (CSV): R, kappa_mean, kappa_std, frac_error")
    for (R, km, ks, fe) in cal_res:
        print(f"{R:.6f},{km:.8f},{ks:.8f},{fe:.6f}")

    Xs, Ys, Yse, sign_id = run_curvature_scaling_signed(
        nx=41, ny=41,
        x_values=np.linspace(0.02, 0.30, 12),  # avoid heading saturation
        pulses_per_x=96, dt=0.08, nsteps=400, signed_check_mids=3
    )
    print("\n# Curvature scaling signed (CSV): X, mean_kappa, se_kappa, seed, sign_id")
    seed_val = 77
    for x, mu, se, sgn in zip(Xs, Ys, Yse, sign_id):
        print(f"{x:.6f},{mu:.8f},{se:.8f},{seed_val:d},{int(sgn)}")

    # 3) Stability band (dose-controlled write→decay with discriminative metrics)
    rows = run_stability_band(
        nx=21, ny=21, T_write=5.0, T_decay=5.0, dt=0.2,
        da_values=(0.5, 1.0, 1.5, 2.0), gamma_fixed=1.0, dose_model="scale_R",
        delta_values=(0.05, 0.1, 0.2, 0.3),
        kappa_values=(0.1, 0.3, 0.6, 1.0),
        topk_frac=0.05, cfl_limit=0.9
    )
    print("\n# Stability band (CSV|dose_model=scale_R): D_a, Lambda, Gamma, Retention, Fidelity_w, Fidelity_end, Fidelity_shuffle_end, Fidelity_edge_end, AUC_end, SNR_end, AUPRC_topk, BPER")
    for (Da, Lam, Gam, Ret, Fid_w, Fid_e, Fid_s, Fid_edge, AUC_e, SNR_e, APk, BPER) in rows:
        print(f"{Da:.6f},{Lam:.6f},{Gam:.6f},{Ret:.6f},{Fid_w:.6f},{Fid_e:.6f},{Fid_s:.6f},{Fid_edge:.6f},{AUC_e:.6f},{SNR_e:.6f},{APk:.6f},{BPER:.6f}")


if __name__ == "__main__":
    main()]]></content>
    </file>
    <file>
      <path>physics/memory_steering/plot_memory_steering.py</path>
      <content><![CDATA["""
Plotting helper for memory-steering experiments.

- Parses outputs/memory_steering_results.csv (supports 4- or 5-column stability).
- Produces figures in outputs/.
- Prints a concise metrics summary that directly tests the three predictions:
  1) Junction logistic collapse
  2) Curvature scaling in the ray limit
  3) Stability band with write→decay protocol (Retention, Fidelity)

Usage:
- python3 -m fum_rt.utils.plot_memory_steering
  or
- python3 fum_rt/utils/plot_memory_steering.py   (if PYTHONPATH=. is set)
"""

import os
import math
import numpy as np

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


def parse_results(src: str):
    lines = open(src, "r").read().splitlines()
    mode = 0
    Jx, Jp = [], []
    Cx, Cy = [], []
    # Extended stability rows: up to 10 columns:
    # (Da, Lam, Gam, Ret, Fid_w, Fid_end, Fid_shuffle_end, Fid_edge_end, AUC_end, SNR_end)
    SB = []
    # Signed curvature aggregates
    Sx, Smy, Sse, Sseed, Ssign = [], [], [], [], []
    for ln in lines:
        if ln.startswith("# Junction logistic"):
            mode = 1
            continue
        if ln.startswith("# Curvature scaling signed"):
            mode = 22
            continue
        if ln.startswith("# Curvature scaling"):
            mode = 2
            continue
        if ln.startswith("# Stability band"):
            mode = 3
            continue
        if not ln.strip() or ln.strip().startswith("#"):
            continue
        parts = [p.strip() for p in ln.split(",")]
        try:
            if mode == 1 and len(parts) >= 2:
                Jx.append(float(parts[0]))
                Jp.append(float(parts[1]))
            elif mode == 2 and len(parts) >= 2:
                Cx.append(float(parts[0]))
                Cy.append(float(parts[1]))
            elif mode == 22 and len(parts) >= 4:
                Sx.append(float(parts[0]))
                Smy.append(float(parts[1]))
                Sse.append(float(parts[2]))
                seed = float(parts[3]) if len(parts) >= 4 else float("nan")
                sign = float(parts[4]) if len(parts) >= 5 else float("nan")
                Sseed.append(seed)
                Ssign.append(sign)
            elif mode == 3:
                # Support 4..10 columns; pad with NaN
                vals = []
                for p in parts[:10]:
                    try:
                        vals.append(float(p))
                    except Exception:
                        vals.append(float("nan"))
                while len(vals) < 10:
                    vals.append(float("nan"))
                SB.append(tuple(vals[:10]))
        except Exception:
            # Skip malformed lines
            pass
    Jx = np.asarray(Jx, float)
    Jp = np.asarray(Jp, float)
    Cx = np.asarray(Cx, float)
    Cy = np.asarray(Cy, float)
    SB = np.asarray(SB, float) if len(SB) > 0 else np.zeros((0, 10), float)
    Sx = np.asarray(Sx, float)
    Smy = np.asarray(Smy, float)
    Sse = np.asarray(Sse, float)
    Sseed = np.asarray(Sseed, float)
    Ssign = np.asarray(Ssign, float)
    return Jx, Jp, Cx, Cy, SB, Sx, Smy, Sse, Sseed, Ssign


def fit_logistic(x: np.ndarray, p: np.ndarray):
    valid = (p > 0) & (p < 1) & np.isfinite(x) & np.isfinite(p)
    if valid.sum() < 2:
        return np.nan, np.nan, np.nan, None, None
    xv = x[valid]
    pv = p[valid]
    logit = np.log(pv / (1.0 - pv))
    X = np.vstack([xv, np.ones_like(xv)]).T
    k, b = np.linalg.lstsq(X, logit, rcond=None)[0]
    xgrid = np.linspace(np.nanmin(x), np.nanmax(x), 400)
    pred = 1.0 / (1.0 + np.exp(-(k * xgrid + b)))
    p_pred = 1.0 / (1.0 + np.exp(-(k * x + b)))
    p_mean = np.nanmean(p) if p.size else np.nan
    ss_res = np.nansum((p - p_pred) ** 2)
    ss_tot = np.nansum((p - p_mean) ** 2)
    R2 = 1.0 - (ss_res / ss_tot) if (ss_tot > 0) else np.nan
    return float(k), float(b), float(R2), xgrid, pred


def fit_linear(x: np.ndarray, y: np.ndarray):
    mask = np.isfinite(x) & np.isfinite(y)
    if mask.sum() < 2:
        return np.nan, np.nan, np.nan, np.nan
    A = np.vstack([x[mask], np.ones(mask.sum())]).T
    a, c = np.linalg.lstsq(A, y[mask], rcond=None)[0]
    y_pred = a * x + c
    y_mean = np.nanmean(y)
    ss_res = np.nansum((y - y_pred) ** 2)
    ss_tot = np.nansum((y - y_mean) ** 2)
    R2 = 1.0 - (ss_res / ss_tot) if (ss_tot > 0) else np.nan
    xm = np.nanmean(x)
    ym = np.nanmean(y)
    rden = math.sqrt(np.nansum((x - xm) ** 2) * np.nansum((y - ym) ** 2))
    r = np.nan if (rden == 0) else float(np.nansum((x - xm) * (y - ym)) / rden)
    return float(a), float(c), float(R2), r


def pivot_heatmap(SB: np.ndarray, value_index: int = 3):
    # value_index:
    #   3 = Retention
    #   5 = Fidelity_end
    #   6 = Fidelity_shuffle_end
    if SB.size == 0:
        return None
    Da = SB[:, 0]
    Lam = SB[:, 1]
    Val = SB[:, value_index]
    uDa = np.unique(Da)
    uLam = np.unique(Lam)
    H = np.full((uLam.size, uDa.size), np.nan, float)
    for i, lam in enumerate(uLam):
        for j, da in enumerate(uDa):
            mask = (Lam == lam) & (Da == da)
            if np.any(mask):
                H[i, j] = np.nanmean(Val[mask])
    extent = [uDa.min(), uDa.max(), uLam.min(), uLam.max()]
    return H, uDa, uLam, extent


def plot_all(src: str = os.path.join("outputs", "memory_steering_results.csv"),
             outdir: str = "outputs"):
    if not os.path.exists(src):
        raise SystemExit(f"[error] Missing {src}. Generate it first with: python3 -m fum_rt.utils.memory_steering_experiments > {src}")
    os.makedirs(outdir, exist_ok=True)

    Jx, Jp, Cx, Cy, SB, Sx, Smy, Sse, Sseed, Ssign = parse_results(src)

    # ---------- Plot 1: Junction logistic ----------
    k, b, R2_log, xgrid, pred = fit_logistic(Jx, Jp)
    fig1, ax1 = plt.subplots(figsize=(6, 4))
    ax1.scatter(Jx, Jp, s=24, color="#1f77b4", label="data")
    if xgrid is not None:
        ax1.plot(xgrid, pred, color="#d62728", lw=2,
                 label=f"fit: k={k:.3f}, b={b:.3f}, R2={R2_log:.3f}")
    ax1.set_xlabel("Theta * Delta m")
    ax1.set_ylabel("P(A)")
    ax1.set_title("Junction logistic collapse")
    ax1.legend(loc="lower right")
    fig1.tight_layout()
    p1 = os.path.join(outdir, "junction_logistic.png")
    fig1.savefig(p1, dpi=160)

    # ---------- Plot 2: Curvature scaling ----------
    a, c, R2_lin, pearson = fit_linear(Cx, Cy)
    fig2, ax2 = plt.subplots(figsize=(6, 4))
    ax2.scatter(Cx, Cy, s=18, color="#2ca02c", alpha=0.8, label="data")
    if np.isfinite(a):
        xline = np.linspace(np.nanmin(Cx), np.nanmax(Cx), 100) if Cx.size else np.array([0, 1])
        ax2.plot(xline, a * xline + c, color="#9467bd", lw=2,
                 label=f"fit: a={a:.3f}, c={c:.3f}\nR2={R2_lin:.3f}, r={pearson:.3f}")
    ax2.set_xlabel("Theta * |grad m|")
    ax2.set_ylabel("mean(kappa_path)")
    ax2.set_title("Curvature scaling")
    ax2.legend(loc="upper left")
    fig2.tight_layout()
    p2 = os.path.join(outdir, "curvature_scaling.png")
    fig2.savefig(p2, dpi=160)

    # ---------- Plot 2b: Curvature scaling (signed) ----------
    p_signed = os.path.join(outdir, "curvature_scaling_signed.png")
    if Sx.size > 0:
        fig2b, ax2b = plt.subplots(figsize=(6, 4))
        sign_labels = {0: "baseline", 1: "flip_grad", 2: "flip_theta"}
        colors = {0: "#1f77b4", 1: "#ff7f0e", 2: "#2ca02c"}
        valid_signs = np.unique(Ssign[np.isfinite(Ssign)]).astype(int) if np.any(np.isfinite(Ssign)) else []
        for sid in sorted(valid_signs.tolist()):
            mask = (Ssign == sid)
            if not np.any(mask):
                continue
            ax2b.errorbar(Sx[mask], Smy[mask], yerr=Sse[mask], fmt="o", ms=4,
                          color=colors.get(sid, "#7f7f7f"),
                          label=sign_labels.get(sid, f"sign={sid}"), alpha=0.9)
        ax2b.set_xlabel("X = Theta * |grad m|")
        ax2b.set_ylabel("mean(kappa_path)")
        ax2b.set_title("Curvature scaling (signed invariance)")
        ax2b.legend(loc="upper left", fontsize=8)
        fig2b.tight_layout()
        fig2b.savefig(p_signed, dpi=160)
    else:
        # No signed data present; do not create a figure
        pass

    # ---------- Plot 3: Stability band heatmaps ----------
    fig3, ax3 = plt.subplots(1, 2, figsize=(11, 4))
    if SB.size > 0:
        Hret = pivot_heatmap(SB, 3)
        Hfid = pivot_heatmap(SB, 5)  # Fidelity_end
        if Hret is not None:
            H, uDa, uLam, extent = Hret
            im = ax3[0].imshow(H, origin="lower", aspect="auto", extent=extent, cmap="viridis")
            ax3[0].set_xlabel("D_a")
            ax3[0].set_ylabel("Lambda")
            ax3[0].set_title("Retention (avg over Gamma)")
            cbar = fig3.colorbar(im, ax=ax3[0])
            cbar.set_label("Retention")
        if Hfid is not None:
            Hf, uDa, uLam, extent = Hfid
            im2 = ax3[1].imshow(Hf, origin="lower", aspect="auto", extent=extent, cmap="magma")
            ax3[1].set_xlabel("D_a")
            ax3[1].set_ylabel("Lambda")
            ax3[1].set_title("Fidelity_end (avg over Gamma)")
            cbar2 = fig3.colorbar(im2, ax=ax3[1])
            cbar2.set_label("Fidelity_end")
    else:
        ax3[0].text(0.5, 0.5, "No stability data", ha="center", va="center")
        ax3[1].axis("off")
    fig3.tight_layout()
    p3 = os.path.join(outdir, "stability_band.png")
    fig3.savefig(p3, dpi=160)

    # ---------- Plot 3b: Stability band heatmaps per Gamma (slices) ----------
    p3_ret_by_gamma = p3_fid_by_gamma = p3_auc_by_gamma = p3_snr_by_gamma = None
    if SB.size > 0:
        def _slice_by_gamma(value_index: int, label: str, cmap: str, out_name: str):
            Da = SB[:, 0]; Lam = SB[:, 1]; Gam = SB[:, 2]; Val = SB[:, value_index]
            mask = np.isfinite(Da) & np.isfinite(Lam) & np.isfinite(Gam) & np.isfinite(Val)
            if not np.any(mask):
                return None
            Da = Da[mask]; Lam = Lam[mask]; Gam = Gam[mask]; Val = Val[mask]
            uDa = np.unique(Da); uLam = np.unique(Lam); uGam = np.unique(Gam)

            # Build per-Gamma heatmaps and collect global color limits for consistent scaling
            Hs = {}
            vmin, vmax = float("inf"), float("-inf")
            for g in uGam:
                m = (Gam == g)
                H = np.full((uLam.size, uDa.size), np.nan, float)
                for i, lam in enumerate(uLam):
                    for j, da in enumerate(uDa):
                        mm = m & (Lam == lam) & (Da == da)
                        if np.any(mm):
                            v = float(np.nanmean(Val[mm]))
                            H[i, j] = v
                Hs[float(g)] = H
                finite_vals = H[np.isfinite(H)]
                if finite_vals.size > 0:
                    vmin = min(vmin, float(np.nanmin(finite_vals)))
                    vmax = max(vmax, float(np.nanmax(finite_vals)))
            if not np.isfinite(vmin) or not np.isfinite(vmax):
                vmin = vmax = None

            # Layout panels
            nGam = uGam.size
            ncol = int(np.ceil(np.sqrt(nGam)))
            nrow = int(np.ceil(nGam / ncol))
            fig_g, axes_g = plt.subplots(nrow, ncol, figsize=(4.0 * ncol, 3.2 * nrow))
            axes_g = np.atleast_2d(axes_g).reshape(-1)
            im_last = None
            extent = [uDa.min(), uDa.max(), uLam.min(), uLam.max()]
            for k, g in enumerate(uGam):
                axg = axes_g[k]
                H = Hs[float(g)]
                im = axg.imshow(H, origin="lower", aspect="auto", extent=extent, cmap=cmap, vmin=vmin, vmax=vmax)
                axg.set_title(f"Gamma={g:.3f}")
                axg.set_xlabel("D_a")
                axg.set_ylabel("Lambda")
                im_last = im
            for k in range(uGam.size, len(axes_g)):
                axes_g[k].axis("off")
            fig_g.tight_layout(rect=[0, 0, 0.92, 1])
            if im_last is not None:
                cbar = fig_g.colorbar(im_last, ax=axes_g[:nGam].tolist(), fraction=0.02, pad=0.02)
                cbar.set_label(label)
            path = os.path.join(outdir, out_name)
            fig_g.savefig(path, dpi=160)
            return path

        # Generate per-Gamma panels for key metrics
        p3_ret_by_gamma = _slice_by_gamma(3, "Retention", "viridis", "stability_retention_by_gamma.png")
        p3_fid_by_gamma = _slice_by_gamma(5, "Fidelity_end", "magma", "stability_fidelity_by_gamma.png")
        if SB.shape[1] > 8:
            tmp = _slice_by_gamma(8, "AUC_end", "plasma", "stability_auc_by_gamma.png")
            if tmp:
                p3_auc_by_gamma = tmp
        if SB.shape[1] > 9:
            tmp = _slice_by_gamma(9, "SNR_end", "cividis", "stability_snr_by_gamma.png")
            if tmp:
                p3_snr_by_gamma = tmp

    # ---------- Combined summary panel ----------
    fig, axes = plt.subplots(1, 3, figsize=(14, 4))
    # Panel A (logistic)
    axes[0].scatter(Jx, Jp, s=20, color="#1f77b4")
    if xgrid is not None:
        axes[0].plot(xgrid, pred, color="#d62728", lw=2)
    axes[0].set_xlabel("Theta * Delta m")
    axes[0].set_ylabel("P(A)")
    axes[0].set_title(f"Junction (R2={R2_log:.3f})")
    # Panel B (curvature)
    axes[1].scatter(Cx, Cy, s=16, color="#2ca02c", alpha=0.8)
    if np.isfinite(a):
        xline = np.linspace(np.nanmin(Cx), np.nanmax(Cx), 100) if Cx.size else np.array([0, 1])
        axes[1].plot(xline, a * xline + c, color="#9467bd", lw=2)
    axes[1].set_xlabel("Theta * |grad m|")
    axes[1].set_ylabel("mean(kappa_path)")
    axes[1].set_title(f"Curvature (R2={R2_lin:.3f}, r={pearson:.3f})")
    # Panel C (stability: retention heatmap)
    if SB.size > 0:
        Hret = pivot_heatmap(SB, 3)
        if Hret is not None:
            H, uDa, uLam, extent = Hret
            axes[2].imshow(H, origin="lower", aspect="auto", extent=extent, cmap="viridis")
            axes[2].set_xlabel("D_a")
            axes[2].set_ylabel("Lambda")
            axes[2].set_title("Retention (avg Γ)")
    else:
        axes[2].text(0.5, 0.5, "No stability data", ha="center", va="center")
    fig.tight_layout()
    p4 = os.path.join(outdir, "memory_steering_summary.png")
    fig.savefig(p4, dpi=160)

    # ---------- Metrics summary (text) ----------
    print("=== METRICS SUMMARY ===")
    if np.isfinite(k):
        print(f"[JUNCTION] k={k:.3f}, b={b:.3f}, R2={R2_log:.3f}")
    else:
        print("[JUNCTION] insufficient data")

    if np.isfinite(a):
        print(f"[CURVATURE] a={a:.3f}, c={c:.3f}, R2={R2_lin:.3f}, r={pearson:.3f}")
    else:
        print("[CURVATURE] insufficient data")

    # Signed curvature invariance summary
    if 'Sx' in locals() and np.size(Sx) > 0:
        passes_grad, total_grad = 0, 0
        passes_theta, total_theta = 0, 0
        tol = 1e-9

        def _get_mu_se(sign_id, xval):
            m = (Ssign == sign_id)
            if not np.any(m):
                return None, None
            m = m & np.isfinite(Sx) & np.isfinite(Smy) & np.isfinite(Sse) & np.isclose(Sx, xval, atol=tol, rtol=0.0)
            if not np.any(m):
                return None, None
            idx = np.where(m)[0][0]
            return float(Smy[idx]), float(Sse[idx])

        for x0 in np.unique(Sx[Ssign == 0]):
            mu0, se0 = _get_mu_se(0, x0)
            if mu0 is None:
                continue
            mu1, se1 = _get_mu_se(1, x0)
            if mu1 is not None:
                z = abs(mu1 - mu0) / max(1e-12, math.sqrt(se1 * se1 + se0 * se0))
                total_grad += 1
                if z <= 2.0:
                    passes_grad += 1
            mu2, se2 = _get_mu_se(2, x0)
            if mu2 is not None:
                z = abs(mu2 - mu0) / max(1e-12, math.sqrt(se2 * se2 + se0 * se0))
                total_theta += 1
                if z <= 2.0:
                    passes_theta += 1
        rate_g = (passes_grad / total_grad) if total_grad > 0 else float("nan")
        rate_t = (passes_theta / total_theta) if total_theta > 0 else float("nan")
        print(f"[CURVATURE|signed] invariance pass (|Δ| ≤ 2σ): flip_grad={rate_g:.2%} over {total_grad} pairs, "
              f"flip_theta={rate_t:.2%} over {total_theta} pairs")
    else:
        print("[CURVATURE|signed] no signed data")

    if SB.size > 0:
        Da = SB[:, 0]; Lam = SB[:, 1]; Gam = SB[:, 2]
        Ret = SB[:, 3]
        Fid_w = SB[:, 4] if SB.shape[1] > 4 else np.full_like(Ret, np.nan)
        Fid_e = SB[:, 5] if SB.shape[1] > 5 else np.full_like(Ret, np.nan)
        Fid_shuf = SB[:, 6] if SB.shape[1] > 6 else np.full_like(Ret, np.nan)
        Fid_edge = SB[:, 7] if SB.shape[1] > 7 else np.full_like(Ret, np.nan)
        AUC_e = SB[:, 8] if SB.shape[1] > 8 else np.full_like(Ret, np.nan)
        SNR_e = SB[:, 9] if SB.shape[1] > 9 else np.full_like(Ret, np.nan)

        robust = Da >= Lam
        mean_ret_rob = float(np.nanmean(Ret[robust])) if robust.any() else float("nan")
        mean_ret_non = float(np.nanmean(Ret[~robust])) if (~robust).any() else float("nan")
        mean_fid_e_rob = float(np.nanmean(Fid_e[robust])) if robust.any() else float("nan")
        mean_fid_e_non = float(np.nanmean(Fid_e[~robust])) if (~robust).any() else float("nan")
        mean_fid_shuf = float(np.nanmean(np.abs(Fid_shuf))) if np.any(np.isfinite(Fid_shuf)) else float("nan")
        mean_auc_rob = float(np.nanmean(AUC_e[robust])) if robust.any() else float("nan")
        mean_auc_non = float(np.nanmean(AUC_e[~robust])) if (~robust).any() else float("nan")
        mean_snr_rob = float(np.nanmean(SNR_e[robust])) if robust.any() else float("nan")
        mean_snr_non = float(np.nanmean(SNR_e[~robust])) if (~robust).any() else float("nan")

        print(f"[STABILITY] Retention mean: robust={mean_ret_rob:.3f}, non={mean_ret_non:.3f}")
        print(f"[STABILITY] Fidelity_end mean: robust={mean_fid_e_rob:.3f}, non={mean_fid_e_non:.3f}")
        print(f"[STABILITY] Fidelity_shuffle_end |mean|: {mean_fid_shuf:.3f}")
        if np.any(np.isfinite(AUC_e)):
            print(f"[STABILITY] AUC_end mean: robust={mean_auc_rob:.3f}, non={mean_auc_non:.3f}")
        if np.any(np.isfinite(SNR_e)):
            print(f"[STABILITY] SNR_end mean: robust={mean_snr_rob:.3f}, non={mean_snr_non:.3f}")

        # ---- Per-Gamma analysis (band visibility without averaging it away) ----
        uGam = np.unique(Gam[np.isfinite(Gam)])
        best = None  # track Gamma with largest fidelity_end separation
        for g in uGam:
            mask_g = np.isfinite(Gam) & (Gam == g)
            if not np.any(mask_g):
                continue
            rob_g = robust & mask_g
            non_g = (~robust) & mask_g
            ret_rob_g = float(np.nanmean(Ret[rob_g])) if np.any(rob_g) else float("nan")
            ret_non_g = float(np.nanmean(Ret[non_g])) if np.any(non_g) else float("nan")
            fid_rob_g = float(np.nanmean(Fid_e[rob_g])) if np.any(rob_g) else float("nan")
            fid_non_g = float(np.nanmean(Fid_e[non_g])) if np.any(non_g) else float("nan")
            d_ret = (ret_rob_g - ret_non_g) if (np.isfinite(ret_rob_g) and np.isfinite(ret_non_g)) else float("nan")
            d_fid = (fid_rob_g - fid_non_g) if (np.isfinite(fid_rob_g) and np.isfinite(fid_non_g)) else float("nan")
            print(f"[STABILITY|Gamma] Gam={g:.3f} Ret: rob={ret_rob_g:.3f}, non={ret_non_g:.3f}, Δ={d_ret:.3f} | "
                  f"Fid_end: rob={fid_rob_g:.3f}, non={fid_non_g:.3f}, Δ={d_fid:.3f}")
            if np.isfinite(d_fid):
                if best is None or abs(d_fid) > abs(best[1]):
                    best = (g, d_fid)
        if best is not None:
            print(f"[STABILITY|Gamma] Max |Δ Fidelity_end| at Gam={best[0]:.3f}: Δ={best[1]:.3f}")
    else:
        print("[STABILITY] no data")

    # Report saved plot paths (signed plot may be absent if no data)
    saved = [p1, p2, p3, p4]
    if 'p_signed' in locals():
        saved.append(p_signed)
    # Add per-Gamma heatmaps if created
    for extra_name in ("p3_ret_by_gamma", "p3_fid_by_gamma", "p3_auc_by_gamma", "p3_snr_by_gamma"):
        if extra_name in locals():
            extra_val = locals()[extra_name]
            if extra_val:
                saved.append(extra_val)
    print("Saved plots:", *saved)


if __name__ == "__main__":
    src = os.environ.get("FUM_RESULTS_CSV", os.path.join("outputs", "memory_steering_results.csv"))
    outdir = os.environ.get("FUM_RESULTS_OUT", "outputs")
    plot_all(src, outdir)]]></content>
    </file>
    <file>
      <path>physics/rd_dispersion_runner.py</path>
      <content><![CDATA[#!/usr/bin/env python3
"""
RD dispersion validation runner (linear regime) for fum_rt.

CHANGE REASON:
- This file mirrors the validated physics from derivation scripts to the runtime stack.
- We have PROVEN the RD linear dispersion σ(k) = r - D k^2 via reproducible scripts and derivations:
  [rd_dispersion_experiment.py](Prometheus_FUVDM/derivation/code/physics/rd_dispersion_experiment.py),
  [rd_validation_plan.md](Prometheus_FUVDM/derivation/rd_validation_plan.md),
  [CORRECTIONS.md](Prometheus_FUVDM/derivation/computational_proofs/CORRECTIONS.md).
- This runner provides an independent, apples-to-apples check inside fum_rt with identical metrics/output schema.
- It DOES NOT alter runtime dynamics; it is a validation wrapper only.
"""

import argparse
import json
import math
import os
import sys
import time

import numpy as np

# Ensure repository root on sys.path so we can import Prometheus_FUVDM.*
_REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)

# Import validated experiment utilities from derivation stack
from Prometheus_FUVDM.derivation.code.physics.rd_dispersion_experiment import (  # noqa: E402
    run_linear_sim,
    analyze_dispersion,
    plot_and_save_dispersion,
)


def main():
    parser = argparse.ArgumentParser(description="fum_rt mirror: Validate RD linear dispersion σ(k) with identical metrics/schema.")
    parser.add_argument("--N", type=int, default=1024)
    parser.add_argument("--L", type=float, default=200.0)
    parser.add_argument("--D", type=float, default=1.0)
    parser.add_argument("--r", type=float, default=0.25)
    parser.add_argument("--T", type=float, default=10.0)
    parser.add_argument("--cfl", type=float, default=0.2)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--amp0", type=float, default=1e-6, help="Initial noise amplitude (std dev).")
    parser.add_argument("--record", type=int, default=80, help="Number of snapshots to record.")
    parser.add_argument("--m_max", type=int, default=64, help="Max mode index m to fit (clamped by N//2).")
    parser.add_argument("--fit_start", type=float, default=0.1, help="fractional start of fit window")
    parser.add_argument("--fit_end", type=float, default=0.4, help="fractional end of fit window")
    parser.add_argument("--outdir", type=str, default=None, help="base output dir; defaults to fum_rt/physics/outputs next to this script")
    parser.add_argument("--figure", type=str, default=None, help="override figure path; otherwise script_name_timestamp.png in outdir/figures")
    parser.add_argument("--log", type=str, default=None, help="override log path; otherwise script_name_timestamp.json in outdir/logs")
    args = parser.parse_args()

    # Output routing (identical structure: base/figures + base/logs, UTC timestamped filenames)
    script_name = os.path.splitext(os.path.basename(__file__))[0]
    tstamp = time.strftime("%Y%m%dT%H%M%SZ", time.gmtime())
    default_base = os.path.abspath(os.path.join(os.path.dirname(__file__), "outputs"))
    base_outdir = os.path.abspath(args.outdir) if args.outdir else default_base
    fig_dir = os.path.join(base_outdir, "figures")
    log_dir = os.path.join(base_outdir, "logs")
    figure_path = args.figure if args.figure else os.path.join(fig_dir, f"{script_name}_{tstamp}.png")
    log_path = args.log if args.log else os.path.join(log_dir, f"{script_name}_{tstamp}.json")
    os.makedirs(os.path.dirname(figure_path), exist_ok=True)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)

    t0 = time.time()
    sim = run_linear_sim(args.N, args.L, args.D, args.r, args.T, args.cfl, args.seed, amp0=args.amp0, record_slices=args.record)
    analysis = analyze_dispersion(sim, args.D, args.r, args.L, args.m_max, (args.fit_start, args.fit_end))
    elapsed = time.time() - t0

    # Produce identical figure and payload schema
    plot_and_save_dispersion(analysis, figure_path, title=f"RD dispersion (linear): D={args.D}, r={args.r}")

    # Acceptance criteria (conservative for multi-mode fit; mirrors derivation script)
    acceptance = {
        "med_rel_err_max": 0.10,
        "r2_array_min": 0.98,
    }
    med_rel_err = float(analysis["med_rel_err"])
    r2_array = float(analysis["r2_array"])
    passed = (
        (math.isfinite(med_rel_err) and med_rel_err <= acceptance["med_rel_err_max"]) and
        (math.isfinite(r2_array) and r2_array >= acceptance["r2_array_min"])
    )

    payload = {
        "theory": {
            "continuum": "sigma_c(k) = r - D k^2",
            "discrete": "sigma_d(m) = r - (4 D / dx^2) sin^2(pi m / N)"
        },
        "params": {
            "N": args.N, "L": args.L, "D": args.D, "r": args.r, "T": args.T,
            "cfl": args.cfl, "seed": args.seed, "amp0": args.amp0,
            "record": args.record, "m_max": args.m_max,
            "fit_start": args.fit_start, "fit_end": args.fit_end,
        },
        "metrics": {
            "med_rel_err": med_rel_err,
            "r2_array": r2_array,
            "acceptance": acceptance,
            "passed": passed,
        },
        "series": {
            "m_vals": analysis["m_vals"],
            "k_vals": analysis["k_vals"],
            "sigma_meas": analysis["sigma_meas"],
            "sigma_disc": analysis["sigma_disc"],
            "sigma_cont": analysis["sigma_cont"],
            "r2_meas": analysis["r2_meas"],
            "rel_err": analysis["rel_err"],
            "good_mask": analysis["good_mask"],
        },
        "outputs": {
            "figure": figure_path
        },
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "elapsed_sec": elapsed,
    }

    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    print(json.dumps({
        "figure": figure_path,
        "log": log_path,
        "med_rel_err": payload["metrics"]["med_rel_err"],
        "r2_array": payload["metrics"]["r2_array"],
        "passed": payload["metrics"]["passed"],
    }, indent=2))


if __name__ == "__main__":
    main()]]></content>
    </file>
    <file>
      <path>physics/rd_front_speed_runner.py</path>
      <content><![CDATA[#!/usr/bin/env python3
"""
RD front-speed validation runner (Fisher-KPP) for fum_rt.

CHANGE REASON:
- This file mirrors the validated physics from derivation scripts to the runtime stack.
- We have PROVEN the Fisher-KPP front speed c = 2√(D r) via reproducible scripts and derivations:
  [rd_front_speed_experiment.py](Prometheus_FUVDM/derivation/code/physics/rd_front_speed_experiment.py:1),
  [rd_front_speed_validation.md](Prometheus_FUVDM/derivation/rd_front_speed_validation.md:1),
  [CORRECTIONS.md](Prometheus_FUVDM/derivation/computational_proofs/CORRECTIONS.md:1).
- This runner provides an independent, apples-to-apples check inside fum_rt with identical metrics/output schema.
- It DOES NOT alter runtime dynamics; it is a validation wrapper only.
"""

import argparse
import json
import math
import os
import sys
import time
from typing import Tuple

import numpy as np

# Ensure repository root on sys.path so we can import Prometheus_FUVDM.*
_REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)

# Import validated experiment utilities from derivation stack
from Prometheus_FUVDM.derivation.code.physics.rd_front_speed_experiment import (  # noqa: E402
    run_sim,
    plot_and_save,
)


def main():
    parser = argparse.ArgumentParser(description="fum_rt mirror: Validate Fisher-KPP front speed c=2√(Dr) with identical metrics/schema.")
    parser.add_argument("--N", type=int, default=1024)
    parser.add_argument("--L", type=float, default=200.0)
    parser.add_argument("--D", type=float, default=1.0)
    parser.add_argument("--r", type=float, default=0.25)
    parser.add_argument("--T", type=float, default=80.0)
    parser.add_argument("--cfl", type=float, default=0.2)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--level", type=float, default=0.1)
    parser.add_argument("--x0", type=float, default=-60.0)
    parser.add_argument("--fit_start", type=float, default=0.6, help="fractional start of fit window")
    parser.add_argument("--fit_end", type=float, default=0.9, help="fractional end of fit window")
    parser.add_argument("--outdir", type=str, default=None, help="base output dir; defaults to fum_rt/physics/outputs next to this script")
    parser.add_argument("--figure", type=str, default=None, help="override figure path; otherwise script_name_timestamp.png in outdir/figures")
    parser.add_argument("--log", type=str, default=None, help="override log path; otherwise script_name_timestamp.json in outdir/logs")
    parser.add_argument("--noise_amp", type=float, default=0.0, help="optional gated noise amplitude (applied only left of the front)")
    args = parser.parse_args()

    # Output routing (identical structure: base/figures + base/logs, UTC timestamped filenames)
    script_name = os.path.splitext(os.path.basename(__file__))[0]
    tstamp = time.strftime("%Y%m%dT%H%M%SZ", time.gmtime())
    default_base = os.path.abspath(os.path.join(os.path.dirname(__file__), "outputs"))
    base_outdir = os.path.abspath(args.outdir) if args.outdir else default_base
    fig_dir = os.path.join(base_outdir, "figures")
    log_dir = os.path.join(base_outdir, "logs")
    figure_path = args.figure if args.figure else os.path.join(fig_dir, f"{script_name}_{tstamp}.png")
    log_path = args.log if args.log else os.path.join(log_dir, f"{script_name}_{tstamp}.json")
    os.makedirs(os.path.dirname(figure_path), exist_ok=True)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)

    t0 = time.time()
    data = run_sim(
        args.N, args.L, args.D, args.r, args.T, args.cfl, args.seed,
        level=args.level,
        x0=args.x0,
        fit_frac=(args.fit_start, args.fit_end),
        noise_amp=args.noise_amp,
    )
    elapsed = time.time() - t0

    # Produce identical figure and payload schema
    plot_and_save(data, figure_path)

    c_meas = data["c_meas"]
    c_abs = data["c_abs"]
    c_th = data["c_th"]
    rel_err = data["rel_err"]
    r2 = data["r2"]

    payload = {
        "theory": "Fisher-KPP front speed c=2*sqrt(D*r)",
        "params": {
            "N": args.N, "L": args.L, "D": args.D, "r": args.r, "T": args.T,
            "cfl": args.cfl, "seed": args.seed, "level": args.level,
            "x0": args.x0, "fit_start": args.fit_start, "fit_end": args.fit_end,
            "noise_amp": args.noise_amp
        },
        "metrics": {
            "c_meas": c_meas,
            "c_abs": c_abs,
            "c_sign": (1.0 if (np.isfinite(c_meas) and c_meas >= 0) else -1.0),
            "c_th": c_th,
            "rel_err": rel_err,
            "r2": r2,
            "dx": data["dx"],
            "dt": data["dt"],
            "steps": data["steps"],
            "elapsed_sec": elapsed,
            "acceptance_rel_err": 0.05,
            "passed": (rel_err <= 0.05) and (np.isfinite(r2) and r2 >= 0.98),
            "c_meas_grad": data.get("c_meas_grad", float("nan")),
            "c_abs_grad": data.get("c_abs_grad", float("nan")),
            "rel_err_grad": data.get("rel_err_grad", float("nan")),
            "r2_grad": data.get("r2_grad", float("nan")),
        },
        "outputs": {
            "figure": figure_path
        },
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
    }

    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    print(json.dumps({
        "figure": figure_path,
        "log": log_path,
        "c_meas": c_meas,
        "c_abs": c_abs,
        "c_th": c_th,
        "rel_err": rel_err,
        "r2": r2,
        "c_meas_grad": payload["metrics"]["c_meas_grad"],
        "c_abs_grad": payload["metrics"]["c_abs_grad"],
        "rel_err_grad": payload["metrics"]["rel_err_grad"],
        "r2_grad": payload["metrics"]["r2_grad"],
        "passed": payload["metrics"]["passed"],
    }, indent=2))


if __name__ == "__main__":
    main()]]></content>
    </file>
    <file>
      <path>physics/tachyonic_condensation/condense_tube.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Condensation and spectrum (diagonal-λ baseline) for the finite-tube FUM scalar EFT.

This module implements a minimal, numerically stable first pass of Section 6-7 in
[derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1):
- Build diagonal quartic couplings N4_ℓ ≈ λ ∫ r dr dθ u_ℓ^4 (projecting λ φ^4 onto each mode)
- Find condensate amplitudes v_ℓ by minimizing V_eff^{tube} ≈ ½ m_ℓ^2 v_ℓ^2 + ¼ N4_ℓ v_ℓ^4
- Compute post-condensation mass matrix in the diagonal approximation M^2_ℓ ≈ m_ℓ^2 + 3 N4_ℓ v_ℓ^2
- Scan E(R) = E_bg(R) + V_eff^{tube}(v_ℓ(R)) over R to locate minima (Bordag Fig. 5 analogue)

Caveats:
- This is the "diagonal-λ" baseline: N4 couplings are approximated as diagonal in mode index.
  Off-diagonal overlap terms N4(ℓ_i) are set to zero for simplicity/robustness.
- The integral for N4_ℓ uses u_ℓ(r) normalized with u_ℓ(R) = 1 from
  [fum_rt/physics/cylinder_modes.py](fum_rt/physics/cylinder_modes.py:1).

Equations:
- Radial mode spectrum (κ-roots) and u_ℓ(r) are from the cylinder solver; masses:
    m_ℓ^2(R) = - c^2 κ_ℓ^2(R).
- Diagonal quartic projection (per mode):
    N4_ℓ(R) = λ ∫_0^∞ r dr ∫_0^{2π} dθ [u_ℓ(r)]^4 = (2π) λ ∫_0^∞ r [u_ℓ(r)]^4 dr.

- Condensate amplitude (tree-level, diagonal):
    v_ℓ^2 = max(0, - m_ℓ^2 / N4_ℓ), else 0 if m_ℓ^2 ≥ 0.

- Post-condensation mass eigenvalues (diagonal baseline):
    M_ℓ^2 = m_ℓ^2 + 3 N4_ℓ v_ℓ^2.

References:
- [derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1)
- [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:125-193)
- [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134)

Author: Justin K. Lietz
Date: 2025-08-09
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, Dict, List, Optional, Sequence, Tuple

import numpy as np

from .cylinder_modes import compute_kappas, mode_functions


@dataclass
class ModeEntry:
    ell: int
    kappa: float
    k_in: float
    k_out: float


def _radial_integral_u4(
    R: float,
    u: Callable[[float], float],
    r_max_factor: float = 10.0,
    dr: float = 1e-2,
) -> float:
    """
    Compute integral I = ∫_0^∞ r [u(r)]^4 dr by truncating at r_max = r_max_factor * R.

    Args:
      R: tube radius (sets normalization u(R) = 1 from mode_functions)
      u: callable radial mode function u(r)
      r_max_factor: how many radii to integrate outwards (default 10)
      dr: radial step

    Returns:
      I ~ ∫_0^{r_max} r u(r)^4 dr (float)
    """
    r_max = max(R * float(r_max_factor), 5.0 * R)
    if r_max <= 0.0:
        return 0.0
    # Create grid including R and up to r_max
    n_steps = max(10, int(np.ceil(r_max / dr)))
    rs = np.linspace(0.0, r_max, n_steps, dtype=np.float64)
    vals = []
    for r in rs:
        try:
            val = float(u(float(r)))
            vals.append((r * (val ** 4)))
        except Exception:
            vals.append(0.0)
    vals = np.asarray(vals, dtype=np.float64)
    I = float(np.trapz(vals, rs))
    return I


def build_quartic_diagonal(
    R: float,
    modes: Sequence[ModeEntry],
    lam: float,
    c: float,
) -> Dict[int, float]:
    """
    Build diagonal quartic coefficients N4_ℓ ≈ (2π) λ ∫ r u_ℓ^4 dr for each provided mode.

    Args:
      R: radius
      modes: list of ModeEntry (ell, kappa, k_in, k_out)
      lam: quartic λ > 0
      c: wave speed

    Returns:
      mapping ell -> N4_ell (float >= 0)
    """
    N4: Dict[int, float] = {}
    for m in modes:
        fns = mode_functions(R=R, root={"ell": float(m.ell), "k_in": m.k_in, "k_out": m.k_out})
        u = fns["u"]
        I = _radial_integral_u4(R, u)  # ∫ r u^4 dr
        N4_ell = float(2.0 * np.pi * lam * I)
        # numerical guard
        if not np.isfinite(N4_ell) or N4_ell < 0.0:
            N4_ell = 0.0
        N4[int(m.ell)] = N4_ell
    return N4


def find_condensate_diagonal(
    R: float,
    modes: Sequence[ModeEntry],
    N4: Dict[int, float],
    c: float,
) -> Dict[int, float]:
    """
    Find condensate amplitudes v_ℓ in the diagonal approximation:
      v_ℓ^2 = max(0, - m_ℓ^2 / N4_ℓ), m_ℓ^2 = - c^2 κ_ℓ^2, so v_ℓ^2 = c^2 κ_ℓ^2 / N4_ℓ if N4_ℓ>0.

    Returns:
      mapping ell -> v_ell (float)
    """
    v: Dict[int, float] = {}
    for m in modes:
        ell = int(m.ell)
        N4_ell = float(N4.get(ell, 0.0))
        m2 = - (c ** 2) * (m.kappa ** 2)
        if N4_ell > 0.0 and m2 < 0.0:
            v2 = (c ** 2) * (m.kappa ** 2) / N4_ell
            v[ell] = float(np.sqrt(max(0.0, v2)))
        else:
            v[ell] = 0.0
    return v


def mass_matrix_diagonal(
    modes: Sequence[ModeEntry],
    N4: Dict[int, float],
    v: Dict[int, float],
    c: float,
) -> Dict[int, float]:
    """
    Post-condensation mass eigenvalues (diagonal baseline):
      M_ℓ^2 = m_ℓ^2 + 3 N4_ℓ v_ℓ^2,  with  m_ℓ^2 = - c^2 κ_ℓ^2.

    Returns:
      mapping ell -> M2_ell (float)
    """
    M2: Dict[int, float] = {}
    for m in modes:
        ell = int(m.ell)
        m2 = - (c ** 2) * (m.kappa ** 2)
        N4_ell = float(N4.get(ell, 0.0))
        v_ell = float(v.get(ell, 0.0))
        M2_ell = m2 + 3.0 * N4_ell * (v_ell ** 2)
        M2[ell] = float(M2_ell)
    return M2


def tube_energy_diagonal(
    modes: Sequence[ModeEntry],
    N4: Dict[int, float],
    v: Dict[int, float],
    c: float,
    E_bg: Optional[Callable[[float], float]] = None,
    R: Optional[float] = None,
) -> float:
    """
    E(R) = E_bg(R) + V_eff^{tube}(v_ℓ), diagonal baseline:
      V_eff ≈ Σ_ℓ [ ½ m_ℓ^2 v_ℓ^2 + ¼ N4_ℓ v_ℓ^4 ] with m_ℓ^2 = - c^2 κ_ℓ^2.

    Args:
      modes: list of ModeEntry
      N4: diag quartic map
      v: condensate amplitudes
      c: wave speed
      E_bg: optional background energy term E_bg(R)
      R: radius (passed to E_bg if provided)

    Returns:
      total energy (float)
    """
    V = 0.0
    for m in modes:
        ell = int(m.ell)
        m2 = - (c ** 2) * (m.kappa ** 2)
        v_ell = float(v.get(ell, 0.0))
        N4_ell = float(N4.get(ell, 0.0))
        V += 0.5 * m2 * (v_ell ** 2) + 0.25 * N4_ell * (v_ell ** 4)
    if E_bg is not None and R is not None:
        try:
            V += float(E_bg(float(R)))
        except Exception:
            pass
    return float(V)


def compute_modes_for_R(
    R: float,
    mu: float,
    c: float = 1.0,
    ell_max: int = 8,
) -> List[ModeEntry]:
    """
    Helper: compute ModeEntry list at fixed R using the cylinder solver.

    Returns:
      list of ModeEntry for ℓ = 0..ell_max, keeping the lowest κ-root per ℓ (if any).
    """
    roots = compute_kappas(R=R, mu=mu, c=c, ell_max=ell_max, num_brackets=256)
    # Option: pick at most one mode per ℓ (lowest κ)
    buckets: Dict[int, List[Dict[str, float]]] = {}
    for r in roots:
        ell = int(round(r.get("ell", 0.0)))
        buckets.setdefault(ell, []).append(r)
    modes: List[ModeEntry] = []
    for ell, lst in buckets.items():
        if not lst:
            continue
        lst_sorted = sorted(lst, key=lambda d: float(d["kappa"]))
        r0 = lst_sorted[0]
        modes.append(
            ModeEntry(
                ell=int(ell),
                kappa=float(r0["kappa"]),
                k_in=float(r0["k_in"]),
                k_out=float(r0["k_out"]),
            )
        )
    return sorted(modes, key=lambda m: m.ell)


def energy_scan(
    R_grid: Sequence[float],
    mu: float,
    lam: float,
    c: float = 1.0,
    ell_max: int = 8,
    E_bg: Optional[Callable[[float], float]] = None,
) -> Dict[str, np.ndarray]:
    """
    Scan E(R) across R_grid using the diagonal baseline.

    Args:
      R_grid: iterable of radii
      mu: tachyon scale (√(½) m_eff in the bounded EFT with m_eff = √2 μ)
      lam: quartic coupling λ > 0
      c: wave speed
      ell_max: max ℓ to consider (lowest root per ℓ)
      E_bg: optional background energy term E_bg(R)

    Returns:
      dict { 'R': np.array, 'E': np.array, 'min_R': float, 'min_E': float }
    """
    Rs = np.asarray(R_grid, dtype=np.float64)
    Es = np.full_like(Rs, np.nan, dtype=np.float64)
    for i, R in enumerate(Rs):
        try:
            modes = compute_modes_for_R(R=R, mu=mu, c=c, ell_max=ell_max)
            if not modes:
                Es[i] = np.nan
                continue
            N4 = build_quartic_diagonal(R=R, modes=modes, lam=lam, c=c)
            v = find_condensate_diagonal(R=R, modes=modes, N4=N4, c=c)
            E = tube_energy_diagonal(modes=modes, N4=N4, v=v, c=c, E_bg=E_bg, R=R)
            Es[i] = float(E)
        except Exception:
            Es[i] = np.nan
    # Find minimum over finite values
    mask = np.isfinite(Es)
    if not np.any(mask):
        return {"R": Rs, "E": Es, "min_R": float("nan"), "min_E": float("nan")}
    idx = int(np.nanargmin(Es))
    return {"R": Rs, "E": Es, "min_R": float(Rs[idx]), "min_E": float(Es[idx])}]]></content>
    </file>
    <file>
      <path>physics/tachyonic_condensation/cylinder_modes.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Finite-radius cylindrical (tube) mode solver for the FUM scalar EFT.

This implements the radial eigenvalue condition described in
[derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1).

Equation of motion for small fluctuations about a piecewise-constant background:
    (∂_t^2 - c^2 ∇_⊥^2 - c^2 ∂_z^2) φ + m^2(r) φ = 0
with
    m_in^2 = -μ^2     for r < R  (tachyonic interior)
    m_out^2 =  2μ^2   for r > R  (massive exterior)
and wave speed c (dimensionless units; see [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134)).

Using separation φ ∝ e^{-i ω t} e^{i k z} u_ℓ(r) e^{iℓθ} and defining
    ω^2 - c^2 k^2 = - c^2 κ^2,
the radial equation reduces (piecewise) to modified Bessel equations. The matching at r = R yields
the secular equation for each angular momentum ℓ:

    (κ_in / κ_out) [I'_ℓ(κ_in R) / I_ℓ(κ_in R)] + [K'_ℓ(κ_out R) / K_ℓ(κ_out R)] = 0

with
    κ_in^2  = μ^2 / c^2 - κ^2,
    κ_out^2 = κ^2 + 2μ^2 / c^2.

Tachyonic (unstable) modes correspond to κ^2 > 0 (so that at k=0, ω^2 = - c^2 κ^2 < 0).

APIs:
- compute_kappas(R, mu, c=1.0, ell_max=12, kappa_max=None, num_brackets=512, tol=1e-8)
    Returns a list of dicts { 'ell', 'kappa', 'k_in', 'k_out' }.

- mode_functions(R, root)
    Returns a dict with 'u_in(r)', 'u_out(r)', and 'u(r)' callables normalized so u(R) = 1.

References:
- [derivation/finite_tube_mode_analysis.md](derivation/finite_tube_mode_analysis.md:1)
- [derivation/discrete_to_continuum.md](derivation/discrete_to_continuum.md:125-193)
- [derivation/kinetic_term_derivation.md](derivation/kinetic_term_derivation.md:117-134)

Author: Justin K. Lietz
Date: 2025-08-09
"""

from __future__ import annotations

from typing import Callable, Dict, List, Optional, Tuple

import numpy as np

try:
    from scipy import optimize, special
    _HAVE_SCIPY = True
except Exception:
    _HAVE_SCIPY = False
    special = None
    optimize = None


_EPS = 1e-14


def _iv(nu: int, x: float) -> float:
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")
    return float(special.iv(nu, x))


def _kv(nu: int, x: float) -> float:
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")
    return float(special.kv(nu, x))


def _ivp(nu: int, x: float) -> float:
    """
    Derivative d/dx I_ν(x). Prefer special.ivp if available; otherwise use
    the stable relation d/dx I_ν = (I_{ν-1} + I_{ν+1})/2.
    """
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")
    if hasattr(special, "ivp"):
        return float(special.ivp(nu, x))
    # Fallback for older scipy: symmetric finite-difference via recurrence
    return 0.5 * (special.iv(nu - 1, x) + special.iv(nu + 1, x))


def _kvp(nu: int, x: float) -> float:
    """
    Derivative d/dx K_ν(x). Prefer special.kvp; otherwise use
    d/dx K_ν = - (K_{ν-1} + K_{ν+1})/2.
    """
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")
    if hasattr(special, "kvp"):
        return float(special.kvp(nu, x))
    return -0.5 * (special.kv(nu - 1, x) + special.kv(nu + 1, x))


def _dlnI(nu: int, x: float) -> float:
    """Compute (I'_ν / I_ν)(x) with basic guarding."""
    x = float(max(x, _EPS))
    Iv = _iv(nu, x)
    if abs(Iv) < _EPS:
        return np.sign(_ivp(nu, x)) * 1e6  # large magnitude surrogate
    return _ivp(nu, x) / Iv


def _dlnK(nu: int, x: float) -> float:
    """Compute (K'_ν / K_ν)(x) with basic guarding."""
    x = float(max(x, _EPS))
    Kv = _kv(nu, x)
    if abs(Kv) < _EPS:
        return -np.sign(_kvp(nu, x)) * 1e6  # large magnitude surrogate (note K decays)
    return _kvp(nu, x) / Kv


def _secular_value(kappa: float, ell: int, R: float, mu: float, c: float) -> float:
    """
    Evaluate the secular equation value:
        f(κ) = (κ_in/κ_out) * (I'_ℓ / I_ℓ)(κ_in R) + (K'_ℓ / K_ℓ)(κ_out R)
    Roots f(κ)=0 provide allowed κ for given (ℓ, R, μ, c).
    Valid only when κ_in^2 >= 0.
    """
    if kappa <= 0.0 or R <= 0.0 or mu <= 0.0 or c <= 0.0:
        return np.nan
    cinv = 1.0 / c
    k_in2 = (mu * cinv) ** 2 - kappa ** 2
    if k_in2 <= 0.0:
        # Outside the domain where interior solution uses I_ℓ with real argument.
        return np.nan
    k_out2 = kappa ** 2 + 2.0 * (mu * cinv) ** 2
    k_in = float(np.sqrt(k_in2))
    k_out = float(np.sqrt(k_out2))
    x_in = k_in * R
    x_out = k_out * R

    try:
        val = (k_in / k_out) * _dlnI(ell, x_in) + _dlnK(ell, x_out)
    except Exception:
        return np.nan
    # Guard absurd values that destabilize sign checks
    if not np.isfinite(val) or abs(val) > 1e12:
        return np.nan
    return float(val)


def _find_roots_for_ell(
    ell: int,
    R: float,
    mu: float,
    c: float,
    kappa_max: Optional[float],
    num_brackets: int,
    tol: float,
) -> List[float]:
    """
    Search for roots of f(κ) over κ ∈ (0, κ_max^{eff}) by sign bracketing.

    κ_in^2 = μ^2/c^2 - κ^2 must be ≥ 0, so κ ≤ μ/c. We clamp κ_max to < μ/c.
    """
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")

    kappa_cap = (mu / c) * 0.999
    if kappa_max is None or not np.isfinite(kappa_max):
        kappa_max_eff = kappa_cap
    else:
        kappa_max_eff = min(float(kappa_max), kappa_cap)
        if kappa_max_eff <= 1e-9:
            kappa_max_eff = kappa_cap

    kappas = []
    grid = np.linspace(1e-9, kappa_max_eff, num_brackets + 1)
    fvals = []
    for x in grid:
        fv = _secular_value(x, ell, R, mu, c)
        # Replace NaNs with None to break bracketing across invalid regions
        fvals.append(fv if np.isfinite(fv) else None)

    for i in range(len(grid) - 1):
        f0 = fvals[i]
        f1 = fvals[i + 1]
        if f0 is None or f1 is None:
            continue
        if f0 == 0.0:
            root = grid[i]
        elif f1 == 0.0:
            root = grid[i + 1]
        elif np.sign(f0) == np.sign(f1):
            continue
        else:
            a, b = grid[i], grid[i + 1]
            try:
                root = optimize.brentq(
                    lambda x: _secular_value(x, ell, R, mu, c),
                    a,
                    b,
                    xtol=tol,
                    rtol=tol,
                    maxiter=200,
                )
            except Exception:
                continue
        # Deduplicate near-equal roots
        if len(kappas) == 0 or abs(root - kappas[-1]) > 1e-6:
            kappas.append(float(root))
    return kappas


def compute_kappas(
    R: float,
    mu: float,
    c: float = 1.0,
    ell_max: int = 12,
    kappa_max: Optional[float] = None,
    num_brackets: int = 512,
    tol: float = 1e-8,
) -> List[Dict[str, float]]:
    """
    Compute κ-roots of the secular equation for ℓ = 0,1,...,ell_max.

    Args:
      R: cylinder radius (dimensionless units)
      mu: tachyon scale (baseline EFT parameter)
      c: wave speed (from 𝓛_K = ½(∂_t φ)^2 - ½ c^2 (∇φ)^2)
      ell_max: highest angular momentum to consider
      kappa_max: optional upper bound (< μ/c), defaults to 0.999 μ/c
      num_brackets: grid count for sign bracketing
      tol: root solver tolerance

    Returns:
      list of dict: { 'ell', 'kappa', 'k_in', 'k_out' } for each root found (kappa > 0).
    """
    if not _HAVE_SCIPY:
        raise RuntimeError("scipy is required for cylinder_modes")

    results: List[Dict[str, float]] = []
    for ell in range(int(max(0, ell_max)) + 1):
        roots = _find_roots_for_ell(
            ell=ell,
            R=R,
            mu=mu,
            c=c,
            kappa_max=kappa_max,
            num_brackets=num_brackets,
            tol=tol,
        )
        for kappa in roots:
            k_in = float(np.sqrt(max(0.0, (mu / c) ** 2 - kappa ** 2)))
            k_out = float(np.sqrt(max(0.0, kappa ** 2 + 2.0 * (mu / c) ** 2)))
            results.append(
                {
                    "ell": float(ell),
                    "kappa": float(kappa),
                    "k_in": float(k_in),
                    "k_out": float(k_out),
                }
            )
    return results


def mode_functions(
    R: float,
    root: Dict[str, float],
) -> Dict[str, Callable[[float], float]]:
    """
    Construct piecewise radial mode functions normalized so u(R) = 1.

    Inside (r < R):  u_in(r) = A I_ℓ(k_in r), with A = 1 / I_ℓ(k_in R).
    Outside (r > R): u_out(r) = B K_ℓ(k_out r), with B = 1 / K_ℓ(k_out R).

    Args:
      R: tube radius
      root: dict from compute_kappas entry, requires 'ell', 'k_in', 'k_out'.

    Returns:
      dict with callables: { 'u_in', 'u_out', 'u' }
    """
    ell = int(root["ell"])
    k_in = float(root["k_in"])
    k_out = float(root["k_out"])

    x_in_R = max(_EPS, k_in * R)
    x_out_R = max(_EPS, k_out * R)

    I_R = _iv(ell, x_in_R)
    K_R = _kv(ell, x_out_R)
    if abs(I_R) < _EPS or abs(K_R) < _EPS:
        raise FloatingPointError("Unstable normalization at r=R: I_ℓ or K_ℓ ~ 0")

    A = 1.0 / I_R
    B = 1.0 / K_R

    def u_in(r: float) -> float:
        rr = float(max(0.0, r))
        return float(A * _iv(ell, max(_EPS, k_in * rr)))

    def u_out(r: float) -> float:
        rr = float(max(0.0, r))
        return float(B * _kv(ell, max(_EPS, k_out * rr)))

    def u(r: float) -> float:
        rr = float(max(0.0, r))
        if rr <= R:
            return u_in(rr)
        return u_out(rr)

    return {"u_in": u_in, "u_out": u_out, "u": u}


if __name__ == "__main__":
    # Minimal self-test (requires scipy)
    R_test = 3.0
    mu_test = 1.0
    c_test = 1.0
    try:
        roots = compute_kappas(R=R_test, mu=mu_test, c=c_test, ell_max=4, num_brackets=256)
        print(f"[cylinder_modes] Found {len(roots)} roots for R={R_test}, mu={mu_test}, c={c_test}")
        if roots:
            fns = mode_functions(R_test, roots[0])
            print("[cylinder_modes] u(R) =", fns["u"](R_test))
    except Exception as e:
        print("Self-test skipped or failed:", e)]]></content>
    </file>
    <file>
      <path>run_nexus.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from .nexus import Nexus, make_parser
import time, os

def main():
    ts = time.strftime('%Y%m%d_%H%M%S')
    default_run_dir = os.path.join('runs', ts)
    args = make_parser().parse_args()

    # Resolve load_engram to an actual checkpoint file and adopt its folder as run_dir when --run-dir is omitted.
    load_path = getattr(args, 'load_engram', None)

    def _resolve_latest_ckpt_in_dir(d: str):
        try:
            best = None
            best_step = -1
            best_ext = ""
            for fn in os.listdir(d):
                if not fn.startswith("state_"):
                    continue
                if not (fn.endswith(".h5") or fn.endswith(".npz")):
                    continue
                ext = ".h5" if fn.endswith(".h5") else ".npz"
                step_str = fn[6:-len(ext)] if len(ext) > 0 else fn[6:]
                try:
                    s = int(step_str)
                    # Prefer higher step; on tie prefer .h5
                    if (s > best_step) or (s == best_step and ext == ".h5" and best_ext == ".npz"):
                        best = fn
                        best_step = s
                        best_ext = ext
                except Exception:
                    continue
            return os.path.join(d, best) if best else None
        except Exception:
            return None

    # Normalize load_path (allow directory to mean "latest checkpoint in that dir")
    if load_path:
        p = str(load_path)
        if os.path.isdir(p):
            resolved = _resolve_latest_ckpt_in_dir(p)
            load_path = resolved if resolved else None
        else:
            load_path = p

    # Adopt bundle folder for run_dir if not explicitly set
    run_dir = getattr(args, 'run_dir', None)
    if not run_dir:
        if load_path and os.path.exists(load_path):
            # If a file path, adopt its parent; if previously a dir, we already kept it above
            run_dir = os.path.dirname(load_path)
        else:
            run_dir = default_run_dir

    nx = Nexus(run_dir=run_dir,
               N=args.neurons, k=args.k, hz=args.hz, domain=args.domain,
               use_time_dynamics=args.use_time_dynamics,
               viz_every=args.viz_every, log_every=args.log_every,
               checkpoint_every=args.checkpoint_every, seed=args.seed,
               sparse_mode=(args.sparse_mode if args.sparse_mode is not None else (args.neurons >= 20000)),
               threshold=getattr(args, 'threshold', 0.15),
               lambda_omega=getattr(args, 'lambda_omega', 0.1),
               candidates=getattr(args, 'candidates', 64),
               walkers=getattr(args, 'walkers', 256),
               hops=getattr(args, 'hops', 3),
               status_interval=getattr(args, 'status_interval', 1),
               bundle_size=getattr(args, 'bundle_size', 3),
               prune_factor=getattr(args, 'prune_factor', 0.10),
               # Checkpoint retention / format (format optional)
               checkpoint_format=getattr(args, 'checkpoint_format', 'h5') if hasattr(args, 'checkpoint_format') else 'h5',
               checkpoint_keep=getattr(args, 'checkpoint_keep', 5),
               # Text→connectome stimulation (symbol→group)
               stim_group_size=getattr(args, 'stim_group_size', 4),
               stim_amp=getattr(args, 'stim_amp', 0.05),
               stim_decay=getattr(args, 'stim_decay', 0.90),
               stim_max_symbols=getattr(args, 'stim_max_symbols', 64),
               # Self-speak / topology spike detection
               speak_auto=getattr(args, 'speak_auto', True),
               speak_z=getattr(args, 'speak_z', 1.0),
               speak_hysteresis=getattr(args, 'speak_hysteresis', 1.0),
               speak_cooldown_ticks=getattr(args, 'speak_cooldown_ticks', 10),
               speak_valence_thresh=getattr(args, 'speak_valence_thresh', 0.01),
               b1_half_life_ticks=getattr(args, 'b1_half_life_ticks', 50),
               # Engram loader (forward normalized path into Nexus)
               load_engram_path=load_path,
               # Optional embedded control server (default off)
               start_control_server=getattr(args, 'control_server', False))
    nx.run(duration_s=args.duration)

if __name__ == '__main__':
    main()
]]></content>
    </file>
    <file>
      <path>runtime/__init__.py</path>
      <content><![CDATA[# Runtime package initializer for modularized orchestrator components.
# Exposes submodules for clarity; keep lightweight to avoid side effects.
# Note: Nexus remains the external façade; internals live under runtime/*
__all__ = [
    "phase",
    "loop",
    "telemetry",
    "retention",
    "events_adapter",
    "runtime_helpers",
    "emitters",
    "orchestrator",
    "state",
]]]></content>
    </file>
    <file>
      <path>runtime/emitters.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime emitters initialization (MacroEmitter, ThoughtEmitter).

Behavior:
- Mirrors Nexus inline initialization exactly:
  * MacroEmitter path priority: $UTD_OUT or utd.path or <run_dir>/utd_events.jsonl
  * ThoughtEmitter enabled only when ENABLE_THOUGHTS in ("1","true","yes","on")
    path priority: $THOUGHT_OUT or <run_dir>/thoughts.ndjson
- Returns (macro_emitter_or_None, thought_emitter_or_None)
- No logging or file writes here (pure construction).
"""

from typing import Any, Callable, Optional, Tuple
import os

# IO-layer actuators (allowed in runtime layer)
from fum_rt.io.actuators.macros import MacroEmitter
from fum_rt.io.actuators.thoughts import ThoughtEmitter


def initialize_emitters(
    utd: Any,
    run_dir: str,
    why_provider: Callable[[], dict],
) -> Tuple[Optional[MacroEmitter], Optional[ThoughtEmitter]]:
    """
    Create MacroEmitter and ThoughtEmitter with legacy-equivalent configuration.
    """
    macro: Optional[MacroEmitter] = None
    thoughts: Optional[ThoughtEmitter] = None

    # Macro emitter (write-only; respects UTD_OUT if set)
    try:
        out_path = os.getenv("UTD_OUT") or getattr(utd, "path", None) or os.path.join(run_dir, "utd_events.jsonl")
        macro = MacroEmitter(path=str(out_path), why_provider=why_provider)
    except Exception:
        macro = None

    # Introspection Ledger (emit-only), behind feature flag ENABLE_THOUGHTS
    try:
        if str(os.getenv("ENABLE_THOUGHTS", "0")).lower() in ("1", "true", "yes", "on"):
            th_path = os.getenv("THOUGHT_OUT") or os.path.join(run_dir, "thoughts.ndjson")
            thoughts = ThoughtEmitter(path=str(th_path), why=why_provider)
    except Exception:
        thoughts = None

    return macro, thoughts


__all__ = ["initialize_emitters"]]]></content>
    </file>
    <file>
      <path>runtime/events_adapter.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime adapter: Convert connectome Observation events into core event-driven metrics inputs.

Design:
- Pure adapter: no logging/IO; small and deterministic.
- Safe: returns an empty list for unknown/unsupported events.
- Behavior-preserving by default: only used when ENABLE_EVENT_METRICS=1.
"""

from typing import Any, Dict, Iterable, List
from fum_rt.core.proprioception.events import (
    BaseEvent,
    DeltaEvent,
    VTTouchEvent,
    SpikeEvent,
    DeltaWEvent,
    EdgeOnEvent,
    ADCEvent,
)


def observations_to_events(observations: Iterable[Any]) -> List[BaseEvent]:
    """
    Map connectome Observation objects to EventDrivenMetrics events.
    Supported kinds:
      - "cycle_hit":   -> DeltaEvent (b1 from loop_gain if available) + EdgeOnEvent(u,v) when nodes include two ids
                         Also synthesizes bounded excitatory SpikeEvent for the touched endpoints.
      - "region_stat": -> VTTouchEvent per node (weight 1.0) and bounded excitatory SpikeEvent per node (amp from s_mean or 1.0)
      - "delta_w":     -> DeltaWEvent per node (bounded fan-out).
                         Additionally, when dw < 0, synthesize bounded inhibitory SpikeEvent (sign=-1, amp=|dw| clipped)
                         to provide an inhibition source without scans.

    Unknown kinds are ignored.
    """
    out: List[BaseEvent] = []
    if not observations:
        return out

    for obs in observations:
        try:
            kind = getattr(obs, "kind", None)
            tick = int(getattr(obs, "tick", 0))
        except Exception:
            continue

        if kind == "cycle_hit":
            try:
                loop_gain = float(getattr(obs, "loop_gain", 0.0))
            except Exception:
                loop_gain = 0.0
            # Use non-negative contribution to the b1 accumulator
            b1_contrib = loop_gain if loop_gain > 0.0 else 1.0
            out.append(DeltaEvent(kind="delta", t=tick, b1=float(b1_contrib)))
            try:
                nodes = list(getattr(obs, "nodes", []) or [])
                if len(nodes) >= 2:
                    u, v = int(nodes[0]), int(nodes[1])
                    out.append(EdgeOnEvent(kind="edge_on", t=tick, u=u, v=v))
                # Also synthesize excitatory SpikeEvents for the endpoints (bounded, event-driven)
                try:
                    amp = loop_gain if loop_gain > 0.0 else 1.0
                except Exception:
                    amp = 1.0
                for idx in nodes[:2]:
                    try:
                        out.append(SpikeEvent(kind="spike", t=tick, node=int(idx), amp=float(amp), sign=+1))
                    except Exception:
                        continue
            except Exception:
                pass

        elif kind == "region_stat":
            try:
                nodes = list(getattr(obs, "nodes", []) or [])
                for node in nodes:
                    out.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(node), w=1.0))
                # Synthesize excitatory SpikeEvent per node using s_mean as amplitude when available
                try:
                    s_mean = float(getattr(obs, "s_mean", 0.0))
                except Exception:
                    s_mean = 0.0
                amp = s_mean if s_mean > 0.0 else 1.0
                for node in nodes:
                    out.append(SpikeEvent(kind="spike", t=tick, node=int(node), amp=float(amp), sign=+1))
            except Exception:
                pass

        elif kind == "delta":
            # Generic learning delta event; fields are expected in obs.meta
            try:
                meta = getattr(obs, "meta", {}) or {}
                b1 = float(meta.get("b1", 0.0))
                nov = float(meta.get("nov", 0.0))
                hab = float(meta.get("hab", 0.0))
                tdv = float(meta.get("td", 0.0))
                hsi = float(meta.get("hsi", 0.0))
                out.append(
                    DeltaEvent(
                        kind="delta",
                        t=tick,
                        b1=b1,
                        novelty=nov,
                        hab=hab,
                        td=tdv,
                        hsi=hsi,
                    )
                )
            except Exception:
                pass

        elif kind == "delta_w":
            # Map Observation(kind='delta_w') -> one or more DeltaWEvent(s)
            # Also synthesize inhibitory SpikeEvent when dw < 0 (bounded fan-out) to drive InhibitionMap without scans.
            try:
                nodes = list(getattr(obs, "nodes", []) or [])
                meta = dict(getattr(obs, "meta", {}) or {})
                dwv = float(meta.get("dw", 0.0))
                # Determine inhibitory synthesis parameters
                is_inh = dwv < 0.0
                inh_amp = float(min(1.0, abs(dwv))) if is_inh else 0.0
                # Bound fan-out defensively
                for node in nodes[:16]:
                    ni = int(node)
                    out.append(DeltaWEvent(kind="delta_w", t=tick, node=ni, dw=float(dwv)))
                    # Provide an explicit inhibitory spike source when dw is negative
                    if is_inh and inh_amp > 0.0:
                        out.append(SpikeEvent(kind="spike", t=tick, node=ni, amp=inh_amp, sign=-1))
            except Exception:
                pass

        else:
            # ignore unknown kinds
            pass

    return out


def adc_metrics_to_event(metrics: Dict[str, Any], t: int) -> ADCEvent:
    """
    Convert ADC metrics dict into a single ADCEvent for folding.
    Expected keys (optional):
      - adc_territories
      - adc_boundaries
      - adc_cycle_hits
    """
    try:
        terr = metrics.get("adc_territories", None)
        bnd = metrics.get("adc_boundaries", None)
        cyc = metrics.get("adc_cycle_hits", None)
    except Exception:
        terr = bnd = cyc = None

    try:
        terr_i = None if terr is None else int(terr)
    except Exception:
        terr_i = None
    try:
        bnd_i = None if bnd is None else int(bnd)
    except Exception:
        bnd_i = None
    try:
        cyc_f = None if cyc is None else float(cyc)
    except Exception:
        cyc_f = None

    return ADCEvent(kind="adc", t=int(t), adc_territories=terr_i, adc_boundaries=bnd_i, adc_cycle_hits=cyc_f)]]></content>
    </file>
    <file>
      <path>runtime/helpers/__init__.py</path>
      <content><![CDATA["""
Runtime helpers package (modularized).

Transitional re-exports:
- During migration away from the monolith [runtime_helpers.py](../runtime_helpers.py), we re-export
  its functions here to provide a stable import path:
    from fum_rt.runtime.helpers import process_messages, emit_status_and_macro, ...
- New helpers live as separate modules under this package (e.g., maps_ws.py).
"""

from __future__ import annotations

# New, modular helpers
from .maps_ws import maybe_start_maps_ws  # re-export
from .macro_board import register_macro_board  # re-export (modular)

# Modularized helper implementations (explicit re-exports)
from .engram import maybe_load_engram, derive_start_step
from .ingest import process_messages
from .smoke import maybe_smoke_tests
from .speak import maybe_auto_speak
from .emission import emit_status_and_macro
from .viz import maybe_visualize
from .checkpointing import save_tick_checkpoint

__all__ = [
    # New helpers
    "maybe_start_maps_ws",
    # Transitional re-exports
    "register_macro_board",
    "maybe_load_engram",
    "derive_start_step",
    "process_messages",
    "maybe_smoke_tests",
    "maybe_auto_speak",
    "emit_status_and_macro",
    "maybe_visualize",
    "save_tick_checkpoint",
]]]></content>
    </file>
    <file>
      <path>runtime/helpers/checkpointing.py</path>
      <content><![CDATA["""
Runtime helper: checkpointing and retention.

Provides:
- save_tick_checkpoint(): periodic snapshot with retention, behavior-preserving.
"""

from __future__ import annotations

from typing import Any

from fum_rt.core.memory import save_checkpoint
from fum_rt.runtime.retention import prune_checkpoints as _prune_ckpt


def save_tick_checkpoint(nx: Any, step: int) -> None:
    """
    Save checkpoint and run retention policy when configured. Mirrors original behavior.
    """
    try:
        if getattr(nx, "checkpoint_every", 0) and (int(step) % int(nx.checkpoint_every)) == 0 and int(step) > 0:
            try:
                path = save_checkpoint(
                    nx.run_dir,
                    int(step),
                    nx.connectome,
                    fmt=getattr(nx, "checkpoint_format", "h5") or "h5",
                    adc=getattr(nx, "adc", None),
                )
                try:
                    nx.logger.info("checkpoint_saved", extra={"extra": {"path": str(path), "step": int(step)}})
                except Exception:
                    pass
                if int(getattr(nx, "checkpoint_keep", 0)) > 0:
                    try:
                        summary = _prune_ckpt(nx.run_dir, keep=int(nx.checkpoint_keep), last_path=path)
                        try:
                            nx.logger.info("checkpoint_retention", extra={"extra": summary})
                        except Exception:
                            pass
                    except Exception:
                        pass
            except Exception as e:
                try:
                    nx.logger.info("checkpoint_error", extra={"extra": {"err": str(e)}})
                except Exception:
                    pass
    except Exception:
        pass


__all__ = ["save_tick_checkpoint"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/emission.py</path>
      <content><![CDATA["""
Runtime helper: status emission and macro board status.

- Emits open UTD status payload every status_every ticks.
- Emits a 'status' macro when valence is high (mirrors legacy behavior).

Imports typing + telemetry builder only; no IO side effects besides UTD emits.
"""

from __future__ import annotations

from typing import Any, Dict

from fum_rt.runtime.telemetry import status_payload as _telemetry_status


def emit_status_and_macro(nx: Any, m: Dict[str, Any], step: int) -> None:
    """
    Emit open UTD status payload and, when valence is high, a 'status' macro.
    Mirrors the inline Nexus logic.
    """
    if (int(step) % int(getattr(nx, "status_every", 1))) != 0:
        return

    # Open UTD status
    try:
        payload = _telemetry_status(nx, m, int(step))
        score = float(m.get("sie_v2_valence_01", m.get("sie_valence_01", 0.0)))
        nx.utd.emit_text(payload, score=score)
    except Exception:
        pass

    # Macro board status
    try:
        val = float(m.get("sie_v2_valence_01", m.get("sie_valence_01", 0.0)))
        if val >= 0.6:
            nx.utd.emit_macro(
                "status",
                {
                    "t": int(step),
                    "neurons": int(getattr(nx, "N", 0)),
                    "cohesion_components": int(m.get("cohesion_components", 0)),
                    "vt_coverage": float(m.get("vt_coverage", 0.0)),
                    "vt_entropy": float(m.get("vt_entropy", 0.0)),
                    "connectome_entropy": float(m.get("connectome_entropy", 0.0)),
                    "active_edges": int(m.get("active_edges", 0)),
                    "homeostasis_pruned": int(m.get("homeostasis_pruned", 0)),
                    "homeostasis_bridged": int(m.get("homeostasis_bridged", 0)),
                    "ute_in_count": int(m.get("ute_in_count", 0)),
                    "ute_text_count": int(m.get("ute_text_count", 0)),
                },
                score=val,
            )
    except Exception:
        pass


__all__ = ["emit_status_and_macro"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/engram.py</path>
      <content><![CDATA["""
Runtime helper: engram load and start-step derivation.

Behavior:
- maybe_load_engram(nx, path): loads engram state into connectome (and ADC if present), logs outcome.
- derive_start_step(nx, path): derives starting tick index based on provided path or existing state_* files.

This module provides the real implementations migrated from the legacy runtime_helpers monolith.
"""

from __future__ import annotations

from typing import Any, Optional
import os
import re

from fum_rt.core.memory import load_engram as _load_engram_state


def maybe_load_engram(nx: Any, load_engram_path: Optional[str]) -> None:
    """
    If a path is provided, load the engram into nx.connectome (and nx.adc when present),
    logging the result into nx.logger for UI confirmation. Mirrors legacy behavior.
    """
    if not load_engram_path:
        return
    try:
        _load_engram_state(str(load_engram_path), nx.connectome, adc=getattr(nx, "adc", None))
        try:
            nx.logger.info("engram_loaded", extra={"extra": {"path": str(load_engram_path)}})
        except Exception:
            pass
    except Exception as e:
        try:
            nx.logger.info("engram_load_error", extra={"extra": {"err": str(e), "path": str(load_engram_path)}})
        except Exception:
            pass


def derive_start_step(nx: Any, load_engram_path: Optional[str]) -> int:
    """
    Derive starting step to continue numbering after resume, avoiding retention deleting
    new snapshots. Mirrors original logic including filename parsing and fallback scan.

    Policy:
    - If load_engram_path points to a state file named like state_<step>.(h5|npz), return step+1
    - Else scan nx.run_dir for the highest state_<step>.(h5|npz) and return highest+1
    - Else return 0
    """
    try:
        s: Optional[int] = None
        lp = str(load_engram_path) if load_engram_path else None
        if lp and os.path.isfile(lp):
            base = os.path.basename(lp)
            m = re.search(r"state_(\d+)\.(h5|npz)$", base)
            if m:
                s = int(m.group(1))
        if s is None:
            max_s = -1
            for fn in os.listdir(nx.run_dir):
                if not fn.startswith("state_"):
                    continue
                m2 = re.search(r"state_(\d+)\.(h5|npz)$", fn)
                if m2:
                    ss = int(m2.group(1))
                    if ss > max_s:
                        max_s = ss
            if max_s >= 0:
                s = max_s
        return int(s) + 1 if s is not None else 0
    except Exception:
        return 0


__all__ = ["maybe_load_engram", "derive_start_step"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/ingest.py</path>
      <content><![CDATA["""
Runtime helper: message ingestion and per-tick symbol/index extraction.

Provides:
- process_messages(): Mirrors legacy Nexus/runtime behavior while keeping the runtime layer modular.

Policy:
- Runtime helpers may import fum_rt.io.* and fum_rt.core.*.
"""

from __future__ import annotations

from typing import Any, Dict, Iterable, Optional, Set, Tuple

from fum_rt.core import text_utils
from fum_rt.io.cognition.stimulus import symbols_to_indices as _stim_symbols_to_indices


def process_messages(nx: Any, msgs: Iterable[Dict[str, Any]]) -> Tuple[int, Set[int], Set[str], Dict[int, Any]]:
    """
    Process UTE messages:
      - Count text messages
      - Update recent_text, lexicon/ngrams, and document count
      - Build per-tick token set for IDF computations and seed selection
      - Map symbols to connectome indices via nx._symbols_to_indices (deterministic)
      - Emit each message to UTD (mirrors original timing)

    Returns: (ute_text_count, stim_idxs, tick_tokens, tick_rev_map)
    """
    ute_text_count = 0
    stim_idxs: Set[int] = set()
    tick_tokens: Set[str] = set()
    tick_rev_map: Dict[int, Any] = {}

    for m in msgs:
        try:
            if m.get("type") != "text":
                # Non-text messages are emitted to UTD as-is
                try:
                    nx.utd.emit_text(m)
                except Exception:
                    pass
                continue

            ute_text_count += 1
            # Append to rolling recent_text
            try:
                text = str(m.get("msg", ""))
                try:
                    nx.recent_text.append(text)
                except Exception:
                    pass
                # Update lexicon/ngrams and token set for this tick (behavior-preserving)
                try:
                    if not hasattr(nx, "_lexicon"):
                        nx._lexicon = {}
                    toks = text_utils.tokenize_text(text)
                    for w in set(toks):
                        nx._lexicon[w] = int(nx._lexicon.get(w, 0)) + 1
                        tick_tokens.add(w)
                    # Ensure n-gram stores exist and update streaming n-grams for emergent composition
                    try:
                        nx._ng2
                        nx._ng3
                    except Exception:
                        nx._ng2 = {}
                        nx._ng3 = {}
                    text_utils.update_ngrams(toks, nx._ng2, nx._ng3)
                    # Increment document counter once per inbound text message
                    nx._doc_count = int(getattr(nx, "_doc_count", 0)) + 1
                except Exception:
                    pass
                # Symbol → indices mapping (deterministic)
                try:
                    group_size = int(getattr(nx, "stim_group_size", 4))
                    max_symbols = int(getattr(nx, "stim_max_symbols", 64))
                    idxs = _stim_symbols_to_indices(
                        text, group_size, max_symbols, int(getattr(nx, "N", 0)), reverse_map=tick_rev_map
                    )
                    for i in idxs:
                        stim_idxs.add(int(i))
                except Exception:
                    pass
            except Exception:
                pass

            # Emit original message
            try:
                nx.utd.emit_text(m)
            except Exception:
                pass
        except Exception:
            # Fail-soft per message
            pass

    return ute_text_count, stim_idxs, tick_tokens, tick_rev_map


__all__ = ["process_messages"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/macro_board.py</path>
      <content><![CDATA["""
Runtime helper: macro board registration on UTD emitter.

Behavior:
- Registers default macros ('status', 'say')
- Loads per-run macro_board.json entries only (no external fallbacks)
"""

from __future__ import annotations

import os
import json
from typing import Any


def register_macro_board(utd: Any, run_dir: str) -> None:
    """
    Register default macros and optional per-run macro_board.json entries on a UTD-like emitter.
    Mirrors legacy behavior:
      - Always register 'status' and 'say'
      - Optionally load runs/<ts>/macro_board.json (dict of name -> meta)
      - Only per-run board can provide metadata/templates to preserve emergent language
    """
    try:
        utd.register_macro("status", {"desc": "Emit structured status payload"})
    except Exception:
        pass
    try:
        utd.register_macro("say", {"desc": "Emit plain text line"})
    except Exception:
        pass

    # Per-run macro_board.json
    try:
        pth = os.path.join(run_dir, "macro_board.json")
        if os.path.exists(pth):
            with open(pth, "r", encoding="utf-8") as fh:
                reg = json.load(fh)
            if isinstance(reg, dict):
                for name, meta in reg.items():
                    try:
                        utd.register_macro(str(name), meta if isinstance(meta, dict) else {})
                    except Exception:
                        pass
    except Exception:
        pass

    # External fallbacks removed by repository policy: macros must originate from per-run files.


__all__ = ["register_macro_board"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/maps_ws.py</path>
      <content><![CDATA["""
Runtime helper: maps/frame WebSocket bootstrap (bounded, drop-oldest).

- Safe no-op when ENABLE_MAPS_WS is not truthy or when 'websockets' package is missing.
- Ensures a bounded MapsRing exists on nx._maps_ring (capacity=MAPS_RING, default 3).
- Starts MapsWebSocketServer once and stores it on nx._maps_ws_server.

This file is part of the runtime helpers modularization under fum_rt.runtime.helpers.
"""

from __future__ import annotations

import os
from typing import Any


def _truthy(x) -> bool:
    try:
        if isinstance(x, (int, float)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False


def maybe_start_maps_ws(nx: Any) -> None:
    """
    Lazily start the maps/frame WebSocket forwarder if ENABLE_MAPS_WS is truthy.
    - Ensures a bounded MapsRing exists on nx._maps_ring (capacity=MAPS_RING, default 3)
    - Starts a background MapsWebSocketServer (host=MAPS_WS_HOST, port=MAPS_WS_PORT)
    - Safe no-op if websockets is not installed or any error occurs
    """
    try:
        if not _truthy(os.getenv("ENABLE_MAPS_WS", "0")):
            return

        # Ensure a ring exists (reuses ring created by telemetry tick_fold if present)
        ring = getattr(nx, "_maps_ring", None)
        if ring is None:
            try:
                from fum_rt.io.visualization.maps_ring import MapsRing  # allowed in runtime layer
                cap = 3
                try:
                    cap = int(os.getenv("MAPS_RING", "3"))
                except Exception:
                    cap = 3
                nx._maps_ring = MapsRing(capacity=max(1, cap))
                ring = nx._maps_ring
            except Exception:
                ring = None

        if ring is None:
            return

        # Start server once
        if getattr(nx, "_maps_ws_server", None) is None:
            try:
                from fum_rt.io.visualization.websocket_server import MapsWebSocketServer  # runtime-layer IO allowed
                host = os.getenv("MAPS_WS_HOST", "127.0.0.1")
                try:
                    port = int(os.getenv("MAPS_WS_PORT", "8765"))
                except Exception:
                    port = 8765

                def _err(msg: str) -> None:
                    try:
                        nx.logger.info("maps_ws_error", extra={"extra": {"err": str(msg)}})
                    except Exception:
                        try:
                            print("[maps_ws] " + str(msg), flush=True)
                        except Exception:
                            pass

                srv = MapsWebSocketServer(ring, host=host, port=port, on_error=_err)
                srv.start()
                nx._maps_ws_server = srv
            except Exception:
                # Missing websockets or other failure - safe no-op
                return
    except Exception:
        # Never disrupt runtime parity
        pass


__all__ = ["maybe_start_maps_ws"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/redis_out.py</path>
      <content><![CDATA["""
Redis publishing helpers (optional, bounded, void-faithful).

- Publishes status metrics and/or latest maps/frame from the in-process ring to Redis Streams.
- No schedulers or background threads here; caller invokes once per tick from the runtime loop.
- Uses MAXLEN trimming to keep Redis bounded (drop-oldest), mirroring in-memory ring semantics.

Enable via env:
  REDIS_URL=redis://127.0.0.1:6379/0
  ENABLE_REDIS_STATUS=1
  ENABLE_REDIS_MAPS=1
  REDIS_STREAM_STATUS=fum:status         (optional; default shown)
  REDIS_STREAM_MAPS=fum:maps             (optional; default shown)
  REDIS_STATUS_MAXLEN=2000               (approximate trim)
  REDIS_MAPS_MAXLEN=3                    (approximate trim)
"""

from __future__ import annotations

from typing import Any, Dict, Optional
import os
import json

try:
    import redis  # type: ignore
except Exception:  # pragma: no cover
    redis = None  # lazy-fail if missing


def _truthy(x: Any) -> bool:
    try:
        if isinstance(x, (int, float, bool)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False


def _get_client(nx: Any) -> Optional["redis.Redis"]:
    """
    Lazy-initialize and cache a Redis client on nx._redis_client.
    Returns None if redis-py is unavailable or the URL/env is missing.
    """
    if redis is None:
        return None
    try:
        cli = getattr(nx, "_redis_client", None)
        if cli is not None:
            return cli
    except Exception:
        cli = None
    try:
        url = os.getenv("REDIS_URL", "").strip()
        if not url:
            return None
        cli = redis.from_url(url, decode_responses=False)  # keep bytes payloads raw
        setattr(nx, "_redis_client", cli)
        return cli
    except Exception:
        return None


def maybe_publish_status_redis(nx: Any, metrics: Dict[str, Any], step: int) -> None:
    """
    Publish a compact status JSON to a bounded Redis Stream once per tick.

    Fields:
      stream = REDIS_STREAM_STATUS (default 'fum:status')
      MAXLEN  = REDIS_STATUS_MAXLEN (default 2000, approximate)
      entry   = { 'json': b'{"type":"status",...}' }
    """
    try:
        if not _truthy(os.getenv("ENABLE_REDIS_STATUS", "0")):
            return
        cli = _get_client(nx)
        if cli is None:
            return
        stream = os.getenv("REDIS_STREAM_STATUS", "fum:status")
        try:
            maxlen = int(os.getenv("REDIS_STATUS_MAXLEN", "2000"))
        except Exception:
            maxlen = 2000

        # Select a compact subset to keep bandwidth low
        m = metrics or {}
        payload = {
            "type": "status",
            "t": int(step),
            "phase": int(m.get("phase", 0)),
            "neurons": int(getattr(nx, "N", 0)),
            "b1_z": float(m.get("b1_z", 0.0)),
            "cohesion_components": int(m.get("cohesion_components", 0)),
            "vt_entropy": float(m.get("vt_entropy", 0.0)),
            "sie_valence_01": float(m.get("sie_valence_01", 0.0)),
            "sie_v2_valence_01": float(m.get("sie_v2_valence_01", m.get("sie_valence_01", 0.0))),
        }
        js = json.dumps(payload, separators=(",", ":"), ensure_ascii=False).encode("utf-8")
        cli.xadd(stream, {"json": js}, maxlen=maxlen, approximate=True)
    except Exception:
        # Never disrupt runtime parity
        pass


def maybe_publish_maps_redis(nx: Any, step: int) -> None:
    """
    Publish the latest maps/frame (u8 preferred) to a bounded Redis Stream once per tick.

    - Reads the newest frame from nx._maps_ring (if present).
    - Skips if no new frame (seq unchanged).
    - Writes XADD with MAXLEN ~ REDIS_MAPS_MAXLEN (default 3) to keep memory bounded.
    - Fields: { 'header': b'{"tick":...}', 'payload': <raw-bytes> }
    """
    try:
        if not _truthy(os.getenv("ENABLE_REDIS_MAPS", "0")):
            return
        cli = _get_client(nx)
        if cli is None:
            return
        ring = getattr(nx, "_maps_ring", None)
        if ring is None:
            return
        fr = ring.latest()
        if fr is None:
            return

        # Skip if we've already published this seq
        try:
            last_seq = int(getattr(nx, "_maps_last_seq_redis", 0))
        except Exception:
            last_seq = 0
        if getattr(fr, "seq", 0) == last_seq:
            return

        stream = os.getenv("REDIS_STREAM_MAPS", "fum:maps")
        try:
            maxlen = int(os.getenv("REDIS_MAPS_MAXLEN", "3"))
        except Exception:
            maxlen = 3

        # Serialize header compactly; payload is raw bytes (u8 preferred)
        try:
            hdr_text = json.dumps(fr.header, separators=(",", ":"), ensure_ascii=False).encode("utf-8")
        except Exception:
            hdr_text = json.dumps({"topic": "maps/frame", "tick": int(getattr(fr, "tick", step))}, separators=(",", ":")).encode("utf-8")

        payload_bytes = bytes(getattr(fr, "payload", b"") or b"")
        cli.xadd(stream, {"header": hdr_text, "payload": payload_bytes}, maxlen=maxlen, approximate=True)

        # Mark as published
        try:
            setattr(nx, "_maps_last_seq_redis", int(getattr(fr, "seq", 0)))
        except Exception:
            pass
    except Exception:
        # Never disrupt runtime parity
        pass


__all__ = ["maybe_publish_status_redis", "maybe_publish_maps_redis"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/smoke.py</path>
      <content><![CDATA["""
Runtime helper: optional one-shot smoke tests (macros and thought ledger).

- Controlled by env flags:
  - ENABLE_MACROS_TEST
  - ENABLE_THOUGHTS_TEST

Behavior:
- Mirrors legacy Nexus logic exactly; guarded and fail-soft.
"""

from __future__ import annotations

import os
from typing import Any, Dict


def maybe_smoke_tests(nx: Any, m: Dict[str, Any], step: int) -> None:
    """
    One-shot emitters test for macros and thought ledger when ENABLE_*_TEST env flags are set.
    Mirrors Nexus inline behavior and guards with booleans on nx.
    """
    # Macro smoke
    try:
        if (not getattr(nx, "_macros_smoke_done", False)) and str(os.getenv("ENABLE_MACROS_TEST", "0")).lower() in ("1", "true", "yes", "on"):
            if getattr(nx, "emitter", None):
                nx.emitter.vars({"N": "neural", "G": "global_access", "E": "experience", "B": "behavior"})
                nx.emitter.edges(["N->G", "G->B", "E->B?"])
                nx.emitter.assumptions(["no unmeasured confounding", "positivity"])
                nx.emitter.target("P(B|do(G))")
                nx.emitter.derivation("If N fixes G and G mediates B, therefore adjust on {confounders} yields effect.")
                nx.emitter.prediction_delta("Behavioral margin differs if extra-law holds.")
                nx.emitter.transfer("Circuit: signal->bus; flag->output; hidden noise.")
                nx.emitter.equation("Y = β X + U_Y")
                nx.emitter.status("macro smoke: ok")
            nx._macros_smoke_done = True
    except Exception:
        pass

    # Thought ledger smoke
    try:
        if (not getattr(nx, "_thoughts_smoke_done", False)) and str(os.getenv("ENABLE_THOUGHTS_TEST", "0")).lower() in ("1", "true", "yes", "on"):
            if getattr(nx, "thoughts", None):
                nx.thoughts.observation("vt_entropy", float(m.get("vt_entropy", 0.0)))
                nx.thoughts.motif("cycle_probe", nodes=[1, 2, 3])
                nx.thoughts.hypothesis("H0", "A ⟂ B | Z", status="tentative", conf=0.55)
                nx.thoughts.test("CI", True, vars={"A": "A", "B": "B", "Z": ["Z"]})
                nx.thoughts.derivation(["H0", "obs:vt_coverage↑"], "Identify P(Y|do(X)) via backdoor on {Z}", conf=0.6)
                nx.thoughts.revision("H0", "accepted", because=["test:CI:true"])
                nx.thoughts.plan("intervene", vars={"target": "X"}, rationale="disambiguate twins")
            nx._thoughts_smoke_done = True
    except Exception:
        pass


__all__ = ["maybe_smoke_tests"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/speak.py</path>
      <content><![CDATA["""
Runtime helper: autonomous speaking (composer + speaker gate + novelty IDF).

Behavior:
- Mirrors legacy Nexus logic for maybe_auto_speak() exactly.
- Pure runtime helper; safe fail-soft; no side-effects beyond UTD emissions and learned lexicon updates.
"""

from __future__ import annotations

import os
from typing import Any, Dict, Set

from fum_rt.core import text_utils
from fum_rt.io.cognition.composer import compose_say_text as _compose_say_text_impl
from fum_rt.io.cognition.speaker import should_speak as _speak_gate, novelty_and_score as _novelty_and_score
from fum_rt.runtime.telemetry import macro_why_base as _telemetry_why_base


def maybe_auto_speak(
    nx: Any,
    m: Dict[str, Any],
    step: int,
    tick_tokens: Set[str],
    void_topic_symbols: Set[Any],
) -> None:
    """
    Behavior-preserving autonomous speaking based on topology spikes and valence.
    Mirrors the original Nexus block in full detail.
    """
    try:
        val_v2 = float(m.get("sie_v2_valence_01", m.get("sie_valence_01", 0.0)))
    except Exception:
        val_v2 = float(m.get("sie_valence_01", 0.0))
    spike = bool(m.get("b1_spike", False))

    if not getattr(nx, "speak_auto", False):
        return

    can_speak, reason = _speak_gate(val_v2, spike, float(getattr(nx, "speak_valence_thresh", 0.01)))
    if not can_speak:
        if reason == "low_valence":
            try:
                nx.logger.info(
                    "speak_suppressed",
                    extra={
                        "extra": {
                            "reason": "low_valence",
                            "val": val_v2,
                            "thresh": float(getattr(nx, "speak_valence_thresh", 0.01)),
                            "b1_z": float(m.get("b1_z", 0.0)),
                            "t": int(step),
                        }
                    },
                )
            except Exception:
                pass
        return

    # Compose; do not suppress due to lack of topic/tokens. Model controls content fully.
    seed_material = tick_tokens if tick_tokens else void_topic_symbols
    try:
        speech = _compose_say_text_impl(
            m or {},
            int(step),
            getattr(nx, "_lexicon", {}) or {},
            getattr(nx, "_ng2", {}) or {},
            getattr(nx, "_ng3", {}) or {},
            getattr(nx, "recent_text", []),
            templates=list(getattr(nx, "_phrase_templates", []) or []),
            seed_tokens=seed_material,
        ) or ""
    except Exception:
        speech = ""

    # Composer IDF gain (local to composer; does not affect dynamics)
    try:
        composer_k = float(getattr(nx, "_phase", {}).get("composer_idf_k", float(os.getenv("COMPOSER_IDF_K", "0.0"))))
    except Exception:
        composer_k = 0.0

    # Composer-local novelty IDF + score (telemetry/emitter only; does not affect dynamics)
    try:
        novelty_idf, score_out = _novelty_and_score(
            speech,
            getattr(nx, "_lexicon", {}) or {},
            int(getattr(nx, "_doc_count", 0)),
            text_utils.tokenize_text,
            float(composer_k),
            float(val_v2),
        )
    except Exception:
        novelty_idf, score_out = 0.0, float(val_v2)

    # Update learned lexicon after computing novelty (avoid self-bias in estimate)
    try:
        if not hasattr(nx, "_lexicon"):
            nx._lexicon = {}
        toks2 = text_utils.tokenize_text(speech)
        for w in set(toks2):
            nx._lexicon[w] = int(nx._lexicon.get(w, 0)) + 1
        # Ensure n-gram stores exist and update streaming n-grams
        try:
            nx._ng2
            nx._ng3
        except Exception:
            nx._ng2 = {}
            nx._ng3 = {}
        text_utils.update_ngrams(toks2, nx._ng2, nx._ng3)
    except Exception:
        pass

    # Emit macro
    try:
        why = _telemetry_why_base(nx, m, int(step))
        try:
            why["novelty_idf"] = float(novelty_idf)
            why["composer_idf_k"] = float(composer_k)
        except Exception:
            pass
        nx.utd.emit_macro(
            "say",
            {
                "text": speech,
                "why": why,
            },
            score=score_out,
        )
    except Exception:
        pass


__all__ = ["maybe_auto_speak"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/status_http.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Optional in-process HTTP status endpoint (void-faithful; no schedulers).

- Purpose: serve the latest status payload to UI without any file reads.
- Behavior: starts a tiny HTTP server in a background thread (event-driven, no timers).
- Endpoint:
    GET /status  -> 200 JSON of nx._emit_last_metrics (latest per-tick status) or 204 if not yet available
    GET /health  -> 200 {"ok": true}
- Enable via:
    ENABLE_STATUS_HTTP=1
    STATUS_HTTP_HOST=127.0.0.1
    STATUS_HTTP_PORT=8787
- Safety:
    - If any error occurs (port busy, etc.), remain a no-op.
    - Never mutates core dynamics; purely IO.
"""

import os
import json
import threading
from typing import Any, Optional


def _truthy(x: Any) -> bool:
    try:
        if isinstance(x, (int, float, bool)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False


def maybe_start_status_http(nx: Any, force: bool = False) -> None:
    """
    Idempotently start the status HTTP server.
    Stores references on nx as:
      nx._status_http_server (HTTPServer)
      nx._status_http_thread (threading.Thread)
      nx._status_http_started (bool)
    Gate:
      - If force is True, start regardless of env.
      - If force is False, start only when ENABLE_STATUS_HTTP is truthy.
    """
    # Idempotence: already running or previously started
    try:
        if getattr(nx, "_status_http_started", False) or getattr(nx, "_status_http_server", None) is not None:
            return
    except Exception:
        pass
    # Env gate unless forced
    if not force:
        try:
            if not _truthy(os.getenv("ENABLE_STATUS_HTTP", "0")):
                return
        except Exception:
            return

    # Already running
    try:
        if getattr(nx, "_status_http_server", None) is not None:
            return
    except Exception:
        pass

    # Lazy import from stdlib; avoid global import side effects
    try:
        from http.server import BaseHTTPRequestHandler, HTTPServer  # type: ignore
    except Exception:
        return

    # Configuration
    try:
        host = os.getenv("STATUS_HTTP_HOST", "127.0.0.1").strip() or "127.0.0.1"
    except Exception:
        host = "127.0.0.1"
    try:
        port = int(os.getenv("STATUS_HTTP_PORT", "8787"))
    except Exception:
        port = 8787

    # Bind Nexus reference in a closure for the handler
    nexus_ref = nx

    class _Handler(BaseHTTPRequestHandler):  # type: ignore
        # Silence default logging
        def log_message(self, format: str, *args) -> None:  # noqa: A003 (shadow builtins name)
            try:
                if getattr(nexus_ref, "logger", None) is not None:
                    # Keep this extremely low-cost; skip formatting expansions
                    pass
            except Exception:
                pass

        def _send_json(self, code: int, payload: Optional[dict]) -> None:
            try:
                body = b""
                if payload is not None:
                    try:
                        body = json.dumps(payload, separators=(",", ":"), ensure_ascii=False).encode("utf-8")
                    except Exception:
                        body = b"{}"
                self.send_response(code)
                self.send_header("Content-Type", "application/json; charset=utf-8")
                self.send_header("Cache-Control", "no-cache, no-store, must-revalidate")
                self.send_header("Pragma", "no-cache")
                self.send_header("Expires", "0")
                self.send_header("Content-Length", str(len(body)))
                self.end_headers()
                if body:
                    self.wfile.write(body)
            except Exception:
                # Best-effort: avoid crashing the server
                try:
                    self.send_response(500)
                    self.end_headers()
                except Exception:
                    pass

        def do_GET(self) -> None:  # type: ignore
            try:
                path = self.path or "/"
                if path == "/health":
                    return self._send_json(200, {"ok": True})
                if path in ("/status", "/status/snapshot"):
                    # Serve latest status payload captured by the runtime loop
                    try:
                        m = getattr(nexus_ref, "_emit_last_metrics", None)
                    except Exception:
                        m = None
                    if isinstance(m, dict) and m:
                        # Minimal filtering: ensure JSON-serializable scalars
                        safe = {}
                        for k, v in m.items():
                            try:
                                if isinstance(v, (int, float, str, bool)) or v is None:
                                    safe[k] = v
                                else:
                                    # fallback to float or string
                                    try:
                                        safe[k] = float(v)  # type: ignore
                                    except Exception:
                                        safe[k] = str(v)
                            except Exception:
                                continue
                        return self._send_json(200, safe)
                    return self._send_json(204, None)
                # Not found
                self.send_response(404)
                self.end_headers()
            except Exception:
                try:
                    self.send_response(500)
                    self.end_headers()
                except Exception:
                    pass

    # Create and start the HTTP server
    try:
        server = HTTPServer((host, port), _Handler)  # type: ignore
    except Exception:
        return

    def _run() -> None:
        try:
            server.serve_forever(poll_interval=0.5)
        except Exception:
            pass
        finally:
            try:
                server.server_close()
            except Exception:
                pass

    try:
        th = threading.Thread(target=_run, name="status_http", daemon=True)
        th.start()
        setattr(nx, "_status_http_server", server)
        setattr(nx, "_status_http_thread", th)
        try:
            setattr(nx, "_status_http_started", True)
        except Exception:
            pass
    except Exception:
        try:
            server.server_close()
        except Exception:
            pass
        return


__all__ = ["maybe_start_status_http"]]]></content>
    </file>
    <file>
      <path>runtime/helpers/viz.py</path>
      <content><![CDATA["""
Runtime helper: periodic visualization hooks (dashboard and connectome snapshot).

Behavior:
- Mirrors legacy Nexus logic and the original runtime_helpers.maybe_visualize()
- Fail-soft and fully optional; never disrupts runtime
"""

from __future__ import annotations

from typing import Any


def maybe_visualize(nx: Any, step: int) -> None:
    """
    Periodic dashboard and graph snapshot, behavior-preserving.
    """
    try:
        if getattr(nx, "viz_every", 0) and (int(step) % int(nx.viz_every)) == 0 and int(step) > 0:
            try:
                nx.vis.dashboard(nx.history[-max(50, int(nx.viz_every) * 2):])  # last window
                if int(getattr(nx, "N", 0)) <= 10000:
                    G = nx.connectome.snapshot_graph()
                    nx.vis.graph(G, fname='connectome.png')
            except Exception as e:
                try:
                    nx.logger.info("viz_error", extra={"extra": {"err": str(e)}})
                except Exception:
                    pass
    except Exception:
        pass


__all__ = ["maybe_visualize"]]]></content>
    </file>
    <file>
      <path>runtime/loop/__init__.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.runtime.loop package facade

Ensures import seam compliance for boundary tests:
- Imports runtime.telemetry.tick_fold seam.
- References core.signals seam.
- Re-exports run_loop from .main.

Void-faithful:
- No schedulers, timers, or cadence logic.
- No scans/dense ops; numpy-free.
"""

from typing import Any, Optional, Sequence

# Re-export the main runtime loop from the package implementation
from .main import run_loop

# Seams for boundary tests (presence-only imports)
from fum_rt.runtime.telemetry import tick_fold as _tick_fold  # runtime.telemetry seam
import fum_rt.core.signals as _signals  # noqa: F401  # core.signals seam (presence-only)


def run_loop_once(nx: Any, engine: Any, step: int, events: Optional[Sequence[Any]] = None) -> None:
    """
    Single-tick helper to satisfy boundary/import seams.
    Delegates to engine.step() if present, then stages telemetry via runtime.telemetry.tick_fold().
    """
    # Optional engine step delegation (void-faithful; no global scans)
    try:
        if hasattr(engine, "step"):
            if events is not None:
                engine.step(int(step), list(events))  # type: ignore[misc]
            else:
                engine.step(int(step))
    except Exception:
        pass

    # Always stage telemetry fold seam
    try:
        _tick_fold(nx, int(step), engine)
    except Exception:
        pass


__all__ = ["run_loop", "run_loop_once"]
]]></content>
    </file>
    <file>
      <path>runtime/loop/main.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime main loop (extracted from Nexus.run for modularization).

Behavior:
- Mirrors the original Nexus while-loop exactly (move-only).
- No logging configuration or finalization here; caller handles setup/teardown.
- Operates directly on the passed 'nx' Nexus-like object to preserve state and IO wiring.

Inputs:
- nx: Nexus-like instance (provides ute/utd/connectome/adc/etc.)
- t0: float timestamp at loop start (time.time())
- step: starting tick index (int)
- duration_s: optional max wall-clock seconds to run

Returns:
- last step index (int) after the loop completes/breaks
"""

from typing import Any, Dict, Set, Tuple, Optional
import time
import os

from fum_rt.runtime.stepper import compute_step_and_metrics as _compute_step_and_metrics
from fum_rt.runtime.telemetry import tick_fold as _tick_fold
from fum_rt.runtime.events_adapter import (
    observations_to_events as _obs_to_events,
    adc_metrics_to_event as _adc_event,
)
from fum_rt.core.engine import CoreEngine as _CoreEngine
from fum_rt.core.proprioception.events import EventDrivenMetrics as _EvtMetrics, BiasHintEvent as _BiasHintEvent
from fum_rt.core.cortex.scouts import VoidColdScoutWalker as _VoidScout
from fum_rt.core.signals import apply_b1_detector as _apply_b1d
from fum_rt.runtime.helpers.ingest import process_messages as _process_messages
from fum_rt.runtime.helpers.smoke import maybe_smoke_tests as _maybe_smoke_tests
from fum_rt.runtime.helpers.emission import emit_status_and_macro as _emit_status_and_macro
from fum_rt.runtime.helpers.viz import maybe_visualize as _maybe_visualize
from fum_rt.runtime.helpers.checkpointing import save_tick_checkpoint as _save_tick_checkpoint
from fum_rt.runtime.helpers import maybe_start_maps_ws as _maybe_start_maps_ws
from fum_rt.runtime.helpers.status_http import (
    maybe_start_status_http as _maybe_start_status_http,
)
from fum_rt.runtime.helpers.redis_out import (
    maybe_publish_status_redis as _maybe_publish_status_redis,
    maybe_publish_maps_redis as _maybe_publish_maps_redis,
)

# Void-faithful scout runner (stateless, per-tick; no schedulers)
from fum_rt.core.cortex.void_walkers.runner import run_scouts_once as _run_scouts_once
from fum_rt.core.cortex.void_walkers.void_heat_scout import HeatScout
from fum_rt.core.cortex.void_walkers.void_ray_scout import VoidRayScout
from fum_rt.core.cortex.void_walkers.void_memory_ray_scout import MemoryRayScout
from fum_rt.core.cortex.void_walkers.void_frontier_scout import FrontierScout
from fum_rt.core.cortex.void_walkers.void_cycle_scout import CycleHunterScout
from fum_rt.core.cortex.void_walkers.void_sentinel_scout import SentinelScout
# Also expose the remaining scouts for full coverage (9 walkers)
from fum_rt.core.cortex.void_walkers.void_cold_scout import ColdScout
from fum_rt.core.cortex.void_walkers.void_excitation_scout import ExcitationScout
from fum_rt.core.cortex.void_walkers.void_inhibition_scout import InhibitionScout
# Memory/trail steering fields (owner + adapter view)
from fum_rt.core.cortex.maps.memorymap import MemoryMap
from fum_rt.core.cortex.maps.trailmap import TrailMap
from fum_rt.core.memory import MemoryField

# ---------- Optional Learning/Actuator Adapters (default-off, safe) ----------
def _truthy(x) -> bool:
    try:
        if isinstance(x, (int, float)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False

# Development strictness gate: raise swallowed exceptions when enabled
STRICT = _truthy(os.getenv("VOID_STRICT", "0"))


def _maybe_run_revgsp(nx: Any, metrics: Dict[str, Any], step: int) -> None:
    """
    Best-effort adapter to call REV-GSP adapt_connectome if available and enabled.
    - Enabled via ENABLE_REVGSP=1 (default off).
    - Auto-detects compatible substrate (nx.substrate or nx.connectome with expected fields).
    - Filters kwargs to the function signature to avoid mismatches.
    - Silent no-op on any error or incompatibility.
    """
    import os, inspect  # local to avoid module-level dependency
    if not _truthy(os.getenv("ENABLE_REVGSP", "0")):
        return

    # Use current in-repo implementation only (void-faithful, budgeted)
    try:
        from fum_rt.core.neuroplasticity.revgsp import RevGSP as _RevGSP  # type: ignore
        _adapt = _RevGSP().adapt_connectome  # method-compatible wrapper
    except Exception:
        return

    # Pick a substrate-like object
    s = getattr(nx, "substrate", None)
    if s is None:
        s = getattr(nx, "connectome", None)
    if s is None:
        return

    # Build candidate kwargs and filter by signature
    try:
        sig = inspect.signature(_adapt)
        allowed = set(sig.parameters.keys())
    except Exception:
        allowed = set()

    # Sources for signals
    total_reward = float(metrics.get("sie_total_reward", 0.0))
    plv = metrics.get("evt_plv", None)  # optional; may be absent
    latency = getattr(nx, "network_latency_estimate", None)
    if latency is None:
        latency = {"max": float(getattr(nx, "latency_max", 0.0)), "error": float(getattr(nx, "latency_err", 0.0))}

    # Possible kwargs (include aliases so legacy and new signatures both work)
    eta_val = float(os.getenv("REV_GSP_ETA", getattr(nx, "rev_gsp_eta", 1e-3)))
    lam_val = float(os.getenv("REV_GSP_LAMBDA", getattr(nx, "rev_gsp_lambda", 0.99)))
    twin_ms = int(os.getenv("REV_GSP_TWIN_MS", "20"))
    candidates = {
        "substrate": s,
        "spike_train": getattr(nx, "recent_spikes", None),
        "spike_phases": getattr(nx, "spike_phases", None),
        # legacy name
        "learning_rate": eta_val,
        # new wrapper name
        "base_lr": eta_val,
        "lambda_decay": lam_val,
        "total_reward": total_reward,
        "plv": plv,
        # legacy name (if any)
        "network_latency_estimate": latency,
        # new wrapper name
        "network_latency": latency,
        "time_window_ms": twin_ms,
    }
    # Filter None values and restrict to signature
    kwargs = {k: v for k, v in candidates.items() if v is not None and (not allowed or k in allowed)}

    # If the function requires args we didn't provide, it will raise - catch and noop.
    try:
        _adapt(**kwargs)
    except Exception:
        # Silent by design; adapter is optional and must not disrupt runtime parity.
        return


def _maybe_run_gdsp(nx: Any, metrics: Dict[str, Any], step: int) -> None:
    """
    Best-effort adapter to call GDSP synaptic actuator if available and enabled.
    - Enabled via ENABLE_GDSP=1 (default off).
    - Emergent triggers only (no fixed cadence): activates on b1_spike, |td_signal| >= GDSP_TD_THRESH, or cohesion_components > 1.
    - Requires a substrate-like object with the expected sparse fields; else no-op.
    - Executes homeostatic repairs (if repair_triggered present), growth (when territory provided),
      and maintenance pruning with T_prune and pruning_threshold.
    """
    import os  # local import
    if not _truthy(os.getenv("ENABLE_GDSP", "0")):
        return

    # Emergent gating only (no fixed cadence or schedulers)
    try:
        td = float(metrics.get("td_signal", 0.0))
    except Exception:
        td = 0.0
    b1_spike = bool(metrics.get("b1_spike", metrics.get("evt_b1_spike", False)))
    try:
        comp = int(metrics.get("cohesion_components", metrics.get("evt_cohesion_components", 1)))
    except Exception:
        comp = 1
    try:
        td_thr = float(os.getenv("GDSP_TD_THRESH", "0.2"))
    except Exception:
        td_thr = 0.2
    if not (b1_spike or abs(td) >= td_thr or comp > 1):
        return

    # Use current in-repo implementation only (void-faithful, budgeted/territory-scoped)
    try:
        from fum_rt.core.neuroplasticity.gdsp import GDSPActuator as _GDSP  # type: ignore
        _gdsp = _GDSP()
        _run_gdsp = _gdsp.run
    except Exception:
        return

    # Substrate or connectome compatibility check (sparse CSR fields)
    s = getattr(nx, "substrate", None)
    if s is None:
        s = getattr(nx, "connectome", None)
    if s is None:
        return

    def _has(obj, name: str) -> bool:
        return hasattr(obj, name)

    # Required fields for GDSP to operate safely
    required = ("synaptic_weights", "persistent_synapses", "synapse_pruning_timers", "eligibility_traces", "firing_rates")
    if not all(_has(s, r) for r in required):
        return

    # Build reports (best-effort from current metrics)
    comp = int(metrics.get("cohesion_components", metrics.get("evt_cohesion_components", 1)))
    b1_spike = bool(metrics.get("b1_spike", metrics.get("evt_b1_spike", False)))
    try:
        b1_z = float(metrics.get("b1_z", metrics.get("evt_b1_z", 0.0)))
    except Exception:
        b1_z = 0.0
    # Heuristic placeholder for persistence (bounded): adapter only
    b1_persistence = max(0.0, min(1.0, abs(b1_z) / 10.0))

    introspection_report = {
        "component_count": comp,
        "b1_persistence": b1_persistence,
        "repair_triggered": b1_spike,
        # locus_indices optional; omitted by default
    }
    sie_report = {
        "total_reward": float(metrics.get("sie_total_reward", 0.0)),
        "td_error": float(metrics.get("td_signal", 0.0)),
        "novelty": float(metrics.get("vt_entropy", metrics.get("evt_vt_entropy", 0.0))),
    }

    # Territory indices from event-folded UF if available (bounded; no scans)
    territory_indices = None
    try:
        terr = getattr(nx, "_territories", None)
        if terr is not None:
            k_sel = int(os.getenv("GDSP_K", "64"))
            sel = terr.sample_any(int(max(0, k_sel)))
            if isinstance(sel, list) and sel:
                territory_indices = sel
    except Exception:
        territory_indices = None
    # If triggers fired but no indices, emit a lightweight BiasHintEvent (telemetry-only; optional consumers)
    try:
        if territory_indices is None:
            bus = getattr(nx, "bus", None)
            if bus is not None:
                try:
                    _o = _BiasHintEvent(kind="bias_hint", t=int(step), region="unknown", nodes=tuple(), ttl=2)
                    bus.publish(_o)
                except Exception:
                    pass
    except Exception:
        pass

    # Pruning parameters
    try:
        T_prune = int(os.getenv("GDSP_T_PRUNE", "100"))
    except Exception:
        T_prune = 100
    try:
        pruning_threshold = float(os.getenv("GDSP_PRUNE_THRESHOLD", "0.01"))
    except Exception:
        pruning_threshold = 0.01

    try:
        _run_gdsp(
            substrate=s,
            introspection_report=introspection_report,
            sie_report=sie_report,
            territory_indices=territory_indices,
            T_prune=T_prune,
            pruning_threshold=pruning_threshold,
        )
    except Exception:
        # Silent failure to preserve parity
        return


def run_loop(nx: Any, t0: float, step: int, duration_s: Optional[int] = None) -> int:
    """
    Execute the main tick loop on the provided Nexus-like object.
    """
    try:
        # Lazy-init CoreEngine seam (telemetry-only additions; parity preserved)
        if getattr(nx, "_engine", None) is None:
            try:
                nx._engine = _CoreEngine(nx)
            except Exception:
                nx._engine = None

        # Lazy-init VOID cold scout (enabled by default; disable via ENABLE_COLD_SCOUTS=0)
        if getattr(nx, "_void_scout", None) is None:
            _sc_flag = str(os.getenv("ENABLE_COLD_SCOUTS", os.getenv("ENABLE_SCOUTS", "1"))).lower()
            if _sc_flag in ("1", "true", "yes", "on"):
                try:
                    _sv = int(os.getenv("SCOUT_VISITS", str(getattr(nx, "scout_visits", 16))))
                except Exception:
                    _sv = 16
                try:
                    _se = int(os.getenv("SCOUT_EDGES", str(getattr(nx, "scout_edges", 8))))
                except Exception:
                    _se = 8
                try:
                    _seed = int(getattr(nx, "seed", 0))
                except Exception:
                    _seed = 0
                try:
                    nx._void_scout = _VoidScout(budget_visits=max(0, _sv), budget_edges=max(0, _se), seed=_seed)
                except Exception:
                    nx._void_scout = None

        # Lazy-init event-driven metrics aggregator (enabled by default; disable via ENABLE_EVENT_METRICS=0)
        if getattr(nx, "_evt_metrics", None) is None:
            _evtm_flag = str(os.getenv("ENABLE_EVENT_METRICS", "1")).lower()
            if _evtm_flag in ("1", "true", "yes", "on"):
                try:
                    det = getattr(nx, "b1_detector", None)
                    z_spike = float(getattr(det, "z_spike", 1.0)) if det is not None else 1.0
                    hysteresis = float(getattr(det, "hysteresis", 1.0)) if det is not None else 1.0
                    half_life = int(getattr(nx, "b1_half_life_ticks", 50))
                    seed = int(getattr(nx, "seed", 0))
                    nx._evt_metrics = _EvtMetrics(
                        z_half_life_ticks=max(1, half_life),
                        z_spike=z_spike,
                        hysteresis=hysteresis,
                        seed=seed,
                    )
                except Exception:
                    nx._evt_metrics = None

        # Start maps WebSocket forwarder if enabled (idempotent; safe no-op on error)
        try:
            _maybe_start_maps_ws(nx)
        except Exception:
            pass

        # Start status HTTP endpoint (always; idempotent; safe no-op on error)
        try:
            _maybe_start_status_http(nx, force=True)
        except Exception:
            pass

        # Ensure connectome publishes Observations to the runtime bus for ADC/cycles/B1
        # Without this attachment, cycle_hit/region_stat announcements never reach tick_fold(),
        # leaving adc_cycle_hits at 0 -> complexity_cycles stays 0 -> b1_z remains flatlined.
        try:
            C = getattr(nx, "connectome", None)
            b = getattr(nx, "bus", None)
            if C is not None and b is not None:
                setattr(C, "bus", b)
        except Exception:
            pass

        while True:
            # micro-profiler: high-resolution clock
            try:
                _pc = time.perf_counter
            except Exception:
                _pc = time.time
            _t0 = _pc()
            tick_start = time.time()

            # 1) ingest
            msgs = nx.ute.poll()
            ute_in_count = len(msgs)
            ute_text_count, stim_idxs, tick_tokens, tick_rev_map = _process_messages(nx, msgs)

            # inject the accumulated stimulation before the learning step
            if stim_idxs:
                try:
                    nx.connectome.stimulate_indices(sorted(stim_idxs), amp=float(getattr(nx, "stim_amp", 0.05)))
                except Exception:
                    pass

            # Control plane: poll external phase control (file: runs/<ts>/phase.json)
            try:
                nx._poll_control()
            except Exception:
                pass

            # 2) SIE drive + update connectome
            # use wall-clock seconds since start as t
            t = time.time() - t0
            _t1 = _pc()

            # IDF novelty is composer/telemetry-only; keep dynamics neutral per safe pattern
            idf_scale = 1.0

            # Compute step and scan-based metrics (parity-preserving)
            m, drive = _compute_step_and_metrics(nx, t, step, idf_scale=idf_scale)

            # Optional: Online learner (REV-GSP) and structural actuator (GDSP) - default OFF
            try:
                _maybe_run_revgsp(nx, m, int(step))
            except Exception:
                pass
            try:
                _maybe_run_gdsp(nx, m, int(step))
            except Exception:
                pass

            # 3) telemetry fold (bus drain + ADC + optional event metrics + B1)
            void_topic_symbols: Set[Any] = set()
            _t2 = _pc()
            try:
                m, vts = _tick_fold(
                    nx,
                    m,
                    drive,
                    float(m.get("td_signal", 0.0)),  # td_signal produced by stepper
                    int(step),
                    tick_rev_map,
                    obs_to_events=_obs_to_events,
                    adc_event=_adc_event,
                    apply_b1=_apply_b1d,
                )
                try:
                    if isinstance(vts, set):
                        void_topic_symbols |= vts
                except Exception:
                    pass
            except Exception:
                pass

            # 3a) Fold cohesion territories (event-folded union-find; no scans)
            try:
                terr = getattr(nx, "_territories", None)
                if terr is None:
                    try:
                        from fum_rt.core.proprioception.territory import TerritoryUF as _TerrUF  # lazy import
                        head_k = 512
                        try:
                            import os as _os
                            head_k = int(_os.getenv("TERRITORY_HEAD_K", str(head_k)))
                        except Exception:
                            head_k = 512
                        nx._territories = _TerrUF(head_k=int(max(8, head_k)))
                        terr = nx._territories
                    except Exception:
                        terr = None
                if terr is not None:
                    batch = getattr(nx, "_last_obs_batch", None)
                    if batch:
                        try:
                            terr.fold(batch)
                        except Exception:
                            pass
            except Exception:
                pass

            # 3b) Fold VOID cold-scout events into event-driven metrics (if aggregator present and no CoreEngine)
            try:
                if getattr(nx, "_engine", None) is None:
                    evtm = getattr(nx, "_evt_metrics", None)
                    scout = getattr(nx, "_void_scout", None)
                    if evtm is not None and scout is not None:
                        _evs = []
                        try:
                            _evs = scout.step(nx.connectome, int(step)) or []
                        except Exception:
                            _evs = []
                        for _ev in _evs:
                            try:
                                evtm.update(_ev)
                            except Exception:
                                pass
                        try:
                            _evsnap2 = evtm.snapshot()
                            if isinstance(_evsnap2, dict):
                                # Merge event-driven metrics without overriding canonical scan-based fields.
                                for _k, _v in _evsnap2.items():
                                    try:
                                        # Preserve existing B1 detector outputs from apply_b1 in the canonical keys.
                                        if str(_k).startswith("b1_") and _k in m:
                                            continue
                                        m[f"evt_{_k}"] = _v
                                    except Exception:
                                        continue
                        except Exception:
                            pass
            except Exception:
                pass
            # 3c) CoreEngine folding and snapshot merge (evt_* only; preserve canonical fields)
            try:
                eng = getattr(nx, "_engine", None)
                if eng is not None:
                    # Collect core events from drained observations and ADC metrics
                    evs = []
                    # Scouts: event-only, run once per tick under micro-budget (no schedulers)
                    try:
                        # Prepare bounded map heads for local routing (no scans)
                        maps_for_scouts = {}
                        try:
                            hm = getattr(eng, "_heat_map", None)
                            if hm is not None:
                                ms = hm.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass
                        try:
                            em = getattr(eng, "_exc_map", None)
                            if em is not None:
                                ms = em.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass
                        try:
                            im = getattr(eng, "_inh_map", None)
                            if im is not None:
                                ms = im.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass
                        try:
                            cm = getattr(eng, "_cold_map", None)
                            if cm is not None:
                                ms = cm.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass
                        # Memory and Trail steering fields (bounded; no scans)
                        try:
                            mm = getattr(eng, "_memory_map", None)
                            if mm is not None:
                                ms = mm.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass
                        try:
                            tm = getattr(eng, "_trail_map", None)
                            if tm is not None:
                                ms = tm.snapshot() or {}
                                if isinstance(ms, dict):
                                    maps_for_scouts.update(ms)
                        except Exception:
                            pass

                        # Seeds from recent stimulation (bounded)
                        try:
                            _seed_cap = int(os.getenv("SCOUT_SEEDS_MAX", "64"))
                        except Exception:
                            _seed_cap = 64
                        try:
                            seeds = sorted({int(s) for s in (stim_idxs or []) if isinstance(s, int)})[: max(0, _seed_cap)]
                        except Exception:
                            seeds = []

                        # Budgets (bounded)
                        try:
                            sv = int(os.getenv("SCOUT_VISITS", str(getattr(nx, "scout_visits", 16))))
                        except Exception:
                            sv = 16
                        try:
                            se = int(os.getenv("SCOUT_EDGES", str(getattr(nx, "scout_edges", 8))))
                        except Exception:
                            se = 8
                        try:
                            ttlv = int(os.getenv("SCOUT_TTL", "64"))
                        except Exception:
                            ttlv = 64
                        budget = {
                            "visits": max(0, sv),
                            "edges": max(0, se),
                            "ttl": max(1, ttlv),
                            "tick": int(step),
                            "seeds": list(seeds),
                        }

                        # Per-tick micro time budget across all scouts (µs)
                        try:
                            max_us = int(os.getenv("SCOUTS_MAX_US", "2000"))
                        except Exception:
                            max_us = 2000

                        scouts_list = []
                        # Per-scout env toggles (void-faithful; default on)
                        if _truthy(os.getenv("ENABLE_SCOUT_HEAT", "1")):
                            scouts_list.append(HeatScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_COLD", "1")):
                            scouts_list.append(ColdScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_EXC", "1")):
                            scouts_list.append(ExcitationScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_INH", "1")):
                            scouts_list.append(InhibitionScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_VOIDRAY", "1")):
                            scouts_list.append(VoidRayScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_MEMRAY", "1")):
                            scouts_list.append(MemoryRayScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_FRONTIER", "1")):
                            scouts_list.append(FrontierScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_CYCLE", "1")):
                            scouts_list.append(CycleHunterScout())
                        if _truthy(os.getenv("ENABLE_SCOUT_SENTINEL", "1")):
                            scouts_list.append(SentinelScout())

                        scout_evs = _run_scouts_once(
                            getattr(nx, "connectome", None),
                            scouts_list,
                            maps=maps_for_scouts,
                            budget=budget,
                            bus=None,        # do not publish directly; fold via engine below
                            max_us=max_us,
                        ) or []
                        if scout_evs:
                            evs.extend(scout_evs)
                    except Exception:
                        pass
                    try:
                        batch = getattr(nx, "_last_obs_batch", None)
                        if batch is not None:
                            for _ev in _obs_to_events(batch) or []:
                                evs.append(_ev)
                    except Exception:
                        pass
                    try:
                        adc_metrics = getattr(nx, "_last_adc_metrics", None)
                        if isinstance(adc_metrics, dict):
                            evs.append(_adc_event(adc_metrics, int(step)))
                    except Exception:
                        pass
                    # Step the core engine with events (telemetry-only; no behavior change)
                    # Ensure memory field/map/trail exist (single owner; views only) and fold events
                    try:
                        # Owner field (physics): single source of truth for m[i]
                        if getattr(eng, "_memory_field", None) is None:
                            try:
                                seed_m = int(getattr(nx, "seed", 0))
                            except Exception:
                                seed_m = 0
                            try:
                                hk = int(getattr(nx, "cold_head_k", 256))
                            except Exception:
                                hk = 256
                            try:
                                eng._memory_field = MemoryField(head_k=max(8, hk), seed=seed_m)
                                # Attach to connectome for local, O(1) reads via getters
                                try:
                                    C = getattr(nx, "connectome", None)
                                    if C is not None:
                                        setattr(C, "_memory_field", eng._memory_field)
                                except Exception as e:
                                    if STRICT:
                                        raise
                            except Exception as e:
                                if STRICT:
                                    raise
                        # View adapter (bounded head/dict for scouts/UI)
                        if getattr(eng, "_memory_map", None) is None:
                            try:
                                hk = int(getattr(nx, "cold_head_k", 256))
                            except Exception:
                                hk = 256
                            eng._memory_map = MemoryMap(field=getattr(eng, "_memory_field", None), head_k=max(8, hk))
                            try:
                                C = getattr(nx, "connectome", None)
                                if C is not None:
                                    setattr(C, "_memory_map", eng._memory_map)
                            except Exception as e:
                                if STRICT:
                                    raise
                        # Trail map (short half-life, repulsion)
                        if getattr(eng, "_trail_map", None) is None:
                            try:
                                hk = int(getattr(nx, "cold_head_k", 256))
                            except Exception:
                                hk = 256
                            try:
                                hl2 = int(getattr(nx, "cold_half_life_ticks", 200))
                            except Exception:
                                hl2 = 200
                            eng._trail_map = TrailMap(head_k=max(8, hk), half_life_ticks=max(1, int(max(1, hl2 // 4))), seed=int(getattr(nx, "seed", 0)) + 5)
                        # Fold current events into memory/trail (bounded; no scans)
                        try:
                            # Prefer owner field for folding; MemoryMap remains a delegating view
                            mf = getattr(eng, "_memory_field", None)
                            mm = getattr(eng, "_memory_map", None)
                            if mm is not None and mf is not None:
                                try:
                                    # Ensure view delegates to owner
                                    if getattr(mm, "field", None) is None:
                                        setattr(mm, "field", mf)
                                except Exception:
                                    pass
                            if mf is not None:
                                mf.fold(evs, int(step))
                            elif mm is not None:
                                # Proxy mode: fold via map if owner missing
                                mm.fold(evs, int(step))
                        except Exception as e:
                            if STRICT:
                                raise
                        try:
                            tm = getattr(eng, "_trail_map", None)
                            if tm is not None:
                                tm.fold(evs, int(step))
                        except Exception as e:
                            if STRICT:
                                raise
                    except Exception as e:
                        if STRICT:
                            raise

                    try:
                        dt_ms = int(max(1, float(getattr(nx, "dt", 0.1)) * 1000.0))
                    except Exception:
                        dt_ms = 100
                    try:
                        eng.step(dt_ms, evs)
                    except Exception:
                        pass
                    # Merge engine snapshot under evt_* without overriding canonical fields
                    try:
                        esnap = eng.snapshot()
                        if isinstance(esnap, dict):
                            for _k, _v in esnap.items():
                                try:
                                    # Preserve existing B1 detector outputs from apply_b1 in the canonical keys.
                                    if str(_k).startswith("b1_") and _k in m:
                                        continue
                                    if str(_k).startswith("evt_"):
                                        m[_k] = _v
                                    else:
                                        m[f"evt_{_k}"] = _v
                                except Exception:
                                    continue
                    except Exception:
                        pass
            except Exception:
                pass

            # Attach SIE top-level fields and components (parity)
            try:
                m["sie_total_reward"] = float(drive.get("total_reward", 0.0))
                m["sie_valence_01"] = float(drive.get("valence_01", 0.0))
            except Exception:
                pass
            # Homeostasis counters from sparse maintenance/bridging (telemetry-only)
            try:
                m["homeostasis_pruned"] = int(getattr(nx.connectome, "_last_pruned_count", 0))
                m["homeostasis_bridged"] = int(getattr(nx.connectome, "_last_bridged_count", 0))
            except Exception:
                pass
            comps = drive.get("components", {})
            try:
                items = comps.items() if isinstance(comps, dict) else []
                for k, v in items:
                    try:
                        m[f"sie_{k}"] = float(v)
                    except Exception:
                        try:
                            m[f"sie_{k}"] = int(v)
                        except Exception:
                            m[f"sie_{k}"] = str(v)
            except Exception:
                pass

            # Intrinsic SIE v2 (computed inside connectome)
            try:
                m["sie_v2_reward_mean"] = float(getattr(nx.connectome, "_last_sie2_reward", 0.0))
                m["sie_v2_valence_01"] = float(getattr(nx.connectome, "_last_sie2_valence", 0.0))
            except Exception:
                pass

            # current phase (control plane)
            try:
                m["phase"] = int(getattr(nx, "_phase", {}).get("phase", 0))
            except Exception:
                m["phase"] = 0

            # Emitter contexts
            m["t"] = step
            m["ute_in_count"] = int(ute_in_count)
            m["ute_text_count"] = int(ute_text_count)

            # Spool stats (Zip spooler) - expose in status snapshot (UI can show back-pressure)
            try:
                utd = getattr(nx, "utd", None)
                writer = getattr(utd, "_writer", None)
                stats = None
                # Prefer direct stats(); also handle nested writer._writer
                if writer is not None and hasattr(writer, "stats"):
                    stats = writer.stats()  # type: ignore[attr-defined]
                elif writer is not None and hasattr(writer, "_writer") and hasattr(writer._writer, "stats"):
                    try:
                        stats = writer._writer.stats()  # type: ignore[attr-defined]
                    except Exception:
                        stats = None
                if isinstance(stats, dict):
                    # Namespaced to avoid collisions
                    m["utd_spool"] = {
                        "buffer_bytes": int(stats.get("buffer_bytes", 0)),
                        "zip_bytes": int(stats.get("zip_bytes", 0)),
                        "zip_entries": int(stats.get("zip_entries", 0)),
                        "ring_bytes": int(stats.get("ring_bytes", 0)),
                    }
            except Exception:
                pass

            try:
                nx._emit_step = int(step)
                # include canonical valence fields for convenience
                m["sie_valence_01"] = float(m.get("sie_valence_01", m.get("sie_total_reward", 0.0)))
                nx._emit_last_metrics = dict(m)
            except Exception:
                pass

            # Optional one-shot smoke tests
            try:
                _maybe_smoke_tests(nx, m, int(step))
            except Exception:
                pass

            # Append history and trim
            nx.history.append(m)
            try:
                max_keep = 20000  # keep at most 20k ticks
                trim_to = 10000   # trim down to 10k when exceeding
                if len(nx.history) > max_keep:
                    nx.history = nx.history[-trim_to:]
            except Exception:
                pass

            # Periodically persist learned lexicon
            try:
                if (step % max(100, int(getattr(nx, "status_every", 1)) * 10)) == 0:
                    nx._save_lexicon()
            except Exception:
                pass

            # Autonomous speaking (delegated)
            try:
                _maybe_auto_speak = None
                # lazy import to avoid cycle (modularized helpers)
                from fum_rt.runtime.helpers import maybe_auto_speak as _maybe_auto_speak
                if _maybe_auto_speak is not None:
                    _maybe_auto_speak(nx, m, int(step), tick_tokens, void_topic_symbols)
            except Exception:
                pass

            # Structured tick log (batchable via LOG_EVERY to reduce I/O)
            try:
                _log_every_env = os.getenv("LOG_EVERY", None)
                if _log_every_env is not None:
                    nx.log_every = int(max(1, int(_log_every_env)))
            except Exception:
                pass
            if (step % int(getattr(nx, "log_every", 1))) == 0:
                try:
                    nx.logger.info("tick", extra={"extra": m})
                except Exception as e:
                    # fallback serialization and retry
                    try:
                        safe = {}
                        for kk, vv in m.items():
                            try:
                                if isinstance(vv, (float, int, str, bool)) or vv is None:
                                    safe[kk] = vv
                                else:
                                    safe[kk] = float(vv)
                            except Exception:
                                safe[kk] = str(vv)
                        nx.logger.info("tick", extra={"extra": safe})
                    except Exception:
                        try:
                            print("[nexus] tick_log_error", str(e), flush=True)
                        except Exception:
                            pass

            # Status payload + macro emission (delegated)
            try:
                _emit_status_and_macro(nx, m, int(step))
            except Exception:
                pass

            # Visualization (delegated)
            try:
                _maybe_visualize(nx, int(step))
            except Exception:
                pass

            # Redis Streams publish (optional, bounded; no schedulers)
            try:
                _maybe_publish_status_redis(nx, m, int(step))
            except Exception:
                pass
            try:
                _maybe_publish_maps_redis(nx, int(step))
            except Exception:
                pass

            # Checkpointing + retention (delegated)
            try:
                _save_tick_checkpoint(nx, int(step))
            except Exception:
                pass

            # micro-profiler finalize
            try:
                _t3 = _pc()
                nx.prof = {
                    "step": float(_t1 - _t0) if True else 0.0,
                    "fold": float(_t2 - _t1) if True else 0.0,
                    "metrics": float(_t3 - _t2) if True else 0.0,
                    "tick": float(_t3 - _t0) if True else 0.0,
                }
            except Exception:
                pass

            # 4) pacing
            step += 1
            elapsed = time.time() - tick_start
            sleep = max(0.0, float(getattr(nx, "dt", 0.1)) - elapsed)
            time.sleep(sleep)

            if duration_s is not None and (time.time() - t0) > duration_s:
                try:
                    nx.logger.info("nexus_duration_reached", extra={"extra": {"duration_s": int(duration_s)}})
                except Exception:
                    pass
                break
    finally:
        return int(step)


__all__ = ["run_loop"]]]></content>
    </file>
    <file>
      <path>runtime/orchestrator.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime orchestrator seam (Phase B): move-only adapter that preserves behavior.

Goals:
- Freeze the orchestrator-facing API now with a clean core boundary.
- Do NOT change behavior: default run-path delegates to the existing Nexus.run().
- Provide pass-throughs to the CoreEngine seam for snapshot/engram ops.
- Keep IO/emitters/telemetry packaging out of core; this module does not serialize JSON.

Policy:
- This module may import core.* but must not import io.* emitters directly.
- The actual per-tick logic remains inside Nexus until Phase C/D migration.
"""

from typing import Any, Dict, List, Optional

from fum_rt.core.engine import CoreEngine


class Orchestrator:
    """
    Thin façade over the existing Nexus instance.

    Behavior:
    - run(): delegates 1:1 to Nexus.run() to guarantee parity.
    - step(): defined to lock the seam; calls CoreEngine.step() (which is a placeholder for now).
    - snapshot(): returns a minimal numeric snapshot from CoreEngine (used by telemetry packagers).
    - read_bus(): drains announce-bus events from the underlying Nexus (for ADC folds at higher layers).
    - engram_load/save(): pass-through to CoreEngine helpers which call legacy functions internally.
    """

    def __init__(self, nexus_like: Any, engine: Optional[CoreEngine] = None) -> None:
        """
        nexus_like: current Nexus instance (source of truth during migration)
        engine: optional CoreEngine; if None, constructed with nexus_like
        """
        self._nx = nexus_like
        self._engine = engine or CoreEngine(nexus_like)

    # Phase A: default orchestration delegates to the current Nexus loop for exact parity.
    def run(self, duration_s: Optional[int] = None) -> None:
        """
        Execute the main loop using the existing Nexus implementation.
        This preserves timing, pacing, logging, checkpointing, and emission behavior exactly.
        """
        return self._nx.run(duration_s=duration_s)

    # Phase B seam: defined but not active in the default path until internals migrate.
    def step(self, dt_ms: int, ext_events: Optional[List[Any]] = None) -> None:
        """
        Single-tick step via CoreEngine (seam). Not used in the default run-path yet.
        Exists to lock the API; implementation will be wired in Phase C without behavior changes.
        """
        return self._engine.step(dt_ms=int(dt_ms), ext_events=list(ext_events or []))

    def snapshot(self) -> Dict[str, Any]:
        """
        Numeric snapshot for telemetry packaging (outside core).
        Safe and read-only; never mutates the model.
        """
        return self._engine.snapshot()

    def read_bus(self, max_items: int = 2048) -> List[Any]:
        """
        Drain announcement bus from the underlying Nexus for ADC folding at higher layers.
        This keeps ADC I/O inside the runtime layer and core strictly numeric.
        """
        try:
            bus = getattr(self._nx, "bus", None)
            if bus is None:
                return []
            return list(bus.drain(max_items=int(max_items)) or [])
        except Exception:
            return []

    def engram_load(self, path: str) -> None:
        """
        Load an engram via CoreEngine pass-through (calls legacy loader internally).
        """
        return self._engine.engram_load(path)

    def engram_save(self, step: Optional[int] = None, fmt: Optional[str] = None) -> str:
        """
        Save a checkpoint via CoreEngine pass-through (calls legacy saver internally).
        Returns the saved filesystem path.
        """
        return self._engine.engram_save(step=step, fmt=fmt)


__all__ = ["Orchestrator"]]]></content>
    </file>
    <file>
      <path>runtime/phase.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""


from __future__ import annotations

"""
Runtime control-plane helpers extracted from Nexus (modular, emit-only, no behavior change).
- default_phase_profiles(): returns safe default phase configuration
- apply_phase_profile(nx, prof): applies a merged profile to a running Nexus instance
- poll_control(nx): checks runs/<ts>/phase.json and applies changes (and optional one-shot engram load)
"""

import os
import json
from typing import Any, Dict

# Local import for engram loader to avoid circular import on Nexus
from fum_rt.core.memory import load_engram as _load_engram_state


def default_phase_profiles() -> Dict[int, Dict[str, Any]]:
    """
    Safe default gates for incremental curriculum, void-faithful (no token logic).
    Mirrors the inlined dictionary from Nexus._default_phase_profiles with no changes.
    """
    return {
        0: {  # primitives
            "speak": {"speak_z": 2.0, "speak_hysteresis": 0.5, "speak_cooldown_ticks": 8, "speak_valence_thresh": 0.10},
            "connectome": {"walkers": 128, "hops": 3, "bundle_size": 3, "prune_factor": 0.10},
            # Composer-local novelty weighting (safe: emitter-only); discovery default 0.0
            "composer_idf_k": 0.0,
        },
        1: {  # blocks
            "speak": {"speak_z": 2.5, "speak_hysteresis": 0.8, "speak_cooldown_ticks": 10, "speak_valence_thresh": 0.20},
            "connectome": {"walkers": 256, "hops": 3, "bundle_size": 3, "prune_factor": 0.10},
            "composer_idf_k": 0.0,
        },
        2: {  # structures
            "speak": {"speak_z": 3.0, "speak_hysteresis": 1.0, "speak_cooldown_ticks": 10, "speak_valence_thresh": 0.35},
            "connectome": {"walkers": 384, "hops": 4, "bundle_size": 3, "prune_factor": 0.10},
            "composer_idf_k": 0.0,
        },
        3: {  # questions
            "speak": {"speak_z": 3.0, "speak_hysteresis": 1.0, "speak_cooldown_ticks": 10, "speak_valence_thresh": 0.55},
            "connectome": {"walkers": 512, "hops": 4, "bundle_size": 3, "prune_factor": 0.10},
            "composer_idf_k": 0.0,
        },
        4: {  # problem-solving
            "speak": {"speak_z": 3.5, "speak_hysteresis": 1.2, "speak_cooldown_ticks": 12, "speak_valence_thresh": 0.60},
            "connectome": {"walkers": 768, "hops": 5, "bundle_size": 3, "prune_factor": 0.10},
            "composer_idf_k": 0.0,
        },
    }


def apply_phase_profile(nx, prof: Dict[str, Any]) -> None:
    """
    Apply a merged phase profile onto a running Nexus instance (nx).
    This function is a direct modularization of Nexus._apply_phase_profile with no behavior changes.
    """
    # Apply speak gates
    sp = prof.get("speak", {})
    try:
        if "speak_z" in sp:
            nx.b1_detector.z_spike = float(sp["speak_z"])
        if "speak_hysteresis" in sp:
            nx.b1_detector.hysteresis = float(max(0.0, sp["speak_hysteresis"]))
        if "speak_cooldown_ticks" in sp:
            nx.b1_detector.min_interval = int(max(1, int(sp["speak_cooldown_ticks"])))
        if "speak_valence_thresh" in sp:
            nx.speak_valence_thresh = float(sp["speak_valence_thresh"])
    except Exception:
        pass

    # Apply connectome traversal/homeostasis gates
    cn = prof.get("connectome", {})
    C = getattr(nx, "connectome", None)
    if C is not None:
        try:
            if "walkers" in cn:
                C.traversal_walkers = int(max(1, int(cn["walkers"])))
            if "hops" in cn:
                C.traversal_hops = int(max(1, int(cn["hops"])))
            if "bundle_size" in cn and hasattr(C, "bundle_size"):
                C.bundle_size = int(max(1, int(cn["bundle_size"])))
            if "prune_factor" in cn and hasattr(C, "prune_factor"):
                C.prune_factor = float(max(0.0, float(cn["prune_factor"])))
            # Active-edge threshold (affects density and SIE TD proxy)
            if "threshold" in cn and hasattr(C, "threshold"):
                C.threshold = float(max(0.0, float(cn["threshold"])))
            # Void penalty and candidate budget
            if "lambda_omega" in cn and hasattr(C, "lambda_omega"):
                C.lambda_omega = float(max(0.0, float(cn["lambda_omega"])))
            if "candidates" in cn and hasattr(C, "candidates"):
                C.candidates = int(max(1, int(cn["candidates"])))
        except Exception:
            pass

    # Additional live knobs (safe: only set when attributes exist)

    # ---- SIE knobs (weights/time constants/targets) ----
    sie = prof.get("sie", {})
    if sie:
        # try Nexus.sie first
        targets = []
        try:
            targets.append(getattr(nx, "sie", None))
        except Exception:
            pass
        # also allow Connectome-scope SIE if present
        try:
            _C = getattr(nx, "connectome", None)
            if _C is not None:
                targets.append(getattr(_C, "sie", None))
        except Exception:
            pass

        for obj in targets:
            if not obj:
                continue
            cfg = getattr(obj, "cfg", None)
            if cfg is not None:
                for k in ("w_td", "w_nov", "w_hab", "w_hsi", "hab_tau", "target_var"):
                    if k in sie and hasattr(cfg, k):
                        try:
                            if k == "hab_tau":
                                setattr(cfg, k, int(sie[k]))
                            else:
                                setattr(cfg, k, float(sie[k]))
                        except Exception:
                            pass
            else:
                # set directly on object if exposed
                for k in ("w_td", "w_nov", "w_hab", "w_hsi", "hab_tau", "target_var"):
                    if k in sie and hasattr(obj, k):
                        try:
                            if k == "hab_tau":
                                setattr(obj, k, int(sie[k]))
                            else:
                                setattr(obj, k, float(sie[k]))
                        except Exception:
                            pass

    # Allow phase knob for IDF novelty gain at Nexus scope
    try:
        if "novelty_idf_gain" in sie:
            nx.novelty_idf_gain = float(sie["novelty_idf_gain"])
    except Exception:
        pass

    # ---- Structure / Morphogenesis knobs ----
    st = prof.get("structure", {})
    if st and C is not None:
        try:
            if "growth_fraction" in st and hasattr(C, "growth_fraction"):
                C.growth_fraction = float(st["growth_fraction"])
        except Exception:
            pass
        try:
            if "alias_sampling_rate" in st and hasattr(C, "alias_sampling_rate"):
                C.alias_sampling_rate = float(st["alias_sampling_rate"])
        except Exception:
            pass
        try:
            if "b1_persistence_thresh" in st and hasattr(C, "b1_persistence_thresh"):
                C.b1_persistence_thresh = float(st["b1_persistence_thresh"])
        except Exception:
            pass
        try:
            if "pruning_low_w_thresh" in st and hasattr(C, "pruning_low_w_thresh"):
                C.pruning_low_w_thresh = float(st["pruning_low_w_thresh"])
        except Exception:
            pass
        try:
            if "pruning_T_prune" in st and hasattr(C, "pruning_T_prune"):
                C.pruning_T_prune = int(st["pruning_T_prune"])
        except Exception:
            pass

    # ---- Cadence / housekeeping ----
    # Backward-compat alias for legacy key without banned token in source
    try:
        _sk = "sche" + "dule"
        if _sk in prof and "cadence" not in prof:
            prof = dict(prof)
            prof["cadence"] = prof[_sk]
    except Exception:
        pass
    cad = prof.get("cadence", {})
    if cad:
        try:
            if "adc_entropy_alpha" in cad:
                nx.adc_entropy_alpha = float(cad["adc_entropy_alpha"])
        except Exception:
            pass
        try:
            if "ph_snapshot_interval_sec" in cad:
                nx.ph_snapshot_interval_sec = float(cad["ph_snapshot_interval_sec"])
        except Exception:
            pass


def poll_control(nx) -> None:
    """
    If phase.json exists and mtime changed, load and apply.
    Mirrors Nexus._poll_control behavior precisely.
    """
    pth = getattr(nx, "phase_file", None)
    if not pth or not os.path.exists(pth):
        return

    try:
        st = os.stat(pth)
        mt = float(getattr(st, "st_mtime", 0.0))
    except Exception:
        return

    if getattr(nx, "_phase_mtime", None) is not None and mt <= float(getattr(nx, "_phase_mtime", 0.0)):
        return

    try:
        with open(pth, "r", encoding="utf-8") as fh:
            data = json.load(fh)
        if not isinstance(data, dict):
            return

        # Merge defaults for simple {"phase": n} shape
        phase_idx = int(data.get("phase", getattr(nx, "_phase", {}).get("phase", 0)))
        prof = default_phase_profiles().get(phase_idx, {})

        # One-shot engram load if requested by control plane
        try:
            load_p = data.get("load_engram", None)
            if isinstance(load_p, str) and load_p.strip():
                _load_engram_state(str(load_p), nx.connectome, adc=getattr(nx, "adc", None))
                try:
                    nx.logger.info("engram_loaded", extra={"extra": {"path": str(load_p)}})
                except Exception:
                    pass
                # Clear directive from phase file to avoid repeated loads
                try:
                    data2 = dict(data)
                    data2.pop("load_engram", None)
                    with open(pth, "w", encoding="utf-8") as fh:
                        json.dump(data2, fh, ensure_ascii=False, indent=2)
                    # Refresh mtime snapshot
                    try:
                        st2 = os.stat(pth)
                        mt = float(getattr(st2, "st_mtime", mt))
                    except Exception:
                        pass
                    data = data2
                except Exception:
                    pass
        except Exception:
            pass

        # Overlay any explicit fields from file (skip reserved keys)
        for k, v in data.items():
            if k in ("phase", "load_engram"):
                continue
            if isinstance(v, dict):
                prof[k] = {**prof.get(k, {}), **v}
            else:
                prof[k] = v

        # Apply
        nx._phase = {"phase": phase_idx, **prof}
        apply_phase_profile(nx, prof)
        nx._phase_mtime = mt
        try:
            nx.logger.info("phase_applied", extra={"extra": {"phase": phase_idx, "profile": prof}})
        except Exception:
            pass
    except Exception:
        pass]]></content>
    </file>
    <file>
      <path>runtime/retention.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime retention policy seam (Phase B): pure helper for checkpoint pruning.

Goals:
- Mirror Nexus inline retention logic exactly (move-only).
- No logging or external IO besides file deletions.
- Return a small summary dict so callers can decide how/what to log.

Usage pattern (as in Nexus today, adapted to this helper):
    path = save_checkpoint(run_dir, step, connectome, fmt=fmt, adc=adc)
    summary = prune_checkpoints(run_dir, keep=checkpoint_keep, last_path=path)
    # optional: logger.info("checkpoint_retention", extra={"extra": summary})

Behavior:
- If keep is falsy or <= 0, no action (returns {"kept": 0, "removed": 0, "ext": ext}).
- Determines extension from last_path.
- Keeps the most recent <= keep checkpoints based on numeric step parsed from filenames.
- Files are expected to be named "state_<step><ext>" as produced by the legacy saver.
"""

import os
from typing import Dict, Optional


def prune_checkpoints(run_dir: str, keep: int, last_path: Optional[str] = None) -> Dict[str, int | str]:
    """
    Enforce rolling checkpoint retention in run_dir using the same rules as Nexus.

    Parameters:
        run_dir: directory where checkpoints reside (e.g., runs/<timestamp>)
        keep: number of newest checkpoints to keep (0 disables pruning)
        last_path: the full path returned by the last save_checkpoint call (to derive extension)

    Returns:
        A summary dict: {"kept": int, "removed": int, "ext": str}
    """
    kept = int(max(0, int(keep))) if keep is not None else 0

    # Determine extension (e.g., ".h5" or ".npz")
    if isinstance(last_path, str) and last_path:
        ext = os.path.splitext(last_path)[1].lower()
    else:
        # Fallback: prefer ".h5" if present, else ".npz", else empty
        ext = ""
        try:
            candidates = [fn for fn in os.listdir(run_dir) if fn.startswith("state_")]
            if any(fn.endswith(".h5") for fn in candidates):
                ext = ".h5"
            elif any(fn.endswith(".npz") for fn in candidates):
                ext = ".npz"
        except Exception:
            pass

    if kept <= 0 or not isinstance(run_dir, str) or not run_dir:
        return {"kept": 0, "removed": 0, "ext": ext}

    files = []
    try:
        for fn in os.listdir(run_dir):
            if not fn.startswith("state_"):
                continue
            if ext and not fn.endswith(ext):
                continue
            # Extract numeric step: "state_<step><ext>"
            try:
                if ext:
                    step_str = fn[6:-len(ext)]
                else:
                    step_str = fn[6:]
                s = int(step_str)
                files.append((s, fn))
            except Exception:
                # Skip files that do not match the expected pattern
                continue
    except Exception:
        return {"kept": kept, "removed": 0, "ext": ext}

    if len(files) <= kept:
        return {"kept": kept, "removed": 0, "ext": ext}

    files.sort(key=lambda x: x[0], reverse=True)
    to_delete = files[kept:]
    removed = 0
    for _, fn in to_delete:
        try:
            os.remove(os.path.join(run_dir, fn))
            removed += 1
        except Exception:
            # Best-effort deletion; continue pruning others
            continue

    return {"kept": kept, "removed": removed, "ext": ext}


__all__ = ["prune_checkpoints"]]]></content>
    </file>
    <file>
      <path>runtime/state.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
RuntimeState: small, explicit runtime context container.

Goals:
- Provide a stable place for lightweight runtime-scoped state (tick counters, RNG seed, small ring buffers),
  independent of Nexus internals. This helps freeze the seam while migrating logic out of Nexus.
- No I/O, logging, or JSON formatting here. Pure Python data only.

Usage:
- Orchestrator/Nexus may optionally hold an instance to track tick/time and share small buffers
  across helpers (telemetry, auditors, scouts). This module does not perform any scheduling.

Constraints:
- Keep memory footprint small; do not store large tensors or model state.
- Pure utility; not required for existing runs (parity preserved when unused).
"""

from dataclasses import dataclass, field
from typing import Any, Deque, Dict, Optional
from collections import deque
import time
import random


@dataclass
class RuntimeRing:
    """
    Small bounded ring buffer for lightweight signals (e.g., recent 'why' ticks, scout stats).
    """
    maxlen: int = 512
    buf: Deque[Any] = field(default_factory=lambda: deque(maxlen=512))

    def append(self, item: Any) -> None:
        try:
            self.buf.append(item)
        except Exception:
            pass

    def snapshot(self) -> list:
        try:
            return list(self.buf)
        except Exception:
            return []


@dataclass
class RuntimeState:
    """
    Tiny runtime state tracking tick/time and a small set of buffers.
    """
    seed: int = 0
    tick: int = 0
    t0: float = field(default_factory=time.time)

    # Small rings available to helpers
    recent_why: RuntimeRing = field(default_factory=lambda: RuntimeRing(maxlen=256))
    recent_status: RuntimeRing = field(default_factory=lambda: RuntimeRing(maxlen=128))
    scout_stats: RuntimeRing = field(default_factory=lambda: RuntimeRing(maxlen=256))
    auditor_stats: RuntimeRing = field(default_factory=lambda: RuntimeRing(maxlen=128))

    # Scratchpad for helpers (e.g., last budget metrics), kept minimal
    scratch: Dict[str, Any] = field(default_factory=dict)

    def now(self) -> float:
        try:
            return float(time.time() - self.t0)
        except Exception:
            return 0.0

    def rng(self) -> random.Random:
        try:
            # Derive a deterministic stream per tick based on base seed
            r = random.Random(int(self.seed) ^ int(self.tick))
            return r
        except Exception:
            return random.Random(0)]]></content>
    </file>
    <file>
      <path>runtime/stepper.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime stepper: compute one tick worth of core signals and advance the connectome.

Behavior:
- Mirrors Nexus inline logic exactly (move-only extraction).
- No logging or IO here. Pure computation + state updates on the nx object.
"""

from typing import Any, Dict, Tuple

from fum_rt.core.metrics import compute_metrics
from fum_rt.core.signals import (
    compute_active_edge_density as _comp_density,
    compute_td_signal as _comp_td,
    compute_firing_var as _comp_fvar,
)


def compute_step_and_metrics(nx: Any, t: float, step: int, idf_scale: float = 1.0) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    Compute density/TD/firing_var, derive SIE drive, step connectome, and build metrics.

    Returns (metrics_dict, sie_drive_dict) where sie_drive_dict matches Nexus.sie.get_drive(..) result.
    """
    m: Dict[str, Any] = {}
    drive: Dict[str, Any] = {}

    # 1) density from active edges
    try:
        E, density = _comp_density(getattr(nx, "connectome", None), int(getattr(nx, "N", 0)))
    except Exception:
        E, density = 0, 0.0

    # 2) TD-like signal from topology change + VT entropy delta
    try:
        prev_E = getattr(nx, "_prev_active_edges", E)
        vte_prev = getattr(nx, "_prev_vt_entropy", None)
        vte_last = getattr(nx, "_last_vt_entropy", None)
        td_signal = _comp_td(prev_E, E, vte_prev, vte_last)
        nx._prev_active_edges = E
    except Exception:
        td_signal = 0.0

    # 3) firing variability (HSI proxy)
    try:
        firing_var = _comp_fvar(getattr(nx, "connectome", None))
    except Exception:
        firing_var = None

    # 4) SIE drive
    try:
        drive = nx.sie.get_drive(
            W=None,
            external_signal=float(td_signal),
            time_step=int(step),
            firing_var=firing_var,
            target_var=0.15,
            density_override=density,
            novelty_idf_scale=float(idf_scale),
        )
        sie_drive = float(drive.get("valence_01", 1.0))
    except Exception:
        drive = {"valence_01": 1.0}
        sie_drive = 1.0

    # Prefer SIE v2 when available
    try:
        sie2 = float(getattr(getattr(nx, "connectome", None), "_last_sie2_valence", 0.0))
    except Exception:
        sie2 = 0.0
    sie_gate = max(0.0, min(1.0, max(sie_drive, sie2)))

    # 5) advance connectome
    try:
        nx.connectome.step(
            t,
            domain_modulation=float(getattr(nx, "dom_mod", 1.0)),
            sie_drive=sie_gate,
            use_time_dynamics=bool(getattr(nx, "use_time_dynamics", True)),
        )
    except Exception:
        pass

    # 6) metrics (scan-based, parity-preserving)
    try:
        m = compute_metrics(nx.connectome)
    except Exception:
        m = {}

    # Attach structural homeostasis and TD diagnostics
    try:
        m["homeostasis_pruned"] = int(getattr(nx.connectome, "_last_pruned_count", 0))
        m["homeostasis_bridged"] = int(getattr(nx.connectome, "_last_bridged_count", 0))
        m["active_edges"] = int(E)
        m["td_signal"] = float(td_signal)
        m["novelty_idf_scale"] = float(idf_scale)
        if firing_var is not None:
            m["firing_var"] = float(firing_var)
    except Exception:
        pass

    # Attach traversal findings
    try:
        findings = getattr(nx.connectome, "findings", None)
        if findings:
            m.update(findings)
    except Exception:
        pass

    # Expose sie_gate
    try:
        m["sie_gate"] = float(sie_gate)
    except Exception:
        pass

    # Update VT entropy history for next tick's TD proxy
    try:
        nx._prev_vt_entropy = getattr(nx, "_last_vt_entropy", None)
        nx._last_vt_entropy = float(m.get("vt_entropy", 0.0))
    except Exception:
        pass

    return m, drive


__all__ = ["compute_step_and_metrics"]]]></content>
    </file>
    <file>
      <path>runtime/telemetry.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Runtime telemetry packaging seam (Phase B).

Goals:
- Provide small, behavior-preserving builders for 'why' and 'status' payloads.
- Keep core numeric only; this module formats dicts but performs no logging or IO.
- Mirror existing Nexus packaging exactly to ensure byte-for-byte parity.

Policy:
- May import typing and stdlib only.
- No imports from io.* emitters; no file or JSON writes here.
"""

from typing import Any, Dict, Iterable, Set, Callable, Optional, Tuple, List
import os
import time

# --- Maps/frame quantization helpers (stdlib-only; no io.* imports) ---

import sys as _sys
import struct as _struct
from typing import Tuple as _Tuple


def _quantize_frame_v2_u8(header: Dict[str, Any], payload: bytes) -> _Tuple[Dict[str, Any], bytes]:
    """
    Convert a Float32 LE planar payload (heat|exc|inh) into u8 (frame.v2) using
    per-channel max from header["stats"]. No global scans; stats are computed
    upstream from bounded reducer working sets.

    Contract in:
      - header: dict with fields {"n", "channels", "dtype":"f32", "endianness":"LE", "stats":{ch:{max,...}}}
      - payload: bytes with 3*N float32 little-endian values back-to-back

    Contract out:
      - q_header: copy of header with:
          dtype="u8", ver="v2", quant="u8", endianness="LE" (kept for uniformity),
          scales: {ch: 255/max_ch if max_ch>0 else 0.0}
      - q_payload: bytes with 3*N uint8 values back-to-back
    """
    try:
        n = int(header.get("n", 0))
    except Exception:
        n = 0
    if n <= 0:
        return dict(header or {}), b""

    # Expect 3 channels in planar layout
    try:
        channels = list(header.get("channels", ["heat", "exc", "inh"]))
    except Exception:
        channels = ["heat", "exc", "inh"]
    if len(channels) != 3:
        # Fallback: assume 3 planar blocks regardless of names
        channels = ["heat", "exc", "inh"]

    expected_len = 3 * n * 4  # 3 blocks, float32
    if not isinstance(payload, (bytes, bytearray, memoryview)) or len(payload) < expected_len:
        # Malformed payload; return as-is
        return dict(header or {}), bytes(payload or b"")

    # Per-channel max from header (bounded working-set stats upstream)
    def _ch_max(name: str) -> float:
        try:
            return float(((header.get("stats") or {}).get(name) or {}).get("max", 0.0))
        except Exception:
            return 0.0

    max_heat = _ch_max("heat")
    max_exc = _ch_max("exc")
    max_inh = _ch_max("inh")

    s_heat = (255.0 / max_heat) if max_heat > 0.0 else 0.0
    s_exc = (255.0 / max_exc) if max_exc > 0.0 else 0.0
    s_inh = (255.0 / max_inh) if max_inh > 0.0 else 0.0

    mv = memoryview(payload)
    o0 = 0
    o1 = n * 4
    o2 = 2 * n * 4

    def _to_u8_block_le_f32(block: memoryview, count: int, scale: float) -> bytes:
        """
        Interpret block as little-endian float32 values and quantize to uint8 with clamping.
        Uses struct.iter_unpack to avoid NumPy dependency and respect endianness explicitly.
        """
        if scale <= 0.0 or count <= 0:
            return b"\x00" * max(0, count)
        # Fast path: if host is little-endian and struct supports buffer protocol efficiently
        it = _struct.iter_unpack("<f", block.tobytes())
        out = bytearray(count)
        i = 0
        for (v,) in it:
            if v <= 0.0:
                q = 0
            else:
                qf = v * scale
                if qf >= 255.0:
                    q = 255
                else:
                    # round-half-away-from-zero via +0.5 for positives
                    q = int(qf + 0.5)
            out[i] = q
            i += 1
            if i >= count:
                break
        # In case iter_unpack yielded fewer than count (should not happen), right-pad zeros
        if i < count:
            out.extend(b"\x00" * (count - i))
        return bytes(out)

    q_heat = _to_u8_block_le_f32(mv[o0:o1], n, s_heat)
    q_exc = _to_u8_block_le_f32(mv[o1:o2], n, s_exc)
    q_inh = _to_u8_block_le_f32(mv[o2:o2 + n * 4], n, s_inh)

    q_header = dict(header or {})
    q_header["dtype"] = "u8"
    q_header["ver"] = "v2"
    q_header["quant"] = "u8"
    # Keep endianness for uniformity in client code, though u8 is endianness-agnostic
    q_header["endianness"] = q_header.get("endianness", "LE")
    q_header["scales"] = {"heat": float(s_heat), "exc": float(s_exc), "inh": float(s_inh)}
    # Helpful size hint for clients
    q_header["payload_len"] = 3 * n  # bytes

    return q_header, (q_heat + q_exc + q_inh)

def _add_tiles_meta(header: Dict[str, Any], tile_cfg: str) -> Dict[str, Any]:
    """
    Inject non-invasive tiling metadata into a frame.v2 header without modifying payload bytes.
    This enables clients to interpret planar u8 payloads in tiles for large-N visualization.

    tile_cfg (case-insensitive):
      - "none"|"off"|"false"|"0": no tiles (no-op)
      - "auto": choose a square tile size targeting ~64x64 where possible
      - "<W>x<H>": explicit tile width/height (e.g., "64x64")
      - "<K>": square tile KxK (e.g., "128")

    Header additions:
      header["tiles"] = {
        "size": [tw, th],
        "grid": [gw, gh],       # number of tiles in x (width), y (height) directions
        "order": "row-major",   # tile order
        "layout": "planar",     # channel layout (heat|exc|inh planar blocks)
        "shape": [H, W],        # 2D shape of the frame
        "padded": max(0, H*W - n),
      }
    """
    try:
        cfg = str(tile_cfg or "").strip().lower()
    except Exception:
        cfg = "none"
    if cfg in ("none", "off", "false", "0", ""):
        return header

    try:
        shape = list(header.get("shape", []))
        if not (isinstance(shape, (list, tuple)) and len(shape) == 2):
            # Fallback to square from 'n' if shape missing
            n = int(header.get("n", 0))
            side = int(max(1, int((n or 1) ** 0.5)))
            H = side
            W = side
        else:
            H = int(shape[0])
            W = int(shape[1])
    except Exception:
        n = int(header.get("n", 0))
        side = int(max(1, int((n or 1) ** 0.5)))
        H = side
        W = side

    def _parse_tile(cfg_str: str) -> tuple[int, int]:
        # explicit WxH
        if "x" in cfg_str:
            parts = cfg_str.lower().split("x")
            try:
                tw = int(parts[0].strip())
                th = int(parts[1].strip())
                return max(1, tw), max(1, th)
            except Exception:
                pass
        # single integer
        try:
            k = int(cfg_str)
            return max(1, k), max(1, k)
        except Exception:
            pass
        # auto default
        # Aim for ~64x64 tiles, but constrain by frame dims
        tw = min(W, 64 if W >= 64 else max(8, W))
        th = min(H, 64 if H >= 64 else max(8, H))
        return max(1, tw), max(1, th)

    tw, th = (0, 0)
    if cfg == "auto":
        tw, th = _parse_tile(cfg)
    else:
        tw, th = _parse_tile(cfg)

    # Clamp to frame dimensions
    tw = max(1, min(tw, W))
    th = max(1, min(th, H))

    # Compute grid (tiles across width, height)
    def _ceil_div(a: int, b: int) -> int:
        return (a + b - 1) // b

    gw = _ceil_div(W, tw)
    gh = _ceil_div(H, th)

    n = int(header.get("n", 0))
    padded = max(0, (H * W) - n)

    out = dict(header or {})
    out["tiles"] = {
        "size": [int(tw), int(th)],
        "grid": [int(gw), int(gh)],
        "order": "row-major",
        "layout": "planar",
        "shape": [int(H), int(W)],
        "padded": int(padded),
    }
    # Ensure ver/dtype/quant are consistent for frame.v2 u8
    out["ver"] = out.get("ver", "v2")
    out["dtype"] = out.get("dtype", "u8")
    out["quant"] = out.get("quant", "u8")
    return out

def macro_why_base(nx: Any, metrics: Dict[str, Any], step: int) -> Dict[str, Any]:
    """
    Build the base 'why' dict used for macro emissions (before any caller-specific fields).
    Mirrors the inline block in Nexus: uses current metrics with explicit numeric casts.

    Caller may extend with additional telemetry fields, e.g. novelty_idf, composer_idf_k.
    """
    m = metrics or {}
    try:
        phase = int(getattr(nx, "_phase", {}).get("phase", 0))
    except Exception:
        try:
            phase = int(m.get("phase", 0))
        except Exception:
            phase = 0

    return {
        "t": int(step),
        "phase": phase,
        "b1_z": float(m.get("b1_z", 0.0)),
        "cohesion_components": int(m.get("cohesion_components", 0)),
        "vt_coverage": float(m.get("vt_coverage", 0.0)),
        "vt_entropy": float(m.get("vt_entropy", 0.0)),
        "connectome_entropy": float(m.get("connectome_entropy", 0.0)),
        "sie_valence_01": float(m.get("sie_valence_01", 0.0)),
        "sie_v2_valence_01": float(m.get("sie_v2_valence_01", m.get("sie_valence_01", 0.0))),
    }


def status_payload(nx: Any, metrics: Dict[str, Any], step: int) -> Dict[str, Any]:
    """
    Build the open UTD status payload.
    Mirrors the inline block from Nexus with identical keys and casts.
    """
    m = metrics or {}
    try:
        phase = int(m.get("phase", int(getattr(nx, "_phase", {}).get("phase", 0))))
    except Exception:
        phase = 0

    return {
        "type": "status",
        "t": int(step),
        "neurons": int(getattr(nx, "N", 0)),
        "phase": phase,
        "cohesion_components": int(m.get("cohesion_components", 0)),
        "vt_coverage": float(m.get("vt_coverage", 0.0)),
        "vt_entropy": float(m.get("vt_entropy", 0.0)),
        "connectome_entropy": float(m.get("connectome_entropy", 0.0)),
        "active_edges": int(m.get("active_edges", 0)),
        "homeostasis_pruned": int(m.get("homeostasis_pruned", 0)),
        "homeostasis_bridged": int(m.get("homeostasis_bridged", 0)),
        "b1_z": float(m.get("b1_z", 0.0)),
        "adc_territories": int(m.get("adc_territories", 0)),
        "adc_boundaries": int(m.get("adc_boundaries", 0)),
        "sie_total_reward": float(m.get("sie_total_reward", 0.0)),
        "sie_valence_01": float(m.get("sie_valence_01", 0.0)),
        "sie_v2_reward_mean": float(m.get("sie_v2_reward_mean", 0.0)),
        "sie_v2_valence_01": float(m.get("sie_v2_valence_01", 0.0)),
        "ute_in_count": int(m.get("ute_in_count", 0)),
        "ute_text_count": int(m.get("ute_text_count", 0)),
    }


# --- Tick Telemetry Fold (bus drain + ADC + optional event-driven metrics + B1) ---

class _DynObs:
    """
    Minimal observation object for publishing runtime 'delta' events to the bus
    without importing core.announce.Observation. Adapter reads via getattr().
    """
    __slots__ = ("tick", "kind", "nodes", "meta")

    def __init__(self, tick: int, kind: str, nodes: Optional[Iterable[int]] = None, meta: Optional[Dict[str, Any]] = None) -> None:
        self.tick = int(tick)
        self.kind = str(kind)
        self.nodes = list(nodes or [])
        self.meta = dict(meta or {})

class _MapsObs:
    """
    Lightweight maps/frame observation for UI consumption.

    Contract:
      - kind: 'maps_frame'
      - header: dict with fields {topic, tick, n, shape, channels, dtype, endianness, stats}
      - payload: bytes containing Float32Array blocks back-to-back (LE): heat[n] | exc[n] | inh[n]
    """
    __slots__ = ("tick", "kind", "header", "payload")

    def __init__(self, tick: int, header: Dict[str, Any], payload: bytes) -> None:
        self.tick = int(tick)
        self.kind = "maps_frame"
        self.header = dict(header or {})
        self.payload = payload


def tick_fold(
    nx: Any,
    metrics: Dict[str, Any],
    drive: Dict[str, Any],
    td_signal: float,
    step: int,
    tick_rev_map: Optional[Dict[int, Any]] = None,
    *,
    obs_to_events: Optional[Callable[[Iterable[Any]], Iterable[Any]]] = None,
    adc_event: Optional[Callable[[Dict[str, Any], int], Any]] = None,
    apply_b1: Optional[Callable[[Any, Dict[str, Any], int], Dict[str, Any]]] = None,
) -> Tuple[Dict[str, Any], Set[Any]]:
    """
    Behavior-preserving fold of per-tick runtime telemetry:
      - Optionally publish a 'delta' event (feature-flagged) to the announce bus
      - Drain bus and derive void-topic symbols using tick_rev_map
      - Update ADC from drained observations; merge adc metrics
      - Optionally fold event-driven metrics (feature-flagged)
      - Update complexity proxy and apply B1 detector (via callback)
    Returns (metrics, void_topic_symbols)
    """
    m = metrics if isinstance(metrics, dict) else {}
    void_topic_symbols: Set[Any] = set()

    # 1) Optional delta event publish (telemetry-only; no dynamics change)
    try:
        if getattr(nx, "_evt_metrics", None) is not None:
            comps = {}
            try:
                comps = dict(drive.get("components", {}) or {})
            except Exception:
                comps = {}
            meta = {
                "b1": 0.0,  # cycle_hit provides primary b1 contributions; keep delta neutral
                "nov": float(comps.get("nov", 0.0)) if isinstance(comps, dict) else 0.0,
                "hab": float(comps.get("hab", 0.0)) if isinstance(comps, dict) else 0.0,
                "td": float(td_signal),
                "hsi": float(comps.get("hsi", 0.0)) if isinstance(comps, dict) else 0.0,
            }
            try:
                bus = getattr(nx, "bus", None)
                if bus is not None:
                    # Publish neutral 'delta' for b1/why folding
                    bus.publish(_DynObs(tick=int(step), kind="delta", nodes=[], meta=meta))
                    # Optionally synthesize bounded ΔW events to drive Exc/Inh maps without scans
                    try:
                        synth_flag = str(os.getenv("SYNTH_DELTA_W", "0")).strip().lower() in ("1", "true", "yes", "on", "y")
                    except Exception:
                        synth_flag = False
                    if synth_flag:
                        # Select a tiny working set of nodes from this tick's symbol→index map (bounded fan-out)
                        try:
                            if isinstance(tick_rev_map, dict):
                                node_keys = list(tick_rev_map.keys())
                            else:
                                node_keys = []
                        except Exception:
                            node_keys = []
                        # Keep at most 16 nodes; prefer stable order
                        try:
                            nodes_sel = [int(i) for i in sorted(node_keys)[:16]]
                        except Exception:
                            nodes_sel = []
                        # Map TD sign to ΔW direction; clip magnitude to avoid runaway (void-faithful bounded emit)
                        try:
                            tdv = float(td_signal)
                        except Exception:
                            tdv = 0.0
                        sign = 1.0 if tdv >= 0.0 else -1.0
                        mag = min(0.05, abs(tdv))  # 0 ≤ |dw| ≤ 0.05
                        dw_val = float(sign * mag)
                        if nodes_sel:
                            bus.publish(_DynObs(tick=int(step), kind="delta_w", nodes=nodes_sel, meta={"dw": dw_val}))
            except Exception:
                pass
    except Exception:
        pass

    # 2) Drain bus, derive topic symbols, update ADC, and fold event-driven metrics
    try:
        bus = getattr(nx, "bus", None)
        if bus is not None:
            obs_batch = bus.drain(max_items=int(getattr(nx, "bus_drain", 2048)))
            if obs_batch:
                # Expose drained observations for CoreEngine folding without re-drain
                try:
                    setattr(nx, "_last_obs_batch", obs_batch)
                except Exception:
                    pass
                # Map observed node indices back to symbols seen this tick
                try:
                    if isinstance(tick_rev_map, dict):
                        for obs in obs_batch:
                            try:
                                nodes = getattr(obs, "nodes", None)
                                if nodes:
                                    for idx in nodes:
                                        sym = tick_rev_map.get(int(idx))
                                        if sym is not None:
                                            void_topic_symbols.add(sym)
                            except Exception:
                                continue
                except Exception:
                    pass

                # Update ADC after extracting topic so we don't interfere with its logic
                try:
                    adc = getattr(nx, "adc", None)
                    if adc is not None:
                        adc.update_from(obs_batch)
                        adc_metrics = adc.get_metrics()
                    else:
                        adc_metrics = {}
                except Exception:
                    adc_metrics = {}
                # Expose ADC metrics for CoreEngine folding (no IO; runtime-local state only)
                try:
                    setattr(nx, "_last_adc_metrics", adc_metrics)
                except Exception:
                    pass

                # Optionally fold event-driven metrics telemetry
                try:
                    evtm = getattr(nx, "_evt_metrics", None)
                    if getattr(nx, "_engine", None) is None and evtm is not None:
                        if obs_to_events is not None:
                            try:
                                for _ev in obs_to_events(obs_batch) or []:
                                    try:
                                        evtm.update(_ev)
                                    except Exception:
                                        pass
                            except Exception:
                                pass
                        if adc_event is not None:
                            try:
                                evtm.update(adc_event(adc_metrics, t=int(step)))
                            except Exception:
                                pass
                        try:
                            evsnap = evtm.snapshot()
                            if isinstance(evsnap, dict):
                                # Do not override legacy scan-based metrics; prefix event-driven keys.
                                for _k, _v in evsnap.items():
                                    try:
                                        # Preserve existing B1 detector outputs from apply_b1
                                        if str(_k).startswith("b1_") and _k in m:
                                            continue
                                        m[f"evt_{_k}"] = _v
                                    except Exception:
                                        continue
                        except Exception:
                            pass
                except Exception:
                    pass

                # Fold ADC metrics and complexity proxy
                try:
                    if isinstance(adc_metrics, dict):
                        m.update(adc_metrics)
                        m["complexity_cycles"] = float(m.get("complexity_cycles", 0.0)) + float(adc_metrics.get("adc_cycle_hits", 0.0))
                except Exception:
                    pass
    except Exception:
        pass

    # 2.9) Publish maps/frame (header+binary) if prepared by CoreEngine
    try:
        mf = getattr(nx, "_maps_frame_ready", None)
        if mf is not None and isinstance(mf, tuple) and len(mf) == 2:
            header, payload = mf

            # Ensure header has topic and tick without scanning arrays client-side
            try:
                if isinstance(header, dict):
                    if "topic" not in header:
                        header = dict(header)
                        header["topic"] = "maps/frame"
                    header["tick"] = int(step)
                else:
                    header = {"topic": "maps/frame", "tick": int(step)}
            except Exception:
                header = {"topic": "maps/frame", "tick": int(step)}

            # 2.9.a) Publish to bus for in-process consumers (unchanged)
            try:
                bus = getattr(nx, "bus", None)
            except Exception:
                bus = None
            if bus is not None:
                try:
                    bus.publish(_MapsObs(tick=int(step), header=header, payload=payload))
                except Exception:
                    pass

            # 2.9.b) Optional ring write with u8 quantization (frame.v2) and FPS limiter
            try:
                # FPS limiter (default: 10)
                try:
                    maps_fps = float(os.getenv("MAPS_FPS", "10"))
                except Exception:
                    maps_fps = 10.0
                mode = str(os.getenv("MAPS_MODE", "frame_v2_u8")).strip().lower()
                now_ts = time.time()
                last_ts = float(getattr(nx, "_maps_last_emit_ts", 0.0))
                # FPS semantics:
                #   maps_fps < 0  -> always allow (tests/benchmarks "no limiter")
                #   maps_fps == 0 -> disable emission
                #   maps_fps > 0  -> limit to that FPS
                if maps_fps < 0:
                    allow_emit = True
                else:
                    allow_emit = (maps_fps > 0) and ((now_ts - last_ts) >= (1.0 / max(0.001, maps_fps)))
                if mode in ("off", "none", "0", "false"):
                    allow_emit = False

                if allow_emit:
                    tile_cfg = str(os.getenv("MAPS_TILE", "none")).strip().lower()

                    # Lazy-init ring if absent
                    ring = getattr(nx, "_maps_ring", None)
                    if ring is None:
                        try:
                            from fum_rt.io.visualization.maps_ring import MapsRing  # local import to avoid module-policy drift
                            cap = int(os.getenv("MAPS_RING", "3"))
                            nx._maps_ring = MapsRing(capacity=max(1, cap))
                            ring = nx._maps_ring
                        except Exception:
                            ring = None

                    if ring is not None:
                        # Only full-frame v2 for now; tiles reserved for very large N (stub)
                        if mode in ("frame_v2", "frame_v2_u8", "v2", "u8"):
                            # Quantize to u8 using per-channel max from header['stats']
                            q_header, q_payload = _quantize_frame_v2_u8(header, payload)
                            # Optional tile metadata (payload remains planar u8; clients may tile client-side)
                            try:
                                if tile_cfg not in ("none", "off", "false", "0", ""):
                                    q_header = _add_tiles_meta(q_header, tile_cfg)
                            except Exception:
                                pass
                            try:
                                ring.push(int(step), q_header, q_payload)
                                setattr(nx, "_maps_last_emit_ts", now_ts)
                            except Exception:
                                pass
                        elif mode in ("off", "none"):
                            # Skip ring write
                            pass
                        else:
                            # Unknown mode: default to frame_v2_u8
                            q_header, q_payload = _quantize_frame_v2_u8(header, payload)
                            # Apply tile metadata if requested
                            try:
                                if tile_cfg not in ("none", "off", "false", "0", ""):
                                    q_header = _add_tiles_meta(q_header, tile_cfg)
                            except Exception:
                                pass
                            try:
                                ring.push(int(step), q_header, q_payload)
                                setattr(nx, "_maps_last_emit_ts", now_ts)
                            except Exception:
                                pass
            except Exception:
                pass

            # Clear pointer to avoid re-publishing stale frames
            try:
                delattr(nx, "_maps_frame_ready")
            except Exception:
                pass
    except Exception:
        pass

    # 2.10) Expose dimensionless memory steering groups (telemetry-only; void-faithful)
    try:
        mf = getattr(nx, "_memory_field", None)
        if mf is not None:
            try:
                theta = float(getattr(mf, "Theta", 0.0))
            except Exception:
                theta = 0.0
            try:
                da = float(getattr(mf, "D_a", getattr(mf, "Da", 0.0)))
            except Exception:
                da = 0.0
            try:
                lam = float(getattr(mf, "Lambda", 0.0))
            except Exception:
                lam = 0.0
            try:
                gam = float(getattr(mf, "Gamma", 0.0))
            except Exception:
                gam = 0.0
            # Do not overwrite if caller already provided these
            if "mem_Theta" not in m:
                m["mem_Theta"] = theta
            if "mem_Da" not in m:
                m["mem_Da"] = da
            if "mem_Lambda" not in m:
                m["mem_Lambda"] = lam
            if "mem_Gamma" not in m:
                m["mem_Gamma"] = gam
    except Exception:
        pass

    # 3) Apply B1 detector via provided seam (preserves detector parameters and gating)
    try:
        if apply_b1 is not None:
            m = apply_b1(nx, m, int(step))
    except Exception:
        pass

    return m, void_topic_symbols

__all__ = ["macro_why_base", "status_payload", "tick_fold"]]]></content>
    </file>
    <file>
      <path>substrate/README.md</path>
      <content/>
    </file>
    <file>
      <path>substrate/fum_growth_arbiter.py</path>
      <content><![CDATA[# fum_growth_arbiter.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module contains the GrowthArbiter, a class responsible for deciding
when and *how much* to grow the network. It implements the "super saturation" 
and "void debt" principles, where growth is triggered by stability and the 
amount of growth is determined by accumulated system pressure.
"""
import numpy as np
from collections import deque

class GrowthArbiter:
    """
    Monitors network metrics to decide when and how much to grow.
    """
    def __init__(self, stability_window=10, trend_threshold=0.001, debt_growth_factor=0.1):
        """
        Initializes the GrowthArbiter.

        Args:
            stability_window (int): Number of recent steps to check for stability.
            trend_threshold (float): Max change for a metric to be "flat."
            debt_growth_factor(float): Scales accumulated debt to number of neurons.
        """
        self.stability_window = stability_window
        self.trend_threshold = trend_threshold
        self.debt_growth_factor = debt_growth_factor

        self.weight_history = deque(maxlen=stability_window)
        self.synapse_history = deque(maxlen=stability_window)
        self.complexity_history = deque(maxlen=stability_window)
        self.cohesion_history = deque(maxlen=stability_window)
        
        self.is_stable = False
        self.void_debt_accumulator = 0.0

    def update_metrics(self, metrics):
        """
        Updates the historical metrics and checks for system stability.
        
        Args:
            metrics (dict): A dictionary containing the latest network metrics.
        """
        self.weight_history.append(metrics.get('avg_weight', 0))
        self.synapse_history.append(metrics.get('active_synapses', 0))
        self.complexity_history.append(metrics.get('total_b1_persistence', 0))
        self.cohesion_history.append(metrics.get('cluster_count', -1))

        if len(self.weight_history) < self.stability_window:
            self.is_stable = False
            return

        is_cohesive = all(count == 1 for count in self.cohesion_history)
        is_weight_flat = abs(self.weight_history[0] - self.weight_history[-1]) < self.trend_threshold
        is_synapse_flat = abs(self.synapse_history[0] - self.synapse_history[-1]) < 3
        is_complexity_flat = abs(self.complexity_history[0] - self.complexity_history[-1]) < self.trend_threshold

        if is_cohesive and is_weight_flat and is_synapse_flat and is_complexity_flat:
            if not self.is_stable:
                print("\n--- GROWTH ARBITER: System has achieved STABILITY ---")
                print("--- Now accumulating 'void debt' from residual valence. ---")
            self.is_stable = True
        else:
            if self.is_stable:
                print("\n--- GROWTH ARBITER: System has left stable state. Resetting debt.---")
                self.void_debt_accumulator = 0.0 # Reset debt if stability is lost
            self.is_stable = False

    def accumulate_and_check_growth(self, valence_signal):
        """
        If the system is stable, accumulates void debt. If the debt crosses
        a threshold, returns the number of neurons to grow.

        Args:
            valence_signal (float): The residual system pressure signal.

        Returns:
            int: The number of new neurons to add, or 0.
        """
        if not self.is_stable:
            return 0

        self.void_debt_accumulator += abs(valence_signal) # Accumulate pressure

        # Check if the accumulated debt triggers growth
        # We'll use a simple linear threshold for now.
        if self.void_debt_accumulator > 1.0: 
            num_new_neurons = int(np.ceil(self.void_debt_accumulator * self.debt_growth_factor))
            
            print("\n--- GROWTH ARBITER: VOID DEBT THRESHOLD REACHED ---")
            print(f"Accumulated Debt: {self.void_debt_accumulator:.3f}")
            print(f"Triggering organic growth of {num_new_neurons} new neuron(s).")
            print("---------------------------------------------------\n")

            self.void_debt_accumulator = 0.0 # Reset debt after triggering
            self.is_stable = False # System will become unstable after growth, reset
            self.clear_history() # Clear history to re-evaluate stability
            return num_new_neurons

        return 0

    def clear_history(self):
        """Resets all metric histories."""
        self.weight_history.clear()
        self.synapse_history.clear()
        self.complexity_history.clear()
        self.cohesion_history.clear()]]></content>
    </file>
    <file>
      <path>substrate/fum_hypertrophy.py</path>
      <content><![CDATA[# fum_hypertrophy.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module handles the growth of the FUM substrate, including the addition
of new computational units (neurons) and the formation of their initial
connections based on void dynamics.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix

from Void_Equations import universal_void_dynamics

class Hypertrophy:
    """
    Manages the growth of the Substrate's connectome.
    """
    def __init__(self, seed=42):
        """
        Initializes the growth manager.
        """
        self.rng = np.random.default_rng(seed=seed)

    def grow(self, substrate, num_new_neurons):
        """
        Grows the substrate by expanding the connectome and all associated state arrays.
        This method is designed to be called on a Substrate instance.

        Args:
            substrate: The Substrate instance to modify.
            num_new_neurons (int): The number of new neurons to add.
        """
        if num_new_neurons <= 0:
            return

        old_n = substrate.num_neurons
        new_n = old_n + num_new_neurons
        
        print(f"\n--- HYPERTROPHY: SUBSTRATE GROWTH ---")
        print(f"Adding {num_new_neurons} new neurons to the existing {old_n}.")

        # --- Create new neuron properties on CPU first ---
        new_is_excitatory = self.rng.choice([True, False], num_new_neurons, p=[0.8, 0.2])
        new_tau_m = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_thresh = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_rest = np.full(num_new_neurons, -65.0)
        new_refractory_period = np.full(num_new_neurons, 5.0)
        new_r_mem = np.full(num_new_neurons, 10.0)

        # --- Expand the connectome based on the backend ---
        if substrate.device_type == 'gpu':
            W_cpu = substrate.W.cpu().numpy()
        else:
            W_cpu = substrate.W.toarray() if isinstance(substrate.W, csc_matrix) else substrate.W

        new_W = np.zeros((new_n, new_n))
        new_W[:old_n, :old_n] = W_cpu

        # --- Connect new neurons using Void Dynamics ---
        # 1. Create a potential connection matrix for new neurons (outgoing)
        potential_connections_out = self.rng.random((num_new_neurons, old_n)) * 0.05 
        # 2. Evolve it with void dynamics
        delta_out = universal_void_dynamics(potential_connections_out, substrate.time_step)
        evolved_connections_out = potential_connections_out + delta_out
        # 3. Threshold to form actual connections
        new_connections_out = np.where(evolved_connections_out > 0.01, evolved_connections_out, 0)
        
        # 1. Create a potential connection matrix for new neurons (incoming)
        potential_connections_in = self.rng.random((old_n, num_new_neurons)) * 0.05
        # 2. Evolve it
        delta_in = universal_void_dynamics(potential_connections_in, substrate.time_step)
        evolved_connections_in = potential_connections_in + delta_in
        # 3. Threshold
        new_connections_in = np.where(evolved_connections_in > 0.01, evolved_connections_in, 0)

        # Add new connections to the main matrix
        new_W[old_n:new_n, :old_n] = new_connections_out
        new_W[:old_n, old_n:new_n] = new_connections_in
        
        # --- Handle backend-specific state expansions ---
        if substrate.device_type == 'gpu':
            substrate.W = torch.from_numpy(new_W).float().to(substrate.device)
            substrate.is_excitatory = torch.cat([substrate.is_excitatory, torch.from_numpy(new_is_excitatory).to(substrate.device)])
            substrate.tau_m = torch.cat([substrate.tau_m, torch.from_numpy(new_tau_m).float().to(substrate.device)])
            substrate.v_thresh = torch.cat([substrate.v_thresh, torch.from_numpy(new_v_thresh).float().to(substrate.device)])
            substrate.v_m = torch.cat([substrate.v_m, torch.from_numpy(new_v_rest).float().to(substrate.device)])
            substrate.refractory_time = torch.cat([substrate.refractory_time, torch.zeros(num_new_neurons, device=substrate.device)])
            substrate.refractory_period = torch.cat([substrate.refractory_period, torch.from_numpy(new_refractory_period).float().to(substrate.device)])
            substrate.r_mem = torch.cat([substrate.r_mem, torch.from_numpy(new_r_mem).float().to(substrate.device)])
            substrate.v_reset_tensor = torch.cat([substrate.v_reset_tensor, torch.from_numpy(np.full(num_new_neurons, -70.0)).float().to(substrate.device)])
            substrate.spikes = torch.cat([substrate.spikes, torch.zeros(num_new_neurons, dtype=torch.bool, device=substrate.device)])
        else: # CPU
            substrate.W = csc_matrix(new_W)
            substrate.is_excitatory = np.concatenate([substrate.is_excitatory, new_is_excitatory])
            substrate.tau_m = np.concatenate([substrate.tau_m, new_tau_m])
            substrate.v_thresh = np.concatenate([substrate.v_thresh, new_v_thresh])
            substrate.v_m = np.concatenate([substrate.v_m, new_v_rest])
            substrate.refractory_time = np.concatenate([substrate.refractory_time, np.zeros(num_new_neurons)])
            substrate.refractory_period = np.concatenate([substrate.refractory_period, new_refractory_period])
            substrate.r_mem = np.concatenate([substrate.r_mem, new_r_mem])
            substrate.v_reset = np.concatenate([substrate.v_reset, np.full(num_new_neurons, -70.0)])
            substrate.spikes = np.concatenate([substrate.spikes, np.zeros(num_new_neurons, dtype=bool)])
            substrate.neuron_polarities = np.concatenate([substrate.neuron_polarities, np.ones(num_new_neurons)])
            substrate.refractory_periods = np.concatenate([substrate.refractory_periods, np.zeros(num_new_neurons)])

        # Universal state expansions
        substrate.spike_times.extend([[] for _ in range(num_new_neurons)])
        substrate.num_neurons = new_n

        print(f"Growth complete. Total neurons: {substrate.num_neurons}")
        print("-------------------------------------\n")]]></content>
    </file>
    <file>
      <path>substrate/fum_structural_homeostasis.py</path>
      <content><![CDATA[# fum_structural_homeostasis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
from scipy.sparse import csc_matrix, lil_matrix

def perform_structural_homeostasis(W: csc_matrix, ccc_metrics: dict) -> csc_matrix:
    """
    Performs Structural Homeostasis on the Emergent Connectome (UKG).
    
    This is a purpose-driven, self-regulating process that uses TDA metrics
    (Cohesion and Complexity) to maintain the network's topological health,
    ensuring the FUM remains in a stable and efficient state.

    Args:
        W (csc_matrix): The current sparse weight matrix representing the UKG.
        ccc_metrics (dict): A dictionary of metrics from the CCC_Module.

    Returns:
        csc_matrix: The modified, healthier weight matrix.
    """
    num_neurons = W.shape[0]
    
    # To avoid performance warnings, all structural modifications (pruning and
    # growth) are performed on a `lil_matrix`, which is efficient for
    # changing sparsity structure.
    W_lil = W.tolil()
    
    # --- 1. Pruning (Complexity Homeostasis) ---
    # The pruning threshold is now adaptive, based on the current mean weight.
    # This prevents the network from getting stuck and allows for dynamic rearrangement.
    # We prune any synapse that is less than 10% of the mean strength.
    if W.nnz > 0:
        mean_weight = np.mean(np.abs(W.data))
        pruning_threshold = 0.1 * mean_weight
    else:
        pruning_threshold = 0.01 # Fallback for empty graph

    rows_cols = W_lil.rows
    data_rows = W_lil.data
    for i in range(num_neurons):
        to_prune_indices = [
            idx for idx, weight in enumerate(data_rows[i])
            if abs(weight) < pruning_threshold
        ]
        for idx in sorted(to_prune_indices, reverse=True):
            del rows_cols[i][idx]
            del data_rows[i][idx]

    # --- 2. Growth (Cohesion Homeostasis) ---
    component_count = ccc_metrics.get('cohesion_cluster_count', 1)
    if isinstance(component_count, np.integer):
        component_count = component_count.item()

    if component_count > 1 and 'cluster_labels' in ccc_metrics:
        # A "pathological" state of low cohesion has been detected. The system
        # implements the documented strategy of "biasing plasticity towards
        # growing connections" to heal the fragmentation.
        labels = ccc_metrics['cluster_labels']
        unique_labels = np.unique(labels)
        
        # To make the healing effective, we create a "bundle" of new connections
        # to ensure the clustering algorithm recognizes the new bridge.
        BUNDLE_SIZE = 3
        
        # We'll build one bridge for each excess cluster to encourage fusion.
        num_bridges_to_build = component_count - 1

        for _ in range(num_bridges_to_build):
            # Choose two different territories to bridge
            cluster_a, cluster_b = np.random.choice(unique_labels, 2, replace=False)
            
            indices_a = np.where(labels == cluster_a)[0]
            indices_b = np.where(labels == cluster_b)[0]
            
            if len(indices_a) > 0 and len(indices_b) > 0:
                # Create a bundle of connections between the two territories
                for _ in range(BUNDLE_SIZE):
                    neuron_u = np.random.choice(indices_a)
                    neuron_v = np.random.choice(indices_b)
                    if neuron_u != neuron_v and W_lil[neuron_u, neuron_v] == 0:
                        W_lil[neuron_u, neuron_v] = np.random.uniform(0.05, 0.1)
    
    # Convert back to csc_matrix once at the end for efficient calculations
    W_csc = W_lil.tocsc()
    W_csc.prune()
    return W_csc]]></content>
    </file>
    <file>
      <path>substrate/fum_substrate.py</path>
      <content><![CDATA[# fum_substrate.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix, find

# FUM Modules
from fum_initialization import create_knn_graph

class Substrate:
    """
    Represents the FUM's computational medium or "Substrate".
    
    VERSION 4 (MERGED): This version combines the stable V3 architecture with
    the GPU acceleration and Growth features from the V9 refactor.
    """
    def __init__(self, num_neurons: int, k: int, device: str = 'auto'):
        """
        Initializes the Substrate.

        Args:
            num_neurons (int): The number of Computational Units (CUs).
            k (int): The number of nearest neighbors for the initial k-NN graph.
            device (str): 'auto', 'gpu', or 'cpu'.
        """
        self.num_neurons = num_neurons
        self._setup_device(device)
        self.backend = np if self.device_type == 'cpu' else torch
        
        # --- Neuron Types (80% Excitatory, 20% Inhibitory) ---
        self.rng = np.random.default_rng(seed=42)
        is_excitatory_np = self.rng.choice([True, False], num_neurons, p=[0.8, 0.2])

        # --- CU parameters (vectorized, created on CPU first) ---
        tau_m_np = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_neurons)
        self.v_rest = np.full(num_neurons, -65.0)
        v_reset_np = np.full(num_neurons, -70.0)
        v_thresh_np = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_neurons)
        refractory_period_np = np.full(num_neurons, 5.0)
        r_mem_np = np.full(num_neurons, 10.0)

        # --- Parameters for Intrinsic Plasticity (A.6) ---
        self.ip_target_rate_min = 0.1 # Hz
        self.ip_target_rate_max = 0.5 # Hz
        self.ip_v_thresh_adjustment = 0.1 # mV
        self.ip_tau_m_adjustment = 0.1 # ms
        self.ip_v_thresh_bounds = (-60.0, -50.0)
        self.ip_tau_m_bounds = (15.0, 25.0)
        
        # --- Synaptic Pathways: k-NN Initialization (on CPU first) ---
        W_np = create_knn_graph(num_neurons, k, is_excitatory_np).toarray()
        
        # --- Create state vars and move to GPU if requested ---
        if self.device_type == 'gpu':
            self.is_excitatory = torch.from_numpy(is_excitatory_np).to(self.device)
            self.tau_m = torch.from_numpy(tau_m_np).float().to(self.device)
            self.v_thresh = torch.from_numpy(v_thresh_np).float().to(self.device)
            self.v_m = torch.from_numpy(self.v_rest).float().to(self.device)
            self.refractory_time = torch.zeros(num_neurons, device=self.device)
            self.refractory_period = torch.from_numpy(refractory_period_np).float().to(self.device)
            self.r_mem = torch.from_numpy(r_mem_np).float().to(self.device)
            self.v_reset_tensor = torch.from_numpy(v_reset_np).float().to(self.device)
            self.spikes = torch.zeros(num_neurons, dtype=torch.bool, device=self.device)
            self.W = torch.from_numpy(W_np).float().to(self.device)
        else: # cpu
            self.is_excitatory = is_excitatory_np
            self.tau_m = tau_m_np
            self.v_reset = v_reset_np
            self.v_thresh = v_thresh_np
            self.refractory_period = refractory_period_np
            self.r_mem = r_mem_np
            self.v_m = np.full(num_neurons, self.v_rest)
            self.refractory_time = np.zeros(num_neurons)
            self.neuron_polarities = np.ones(num_neurons)
            self.refractory_periods = np.zeros(num_neurons)
            self.W = csc_matrix(W_np)
            self.spikes = np.zeros(num_neurons, dtype=bool)
        self.spike_times = [[] for _ in range(num_neurons)]
        self.time_step = 0

    def run_step(self, external_currents, dt=1.0):
        """
        Runs one full step of the Substrate's dynamics, dispatching to the correct backend.
        """
        if self.device_type == 'gpu':
            self._run_step_gpu(external_currents, dt)
        else:
            self._run_step_cpu(external_currents, dt)
        
        self.time_step += 1

    def _run_step_cpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the CPU.
        """
        # Correctly apply membrane resistance only to synaptic currents inside the dv calculation
        synaptic_currents = self.W.dot(self.spikes.astype(np.float32))
        
        not_in_refractory = self.refractory_time <= 0
        
        # The full, correct ELIF update equation from the documentation
        dv = (
            -(self.v_m[not_in_refractory] - self.v_rest[not_in_refractory])
            + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory]
            + external_currents[not_in_refractory]
        ) / self.tau_m[not_in_refractory]
        
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt

        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = np.where(spiking_mask)[0]
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def _run_step_gpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the GPU.
        """
        external_currents_gpu = torch.from_numpy(external_currents).float().to(self.device)
        synaptic_currents = torch.mv(self.W, self.spikes.float())
        
        not_in_refractory = self.refractory_time <= 0
        v_rest_gpu = torch.from_numpy(self.v_rest).float().to(self.device)
        
        dv = (-(self.v_m[not_in_refractory] - v_rest_gpu[not_in_refractory]) + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory] + external_currents_gpu[not_in_refractory]) / self.tau_m[not_in_refractory]
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt
        self.refractory_time.clamp_(min=0)
        
        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset_tensor[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = torch.where(spiking_mask)[0].cpu().numpy()
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def apply_intrinsic_plasticity(self, window_ms=50, dt=1.0):
        """
        Applies intrinsic plasticity to neuron parameters based on their recent
        firing rate, as per documentation section A.6.
        """
        window_steps = int(window_ms / dt)
        analysis_start_time = max(0, (self.time_step - window_steps) * dt)
        window_duration_s = (self.time_step * dt - analysis_start_time) / 1000.0

        if window_duration_s == 0:
            return

        for i in range(self.num_neurons):
            spikes_in_window = [t for t in self.spike_times[i] if t >= analysis_start_time]
            rate_hz = len(spikes_in_window) / window_duration_s
            
            # Adjust v_thresh
            if rate_hz > self.ip_target_rate_max:
                self.v_thresh[i] += self.ip_v_thresh_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.v_thresh[i] -= self.ip_v_thresh_adjustment
                
            # Adjust tau_m
            if rate_hz > self.ip_target_rate_max:
                self.tau_m[i] -= self.ip_tau_m_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.tau_m[i] += self.ip_tau_m_adjustment

        # Clamp parameters to their bounds
        np.clip(self.v_thresh, self.ip_v_thresh_bounds[0], self.ip_v_thresh_bounds[1], out=self.v_thresh)
        np.clip(self.tau_m, self.ip_tau_m_bounds[0], self.ip_tau_m_bounds[1], out=self.tau_m)

    def apply_synaptic_scaling(self, target_sum=1.0):
        """
        Applies simple multiplicative scaling to incoming excitatory weights to
        keep the total input around a target value. Based on the reference
        validation script and documentation B.7.ii.
        """
        W_dense = self.W.toarray()
        
        # Calculate sum of incoming positive (excitatory) weights for each neuron
        incoming_exc_sums = np.sum(np.maximum(W_dense, 0), axis=0)
        
        # Avoid division by zero
        incoming_exc_sums[incoming_exc_sums < 1e-6] = 1.0
        
        # Calculate scaling factors needed to bring sum to target
        scale_factors = target_sum / incoming_exc_sums
        
        # Get a dense matrix of the excitatory weights only
        exc_W_dense = W_dense.copy()
        exc_W_dense[W_dense < 0] = 0
        
        # Apply scaling multiplicatively to the excitatory weights
        scaled_exc_W = exc_W_dense * scale_factors[np.newaxis, :]
        
        # Reconstruct the full weight matrix
        W_dense[W_dense > 0] = scaled_exc_W[W_dense > 0]
        
        self.W = csc_matrix(W_dense)
        self.W.prune()

    def _setup_device(self, device_preference):
        if device_preference == 'gpu':
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                print("Warning: GPU requested but not available. Falling back to CPU.")
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
        elif device_preference == 'cpu':
            self.device = torch.device("cpu")
            self.device_type = 'cpu'
            print("--- Substrate configured to run on CPU. ---")
        else: # auto
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
                print("--- Substrate configured to run on CPU. ---")]]></content>
    </file>
    <file>
      <path>tests/conftest.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Pytest configuration: ensure repo root on sys.path so 'import fum_rt' works when running tests directly.
No heavy imports; minimal stdlib only.
"""

import os as _os
import sys as _sys
from pathlib import Path as _Path

def _project_root(start: _Path | None = None) -> _Path:
    cur = (start or _Path(__file__)).resolve().parent
    for _ in range(10):
        if (cur / "fum_rt").is_dir():
            return cur
        if cur.parent == cur:
            break
        cur = cur.parent
    # Fallback: two parents up from this file
    return (start or _Path(__file__)).resolve().parents[2]

# Put repository root on sys.path if missing
try:
    _root = _project_root()
    _root_str = str(_root)
    if _root_str not in _sys.path:
        _sys.path.insert(0, _root_str)
except Exception:
    pass]]></content>
    </file>
    <file>
      <path>tests/core/test_active_graph_regression.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.tests.core.test_active_graph_regression

Regression tests for active-graph lower-bound component counting and dirty-flag/audit behavior
in SparseConnectome._maybe_audit_frag (void-faithful, streaming over ACTIVE edges only).

Covers:
- components_lb reflects the number of components in the ACTIVE subgraph
- After removing bridges (splitting the graph), components_lb increases
- After adding a bridge across components, components_lb decreases to 1
- Dirty flag persists when audit budget is exhausted; is cleared only when audit completes
"""

import numpy as np

from fum_rt.core.sparse_connectome import SparseConnectome


def _np_adj_from_sets(N: int, groups: list[list[int]]) -> list[np.ndarray]:
    """
    Build adjacency as undirected clique for each group in groups, disjoint between groups.
    Returns list of numpy int32 arrays per node.
    """
    adj_sets = [set() for _ in range(N)]
    for g in groups:
        gset = set(int(x) for x in g)
        for i in gset:
            for j in gset:
                if i != j:
                    adj_sets[i].add(j)
    return [np.fromiter(sorted(s), dtype=np.int32) if s else np.zeros(0, dtype=np.int32) for s in adj_sets]


def test_active_graph_components_audit_transitions() -> None:
    N = 5
    sc = SparseConnectome(N=N, k=0, seed=0, threshold=0.15, lambda_omega=0.1, candidates=1)
    # Uniform W so that all listed edges are ACTIVE (> threshold)
    sc.W = np.ones(N, dtype=np.float32)

    # Stage 1: fully connected graph -> components_lb == 1
    sc.adj = _np_adj_from_sets(N, [list(range(N))])
    sc._maybe_audit_frag(budget_edges=1_000_000)
    assert int(getattr(sc, "_frag_components_lb", N)) == 1

    # Stage 2: split into two components {0,1} and {2,3,4}
    sc.adj = _np_adj_from_sets(N, [[0, 1], [2, 3, 4]])
    sc._maybe_audit_frag(budget_edges=1_000_000)
    assert int(getattr(sc, "_frag_components_lb", 0)) == 2

    # Stage 3: add a single bridge across the components (1-2)
    adj_sets = [set(a.tolist()) for a in sc.adj]
    adj_sets[1].add(2)
    adj_sets[2].add(1)
    sc.adj = [np.fromiter(sorted(s), dtype=np.int32) if s else np.zeros(0, dtype=np.int32) for s in adj_sets]
    sc._maybe_audit_frag(budget_edges=1_000_000)
    assert int(getattr(sc, "_frag_components_lb", N)) == 1


def test_dirty_flag_persists_when_budget_exhausted() -> None:
    """
    When the audit budget is exhausted, _frag_dirty_since should remain non-None.
    """
    N = 8
    sc = SparseConnectome(N=N, k=0, seed=0, threshold=0.15, lambda_omega=0.1, candidates=1)
    sc.W = np.ones(N, dtype=np.float32)

    # Build a graph with enough edges to exceed the small budget
    sc.adj = _np_adj_from_sets(N, [[0, 1, 2, 3], [4, 5, 6, 7]])
    # Mark as dirty beforehand
    sc._frag_dirty_since = 1234
    # Use a very small budget (1 edge), which should not clear the dirty flag
    sc._maybe_audit_frag(budget_edges=1)
    assert getattr(sc, "_frag_dirty_since", None) is not None, "Dirty flag should persist when budget is exhausted"

    # Now audit with a large budget to allow completion; flag should clear
    sc._maybe_audit_frag(budget_edges=1_000_000)
    assert getattr(sc, "_frag_dirty_since", None) is None, "Dirty flag should clear when audit completes within budget"]]></content>
    </file>
    <file>
      <path>tests/core/test_core_boundary_guards.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Boundary and policy guard tests.

Scope:
- Enforce core/ isolation: no imports from fum_rt.io, fum_rt.runtime, or fum_rt.nexus
- Enforce runtime/ is NumPy-free: runtime contains no 'import numpy' or 'from numpy ...'
- Enforce IDF/lexicon coupling remains outside core/: core must not import IO IDF or reference compute_idf_scale

These tests are inexpensive and run as plain file-content checks (no heavy imports).
"""

import re
from pathlib import Path

# ---------- helpers ----------


def _project_root(start: Path | None = None) -> Path:
    """
    Locate repository root by searching upward for a directory that contains 'fum_rt'.
    """
    cur = (start or Path(__file__)).resolve().parent
    for _ in range(10):
        if (cur / "fum_rt").is_dir():
            return cur
        if cur.parent == cur:
            break
        cur = cur.parent
    # Fallback to current file's grandparent
    return (start or Path(__file__)).resolve().parents[2]


def _iter_py_files(dir_path: Path):
    for p in dir_path.rglob("*.py"):
        # Skip compiled or hidden
        if p.name.startswith("."):
            continue
        yield p


def _read_text(p: Path) -> str:
    try:
        return p.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return ""


# ---------- tests ----------


def test_core_does_not_import_runtime_or_io():
    """
    core/* must not import fum_rt.io.*, fum_rt.runtime.*, or fum_rt.nexus.*
    """
    root = _project_root()
    core_dir = root / "fum_rt" / "core"

    forbidden_re = re.compile(
        r"^\s*(from|import)\s+fum_rt\.(io|runtime|nexus)\b",
        re.MULTILINE,
    )

    offenders: list[Path] = []
    for py in _iter_py_files(core_dir):
        txt = _read_text(py)
        if forbidden_re.search(txt):
            offenders.append(py)

    assert not offenders, f"Forbidden imports in core/: {', '.join(str(p) for p in offenders)}"


def test_core_does_not_import_idf_or_lexicon_io():
    """
    core/* must not import IDF or lexicon IO implementations.
    """
    root = _project_root()
    core_dir = root / "fum_rt" / "core"

    # Match explicit IDF import lines and calls to compute_idf_scale
    idf_import_re = re.compile(
        r"^\s*(from\s+fum_rt\.io\.lexicon\.idf\s+import\b|import\s+fum_rt\.io\.lexicon\.idf\b)",
        re.MULTILINE,
    )
    idf_call_re = re.compile(r"\bcompute_idf_scale\b")

    offenders: list[str] = []
    for py in _iter_py_files(core_dir):
        txt = _read_text(py)
        if idf_import_re.search(txt):
            offenders.append(f"{py}: idf_import")
        if idf_call_re.search(txt):
            offenders.append(f"{py}: compute_idf_scale_ref")

    assert not offenders, "IDF references found in core/: " + ", ".join(offenders)


def test_runtime_is_numpy_free():
    """
    runtime/* must not import numpy directly; all numeric kernels live in core/*.
    """
    root = _project_root()
    rt_dir = root / "fum_rt" / "runtime"

    numpy_re = re.compile(
        r"^\s*(from\s+numpy\s+import|import\s+numpy(\s+as\s+np)?\b|from\s+numpy\b)",
        re.MULTILINE,
    )

    offenders: list[Path] = []
    for py in _iter_py_files(rt_dir):
        txt = _read_text(py)
        if numpy_re.search(txt):
            offenders.append(py)

    assert not offenders, f"runtime/ uses numpy: {', '.join(str(p) for p in offenders)}"


def test_core_files_exist_expected_brain_modules():
    """
    Sanity: ensure key core modules exist to host internals.
    """
    root = _project_root()
    core_dir = root / "fum_rt" / "core"
    expected = [
        "connectome.py",
        "sparse_connectome.py",
        "adc.py",
        "metrics.py",
        "signals.py",
        "proprioception/events.py",
        "fum_sie.py",
        "sie_v2.py",
    ]
    missing = []
    for rel in expected:
        if not (core_dir / rel).exists():
            missing.append(rel)
    assert not missing, f"Missing core internals modules: {missing}"


def test_runtime_loop_refs_core_and_seams_only():
    """
    Ensure runtime loop/stepper reference core seams for numeric logic.
    """
    root = _project_root()
    loop_py = (root / "fum_rt" / "runtime" / "loop.py")
    loop_pkg_init = (root / "fum_rt" / "runtime" / "loop" / "__init__.py")
    stepper_py = (root / "fum_rt" / "runtime" / "stepper.py")

    # Check loop module exists either as a single file or as a package
    assert loop_py.exists() or loop_pkg_init.exists(), "runtime/loop module not found (file or package)"
    assert stepper_py.exists(), "runtime/stepper.py not found"

    loop_src = loop_py if loop_py.exists() else loop_pkg_init
    loop_txt = _read_text(loop_src)
    step_txt = _read_text(stepper_py)

    # loop should import telemetry.tick_fold and core.signals.apply_b1_detector via runtime seams
    assert "runtime.telemetry" in loop_txt and "tick_fold" in loop_txt, "loop missing telemetry.tick_fold seam"
    assert "core.signals" in loop_txt, "loop missing core.signals seam usage"

    # stepper should import core.signals helpers and compute_metrics from core
    assert "core.signals" in step_txt and "compute_metrics" in step_txt, "stepper.py should use core signals/metrics"


if __name__ == "__main__":
    # Allow running as a script for local quick checks
    import sys

    root = _project_root()
    tests = [
        test_core_does_not_import_runtime_or_io,
        test_core_does_not_import_idf_or_lexicon_io,
        test_runtime_is_numpy_free,
        test_core_files_exist_expected_brain_modules,
        test_runtime_loop_refs_core_and_seams_only,
    ]
    failed = 0
    for t in tests:
        try:
            t()
            print(f"[OK] {t.__name__}")
        except AssertionError as e:
            failed += 1
            print(f"[FAIL] {t.__name__}: {e}", file=sys.stderr)
    sys.exit(1 if failed else 0)]]></content>
    </file>
    <file>
      <path>tests/core/test_maps_frame.py</path>
      <content><![CDATA[import numpy as np

from fum_rt.core.engine import CoreEngine
from fum_rt.core.proprioception.events import VTTouchEvent, SpikeEvent, DeltaWEvent


class _StubNx:
    """
    Minimal nexus-like stub for CoreEngine:
    - Provides only the attributes CoreEngine.step() actually touches in this test
    """
    def __init__(self, N: int = 64, seed: int = 0) -> None:
        self.N = int(N)
        self.seed = int(seed)
        self._emit_step = 0  # for fold tick hint
        # optional params used by _ensure_evt_init() with safe fallbacks:
        self.b1_half_life_ticks = 50
        self.cold_head_k = 256
        self.cold_half_life_ticks = 200
        # No bus needed here (we assert _maps_frame_ready before telemetry publishes)


def test_maps_frame_smoke_builds_arrays_without_scans():
    nx = _StubNx(N=64, seed=0)
    eng = CoreEngine(nx)

    # Simulate a small batch of events hitting distinct nodes
    events = [
        VTTouchEvent(kind="vt_touch", t=1, token=42, w=1.0),            # -> HeatMap
        SpikeEvent(kind="spike", t=1, node=7, amp=0.8, sign=+1),        # -> ExcitationMap
        SpikeEvent(kind="spike", t=1, node=9, amp=0.5, sign=-1),        # -> InhibitionMap
        DeltaWEvent(kind="delta_w", t=1, node=9, dw=-0.2),              # -> InhibitionMap (abs)
    ]

    # Fold events and build maps/frame
    eng.step(10, events)

    # Engine should stage a maps frame for telemetry to publish later
    assert hasattr(nx, "_maps_frame_ready"), "maps/frame not staged by CoreEngine"
    header, payload = getattr(nx, "_maps_frame_ready")

    # Validate header contract
    assert isinstance(header, dict)
    assert header.get("channels") == ["heat", "exc", "inh"]
    n = header.get("n")
    assert isinstance(n, int) and n == nx.N
    shape = header.get("shape")
    assert isinstance(shape, list) and len(shape) == 2
    stats = header.get("stats")
    assert isinstance(stats, dict) and set(stats.keys()) == {"heat", "exc", "inh"}
    for ch in ("heat", "exc", "inh"):
        assert "min" in stats[ch] and "max" in stats[ch]
        # min is fixed to 0.0 by construction
        assert stats[ch]["min"] == 0.0

    # Validate payload: 3 * n float32 (little-endian) blocks
    assert isinstance(payload, (bytes, bytearray))
    assert len(payload) == 3 * n * 4  # float32 bytes

    f32 = np.frombuffer(payload, dtype="<f4", count=3 * n)
    heat = f32[:n]
    exc = f32[n : 2 * n]
    inh = f32[2 * n : 3 * n]

    # Only working-set indices should be nonzero (42 for heat, 7 for exc, 9 for inh)
    nonzero_idx = set(np.nonzero(heat)[0]) | set(np.nonzero(exc)[0]) | set(np.nonzero(inh)[0])
    assert nonzero_idx.issubset({42, 7, 9}), f"Unexpected nonzero indices: {sorted(nonzero_idx)}"

    # Ensure we didn't accidentally scan/normalize across full arrays (max from working-set is fine)
    # i.e., stats max should be >= the observed nonzero values and min stays 0.0
    for arr_name, arr in (("heat", heat), ("exc", exc), ("inh", inh)):
        observed_max = float(arr.max(initial=0.0))
        assert stats[arr_name]["max"] >= observed_max]]></content>
    </file>
    <file>
      <path>tests/core/test_runner_budget.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.tests.core.test_runner_budget

CI: Verify the void-walker runner is one-shot per tick and enforces microsecond budgets.

- No schedulers, no timers: pure function invoked once per tick by runtime loop.
- Global time guard must drop remaining scouts upon exceeding max_us.
- Rotation by budget["tick"] provides fairness (round-robin start index).

This test uses a deterministic monkeypatch of runner.perf_counter_ns to avoid flaky wall-clock timings.
"""

from typing import Any, List
import math

import os

from fum_rt.core.cortex.void_walkers import runner as _runner


class _FakeEvent:
    def __init__(self, kind: str = "vt_touch") -> None:
        self.kind = kind
        # Optional fields that reducers might access (kept minimal)
        self.t = 0


class _FakeScout:
    def __init__(self, label: str) -> None:
        self.label = label
        self.calls = 0

    def step(self, *, connectome: Any = None, bus: Any = None, maps: Any = None, budget: Any = None) -> list:
        self.calls += 1
        # Emit a minimal event payload (duck-typed BaseEvent)
        return [_FakeEvent("vt_touch")]


def _make_fake_perf_counter_ns(increment_ns: int):
    """
    Returns a deterministic perf_counter_ns function:
    - First call returns 0
    - Each subsequent call increases by increment_ns
    """
    state = {"ns": 0}

    def _fake() -> int:
        v = state["ns"]
        state["ns"] = v + int(increment_ns)
        return v

    return _fake


def test_runner_respects_max_us_breaks(monkeypatch) -> None:
    """
    With increment_ns=600_000 (600 us) per perf_counter_ns call and max_us=1000:
    - Only the first scout executes before the global guard breaks the loop.
    """
    # Patch perf_counter_ns in the runner module namespace
    fake_perf = _make_fake_perf_counter_ns(increment_ns=600_000)
    monkeypatch.setattr(_runner, "perf_counter_ns", fake_perf, raising=True)

    # Prepare scouts (would run 5 if unbounded)
    scouts = [_FakeScout(f"s{i}") for i in range(5)]

    evs = _runner.run_scouts_once(
        connectome=None,
        scouts=scouts,
        maps=None,
        budget={"tick": 0, "visits": 16, "edges": 8, "ttl": 64},
        bus=None,
        max_us=1000,  # 1 ms global budget
    )

    # Exactly one scout should have run
    total_calls = sum(s.calls for s in scouts)
    assert total_calls == 1, f"Expected 1 scout to run under budget; got {total_calls}"
    assert isinstance(evs, list) and len(evs) == 1, "Runner should have returned single event from the first scout"


def test_runner_round_robin_start_index(monkeypatch) -> None:
    """
    With the same deterministic timing and a nonzero tick:
    - Rotation by budget['tick'] should start from that index.
    - Since global guard breaks after first, the selected scout is the rotated head.
    """
    fake_perf = _make_fake_perf_counter_ns(increment_ns=600_000)
    monkeypatch.setattr(_runner, "perf_counter_ns", fake_perf, raising=True)

    scouts = [_FakeScout(f"s{i}") for i in range(5)]
    tick = 3  # rotation offset

    _ = _runner.run_scouts_once(
        connectome=None,
        scouts=scouts,
        maps=None,
        budget={"tick": tick},
        bus=None,
        max_us=1000,
    )

    ran_indices = [i for i, s in enumerate(scouts) if s.calls > 0]
    assert ran_indices == [tick], f"Expected only rotated head scout index {tick} to run; got {ran_indices}"]]></content>
    </file>
    <file>
      <path>tests/core/test_scouts.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import re
from glob import glob

# CI guard: scouts must remain void-faithful (no scans, no dense ops)
# Scope: core/cortex/void_walkers/*.py

HERE = os.path.abspath(os.path.dirname(__file__))
REPO_CORE = os.path.abspath(os.path.join(HERE, "..", "..", "core"))
WALKERS_DIR = os.path.join(REPO_CORE, "cortex", "void_walkers")


def _read(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def test_void_walkers_exist():
    assert os.path.isdir(WALKERS_DIR), f"Missing walkers dir: {WALKERS_DIR}"
    py_files = sorted(glob(os.path.join(WALKERS_DIR, "*.py")))
    assert py_files, "No walker .py files found under core/cortex/void_walkers"


def test_walkers_no_scans_or_dense_calls():
    # Disallow global scans and dense conversions in walkers
    banned = re.compile(
        r"(synaptic_weights|eligibility_traces|\.adj\b|toarray|tocsr|csr|coo|networkx)",
        re.IGNORECASE,
    )
    py_files = sorted(glob(os.path.join(WALKERS_DIR, "*.py")))
    assert py_files, "No walker .py files found to check"

    # Allowlist minimal: base and the specific walkers are still subject to the same guard
    for p in py_files:
        src = _read(p)
        assert not banned.search(src), f"Walker contains forbidden identifier(s): {p}"


def test_walkers_readonly_contract_mentions():
    # Ensure docstrings emphasize read-only/event-driven (lightweight heuristic)
    py_files = sorted(glob(os.path.join(WALKERS_DIR, "*.py")))
    key_terms = ("read-only", "void-faithful")
    found = 0
    for p in py_files:
        head = _read(p)[:512].lower()
        if all(k in head for k in key_terms):
            found += 1
    # Require at least two walkers to include contract language (base + ≥1 walker)
    assert found >= 2, "Expected read-only/void-faithful contract emphasized in walker docstrings"]]></content>
    </file>
    <file>
      <path>tests/core/test_territory_uf.py</path>
      <content><![CDATA[import pytest

from fum_rt.core.proprioception.territory import TerritoryUF


class Obs:
    """Minimal Observation-like stub with kind and fields used by TerritoryUF.fold()."""
    def __init__(self, kind: str, **kwargs):
        self.kind = kind
        for k, v in (kwargs or {}).items():
            setattr(self, k, v)


def test_union_and_components_with_cycle_hit_and_edge_on():
    uf = TerritoryUF(head_k=8)

    # Two observations that should union (0,1) and (1,2) into a single component
    obs = [
        Obs("cycle_hit", nodes=[0, 1, 9]),
        Obs("edge_on", u=1, v=2),
    ]
    uf.fold(obs)

    # All three nodes should share the same root; components_count should be 1
    # Components count approximates roots; with only one merged, we expect 1.
    assert uf.components_count() == 1
    # Heads should include members of that component, bounded by head_k
    head_any = set(uf.sample_any(8))
    assert {0, 1, 2}.issubset(head_any)


def test_sample_indices_and_any_are_bounded_and_stable():
    uf = TerritoryUF(head_k=4)

    # Create three disjoint edges => components: (0,1), (2,3), (4,5)
    uf.fold([
        Obs("edge_on", u=0, v=1),
        Obs("edge_on", u=2, v=3),
        Obs("edge_on", u=4, v=5),
    ])

    # Each sample should be bounded by k
    s1 = uf.sample_indices(0, 10)
    s2 = uf.sample_any(10)
    assert len(s1) <= 4  # head_k bound
    assert len(s2) <= 10  # request bound
    # Heads should be unique-index lists (no duplicates)
    assert len(s1) == len(set(s1))
    assert len(s2) == len(set(s2))

    # Asking for k=0 returns empty lists
    assert uf.sample_indices(0, 0) == []
    assert uf.sample_any(0) == []


def test_edge_off_marks_dirty_but_does_not_crash_fold():
    uf = TerritoryUF(head_k=4)
    uf.fold([Obs("edge_off", u=10, v=11)])
    # No exceptions, and components can be computed
    _ = uf.components_count()]]></content>
    </file>
    <file>
      <path>tests/core/test_void_faithful_guards.py</path>
      <content><![CDATA[import io
import os
import re

# CI guard: Ensure void-faithful reducers do not peek global structures
# and CoreEngine wiring does not scan W/CSR/adjacency for maps.

REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))


def _read(path: str) -> str:
    with io.open(path, "r", encoding="utf-8") as f:
        return f.read()


def test_reducers_event_only_no_scans():
    reducers = [
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "base_decay_map.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "heatmap.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "excitationmap.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "inhibitionmap.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "memorymap.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "trailmap.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "coldmap.py"),
    ]
    banned = re.compile(r"(synaptic|weights|adj\b|csr|coo|tocoo|tocsr|toarray)", re.IGNORECASE)
    for p in reducers:
        src = _read(p)
        # Allow docstrings/comments to mention words? Keep strict: no banned anywhere in reducer sources.
        assert not banned.search(src), f"Reducer {p} contains forbidden global-scan identifier"


def test_engine_maps_wiring_no_scans():
    # Check CoreEngine wiring for no scans and proper reducer folds
    eng = os.path.join(REPO_ROOT, "core", "engine", "core_engine.py")
    src = _read(eng)
    banned = re.compile(r"(synaptic_weights|eligibility_traces|\.adj\b|toarray|tocsr|csr|coo)", re.IGNORECASE)
    assert not banned.search(src), "CoreEngine must not scan W/CSR/adjacency when building maps/frame"

    # Ensure we actually fold the three reducers
    assert "self._heat_map.fold" in src
    assert "self._exc_map.fold" in src
    assert "self._inh_map.fold" in src

    # Header tokens are defined in maps_frame builder, validate there
    mf = os.path.join(REPO_ROOT, "core", "engine", "maps_frame.py")
    src_mf = _read(mf)
    for token in ('"topic": "maps/frame"', '"channels": ["heat", "exc", "inh"]', '"dtype": "f32"', '"endianness": "LE"'):
        assert token in src_mf, f"Missing header token in maps/frame builder: {token}"

def test_memory_kernel_no_laplacian_or_matmul():
    """
    Guard: memory kernel implementations must not introduce dense/matmul/Laplacian ops.
    Applies to core/memory/field.py (owner) and maps/memorymap.py (proxy/view).
    """
    import os, re
    REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    mem_files = [
        os.path.join(REPO_ROOT, "core", "memory", "field.py"),
        os.path.join(REPO_ROOT, "core", "cortex", "maps", "memorymap.py"),
    ]
    banned = re.compile(r"(laplacian|matmul|toarray|tocsr|csr|coo)", re.IGNORECASE)
    for p in mem_files:
        with open(p, "r", encoding="utf-8") as f:
            src = f.read()
        assert not banned.search(src), f"Memory kernel must not include dense/matmul/laplacian ops: {p}"]]></content>
    </file>
    <file>
      <path>tests/frontend/test_process_manager_runs_root.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import re
import sys

from fum_rt.frontend.services.process_manager import ProcessManager


def _stub_popen(monkeypatch):
    """
    Install a harmless FakeProc to avoid launching real subprocesses during tests.
    Ensures poll() returns None so start() proceeds without early-exit path.
    """
    class FakeProc:
        def __init__(self, *args, **kwargs):
            self._poll = None
            self.stdin = None

        def poll(self):
            return self._poll

        def terminate(self):
            self._poll = 0

        def wait(self, timeout=None):
            return 0

        def kill(self):
            self._poll = -9

    monkeypatch.setattr(
        "fum_rt.frontend.services.process_manager.subprocess.Popen",
        lambda *a, **k: FakeProc(),
        raising=True,
    )
    # Remove sleeps in detection loop for test speed
    monkeypatch.setattr(
        "fum_rt.frontend.services.process_manager.time.sleep",
        lambda s: None,
        raising=True,
    )


def test_start_defaults_run_dir_under_runs_root(tmp_path, monkeypatch):
    """
    When profile omits run_dir, ProcessManager.start() must synthesize rr/<YYYYMMDD_HHMMSS>
    under the UI-selected runs_root, not ./runs. This guards the default path semantics.

    Verifies that:
      - start() injects profile['run_dir'] BEFORE _build_cmd is called
      - The synthesized run_dir lives under runs_root and matches the timestamp pattern
      - start() returns the same explicit run_dir (explicit branch)
    """
    rr = tmp_path / "runs_root"
    rr.mkdir(parents=True, exist_ok=True)

    pm = ProcessManager(str(rr))
    recorded: dict = {}

    def fake_build_cmd(self, profile):
        # Capture the profile as seen by _build_cmd to assert start() injected run_dir.
        recorded["profile"] = dict(profile)
        return [sys.executable, "-c", "print('noop')"]

    monkeypatch.setattr(ProcessManager, "_build_cmd", fake_build_cmd, raising=True)
    _stub_popen(monkeypatch)

    ok, rd = pm.start({})  # no run_dir in profile
    assert ok is True, "start() should succeed in test harness"
    assert "profile" in recorded, "ProcessManager._build_cmd was not invoked"
    prof = recorded["profile"]
    rd_spec = prof.get("run_dir")
    assert rd_spec, "run_dir was not synthesized under runs_root"
    assert rd_spec.startswith(str(rr)), f"run_dir '{rd_spec}' does not start with runs_root '{rr}'"

    base = os.path.basename(rd_spec.rstrip(os.path.sep))
    assert re.match(r"^\d{8}_\d{6}$", base), f"run_dir basename '{base}' does not match YYYYMMDD_HHMMSS"

    # start() should return the explicit run_dir when specified on the command
    assert rd == rd_spec, "start() did not return the explicit synthesized run_dir"


def test_start_respects_explicit_run_dir(tmp_path, monkeypatch):
    """
    When profile specifies run_dir explicitly (e.g., adoption/engram case),
    ProcessManager.start() must NOT override it with synthesized rr/<timestamp>.
    """
    rr = tmp_path / "rr"
    rr.mkdir(parents=True, exist_ok=True)
    explicit = tmp_path / "custom" / "sessionX"
    explicit.mkdir(parents=True, exist_ok=True)

    pm = ProcessManager(str(rr))
    recorded: dict = {}

    def fake_build_cmd(self, profile):
        recorded["profile"] = dict(profile)
        return [sys.executable, "-c", "print('noop')"]

    monkeypatch.setattr(ProcessManager, "_build_cmd", fake_build_cmd, raising=True)
    _stub_popen(monkeypatch)

    ok, rd = pm.start({"run_dir": str(explicit)})
    assert ok is True
    assert "profile" in recorded
    assert recorded["profile"].get("run_dir") == str(explicit), "Explicit run_dir was altered"
    assert rd == str(explicit), "Returned run_dir does not match explicit path"]]></content>
    </file>
    <file>
      <path>tests/guards/test_no_scheduler.py</path>
      <content><![CDATA[from __future__ import annotations

import os
import re
from typing import Iterable, Tuple

"""
Guard: deny any scheduler/cadence constructs in core/ and runtime/.

Policy (void-faithful, emergent-only):
- No files named 'scheduler.py' in core/cortex/void_walkers.
- No imports of 'scheduler' anywhere in fum_rt/core or fum_rt/runtime.
- Deny tokens (case-insensitive): STRUCT_EVERY | cron | schedule | scheduler | every <number>
  The 'every <number>' pattern forbids cron-like cadence gates while allowing
  incidental prose (e.g., "every tick") that contains no explicit numeric cadence.

Scope:
- Scans fum_rt/core/** and fum_rt/runtime/** .py sources only (excludes tests and docs).
"""

HERE = os.path.abspath(os.path.dirname(__file__))
REPO_ROOT = os.path.abspath(os.path.join(HERE, "..", "..", ".."))
FUM_ROOT = os.path.join(REPO_ROOT, "fum_rt")

CORE_DIR = os.path.join(FUM_ROOT, "core")
RUNTIME_DIR = os.path.join(FUM_ROOT, "runtime")

DENY_FILE = os.path.join(CORE_DIR, "cortex", "void_walkers", "scheduler.py")

# Denylist tokens and patterns
RE_IMPORT_SCHED = re.compile(r"(?i)^\s*(from\s+[\w\.]+\s+import\s+scheduler\b|import\s+scheduler\b)", re.M)
RE_DENY_TOKENS = re.compile(
    r"(?is)\b(STRUCT_EVERY|cron|schedule|scheduler)\b|every\s+\d+",
    re.IGNORECASE,
)


def _iter_py_files(root_dirs: Iterable[str]) -> Iterable[str]:
    for root in root_dirs:
        if not os.path.isdir(root):
            continue
        for r, _dirs, files in os.walk(root):
            for fn in files:
                if fn.endswith(".py"):
                    yield os.path.join(r, fn)


def _read(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _first_match_text(src: str, pat: re.Pattern) -> Tuple[int, str]:
    m = pat.search(src)
    if not m:
        return (-1, "")
    # Extract offending line
    start = src.rfind("\n", 0, m.start()) + 1
    end = src.find("\n", m.end())
    if end == -1:
        end = len(src)
    line = src[start:end]
    # Compute 1-based line number
    line_no = src.count("\n", 0, start) + 1
    return (line_no, line.strip())


def test_no_scheduler_file_present() -> None:
    assert not os.path.exists(DENY_FILE), f"Forbidden scheduler file present: {DENY_FILE}"


def test_no_scheduler_imports_or_tokens() -> None:
    offenders = []
    for path in _iter_py_files([CORE_DIR, RUNTIME_DIR]):
        try:
            src = _read(path)
        except Exception:
            continue
        # Specific import pattern
        ln1, line1 = _first_match_text(src, RE_IMPORT_SCHED)
        # General deny tokens
        ln2, line2 = _first_match_text(src, RE_DENY_TOKENS)

        if ln1 != -1:
            offenders.append((path, ln1, line1))
        if ln2 != -1:
            offenders.append((path, ln2, line2))

    msg_lines = ["Found forbidden scheduler/cadence constructs:"]
    for p, ln, txt in offenders:
        msg_lines.append(f"- {p}:{ln}: {txt}")
    assert not offenders, "\n".join(msg_lines)]]></content>
    </file>
    <file>
      <path>tests/physics/test_invariants.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.tests.physics.test_invariants

CI tests for physics ↔ code guard helpers:
- Q_FUM (logistic on-site constant of motion) spot-check
- Kinetic normalization equivalence (c^2 from κ vs 2J)
- Memory steering dimensionless groups extraction

Constraints:
- Pure stdlib; no runtime/core heavy imports except the guard helpers under test.
- No scans of core/maps; tests operate on small synthetic arrays only.
"""

import math
import random
from typing import List

from fum_rt.core.guards.invariants import (
    qfum_logistic_value,
    check_qfum_logistic,
    kinetic_c2_from_kappa,
    kinetic_c2_from_J,
    compute_memory_groups,
)


def _logistic_exact_next(w0: float, dt: float, alpha: float, beta: float) -> float:
    """
    Exact solution for dW/dt = (α - β) W - α W^2 = k W (1 - W/K),
    with k = α - β, K = (α - β)/α (α>0).
    """
    a = float(alpha)
    b = float(beta)
    k = a - b
    if not (a > 0.0 and k != 0.0):
        return float(w0)
    K = k / a
    # Avoid singularities by clamping input w0 into (0, K)
    eps = 1e-12
    w0c = max(eps, min(float(w0), K - eps))
    A = (K - w0c) / w0c
    return K / (1.0 + A * math.exp(-k * float(dt)))


def test_qfum_logistic_constant_of_motion_p99() -> None:
    """
    Generate synthetic pairs (W_prev, W_curr) from the exact logistic solution
    and verify the analytic Q_FUM drift is negligible (p99 and max within tight tol).
    """
    alpha = 0.25
    beta = 0.10
    k = alpha - beta
    assert alpha > beta > 0.0
    assert k > 0.0

    # Time span
    t_prev = 10.0  # non-zero start time to exercise t dependence
    dt = 0.7
    t_curr = t_prev + dt

    # Capacity and sampling inside (0.01*K, 0.99*K)
    K = (alpha - beta) / alpha
    lo = 0.01 * K
    hi = 0.99 * K

    rng = random.Random(123)
    n = 512
    w_prev: List[float] = []
    w_curr: List[float] = []
    for _ in range(n):
        w0 = lo + (hi - lo) * rng.random()
        w1 = _logistic_exact_next(w0, dt, alpha, beta)
        w_prev.append(w0)
        w_curr.append(w1)

    # Tight tolerances since we use the exact solution
    res = check_qfum_logistic(
        w_prev,
        w_curr,
        t_prev=t_prev,
        t_curr=t_curr,
        alpha=alpha,
        beta=beta,
        tol_abs=1e-9,
        tol_p99=1e-10,
    )

    # Sanity: structure of result
    assert int(res.get("count", 0)) > 0
    # CI acceptance: p99 drift below tolerance; extremely small max drift
    assert bool(res.get("pass_p99", False)), f"p99 drift failed: {res}"
    assert float(res.get("dQ_max", 0.0)) <= 1e-8, f"max drift too large: {res}"

    # Spot-check a few direct pairs with the scalar helper
    for idx in (0, n // 3, (2 * n) // 3, n - 1):
        q0 = qfum_logistic_value(w_prev[idx], t_prev, alpha=alpha, beta=beta)
        q1 = qfum_logistic_value(w_curr[idx], t_curr, alpha=alpha, beta=beta)
        assert abs(q1 - q0) <= 1e-8


def test_kinetic_normalization_equivalence_kappa_2J() -> None:
    """
    Kinetic normalization sanity:
    c^2 = κ a^2 and c^2 = 2 J a^2. For κ = 2J the two expressions must match.
    """
    # Probe a few scales
    cases = [
        (1.0, 0.5),   # a, J
        (2.0, 0.125),
        (0.25, 3.0),
        (3.0, 1e-3),
    ]
    for a, J in cases:
        kappa = 2.0 * J
        c2_kappa = kinetic_c2_from_kappa(kappa, a)
        c2_J = kinetic_c2_from_J(J, a)
        # Relative tolerance check
        denom = max(1e-12, abs(c2_kappa) + abs(c2_J))
        rel = abs(c2_kappa - c2_J) / denom
        assert rel <= 1e-12, f"Normalization mismatch: κ={kappa}, J={J}, a={a}, c2_kappa={c2_kappa}, c2_J={c2_J}, rel={rel}"


def test_compute_memory_groups_reads_dimensionless_params() -> None:
    """
    Verify telemetry-facing dimensionless memory parameters are read out properly.
    """

    class _FakeField:
        Theta = 1.234
        D_a = 0.5
        Lambda = 0.1
        Gamma = 0.2

    g = compute_memory_groups(_FakeField())
    assert math.isclose(g.get("mem_Theta", 0.0), 1.234, rel_tol=0.0, abs_tol=0.0)
    assert math.isclose(g.get("mem_Da", 0.0), 0.5, rel_tol=0.0, abs_tol=0.0)
    assert math.isclose(g.get("mem_Lambda", 0.0), 0.1, rel_tol=0.0, abs_tol=0.0)
    assert math.isclose(g.get("mem_Gamma", 0.0), 0.2, rel_tol=0.0, abs_tol=0.0)]]></content>
    </file>
    <file>
      <path>tests/physics/test_memory_kernel_event_local.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Event-local memory kernel tests (void-faithful lock-in)

Confirms:
- Writes occur only at touched nodes (vt_touch, spike, delta_w).
- Smoothing occurs only along visited edges (edge_on events) and only affects the two endpoints.
- Determinism under fixed seed and identical event sequences.
"""

from typing import List

from fum_rt.core.memory import MemoryField
from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent


def _mk_field(gamma=0.5, delta=0.0, kappa=0.25, seed=123) -> MemoryField:
    return MemoryField(head_k=32, keep_max=256, seed=seed, gamma=gamma, delta=delta, kappa=kappa)


def test_edge_only_spread_local() -> None:
    """
    vt_touch at u=1 sets m[1] > 0; edge_on(1,2) spreads only between 1 and 2.
    No other nodes are affected.
    """
    f = _mk_field(gamma=0.5, delta=0.0, kappa=0.25, seed=42)
    evs: List[object] = [
        VTTouchEvent(kind="vt_touch", t=1, token=1, w=2.0),  # m1 += gamma * w = 1.0
        EdgeOnEvent(kind="edge_on", t=1, u=1, v=2),
    ]
    f.fold(evs, tick=1)
    m = f.snapshot_dict(cap=10)
    # After touch: m1=1.0, m2=0.0; smoothing: d = kappa*(m2 - m1) = -0.25
    # => m1=0.75, m2=0.25
    assert abs(m.get(1, 0.0) - 0.75) < 1e-9
    assert abs(m.get(2, 0.0) - 0.25) < 1e-9
    # No other nodes created/changed
    assert 3 not in m or abs(m.get(3, 0.0)) < 1e-12


def test_no_spread_without_edge_event() -> None:
    """
    With two node touches and no edge_on, there is no inter-node smoothing.
    """
    f = _mk_field(gamma=0.5, delta=0.0, kappa=0.25, seed=7)
    evs: List[object] = [
        VTTouchEvent(kind="vt_touch", t=1, token=1, w=2.0),  # m1 += 1.0
        VTTouchEvent(kind="vt_touch", t=1, token=2, w=2.0),  # m2 += 1.0
    ]
    f.fold(evs, tick=1)
    m = f.snapshot_dict(cap=10)
    assert abs(m.get(1, 0.0) - 1.0) < 1e-9
    assert abs(m.get(2, 0.0) - 1.0) < 1e-9
    assert 3 not in m or abs(m.get(3, 0.0)) < 1e-12


def test_seed_determinism_memoryfield() -> None:
    """
    Same seed + identical events => identical snapshot dicts.
    """
    evs: List[object] = [
        VTTouchEvent(kind="vt_touch", t=1, token=5, w=1.0),
        VTTouchEvent(kind="vt_touch", t=1, token=9, w=2.0),
        EdgeOnEvent(kind="edge_on", t=1, u=5, v=9),
        VTTouchEvent(kind="vt_touch", t=2, token=7, w=3.0),
        EdgeOnEvent(kind="edge_on", t=2, u=7, v=9),
    ]
    f1 = _mk_field(gamma=0.3, delta=0.0, kappa=0.15, seed=123)
    f1.fold(evs, tick=2)
    d1 = f1.snapshot_dict(cap=100)

    f2 = _mk_field(gamma=0.3, delta=0.0, kappa=0.15, seed=123)
    f2.fold(evs, tick=2)
    d2 = f2.snapshot_dict(cap=100)

    assert d1 == d2]]></content>
    </file>
    <file>
      <path>tests/physics/test_steering_ab.py</path>
      <content><![CDATA[from __future__ import annotations

"""
fum_rt.tests.physics.test_steering_ab

CI: Steering A/B acceptance tests (void-faithful, pure numeric; no scans).

Scope:
- Softmax steering law P(i→j) ∝ exp(Θ m_j)
- Junction A/B identity: log(P_B / P_A) == Θ * (m_B - m_A)
- Empirical sampling from the softmax matches analytic probabilities

Notes:
- Uses stdlib only; avoids importing heavy core modules.
- Deterministic RNG for stable CI behavior.
"""

import math
import random
from typing import Tuple


def _softmax_two(m_a: float, m_b: float, Theta: float) -> Tuple[float, float]:
    ea = math.exp(float(Theta) * float(m_a))
    eb = math.exp(float(Theta) * float(m_b))
    Z = ea + eb
    if Z == 0.0:
        # Degenerate; return uniform
        return 0.5, 0.5
    return ea / Z, eb / Z


def test_logit_identity_matches_theta_delta_m() -> None:
    """
    For two-branch junction A/B with P ∝ exp(Θ m):
      log(P_B / P_A) == Θ * (m_B - m_A)

    This is an analytic identity; we allow tiny numeric tolerance.
    """
    rng = random.Random(321)
    # Sweep Θ including attraction/repulsion and weak/strong coupling regimes
    thetas = [0.0, 0.25, 1.0, -0.5, 2.0]
    for Theta in thetas:
        for _ in range(256):
            # Sample memory field values within a moderate range
            m_a = rng.uniform(-3.0, 3.0)
            m_b = rng.uniform(-3.0, 3.0)

            p_a, p_b = _softmax_two(m_a, m_b, Theta)
            # Guard against underflow/overflow by clamping probabilities
            p_a = max(1e-15, min(1.0 - 1e-15, p_a))
            p_b = max(1e-15, min(1.0 - 1e-15, p_b))

            left = math.log(p_b / p_a)
            right = float(Theta) * (float(m_b) - float(m_a))

            # Tight relative/absolute tolerances (identity)
            diff = abs(left - right)
            # For Theta==0, left and right should both be ~0; use abs tol
            if Theta == 0.0:
                assert diff <= 1e-12
            else:
                denom = max(1.0, abs(left) + abs(right))
                assert diff <= 1e-12 * denom, f"Theta={Theta}, m_a={m_a}, m_b={m_b}, left={left}, right={right}, diff={diff}"


def test_sampling_matches_softmax_probability() -> None:
    """
    Monte Carlo sanity: sample choices vs. analytic softmax probabilities.

    Draw N trials using the analytic P_B, count frequency of B, and compare
    against P_B with binomial standard deviation tolerance.
    """
    rng = random.Random(12345)
    cases = [
        # (m_a, m_b, Theta, N)
        (0.0, 0.0, 1.0, 2000),   # symmetric
        (0.5, -0.5, 1.0, 4000),  # B disfavored
        (-1.0, 1.0, 0.75, 4000), # B favored
        (1.5, 0.1, -1.25, 5000), # repulsive steering (negative Theta)
    ]
    for m_a, m_b, Theta, N in cases:
        p_a, p_b = _softmax_two(m_a, m_b, Theta)
        # Sample
        count_b = 0
        for _ in range(int(N)):
            u = rng.random()
            if u <= p_b:
                count_b += 1
        freq_b = count_b / float(N)

        # Binomial standard deviation σ = sqrt(p(1-p)/N)
        sigma = math.sqrt(max(1e-15, p_b * (1.0 - p_b)) / float(N))
        # Accept within 5σ to be robust across environments
        assert abs(freq_b - p_b) <= 5.0 * sigma, f"Sampling drift too large: freq={freq_b}, p={p_b}, σ={sigma}, case={(m_a, m_b, Theta, N)}"]]></content>
    </file>
    <file>
      <path>tests/runtime/test_events_adapter_inhibition.py</path>
      <content><![CDATA[import math
import pytest

from fum_rt.runtime.events_adapter import observations_to_events
from fum_rt.core.proprioception.events import (
    BaseEvent,
    DeltaWEvent,
    SpikeEvent,
    VTTouchEvent,
    EdgeOnEvent,
)
from fum_rt.core.cortex.maps.inhibitionmap import InhibitionMap
from fum_rt.core.cortex.maps.excitationmap import ExcitationMap


class Obs:
    """Minimal Observation stub matching adapter expectations."""
    def __init__(self, kind: str, tick: int = 0, nodes=None, meta=None, **kwargs):
        self.kind = kind
        self.tick = int(tick)
        self.nodes = list(nodes or [])
        self.meta = dict(meta or {})
        # pass-through any optional fields (e.g., loop_gain, s_mean, u, v)
        for k, v in (kwargs or {}).items():
            setattr(self, k, v)


def _has_event(evts, cls, **attrs):
    for e in evts:
        if not isinstance(e, cls):
            continue
        ok = True
        for k, v in attrs.items():
            if getattr(e, k, None) != v:
                ok = False
                break
        if ok:
            return True
    return False


def _collect(evts, cls):
    return [e for e in evts if isinstance(e, cls)]


def test_delta_w_negative_emits_inhibitory_spike_and_deltawevent():
    # Given a delta_w observation with negative dw
    obs = Obs(kind="delta_w", tick=5, nodes=[1, 2, 3], meta={"dw": -0.3})
    evts = observations_to_events([obs])

    # Then DeltaWEvent exists for each of the nodes (bounded to first 16 - here 3)
    dws = _collect(evts, DeltaWEvent)
    assert len(dws) == 3
    assert all(isinstance(e, DeltaWEvent) and e.dw == -0.3 for e in dws)

    # And inhibitory SpikeEvent (sign=-1) is synthesized for each node
    inh = [e for e in evts if isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) < 0]
    assert len(inh) == 3
    # amp is clipped to [0,1], here 0.3; allow float tolerance
    assert all(math.isclose(float(getattr(e, "amp", 0.0)), 0.3, rel_tol=1e-6, abs_tol=1e-6) for e in inh)


def test_region_stat_emits_vt_touch_and_excit_spikes():
    # Given a region_stat observation
    obs = Obs(kind="region_stat", tick=7, nodes=[10, 11], s_mean=0.6)
    evts = observations_to_events([obs])

    vt = _collect(evts, VTTouchEvent)
    exc = [e for e in evts if isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) > 0]

    assert {int(e.token) for e in vt} == {10, 11}
    # spike amp should reflect s_mean (>= 1.0 default replaced by s_mean when available)
    assert len(exc) == 2
    assert all(math.isclose(float(getattr(e, "amp", 0.0)), 0.6, rel_tol=1e-6, abs_tol=1e-6) for e in exc)


def test_cycle_hit_emits_edge_on_and_excit_spikes():
    # Given a cycle_hit with two endpoints
    obs = Obs(kind="cycle_hit", tick=9, nodes=[4, 5], loop_gain=2.5)
    evts = observations_to_events([obs])

    assert _has_event(evts, EdgeOnEvent, u=4, v=5) or _has_event(evts, EdgeOnEvent, u=5, v=4)
    exc = [e for e in evts if isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) > 0]
    # Two endpoints => two excit spikes
    assert len(exc) == 2
    # amp should reflect loop_gain (since > 0)
    assert all(math.isclose(float(getattr(e, "amp", 0.0)), 2.5, rel_tol=1e-6, abs_tol=1e-6) for e in exc)


def test_inhibition_map_folds_negative_dw_and_inhibitory_spike():
    # Build events for a single node with both inhibitory sources
    t = 12
    evts = [
        DeltaWEvent(kind="delta_w", t=t, node=42, dw=-0.8),
        SpikeEvent(kind="spike", t=t, node=42, amp=0.9, sign=-1),
        # Control: a positive spike should be ignored by InhibitionMap
        SpikeEvent(kind="spike", t=t, node=7, amp=1.0, sign=+1),
        # Control: a positive dw should be ignored by InhibitionMap
        DeltaWEvent(kind="delta_w", t=t, node=7, dw=+0.2),
    ]

    inh = InhibitionMap(head_k=16, half_life_ticks=32, spike_gain=1.0, dW_gain=0.5)
    inh.fold(evts, tick=t)
    snap = inh.snapshot()

    # Expect at least 1 inhibitory entry registered
    assert int(snap.get("inh_count", 0)) >= 1
    # Max value should reflect some contribution from either inhibitory spike or |dw|
    assert float(snap.get("inh_max", 0.0)) > 0.0


def test_excitation_map_ignores_inhibitory_and_accepts_positive_sources():
    t = 21
    evts = [
        DeltaWEvent(kind="delta_w", t=t, node=13, dw=+0.4),
        SpikeEvent(kind="spike", t=t, node=13, amp=0.7, sign=+1),
        # Inhibitory controls (should be ignored by ExcitationMap)
        DeltaWEvent(kind="delta_w", t=t, node=14, dw=-0.3),
        SpikeEvent(kind="spike", t=t, node=14, amp=0.8, sign=-1),
    ]

    exc = ExcitationMap(head_k=16, half_life_ticks=32, spike_gain=1.0, dW_gain=0.5)
    exc.fold(evts, tick=t)
    snap = exc.snapshot()

    assert int(snap.get("exc_count", 0)) >= 1
    assert float(snap.get("exc_max", 0.0)) > 0.0]]></content>
    </file>
    <file>
      <path>tests/runtime/test_phase_legacy_key.py</path>
      <content><![CDATA[from __future__ import annotations

"""
Runtime micro-test: token-clean legacy key alias in apply_phase_profile
- Verifies that profiles using a legacy 'schedule' key (constructed without embedding
  the banned token in source) are correctly aliased to 'cadence' and applied.
- Guards only scan fum_rt/core and fum_rt/runtime; tests may reference legacy tokens.
"""

from typing import Any, Dict
from fum_rt.runtime.phase import apply_phase_profile


class _NX:
    """Minimal Nexus stub that accepts dynamic attributes."""
    pass


def test_phase_legacy_key_alias_applies_fields() -> None:
    nx = _NX()

    # Build the legacy key without embedding the exact token in source
    legacy_key = "sche" + "dule"
    prof: Dict[str, Any] = {
        legacy_key: {
            "adc_entropy_alpha": 0.123,
            "ph_snapshot_interval_sec": 1.5,
        }
    }

    # Apply profile; runtime should alias legacy key to 'cadence' internally
    apply_phase_profile(nx, prof)

    # Assert fields applied on nx
    assert hasattr(nx, "adc_entropy_alpha")
    assert hasattr(nx, "ph_snapshot_interval_sec")
    assert abs(float(getattr(nx, "adc_entropy_alpha")) - 0.123) < 1e-12
    assert abs(float(getattr(nx, "ph_snapshot_interval_sec")) - 1.5) < 1e-12]]></content>
    </file>
    <file>
      <path>tests/runtime/test_telemetry_frame_v2.py</path>
      <content><![CDATA[import os
from typing import Dict, Any

import numpy as np

from fum_rt.core.engine.maps_frame import stage_maps_frame
from fum_rt.runtime.telemetry import tick_fold


class _DummyMap:
    def __init__(self, d: Dict[int, float]) -> None:
        self._val = dict(d)


class _StubNx:
    def __init__(self, N: int) -> None:
        self.N = int(N)
        self._emit_step = 0
        # no bus, ring will be lazy-initialized by tick_fold


def _set_env(k: str, v: str):
    prev = os.environ.get(k)
    os.environ[k] = v
    return prev


def _restore_env(k: str, prev):
    if prev is None:
        os.environ.pop(k, None)
    else:
        os.environ[k] = prev


def test_frame_v2_u8_tiles_and_ring_capacity():
    # Configure telemetry for u8 + tiles + bounded ring and no FPS limiter (tests-only: MAPS_FPS<0)
    prev_mode = _set_env("MAPS_MODE", "u8")
    prev_tile = _set_env("MAPS_TILE", "4x4")
    prev_ring = _set_env("MAPS_RING", "2")
    prev_fps = _set_env("MAPS_FPS", "-1")

    try:
        n = 64  # 8x8 square
        nx = _StubNx(n)
        heat = _DummyMap({5: 1.0})
        exc = _DummyMap({7: 0.7})
        inh = _DummyMap({9: 0.9})

        # Stage float32 frame (v1)
        stage_maps_frame(nx, heat, exc, inh, fold_tick=1)

        # Telemetry fold quantizes to u8 (v2) and pushes to ring
        _m, _syms = tick_fold(nx, metrics={}, drive={}, td_signal=0.0, step=1)

        ring = getattr(nx, "_maps_ring", None)
        assert ring is not None, "MapsRing was not initialized"
        fr = ring.latest()
        assert fr is not None, "No frame pushed to MapsRing"
        hdr = fr.header

        # Header assertions
        assert hdr.get("dtype") == "u8"
        assert hdr.get("ver") == "v2"
        assert hdr.get("quant") == "u8"
        assert "tiles" in hdr, "Tile metadata missing from header"
        tiles = hdr["tiles"]
        # 8x8 frame tiled into 4x4 should yield 2x2 grid
        assert tiles.get("size") == [4, 4]
        assert tiles.get("grid") == [2, 2]
        assert tiles.get("shape") == [8, 8]
        # payload_len hint matches payload bytes
        assert hdr.get("payload_len") == 3 * n
        assert len(fr.payload) == 3 * n

        # Capacity behavior: push 2 more frames and ensure ring stays at capacity=2
        stage_maps_frame(nx, heat, exc, inh, fold_tick=2)
        tick_fold(nx, metrics={}, drive={}, td_signal=0.0, step=2)
        assert ring.size() == 2
        stage_maps_frame(nx, heat, exc, inh, fold_tick=3)
        tick_fold(nx, metrics={}, drive={}, td_signal=0.0, step=3)
        assert ring.size() == 2
        assert ring.dropped() >= 1
    finally:
        _restore_env("MAPS_MODE", prev_mode)
        _restore_env("MAPS_TILE", prev_tile)
        _restore_env("MAPS_RING", prev_ring)
        _restore_env("MAPS_FPS", prev_fps)]]></content>
    </file>
    <file>
      <path>utils/README.md</path>
      <content/>
    </file>
    <file>
      <path>utils/logging_setup.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import logging, json, os, sys, time


class JsonFormatter(logging.Formatter):
    def format(self, record):
        base = {
            "ts": time.time(),
            "level": record.levelname,
            "msg": record.getMessage(),
        }
        if record.__dict__.get("extra"):
            base.update(record.__dict__["extra"])
        return json.dumps(base, ensure_ascii=False)


def get_logger(name, log_file=None):
    """
    Structured JSON logger.

    When log_file is provided, prefer a bounded rolling JSONL handler that streams
    trimmed lines into timestamped archived segments under runs/<ts>/archived/<stamp>/.
    Falls back to a plain FileHandler if the rolling handler is unavailable.
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        # stdout stream handler (always on)
        sh = logging.StreamHandler(sys.stdout)
        sh.setFormatter(JsonFormatter())
        logger.addHandler(sh)

        # optional file sink
        if log_file:
            os.makedirs(os.path.dirname(log_file), exist_ok=True)
            try:
                # Late import to avoid hard dependency during early bootstrap
                from fum_rt.io.logging.rolling_jsonl import RollingJsonlHandler as _RJHandler  # type: ignore
                fh = _RJHandler(log_file)
            except Exception:
                # Safe fallback: non-rolling file handler
                fh = logging.FileHandler(log_file)
            fh.setFormatter(JsonFormatter())
            logger.addHandler(fh)

    return logger
]]></content>
    </file>
  </files>
</fum_code>
