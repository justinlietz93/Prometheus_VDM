***Unit 6 - Appendix A***

# The FUM Nomenclature

### Version 1.0

This document provides a definitive lexicon for the novel concepts introduced in the Fully Unified Model (FUM) architecture. To avoid ambiguity and the chronic misinterpretation that arises from using standard terminology for novel mechanics, this nomenclature establishes a unique vocabulary. Each entry defines a FUM-specific term, clarifies the traditional concept from which it departs, and explains the novel mechanisms that justify the new name.

---

***Section A.***

**A.1. Valence-Gated Synaptic Plasticity (VGSP)**

*   **Traditional Concept:** Spike-Timing-Dependent Plasticity (STDP). Standard STDP is a local learning rule where the change in a synapse's strength is determined solely by the relative timing of pre-synaptic and post-synaptic spikes.
*   **FUM Departure and Justification:** **VGSP** is the core learning principle in FUM. It departs from STDP by making the final synaptic weight update dependent on a global reward signal (a "valence gate") from the Self-Improvement Engine (SIE). The term **VGSP** is often used to refer specifically to the **"classic" implementation**: a simple but computationally intensive (`O(s^2)`) version used in early validation demos. It has been superseded by the canonical RE-VGSP implementation.

***Section B.***

**B.1. Evolving LIF Neuron (ELIF)**

*   **Traditional Concept:** Leaky Integrate-and-Fire (LIF) Neuron. A standard LIF neuron is a simple computational model with a fixed existence and pre-defined connections within a static network architecture.
*   **FUM Departure and Justification:** Neurons in FUM are not static components. An ELIF neuron is a dynamic entity within the Emergent connectome (UKG). Its existence is conditional; it can be created or pruned based on its contribution to the system's goals. Its connections are not fixed but are formed, strengthened, weakened, and eliminated through novel plasticity mechanisms. The term **Evolving LIF Neuron (ELIF)** is used to denote that the neuron itself is subject to an evolutionary process at the architectural level, fundamentally departing from the static nature of a standard LIF model.

***Section C.***

**C.1. Synaptic Actuator (GDSP)**

*   **Traditional Concept:** Structural Plasticity. In neuroscience and some AI models, this refers to the ability of the network to change its physical structure, often driven by simple, local homeostatic rules (e.g., pruning inactive synapses).
*   **FUM Departure and Justification:** Structural changes in FUM are not random or based on simple local rules alone. They are explicitly **goal-directed**. The triggers for creating or pruning neurons and synapses are directly tied to performance metrics from the Self-Improvement Engine. For example, growth may be triggered in a region exhibiting high novelty and high error, representing a direct, system-level attempt to solve a problem. This mechanism is a form of architectural optimization guided by the system's overall objectives, justifying the name **Synaptic Actuator (GDSP)**.

***Section D.***

**D.1. Homeostatic Stability Index (HSI)**

*   **Traditional Concept:** A general reward component for `self_benefit` or stability. In reinforcement learning, such terms are often abstract placeholders for stability-promoting rewards.
*   **FUM Departure and Justification:** The HSI provides a clear, mechanistic drive toward stable, efficient network operation. Its formal definition is a computable **Index** that measures **Homeostatic Stability**: `1 - |var(firing_rates) - target_var| / target_var`. This rewards the network for maintaining its firing rate variance near a target, biologically-plausible setpoint. In the context of early-phase validation and testing, a simpler implementation is often used which rewards network sparsity as a proxy for efficiency. This version is often referred to by its implementation name, `self_benefit`. The formal name, **HSI**, is used to describe the official mechanism in its complete form.

***Section E.***

**E.1. Plasticity Impulse (PI) vs. Eligibility Trace (`e_ij`)**

*   **Traditional Concept:** Eligibility Trace. In standard reinforcement learning, an eligibility trace (`e_ij`) is a synaptic variable that accumulates a memory of recent, locally-correlated activity (e.g., pre- and post-synaptic spikes), marking the synapse as "eligible" for modification by a future, delayed reward.
*   **FUM Departure and Justification:** FUM refines this concept by explicitly separating the instantaneous event from the accumulated memory.
    *   **Plasticity Impulse (PI):** This is the **instantaneous potential** for a weight change generated by a single, local event (e.g., a pre-post spike pair in RE-VGSP). It is a discrete value calculated at a point in time, representing the immediate credit or blame assigned to that event. It is *not* a trace.
    *   **Eligibility Trace (`e_ij`):** This is the memory-based mechanism. It is a local synaptic variable that accumulates and decays the **Plasticity Impulses** over time, according to the rule: `e_ij(t) = gamma(PLV) * e_ij(t-1) + PI(t)`. This trace is what allows a delayed, global `total_reward` signal from the SIE to be correctly applied to the synapses that were active in the recent past.

This distinction is critical: **PI** is the signal, and the **Eligibility Trace** is the memory that integrates the signal. The deprecated term "CRET" was an attempt to capture this but created ambiguity; the formal separation into PI and `e_ij` is the canonical approach.

***Section F.***

**F.1. Resonance-Enhanced Valence-Gated Synaptic Plasticity (RE-VGSP)**

*   **Traditional Concept:** Efficient STDP variants or eligibility traces.
*   **FUM Departure and Justification:** **RE-VGSP** is the canonical, computationally efficient (`O(N)`) implementation of the VGSP learning principle. It improves upon the classic version by using **eligibility traces** instead of direct spike-pair comparisons. Furthermore, it introduces a novel modulation mechanism where the stability of these traces is dynamically controlled by the network's local **resonance** (measured via Phase-Locking Value). This allows the system to autonomously stabilize learning for coherent patterns while remaining flexible to novel inputs. The name signifies this unique combination of Valence-Gating and Resonance-Enhancement.

---

***End of Appendix A***