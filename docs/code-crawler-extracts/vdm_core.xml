<?xml version="1.0" ?>
<fum_code_report>
  <generated_timestamp>2025-10-27 12:55:09</generated_timestamp>
  <metadata>
    <global_stats>
      <total_files>81</total_files>
      <total_size_bytes>520843</total_size_bytes>
      <total_loc>13235</total_loc>
    </global_stats>
    <chunk_stats>
      <files_in_chunk>81</files_in_chunk>
      <size_in_chunk_bytes>520843</size_in_chunk_bytes>
      <loc_in_chunk>13235</loc_in_chunk>
    </chunk_stats>
  </metadata>
  <ascii_map><![CDATA[   core/
>> ├── README.md
   │   (LOC: 1, Size: 25 B)
>> ├── Void_Debt_Modulation.py
   │   (LOC: 136, Size: 5.8 KB)
>> ├── Void_Equations.py
   │   (LOC: 107, Size: 3.9 KB)
>> ├── __init__.py
   │   (LOC: 9, Size: 373 B)
>> ├── adc.py
   │   (LOC: 205, Size: 7.9 KB)
>> ├── announce.py
   │   (LOC: 76, Size: 3.0 KB)
>> ├── bus.py
   │   (LOC: 62, Size: 2.0 KB)
>> ├── connectome.py
   │   (LOC: 403, Size: 17.0 KB)
>> ├── control_server.py
   │   (LOC: 296, Size: 10.4 KB)
   ├── cortex/
>> │   ├── IMPORTANT_TODO.md
   │   │   (LOC: 112, Size: 6.0 KB)
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── Void-Walker-Rules.md
   │   │   (LOC: 456, Size: 32.6 KB)
>> │   ├── Void-Walkers.md
   │   │   (LOC: 166, Size: 14.9 KB)
>> │   ├── __init__.py
   │   │   (LOC: 9, Size: 373 B)
   │   ├── maps/
>> │   │   ├── README.md
   │   │   │   (LOC: 0, Size: 0 B)
>> │   │   ├── __init__.py
   │   │   │   (LOC: 15, Size: 580 B)
>> │   │   ├── base_decay_map.py
   │   │   │   (LOC: 139, Size: 5.0 KB)
>> │   │   ├── coldmap.py
   │   │   │   (LOC: 174, Size: 6.0 KB)
>> │   │   ├── excitationmap.py
   │   │   │   (LOC: 74, Size: 2.6 KB)
>> │   │   ├── heatmap.py
   │   │   │   (LOC: 79, Size: 2.9 KB)
>> │   │   ├── inhibitionmap.py
   │   │   │   (LOC: 74, Size: 2.6 KB)
>> │   │   ├── memorymap.py
   │   │   │   (LOC: 333, Size: 11.8 KB)
>> │   │   └── trailmap.py
   │   │       (LOC: 111, Size: 4.2 KB)
>> │   ├── scouts.py
   │   │   (LOC: 87, Size: 2.9 KB)
   │   └── void_walkers/
>> │       ├── README.md
   │       │   (LOC: 0, Size: 0 B)
>> │       ├── base.py
   │       │   (LOC: 250, Size: 8.6 KB)
>> │       ├── frontier_scout.py
   │       │   (LOC: 19, Size: 639 B)
>> │       ├── runner.py
   │       │   (LOC: 148, Size: 5.0 KB)
>> │       ├── void_cold_scout.py
   │       │   (LOC: 64, Size: 2.0 KB)
>> │       ├── void_cycle_scout.py
   │       │   (LOC: 170, Size: 5.3 KB)
>> │       ├── void_excitation_scout.py
   │       │   (LOC: 122, Size: 4.4 KB)
>> │       ├── void_frontier_scout.py
   │       │   (LOC: 302, Size: 9.5 KB)
>> │       ├── void_heat_scout.py
   │       │   (LOC: 276, Size: 8.9 KB)
>> │       ├── void_inhibition_scout.py
   │       │   (LOC: 122, Size: 4.4 KB)
>> │       ├── void_memory_ray_scout.py
   │       │   (LOC: 260, Size: 8.2 KB)
>> │       ├── void_ray_scout.py
   │       │   (LOC: 247, Size: 7.8 KB)
>> │       └── void_sentinel_scout.py
   │           (LOC: 174, Size: 5.6 KB)
   ├── cosmology/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── __init__.py
   │   │   (LOC: 40, Size: 1.0 KB)
>> │   ├── events.py
   │   │   (LOC: 136, Size: 4.7 KB)
>> │   └── router.py
   │       (LOC: 505, Size: 16.5 KB)
>> ├── diagnostics.py
   │   (LOC: 292, Size: 10.5 KB)
   ├── engine/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── __init__.py
   │   │   (LOC: 19, Size: 610 B)
>> │   ├── core_engine.py
   │   │   (LOC: 532, Size: 21.4 KB)
>> │   ├── evt_snapshot.py
   │   │   (LOC: 123, Size: 3.5 KB)
>> │   └── maps_frame.py
   │       (LOC: 130, Size: 4.0 KB)
>> ├── fum_growth_arbiter.py
   │   (LOC: 148, Size: 6.2 KB)
>> ├── fum_sie.py
   │   (LOC: 298, Size: 13.1 KB)
>> ├── fum_structural_homeostasis.py
   │   (LOC: 166, Size: 6.8 KB)
>> ├── global_system.py
   │   (LOC: 251, Size: 9.8 KB)
   ├── guards/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   └── invariants.py
   │       (LOC: 305, Size: 9.5 KB)
   ├── memory/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── __init__.py
   │   │   (LOC: 26, Size: 855 B)
>> │   ├── engram_io.py
   │   │   (LOC: 440, Size: 16.3 KB)
>> │   └── field.py
   │       (LOC: 362, Size: 12.2 KB)
>> ├── metrics.py
   │   (LOC: 115, Size: 4.3 KB)
   ├── neuroplasticity/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── __init__.py
   │   │   (LOC: 9, Size: 373 B)
>> │   ├── gdsp.py
   │   │   (LOC: 531, Size: 22.3 KB)
>> │   └── revgsp.py
   │       (LOC: 319, Size: 12.3 KB)
   ├── primitives/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   └── dsu.py
   │       (LOC: 101, Size: 3.3 KB)
   ├── proprioception/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── events.py
   │   │   (LOC: 486, Size: 15.5 KB)
>> │   └── territory.py
   │       (LOC: 184, Size: 6.4 KB)
>> ├── sie_v2.py
   │   (LOC: 100, Size: 3.9 KB)
>> ├── signals.py
   │   (LOC: 326, Size: 10.0 KB)
>> ├── sparse_connectome.py
   │   (LOC: 706, Size: 28.5 KB)
   ├── substrate/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   ├── growth_arbiter.py
   │   │   (LOC: 108, Size: 4.6 KB)
>> │   ├── neurogenesis.py
   │   │   (LOC: 112, Size: 6.0 KB)
>> │   ├── structural_homeostasis.py
   │   │   (LOC: 92, Size: 3.9 KB)
>> │   └── substrate.py
   │       (LOC: 240, Size: 10.4 KB)
   ├── tests/
>> │   ├── README.md
   │   │   (LOC: 0, Size: 0 B)
>> │   └── test_conservation_flux.py
   │       (LOC: 96, Size: 3.2 KB)
>> ├── text_utils.py
   │   (LOC: 122, Size: 4.0 KB)
>> ├── visualizer.py
   │   (LOC: 89, Size: 3.3 KB)
>> ├── void_b1.py
   │   (LOC: 377, Size: 13.0 KB)
>> └── void_dynamics_adapter.py
       (LOC: 71, Size: 3.1 KB)]]></ascii_map>
  <files>
    <file>
      <path>README.md</path>
      <content><![CDATA[### fum_rt/core/README.md]]></content>
    </file>
    <file>
      <path>Void_Debt_Modulation.py</path>
      <content><![CDATA["""
FUM Universal Domain Modulation System
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

Universal derivation of domain_modulation factors based on void debt theory
and learning stability principles. This replaces arbitrary scaling with 
mathematically derived modulation factors.
"""
import numpy as np
from Void_Equations import get_universal_constants

class VoidDebtModulation:
    """Class to derive domain modulation factors from void debt principles."""
    
    def __init__(self):
        self.constants = get_universal_constants()
        self.ALPHA = self.constants['ALPHA']  # e.g., 0.25
        self.BETA = self.constants['BETA']    # e.g., 0.1
        self.VOID_DEBT_RATIO = self.BETA / self.ALPHA  # e.g., 0.4

    def get_universal_domain_modulation(self, physics_domain, target_sparsity_pct=None):
        """
        Derive domain modulation factor from universal void debt principles.
        
        Args:
            physics_domain: One of 'quantum', 'standard_model', 'dark_matter', 
                        'biology_consciousness', 'cosmogenesis', 'higgs'
            target_sparsity_pct: Target sparsity percentage for this domain
        
        Returns:
            domain_modulation: Universal scaling factor
        """
        constants = get_universal_constants()
        ALPHA, BETA = constants['ALPHA'], constants['BETA']
        
        # Domain-specific target sparsities (from empirical physics)
        domain_targets = {
            'quantum': 15.0,           # Low sparsity due to wave-particle duality
            'standard_model': 22.0,    # Moderate sparsity for gauge interactions  
            'dark_matter': 27.0,       # High sparsity matching cosmic DM density
            'biology_consciousness': 20.0,  # Biological complexity patterns
            'cosmogenesis': 84.0,      # Very high sparsity from inherited debt
            'higgs': 80.0             # High sparsity due to symmetry breaking
        }
        
        if target_sparsity_pct is None:
            target_sparsity_pct = domain_targets.get(physics_domain, 25.0)
        
        # Universal void debt derivation formula (from our 22.2% error method)
        # modulation = 1.0 + (sparsity²)/(BETA/ALPHA)
        sparsity_fraction = target_sparsity_pct / 100.0
        void_debt_ratio = BETA / ALPHA  # = 0.1 / 0.25 = 0.4
        
        domain_modulation = 1.0 + (sparsity_fraction ** 2) / void_debt_ratio
        
        return {
            'domain': physics_domain,
            'target_sparsity_pct': target_sparsity_pct,
            'domain_modulation': domain_modulation,
            'void_debt_ratio': void_debt_ratio,
            'derivation_method': 'universal_void_debt',
            'formula': 'modulation = 1.0 + (sparsity²)/(β/α)'
        }

    def get_all_domain_modulations(self):
        """Get all domain modulation factors for systematic comparison."""
        domains = ['quantum', 'standard_model', 'dark_matter', 
                'biology_consciousness', 'cosmogenesis', 'higgs']
        
        modulations = {}
        for domain in domains:
            modulations[domain] = self.get_universal_domain_modulation(domain)
        
        return modulations

    def print_modulation_table(self):
        """Print formatted table of all domain modulation factors."""
        modulations = self.get_all_domain_modulations()
        
        print("="*80)
        print("FUM UNIVERSAL DOMAIN MODULATION FACTORS")
        print("Derived from Void Debt Theory: modulation = 1.0 + (sparsity²)/(β/α)")
        print("="*80)
        print(f"{'Domain':<20} {'Target %':<10} {'Modulation':<12} {'Formula Application'}")
        print("-"*80)
        
        for domain, data in modulations.items():
            domain_display = domain.replace('_', ' ').title()
            target = data['target_sparsity_pct']
            mod = data['domain_modulation']
            formula_app = f"1.0 + ({target/100:.2f}²)/0.4"
            print(f"{domain_display:<20} {target:<10.1f} {mod:<12.3f} {formula_app}")
        
        print("-"*80)
        print("Note: These factors emerge from universal learning stability,")
        print("not arbitrary physics assumptions. Same math governs cognition & cosmos.")
        print("="*80)

    def validate_modulation_consistency(self):
        """Validate that our modulation factors are consistent with physics."""
        modulations = self.get_all_domain_modulations()
        
        # Extract just the modulation values
        values = [data['domain_modulation'] for data in modulations.values()]
        domains = list(modulations.keys())
        
        print("\n=== DOMAIN MODULATION VALIDATION ===")
        print(f"Range: {min(values):.3f} to {max(values):.3f}")
        print(f"Mean: {np.mean(values):.3f} ± {np.std(values):.3f}")
        
        # Physics consistency checks
        qm_mod = modulations['quantum']['domain_modulation']
        cos_mod = modulations['cosmogenesis']['domain_modulation']
        
        print(f"\nPhysics Consistency:")
        print(f"• Quantum < Cosmic: {qm_mod:.3f} < {cos_mod:.3f} = {qm_mod < cos_mod}")
        print(f"• Reasonable range: All factors 1.0-2.0 = {all(1.0 <= v <= 2.0 for v in values)}")
        
        return {
            'values': values,
            'domains': domains,
            'range': (min(values), max(values)),
            'mean': np.mean(values),
            'std': np.std(values),
            'physics_consistent': qm_mod < cos_mod and all(1.0 <= v <= 2.0 for v in values)
        }

if __name__ == "__main__":
    modulator = VoidDebtModulation()
    modulator.print_modulation_table()
    validation = modulator.validate_modulation_consistency()
    
    if validation['physics_consistent']:
        print("\n✓ VALIDATION PASSED: Domain modulations are physically consistent")
    else:
        print("\n⚠ VALIDATION FAILED: Domain modulations need adjustment")]]></content>
    </file>
    <file>
      <path>Void_Equations.py</path>
      <content><![CDATA["""
FUM Void Dynamics Library
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This library contains the main functions governing the Void Dynamics Model.
These functions represent the unchanging laws of the system.

These functions demonstrate the two opposing, yet synergistic forces that
drive void dynamics across all scales of the model. Void dynamics are inherently 
stochastic and generative. 
"""
import numpy as np

# ===== UNIVERSAL PHYSICAL CONSTANTS =====
# These are NOT arbitrary - they come from VDM learning stability
# requirements
ALPHA = 0.25      # Universal learning rate for RE-VGSP (Resonance-Enhanced dynamics)
BETA = 0.1        # Universal plasticity rate for GDSP (Goal-Directed dynamics)
F_REF = 0.02      # Universal reference frequency for time modulation
PHASE_SENS = 0.5  # Universal phase sensitivity for time modulation

def delta_re_vgsp(W, t, alpha=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
    """
    Void Alpha Function: Synchronizes with Void Omega
    Universal function for Resonance-Enhanced Valence-Gated Synaptic Plasticity.
    Models the fractal energy drain/pull (learning rule).
    
    Args:
        W: Current void state
        t: Time step
        alpha: Learning rate (defaults to universal constant)
        f_ref: Reference frequency (defaults to universal constant)
        phase_sens: Phase sensitivity (defaults to universal constant)
        use_time_dynamics: Enable time modulation
        domain_modulation: Domain-specific scaling factor
    """
    # Use universal constants as defaults
    if alpha is None:
        alpha = ALPHA
    if f_ref is None:
        f_ref = F_REF
    if phase_sens is None:
        phase_sens = PHASE_SENS
    
    # Apply domain modulation to alpha
    effective_alpha = alpha * domain_modulation
    
    noise = np.random.uniform(-0.02, 0.02)
    base_delta = effective_alpha * W * (1 - W) + noise
    
    if use_time_dynamics:
        phase = np.sin(2 * np.pi * f_ref * t)
        return base_delta * (1 + phase_sens * phase)
    return base_delta

def delta_gdsp(W, t, beta=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
    """
    Void Omega Function: Synchronizes with Void Alpha
    Universal function for Goal-Directed Structural Plasticity.
    Models the weak closure for persistent voids (structural rule).
    
    Args:
        W: Current void state
        t: Time step
        beta: Plasticity rate (defaults to universal constant)
        f_ref: Reference frequency (defaults to universal constant)
        phase_sens: Phase sensitivity (defaults to universal constant)
        use_time_dynamics: Enable time modulation
        domain_modulation: Domain-specific scaling factor
    """
    # Use universal constants as defaults
    if beta is None:
        beta = BETA
    if f_ref is None:
        f_ref = F_REF
    if phase_sens is None:
        phase_sens = PHASE_SENS
    
    # Apply domain modulation to beta
    effective_beta = beta * domain_modulation
    
    base_delta = -effective_beta * W
    
    if use_time_dynamics:
        phase = np.sin(2 * np.pi * f_ref * t)
        return base_delta * (1 + phase_sens * phase)
    return base_delta

# ===== SIMPLIFIED INTERFACES FOR COMMON USE CASES =====

def universal_void_dynamics(W, t, domain_modulation=1.0, use_time_dynamics=True):
    """
    Simplified interface that applies both RE-VGSP and GDSP with universal constants.
    Returns combined delta for single-step evolution.
    """
    dw_re = delta_re_vgsp(W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
    dw_gdsp = delta_gdsp(W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
    return dw_re + dw_gdsp

def get_universal_constants():
    """Returns the universal constants as a dictionary."""
    return {
        'ALPHA': ALPHA,
        'BETA': BETA,
        'F_REF': F_REF,
        'PHASE_SENS': PHASE_SENS
    }]]></content>
    </file>
    <file>
      <path>__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>adc.py</path>
      <content><![CDATA[# adc.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Active Domain Cartography (ADC) - incremental, void-faithful reducer.

Design
- ADC consumes compact Observation events from the void-walker announcement bus.
- It never inspects raw W or dense adjacency; all inputs are announcements.
- Territories and boundaries are updated locally per event (O(1) per event).
- Provides lightweight map metrics for Nexus logging and self-speak decisions.

Territories
- Coarse "concept regions" indexed by a composite key (domain_hint, coverage_id).
- Each territory tracks EWMA stats (w_mean/var, s_mean), a mass (support), a confidence,
  and a TTL (decays unless reinforced).

Boundaries
- Abstract edges between territories tracking cut_strength EWMA, churn, and TTL.

Events
- region_stat: assimilation into a nearest territory (or create).
- boundary_probe: update boundary signal (if 2+ territories exist).
- cycle_hit: bumps a cycle counter (B1 proxy) surfaced in map metrics.
- novel_frontier: creates/boosts a new/sibling territory with low initial confidence.

This is a minimal, safe baseline. You can evolve the territory identity function
from (domain_hint, coverage_id) to a learned centroid distance when you add one.
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Dict, Tuple, Optional, Iterable
import math
from .announce import Observation


@dataclass
class _EWMA:
    """Numerically stable EWMA with alpha in (0,1]."""
    alpha: float
    mean: float = 0.0
    var: float = 0.0
    init: bool = False

    def update(self, x: float):
        a = self.alpha
        if not self.init:
            self.mean = float(x)
            self.var = 0.0
            self.init = True
            return
        # Welford-style EMA
        delta = float(x) - self.mean
        self.mean += a * delta
        self.var = (1 - a) * (self.var + a * delta * delta)


@dataclass
class Territory:
    key: Tuple[str, int]  # (domain_hint, coverage_id)
    id: int
    mass: float = 0.0
    conf: float = 0.0
    ttl: int = 120
    w_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.15))
    s_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.15))

    def reinforce(self, w_mean: float, s_mean: float, add_mass: float, add_conf: float, ttl_init: int):
        self.w_stats.update(w_mean)
        self.s_stats.update(s_mean)
        self.mass += max(0.0, add_mass)
        self.conf = min(1.0, self.conf + max(0.0, add_conf))
        self.ttl = max(self.ttl, int(ttl_init))


@dataclass
class Boundary:
    a: int
    b: int
    cut_stats: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.2))
    churn: _EWMA = field(default_factory=lambda: _EWMA(alpha=0.2))
    ttl: int = 120

    def reinforce(self, cut_strength: float, ttl_init: int):
        prev = float(self.cut_stats.mean) if self.cut_stats.init else 0.0
        self.cut_stats.update(cut_strength)
        self.churn.update(abs(float(self.cut_stats.mean) - prev))
        self.ttl = max(self.ttl, int(ttl_init))


class ADC:
    def __init__(self, r_attach: float = 0.25, ttl_init: int = 120, split_patience: int = 6):
        self.r_attach = float(r_attach)
        self.ttl_init = int(max(1, ttl_init))
        self.split_patience = int(max(1, split_patience))

        self._territories: Dict[Tuple[str, int], Territory] = {}
        self._id_seq: int = 1
        self._boundaries: Dict[Tuple[int, int], Boundary] = {}
        self._frontier_counter: Dict[Tuple[str, int], int] = {}
        self._cycle_events: int = 0  # accumulated since last metrics call

    # --- Public API ---

    def update_from(self, observations: Iterable[Observation]) -> None:
        for o in observations:
            kind = getattr(o, "kind", "")
            if kind == "region_stat":
                self._accumulate_region(o)
            elif kind == "boundary_probe":
                self._accumulate_boundary(o)
            elif kind == "cycle_hit":
                self._note_cycle(o)
            elif kind == "novel_frontier":
                self._note_frontier(o)
        self._decay()

    def get_metrics(self) -> Dict[str, float]:
        """Return a small metrics dict and reset transient counters."""
        terr_count = len(self._territories)
        bnd_count = len(self._boundaries)
        cycles = self._cycle_events
        self._cycle_events = 0
        return {
            "adc_territories": int(terr_count),
            "adc_boundaries": int(bnd_count),
            "adc_cycle_hits": int(cycles),
        }

    # --- Internals ---

    def _territory_for(self, domain_hint: str, cov_id: int) -> Territory:
        key = (str(domain_hint or ""), int(cov_id))
        t = self._territories.get(key)
        if t is None:
            t = Territory(key=key, id=self._id_seq, ttl=self.ttl_init)
            self._id_seq += 1
            self._territories[key] = t
        return t

    def _accumulate_region(self, o: Observation):
        t = self._territory_for(o.domain_hint, o.coverage_id)
        # Attach if "close": here closeness is discretized by coverage bin match via key.
        # When you add a real centroid, use a distance threshold compared to r_attach.
        add_mass = max(1.0, float(len(o.nodes)))
        add_conf = 0.02
        t.reinforce(w_mean=float(o.w_mean), s_mean=float(o.s_mean),
                    add_mass=add_mass, add_conf=add_conf, ttl_init=self.ttl_init)

    def _accumulate_boundary(self, o: Observation):
        # Pick two "closest" territories by coverage bin neighborhood:
        # Here we approximate by choosing (domain_hint, cov_id) and (domain_hint, cov_id±1)
        t1 = self._territory_for(o.domain_hint, o.coverage_id)
        # neighbor bin
        neighbor_cov = int(max(0, min(9, int(o.coverage_id + (1 if (o.coverage_id % 2 == 0) else -1)))))
        t2 = self._territory_for(o.domain_hint, neighbor_cov)
        a, b = (t1.id, t2.id) if t1.id < t2.id else (t2.id, t1.id)
        key = (a, b)
        bnd = self._boundaries.get(key)
        if bnd is None:
            bnd = Boundary(a=a, b=b, ttl=self.ttl_init)
            self._boundaries[key] = bnd
        bnd.reinforce(float(o.cut_strength), ttl_init=self.ttl_init)

    def _note_cycle(self, o: Observation):
        self._cycle_events += 1
        # Optionally: attach cycles to a territory using coverage bin
        t = self._territory_for(o.domain_hint, o.coverage_id)
        t.conf = min(1.0, t.conf + 0.01)
        t.ttl = max(t.ttl, self.ttl_init)

    def _note_frontier(self, o: Observation):
        key = (str(o.domain_hint or ""), int(o.coverage_id))
        cnt = self._frontier_counter.get(key, 0) + 1
        self._frontier_counter[key] = cnt
        if cnt >= self.split_patience:
            # Create or boost a new sibling territory by nudging coverage bin
            sib_cov = int(max(0, min(9, int(o.coverage_id + 1))))
            sib = self._territory_for(o.domain_hint, sib_cov)
            sib.reinforce(w_mean=float(o.w_mean), s_mean=float(o.s_mean),
                          add_mass=max(1.0, float(len(o.nodes))), add_conf=0.05, ttl_init=self.ttl_init)
            self._frontier_counter[key] = 0

    def _decay(self):
        # TTL decay and garbage collection for stale items with low confidence/mass
        drop_terr = []
        for key, t in self._territories.items():
            t.ttl -= 1
            if t.ttl <= 0 and (t.conf < 0.05 or t.mass < 5.0):
                drop_terr.append(key)
        for key in drop_terr:
            self._territories.pop(key, None)

        drop_bnd = []
        for key, b in self._boundaries.items():
            b.ttl -= 1
            if b.ttl <= 0 and (not b.cut_stats.init or b.cut_stats.mean < 1e-4):
                drop_bnd.append(key)
        for key in drop_bnd:
            self._boundaries.pop(key, None)]]></content>
    </file>
    <file>
      <path>announce.py</path>
      <content><![CDATA[# announce.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event schema for the void-walker announcement bus (Active Domain Cartography input).

Blueprint alignment:
- Use void equations for traversal/measuring: walkers traverse using your RE-VGSP/GDSP deltas
  and publish compact observations only (no W dumps).
- ADC consumes only these observations to maintain territories/boundaries incrementally.
- This keeps introspection cost proportional to the number of announcements, not to N.

Kinds:
- "region_stat": aggregate stats for a small visited set (mean/var over W, mean coupling S_ij)
- "boundary_probe": evidence of a low-coupling cut between neighborhoods (candidate boundary)
- "cycle_hit": a loop was closed during a walk (B1 proxy event)
- "novel_frontier": sustained novelty ridge / new subdomain frontier

Notes:
- All floats are plain Python floats to keep JSON-friendly if logged.
- nodes is small (sampled IDs visited during the walk). Keep it compact (<= ~64).
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Dict, Any


@dataclass
class Observation:
    tick: int
    kind: str  # "region_stat" | "boundary_probe" | "cycle_hit" | "novel_frontier"
    # Small, representative subset of node ids touched during the walk or boundary sample
    nodes: List[int] = field(default_factory=list)

    # Optional centroid in an embedding space if available (not required)
    centroid: Optional[Tuple[float, float, float]] = None

    # Aggregate stats over the local visited set
    w_mean: float = 0.0
    w_var: float = 0.0
    s_mean: float = 0.0  # mean positive coupling encountered during the walk

    # Boundary-specific signal (strength of cut across a small boundary sample)
    cut_strength: float = 0.0

    # Cycle-specific fields
    loop_len: int = 0
    loop_gain: float = 0.0  # accumulated positive transition weights along the loop

    # Coverage bin for ADC scheduling and map updates (e.g., int(vt_coverage*10))
    coverage_id: int = 0

    # Optional hint from the domain/cartographer
    domain_hint: str = ""

    # Extra metadata bag for future-proofing (small; JSON-serializable only)
    meta: Dict[str, Any] = field(default_factory=dict)


def validate_observation(o: Observation) -> bool:
    """Light sanity checks to avoid corrupting the bus/ADC with malformed events."""
    if not isinstance(o.tick, int) or o.tick < 0:
        return False
    if o.kind not in ("region_stat", "boundary_probe", "cycle_hit", "novel_frontier"):
        return False
    if not isinstance(o.nodes, list):
        return False
    # keep nodes tiny to avoid large payloads by accident
    if len(o.nodes) > 256:
        return False
    return True]]></content>
    </file>
    <file>
      <path>bus.py</path>
      <content><![CDATA[# bus.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Lock-free announcement bus for void-walker observations (ADC input).

Blueprint alignment:
- Walkers traverse with void equations and publish compact Observation packets.
- ADC consumes only these announcements to maintain territories/boundaries incrementally.
- Cost is proportional to number of announcements, not to graph size.

Usage
- Producer (connectome walkers):
    bus.publish(observation)
- Consumer (Nexus/ADC loop):
    batch = bus.drain(max_items=2048)

Behavior
- Bounded deque with overwrite-on-full semantics (drop oldest) to keep runtime stable.
"""

from __future__ import annotations
from collections import deque
from typing import Deque, List, Any, Optional


class AnnounceBus:
    """Bounded, overwrite-on-full FIFO for Observation events."""
    def __init__(self, capacity: int = 65536):
        self._q: Deque[Any] = deque(maxlen=int(max(1, capacity)))

    @property
    def capacity(self) -> int:
        return int(self._q.maxlen or 0)

    def size(self) -> int:
        return len(self._q)

    def publish(self, obs: Any) -> None:
        """
        Append an event; when full, the oldest is dropped automatically.
        This keeps the system stable under load without backpressure deadlocks.
        """
        self._q.append(obs)

    def drain(self, max_items: int = 2048) -> List[Any]:
        """
        Pop up to max_items from the left, returning them in arrival order.
        """
        n = min(int(max_items), len(self._q))
        out: List[Any] = []
        append = out.append
        for _ in range(n):
            append(self._q.popleft())
        return out

    def clear(self) -> None:
        self._q.clear()]]></content>
    </file>
    <file>
      <path>connectome.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os as _os

# Dense backend is validation-only; forbid import unless FORCE_DENSE is explicitly set.
# This prevents accidental dense usage in runtime paths and enforces sparse-first policy.
if str(_os.getenv("FORCE_DENSE", "0")).strip().lower() not in ("1", "true", "yes", "on", "y", "t"):
    raise RuntimeError("Dense connectome is validation-only. Set FORCE_DENSE=1 to enable for tests.")

import numpy as np
import networkx as nx
from .void_dynamics_adapter import universal_void_dynamics, delta_re_vgsp, delta_gdsp
from .fum_structural_homeostasis import perform_structural_homeostasis
from .announce import Observation  # event schema for ADC bus

# TODO THIS ENTIRE FILE IS DEPRECATED AND WILL NEED TO BE REMOVED. USE /mnt/ironwolf/git/Void_Unity_Proofs/fum_rt/core/sparse_connectome.py
class Connectome:
    def __init__(self, N: int, k: int, seed: int = 0, threshold: float = 0.15, lambda_omega: float = 0.1, candidates: int = 64, structural_mode: str = "alias", traversal_walkers: int = 256, traversal_hops: int = 3, bundle_size: int = 3, prune_factor: float = 0.10):
        self.N = N
        self.k = k
        self.rng = np.random.default_rng(seed)
        self.threshold = threshold  # active synapse threshold on edge weights
        self.lambda_omega = lambda_omega  # penalty weight for structural plasticity (Ω)
        self.candidates = int(max(1, candidates))  # candidate samples per node for structure search
        self.structural_mode = structural_mode      # "alias" (default) | "dense"
        # Node state
        self.W = self.rng.uniform(0.0, 1.0, size=(N,)).astype(np.float32)
        # Start with empty topology; growth is dictated purely by Void Equations
        self.A = np.zeros((N, N), dtype=np.int8)
        # Graph objects are built on demand for visualization/metrics
        self.G = nx.Graph()
        # Edge weights derived from node states initially (zeros)
        self.E = self._edge_weights_from_W()
        # Void‑equation traversal configuration (Blueprint: "void equations for traversal and measuring")
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))
        # Findings propagated each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Void‑equation traversal configuration (Blueprint: "void equations for traversal and measuring")
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))
        # Findings propagated each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Local tick counter for announcements (incremented each step)
        self._tick = 0
        # Homeostasis tuning (Blueprint Rule 4/4.1)
        self.bundle_size = int(max(1, bundle_size))
        self.prune_factor = float(max(0.0, prune_factor))
        # External stimulation buffer (deterministic symbol→group)
        self._stim = np.zeros(N, dtype=np.float32)
        self._stim_decay = 0.90

    def _ring_lattice(self, N, k):
        # Each node connected to k nearest neighbors (k must be even)
        k = max(2, k + (k % 2))
        A = np.zeros((N, N), dtype=np.int8)
        for i in range(N):
            for d in range(1, k//2 + 1):
                j = (i + d) % N
                h = (i - d) % N
                A[i, j] = 1
                A[i, h] = 1
        return A

    def _edge_weights_from_W(self):
        # Symmetric edge weights derived from node states
        E = (np.outer(self.W, self.W) * self.A).astype(np.float32)
        return E

    def stimulate_indices(self, idxs, amp: float = 0.05):
        """
        Deterministic stimulus injection:
        - idxs: iterable of neuron indices to stimulate
        - amp: additive boost to the stimulus buffer (decays each tick)
        """
        try:
            if idxs is None:
                return
            idxs = np.asarray(list(set(int(i) % self.N for i in idxs)), dtype=np.int64)
            if idxs.size == 0:
                return
            # accumulate into a decaying stimulus buffer that feeds ReLU(Δalpha)
            self._stim[idxs] = np.clip(self._stim[idxs] + float(amp), 0.0, 1.0)
            # small immediate bump to W to seed associations
            self.W[idxs] = np.clip(self.W[idxs] + 0.01 * float(amp), 0.0, 1.0)
        except Exception:
            # maintain runtime continuity
            pass

    # --- Alias sampler (Vose) to sample candidates ~ ReLU(Δalpha) in O(N) build + O(1) draw ---
    def _build_alias(self, p: np.ndarray):
        n = p.size
        if n == 0:
            return np.array([], dtype=np.float32), np.array([], dtype=np.int32)
        p = p.astype(np.float64, copy=False)
        s = float(p.sum())
        if s <= 0:
            p = np.full(n, 1.0 / n, dtype=np.float64)
        else:
            p = p / s
        prob = np.zeros(n, dtype=np.float64)
        alias = np.zeros(n, dtype=np.int32)
        scaled = p * n
        small = [i for i, v in enumerate(scaled) if v < 1.0]
        large = [i for i, v in enumerate(scaled) if v >= 1.0]
        while small and large:
            s_idx = small.pop()
            l_idx = large.pop()
            prob[s_idx] = scaled[s_idx]
            alias[s_idx] = l_idx
            scaled[l_idx] = scaled[l_idx] - (1.0 - prob[s_idx])
            if scaled[l_idx] < 1.0:
                small.append(l_idx)
            else:
                large.append(l_idx)
        for i in large:
            prob[i] = 1.0
        for i in small:
            prob[i] = 1.0
        return prob.astype(np.float32), alias

    def _alias_draw(self, prob: np.ndarray, alias: np.ndarray, s: int):
        n = prob.size
        if n == 0 or s <= 0:
            return np.array([], dtype=np.int64)
        k = self.rng.integers(0, n, size=s, endpoint=False)
        u = self.rng.random(s)
        choose_alias = (u >= prob[k])
        out = k.copy()
        out[choose_alias] = alias[k[choose_alias]]
        return out.astype(np.int64)

    def _void_traverse(self, a: np.ndarray, om: np.ndarray):
        """
        Continuous void‑equation traversal (Blueprint mandate: use void equations for traversal/measuring)
        - Seeds walkers proportionally to ReLU(Δalpha)
        - Transition weight to neighbor j: max(0, a[i]*a[j] - λ*|ω_i-ω_j|)
        - Simulates small number of hops per walker; accumulates visit histogram
        Complexity: ~O(N*k + walkers*hops). k is current degree bound per node.
        Produces findings propagated to metrics/SIE/ADC: coverage, entropy, unique_nodes, mean_a/omega.
        Also publishes compact Observation events to the ADC bus if present.
        """
        N = self.N
        walkers = self.traversal_walkers
        hops = self.traversal_hops

        # Seed distribution ~ a (ReLU(Δalpha))
        prob, alias = self._build_alias(a)
        seeds = self._alias_draw(prob, alias, walkers)
        visit = np.zeros(N, dtype=np.int32)

        # Optional ADC bus
        bus = getattr(self, "bus", None)
        tick = int(getattr(self, "_tick", 0))

        # Event accumulators (kept small)
        sample_cap = 64
        sel_w_sum = 0.0
        sel_steps = 0
        cycle_events = 0
        sample_nodes = set()

        for s in seeds:
            i = int(s)
            cur = i
            seen = {cur: 0}  # for simple loop detection within this walk
            path = [cur]
            for step_idx in range(1, hops + 1):
                nbrs = np.nonzero(self.A[cur])[0]
                if nbrs.size == 0:
                    break
                # weights by void affinity
                w = a[cur] * a[nbrs] - self.lambda_omega * np.abs(om[cur] - om[nbrs])
                w = np.clip(w, 0.0, None)
                if np.all(w <= 0):
                    break
                # sample next neighbor proportional to w
                wp = w / (w.sum() + 1e-12)
                r = self.rng.random()
                cdf = np.cumsum(wp)
                idx = int(np.searchsorted(cdf, r, side="right"))
                nxt = int(nbrs[min(idx, nbrs.size - 1)])
                visit[nxt] += 1

                # Accumulate event stats
                sel_w = float(w[min(idx, nbrs.size - 1)])
                sel_w_sum += max(0.0, sel_w)
                sel_steps += 1
                if len(sample_nodes) < sample_cap:
                    sample_nodes.add(nxt)

                # Simple cycle detection: return to a previously visited node on this walk
                if nxt in seen:
                    cycle_events += 1
                    # Publish cycle_hit immediately if bus is present (use lightweight payload)
                    if bus is not None:
                        try:
                            obs = Observation(
                                tick=tick,
                                kind="cycle_hit",
                                nodes=[cur, nxt],
                                w_mean=float(a.mean()),
                                w_var=float(a.var()),
                                s_mean=0.0,
                                loop_len=int(len(path) - seen[nxt] + 1),
                                loop_gain=float(sel_w),
                                coverage_id=0,
                                domain_hint=""
                            )
                            bus.publish(obs)
                        except Exception:
                            pass
                else:
                    seen[nxt] = step_idx
                    path.append(nxt)

                cur = nxt

        total_visits = int(visit.sum())
        unique = int(np.count_nonzero(visit))
        coverage = float(unique) / float(max(1, N))
        if total_visits > 0:
            p = visit.astype(np.float64) / float(total_visits)
            p = p[p > 0]
            vt_entropy = float(-(p * np.log(p)).sum())
        else:
            vt_entropy = 0.0

        # Store findings for external consumers (Nexus metrics/SIE/ADC)
        self.findings = {
            "vt_visits": total_visits,
            "vt_unique": unique,
            "vt_coverage": coverage,
            "vt_entropy": vt_entropy,
            "vt_walkers": float(walkers),
            "vt_hops": float(hops),
            "a_mean": float(a.mean()),
            "omega_mean": float(om.mean()),
        }

        # Publish a compact region_stat at end of traversal
        if bus is not None:
            try:
                s_mean = float(sel_w_sum / max(1, sel_steps))
                cov_id = int(min(9, max(0, int(coverage * 10.0))))
                obs = Observation(
                    tick=tick,
                    kind="region_stat",
                    nodes=list(sample_nodes),
                    w_mean=float(self.W.mean()),
                    w_var=float(self.W.var()),
                    s_mean=s_mean,
                    coverage_id=cov_id,
                    domain_hint=""
                )
                bus.publish(obs)
            except Exception:
                pass
    
            # Increment local tick for announcement timestamps
            try:
                self._tick += 1
            except Exception:
                pass


    def step(self, t: float, domain_modulation: float, sie_drive: float = 1.0, use_time_dynamics: bool=True):
        """
        Apply one update tick driven entirely by Void Equations:
        - Structural growth/rewiring: candidates sampled via alias table built from ReLU(Δalpha)
        - Affinity S_ij = ReLU(Δalpha_i) * ReLU(Δalpha_j) - λ * |Δomega_i - Δomega_j|
        - Top‑k neighbors per node; symmetric adjacency
        - Node field update via universal_void_dynamics, multiplicatively gated by SIE valence (Rule 3)
        """
        # 1) Compute elemental deltas from your void equations
        d_alpha = delta_re_vgsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        d_omega = delta_gdsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        a = np.maximum(0.0, d_alpha.astype(np.float32))  # ReLU(Δalpha)
        om = d_omega.astype(np.float32)
        # External stimulation: add and decay deterministic symbol→group drive
        try:
            a = np.clip(a + self._stim, 0.0, None)
            self._stim *= getattr(self, "_stim_decay", 0.90)
        except Exception:
            pass

        # 2) Build candidate sampler ~ a (ReLU(Δalpha))
        prob, alias = self._build_alias(a)

        # 3) Per-node top-k neighbors by sampling; complexity ~ O(N * candidates)
        N = self.N
        k = int(max(1, self.k))
        s = int(max(self.candidates, 2 * k))
        A_new = np.zeros((N, N), dtype=np.int8)

        if self.structural_mode == "dense" and N <= 4096:
            # Exact affinity (validation/small N)
            S = a[:, None] * a[None, :] - self.lambda_omega * np.abs(om[:, None] - om[None, :])
            np.fill_diagonal(S, -np.inf)
            idx_topk = np.argpartition(S, -k, axis=1)[:, -k:]
            rows = np.repeat(np.arange(N), k)
            cols = idx_topk.reshape(-1)
            A_new[rows, cols] = 1
        else:
            # Alias sampling per node (default, efficient, void‑guided traversal)
            for i in range(N):
                js = self._alias_draw(prob, alias, s)
                # drop self and dupes
                js = js[js != i]
                if js.size == 0:
                    continue
                js = np.unique(js)  # s is small; set semantics OK
                # score by void affinity
                Si = a[i] * a[js] - self.lambda_omega * np.abs(om[i] - om[js])
                take = min(k, Si.size)
                idx = np.argpartition(Si, -take)[-take:]
                nbrs = js[idx]
                A_new[i, nbrs] = 1

        # Undirected symmetrization
        A_new = np.maximum(A_new, A_new.T)
        self.A = A_new

        # 4) Update node field with combined universal dynamics, gated by SIE valence (in [0,1])
        dW = universal_void_dynamics(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        dW = (float(max(0.0, min(1.0, sie_drive))) * dW).astype(np.float32)
        self.W = np.clip(self.W + dW, 0.0, 1.0)

        # 4.1) SIE v2 intrinsic reward/valence from W and dW (void-native)
        try:
            if not hasattr(self, "_sie2"):
                from .sie_v2 import SIECfg, SIEState
                self._sie2 = SIEState(self.N, SIECfg())
            from .sie_v2 import sie_step
            r_vec, v01 = sie_step(self._sie2, self.W, dW)
            self._last_sie2_reward = float(np.mean(r_vec))
            self._last_sie2_valence = float(v01)
        except Exception:
            pass

        # 5) Edge weights follow nodes on the updated topology
        self.E = self._edge_weights_from_W()

        # 5.1) Stage‑1 cohesion repair + pruning via structural homeostasis (void‑affinity)
        try:
            labels = self.component_labels()
            perform_structural_homeostasis(
                self,
                labels=labels,
                d_alpha=d_alpha,
                d_omega=d_omega,
                lambda_omega=self.lambda_omega,
                bundle_size=int(getattr(self, "bundle_size", 3)),
                prune_factor=float(getattr(self, "prune_factor", 0.10))
            )
        except Exception:
            # Keep runtime alive even if homeostasis step fails
            pass
        
        # 6) Continuous void‑equation traversal to propagate findings for Global System
        try:
            self._void_traverse(a, om)
        except Exception:
            # Keep system alive even if traversal fails; findings may be stale
            pass

    def active_edge_count(self):
        return int((self.E > self.threshold).sum() // 2)  # undirected

    def connected_components(self):
        # Cohesion via topology-only graph (Stage 1)
        G = nx.from_numpy_array(self.A.astype(int), create_using=nx.Graph)
        return nx.number_connected_components(G)
    
    def component_labels(self):
        # Labels for topology-only cohesion (Stage 1)
        G = nx.from_numpy_array(self.A.astype(int), create_using=nx.Graph)
        labels = np.zeros(self.N, dtype=int)
        for idx, comp in enumerate(nx.connected_components(G)):
            for n in comp:
                labels[int(n)] = idx
        return labels
    
    def cyclomatic_complexity(self):
        # For the active subgraph: cycles = E - N + C
        mask = (self.E > self.threshold)
        G_act = nx.from_numpy_array(mask.astype(int), create_using=nx.Graph)
        n = G_act.number_of_nodes()
        e = G_act.number_of_edges()
        c = nx.number_connected_components(G_act)
        return max(0, e - n + c)
    
    def snapshot_graph(self):
        # Return a NetworkX graph (active subgraph) for drawing
        mask = (self.E > self.threshold)
        G_act = nx.from_numpy_array(mask.astype(int), create_using=nx.Graph)
        return G_act
]]></content>
    </file>
    <file>
      <path>control_server.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

# control_server.py
# Lightweight local control server to expose a "Load Engram" button/page.
# - Serves a minimal HTML UI at http://127.0.0.1:<port>/
# - Accepts POST /api/load_engram with JSON {"path": "<engram file path>"}
# - Writes/updates runs/<ts>/phase.json with {"load_engram": "<path>"} so Nexus control plane will pick it up.

import os
import json
import threading
from http.server import HTTPServer, BaseHTTPRequestHandler
import socketserver
from urllib.parse import urlparse

_HTML = r'''<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FUM Control - Load Engram</title>
  <style>
    :root { --fg: #e6edf3; --bg: #0d1117; --muted: #8b949e; --accent: #2f81f7; --danger: #f85149; }
    body { background: var(--bg); color: var(--fg); font: 14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, Cantarell, Noto Sans, Helvetica Neue, Arial, sans-serif; margin: 0; padding: 0; }
    .wrap { max-width: 880px; margin: 32px auto; padding: 16px 20px; }
    h1 { margin: 0 0 16px 0; font-size: 20px; }
    p.note { color: var(--muted); }
    .card { border: 1px solid #30363d; border-radius: 8px; padding: 16px; margin: 16px 0; background: #161b22; }
    label { display: block; margin-bottom: 6px; color: var(--muted); }
    input[type=text] {
      width: 100%; padding: 10px 12px; border-radius: 6px; border: 1px solid #30363d; background: #0d1117; color: var(--fg);
    }
    .row { display: flex; gap: 8px; align-items: center; margin-top: 10px; }
    button {
      padding: 10px 16px; border-radius: 6px; border: 1px solid #30363d; background: var(--accent); color: white; cursor: pointer;
    }
    button:disabled { opacity: 0.6; cursor: not-allowed; }
    .status { margin-top: 12px; min-height: 20px; }
    .ok { color: #3fb950; }
    .err { color: var(--danger); }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, Liberation Mono, monospace; }
    .footer { margin-top: 24px; color: var(--muted); font-size: 12px; }
    code { background: #0b1220; padding: 2px 6px; border-radius: 4px; border: 1px solid #30363d; }
  </style>
</head>
<body>
  <div class="wrap">
    <h1>FUM Control - Load Engram</h1>
    <p class="note">Run directory: <span class="mono" id="runDir"></span></p>

    <div class="card">
      <label for="engram">Engram path (.h5 or .npz)</label>
      <input id="engram" type="text" placeholder="e.g. runs/20250811_155023/state_23220.h5" />
      <div class="row">
        <button id="btnLoad">Load Engram</button>
        <span class="mono" id="busy" style="display:none">loading…</span>
      </div>
      <div class="status" id="status"></div>
    </div>

    <div class="footer">
      The button sets <code>load_engram</code> in your run's <code>phase.json</code>; Nexus will hot-load it on the next poll and then clear the field.
    </div>
  </div>

  <script>
    const runDirSpan = document.getElementById('runDir');
    fetch('/api/status').then(r => r.json()).then(js => {
      runDirSpan.textContent = js.run_dir || '(unknown)';
    }).catch(() => { runDirSpan.textContent = '(unknown)'; });

    const el = (id) => document.getElementById(id);
    el('btnLoad').addEventListener('click', async () => {
      const path = el('engram').value.trim();
      const btn = el('btnLoad');
      const busy = el('busy');
      const status = el('status');
      status.textContent = '';
      status.className = 'status';
      if (!path) {
        status.textContent = 'Please enter a file path.';
        status.classList.add('err');
        return;
      }
      btn.disabled = true; busy.style.display = 'inline';
      try {
        const res = await fetch('/api/load_engram', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ path })
        });
        const js = await res.json().catch(() => ({}));
        if (res.ok && js.ok) {
          status.textContent = 'Queued load_engram: ' + (js.path || path);
          status.classList.add('ok');
        } else {
          status.textContent = 'Error: ' + (js.error || ('HTTP ' + res.status));
          status.classList.add('err');
        }
      } catch (err) {
        status.textContent = 'Request failed';
        status.classList.add('err');
      } finally {
        btn.disabled = false; busy.style.display = 'none';
      }
    });
  </script>
</body>
</html>
'''

class ThreadingHTTPServer(socketserver.ThreadingMixIn, HTTPServer):
    daemon_threads = True
    allow_reuse_address = True


class _Handler(BaseHTTPRequestHandler):
    # Server context attached at runtime: self.server.ctx = { 'run_dir': ..., 'phase_file': ... }

    def _json(self, code: int, obj: dict):
        try:
            payload = json.dumps(obj).encode("utf-8")
        except Exception:
            payload = b'{}'
        self.send_response(code)
        self.send_header("Content-Type", "application/json; charset=utf-8")
        self.send_header("Cache-Control", "no-store")
        self.end_headers()
        try:
            self.wfile.write(payload)
        except Exception:
            pass

    def _text(self, code: int, html: str):
        try:
            data = html.encode("utf-8")
        except Exception:
            data = b""
        self.send_response(code)
        self.send_header("Content-Type", "text/html; charset=utf-8")
        self.end_headers()
        try:
            self.wfile.write(data)
        except Exception:
            pass

    def do_GET(self):
        try:
            path = urlparse(self.path).path
        except Exception:
            path = "/"
        if path in ("/", "/index", "/index.html"):
            # Fill in run_dir client-side via /api/status
            return self._text(200, _HTML)
        if path == "/api/status":
            ctx = getattr(self.server, "ctx", {})
            run_dir = ctx.get("run_dir", "")
            return self._json(200, {"ok": True, "run_dir": run_dir})
        return self._json(404, {"ok": False, "error": "not_found"})

    def do_POST(self):
        try:
            path = urlparse(self.path).path
        except Exception:
            path = "/"
        if path != "/api/load_engram":
            return self._json(404, {"ok": False, "error": "not_found"})

        # parse JSON
        try:
            length = int(self.headers.get("Content-Length", "0"))
        except Exception:
            length = 0
        try:
            body = self.rfile.read(length) if length > 0 else b"{}"
        except Exception:
            body = b"{}"
        try:
            data = json.loads(body.decode("utf-8"))
        except Exception:
            data = {}
        raw_path = data.get("path")
        if not isinstance(raw_path, str) or not raw_path.strip():
            return self._json(400, {"ok": False, "error": "path_missing"})

        # Normalize path
        p = raw_path.strip()
        try:
            p = os.path.expanduser(p)
        except Exception:
            pass
        # Allow relative paths; make them absolute relative to CWD
        try:
            if not os.path.isabs(p):
                p = os.path.abspath(p)
        except Exception:
            pass

        # Optional existence check to reduce confusion
        if not os.path.exists(p):
            return self._json(400, {"ok": False, "error": "path_not_found", "path": p})

        # Write/merge phase.json with load_engram directive
        ctx = getattr(self.server, "ctx", {})
        phase_file = ctx.get("phase_file")
        if not isinstance(phase_file, str) or not phase_file:
            return self._json(500, {"ok": False, "error": "phase_file_unavailable"})

        obj = {}
        try:
            if os.path.exists(phase_file):
                with open(phase_file, "r", encoding="utf-8") as fh:
                    obj = json.load(fh)
                    if not isinstance(obj, dict):
                        obj = {}
        except Exception:
            obj = {}

        obj["load_engram"] = p
        try:
            os.makedirs(os.path.dirname(phase_file), exist_ok=True)
        except Exception:
            pass

        try:
            with open(phase_file, "w", encoding="utf-8") as fh:
                json.dump(obj, fh, ensure_ascii=False, indent=2)
        except Exception as e:
            return self._json(500, {"ok": False, "error": "write_failed", "detail": str(e)})

        return self._json(200, {"ok": True, "path": p})

    # Quiet server logs
    def log_message(self, fmt, *args):
        try:
            # Suppress default stderr chatter
            return
        except Exception:
            pass


class ControlServer:
    """
    Spawn a local HTTP control server in a background thread.
    Exposes:
      - url: http://127.0.0.1:<port>/
      - stop(): shutdown server
    """
    def __init__(self, run_dir: str, host: str = "127.0.0.1", port: int = 8765):
        self.run_dir = run_dir
        self.phase_file = os.path.join(run_dir, "phase.json")
        self.host = host
        self.port = None
        self._server = None
        self._thread = None

        # Bind first available port in a small range
        last_err = None
        for p in range(int(port), int(port) + 16):
            try:
                server = ThreadingHTTPServer((host, p), _Handler)
                server.ctx = {"run_dir": self.run_dir, "phase_file": self.phase_file}
                self._server = server
                self.port = p
                break
            except OSError as e:
                last_err = e
                continue

        if self._server is None:
            raise RuntimeError(f"Failed to bind control server on {host}:{port} (+15) - last error: {last_err}")

        t = threading.Thread(target=self._server.serve_forever, name="fum-control-server", daemon=True)
        t.start()
        self._thread = t

        self.url = f"http://{host}:{self.port}/"

    def stop(self):
        try:
            if self._server:
                self._server.shutdown()
        except Exception:
            pass
        try:
            if self._server:
                self._server.server_close()
        except Exception:
            pass
        self._server = None]]></content>
    </file>
    <file>
      <path>cortex/IMPORTANT_TODO.md</path>
      <content><![CDATA[# TODO

* **You already have spikes.** The scouts/maps produce `SpikeEvent`s and the tick logs summarize them (e.g., `evt_exc_head`, `evt_inh_head`, `evt_heat_head`, `evt_*_count`). I incorrectly implied otherwise.
* **Walkers matter.** In your design they’re not passive samplers; they publish findings (heat/cold/trail/frontier/salience), tag nodes/edges for GDSP, and their field measurements are part of the control loop. Saying “don’t attach spiking to walkers” was a bad take for your system.
* **`td_signal`** in your logs travels with `sie_td_error`, so it’s a TD-style control channel (not “total decay”). That was my misread.

Thanks for calling this out.

---

## What to do now (with the logs you already have)

Below are analyses that produce **non-boring, model-specific** insights using exactly what’s in `events.jsonl` and `utd_events.jsonl`-no new instrumentation, no dense scans.

## 1) Hotspot dynamics & frontier churn

**Goal:** find stable hubs, reactivation latencies, and boundary reshaping.

* From each tick, take `evt_heat_head` and `evt_exc_head` (lists of `[neuron_id, score]`).
* Per neuron:

  * **Dwell time:** number of ticks it appears in `heat_head`.
  * **Reactivation latency:** time from last appearance in `cold_head` to next appearance in `heat_head` (distribution per neuron).
* From `adc_territories / adc_boundaries / adc_cycle_hits`:

  * **Boundary churn rate:** Δ in `adc_boundaries` per tick; correlate with spikes and with any `b1_spike` consolidation events.

**Delivers:** a ranked table of persistent “structural” neurons, a histogram of cold→hot latencies (novelty rebounds), and how ADC frontier movement co-varies with spiking bursts.

## 2) EI balance & control coupling

**Goal:** quantify how system control channels shape activity.

* Construct per-tick features: `vt_walkers`, `vt_coverage`, `td_signal`, `sie_valence_01`, `evt_exc_count`, `evt_inh_count`, `evt_heat_count`, `evt_trail_count`.
* Compute:

  * **EI ratio:** `evt_exc_count / (evt_inh_count + 1)`.
  * **Lag correlations / VAR Granger:** `{vt_*, td_signal, sie_valence_01}` → `{EI ratio, evt_*_count}` with BH-FDR control.
  * **Response curves:** bin `vt_coverage` and plot median `evt_exc_count` (and EI ratio) vs coverage.

**Delivers:** concrete lead/lag relationships (e.g., coverage and walkers leading excitation) and operating curves that explain *when* your system tips into high-activity regimes.

## 3) Memory consolidation precursors

**Goal:** find what predicts **`b1_spike=true`** (loop closure / fragment consolidation).

* Around each tick with `b1_spike=true`, compute pre/post deltas (±K ticks) for `evt_memory_head` aggregates (sum of values), `evt_*_count`, `vt_*`, `td_signal`, `sie_valence_01`.
* Train a simple logistic model with lagged features to see which signals **precede** consolidation.

**Delivers:** a short list of leading indicators (e.g., memory build-up + rising td_signal) that you can watch in real time.

## 4) Emission-aligned analysis (`events.jsonl` ↔ `utd_events.jsonl`)

**Goal:** understand the internal “signature” of text output.

* Align streams by tick index (`evt_t`) or timestamp window.
* For each emission window (emission tick ±K):

  * Summarize changes in `vt_coverage`, `vt_walkers`, `td_signal`, `sie_valence_01`, `evt_*_count`.
  * Count which neuron IDs recur in `heat_head` right before emissions (top-K “speaker-adjacent” neurons).
* Fit a lightweight classifier to predict “emit at t” from lagged `{vt_*, td_signal, sie_valence_01, evt_*_count}`.

**Delivers:** a compact “will-speak soon” signature and the subgraph (by neuron IDs) most often active before speech.

## 5) Co-activation communities (no spikes needed)

**Goal:** recover structure without pre→post edges.

* Build a binary presence matrix **N × T** where entry (i,t)=1 if neuron *i* is in `heat_head` or `exc_head` at tick *t* (use an IDF-like weight if you want).
* Compute pairwise Jaccard or PMI; run community detection (e.g., Louvain) on that similarity graph.

**Delivers:** functional assemblies that co-activate across ticks; plot them as communities and track their activation over time (and vs emissions).

---

## If you can stomach **one** tiny log tweak (later)

You said you’ll park implementation changes as TODOs; here are two **surgical** ones that don’t require dense scans:

1. **Spike sample per tick**
   At the end of a tick, serialize **up to K** spike triples (reservoir sample) already seen by the scouts/maps:

```json
"spike_sample": [[node, sign, amp], ...]  // K≈500–2000
```

This unlocks a spike raster without touching substrate scans.

2. **Walker observation patch**
   When a walker publishes its trail, optionally attach any spikes it *observed* along the trail:

```json
"trail_obs": {"spikes":[[node, sign, amp], ...], "frontiers":[...], "salience":[...]}
```

This respects your design (walkers announce findings) and keeps spikes tied to measurement context.

> You **do not** need to add a “walker spiked? yes/no” flag; walkers don’t fire. But letting a walker message carry the *spikes it observed* preserves the very coupling you care about.

---

## Quick key–meaning recap (from your logs)

* **`vt_walkers` / `vt_coverage`** – number of active walkers and fraction of graph touched this tick.
* **`evt_heat_head` / `evt_exc_head` / `evt_inh_head`** – top-K neuron IDs with scores (heat = “any spike/energy”; exc/inh = signed spike content).
* **`evt_*_count`** – per-tick counts over all neurons for that class of event.
* **`evt_cold_head`** – neurons quiet for a while (good targets for novelty/reactivation analysis).
* **`evt_memory_head` / `evt_memory_dict`** – per-neuron memory weights that often ramp before consolidation.
* **`adc_*`** – territory and boundary stats for your frontier detector.
* **`b1_*`** – complexity/cycle metrics; `b1_spike=true` marks consolidation/loop-closure events.
* **`td_signal`, `sie_valence_01`, `sie_td_error`** – control/reward channels from SIE; td is a TD-like signal, valence is total reward proxy.

]]></content>
    </file>
    <file>
      <path>cortex/README.md</path>
      <content/>
    </file>
    <file>
      <path>cortex/Void-Walker-Rules.md</path>
      <content><![CDATA[# Technical Summary Report

**Generated on:** October 4, 2025 at 2:23 AM CDT

---

### Architecture
*   Do not perform global scans.
*   Do not perform dense conversions.
*   Do not directly access raw weight arrays.
*   Do not directly access external graph libraries.
*   Operate only on local neighbor reads provided by the active graph.
*   Emit only small, foldable events.
*   The `bus` parameter must not be used for writes by read-only scouts.
*   Do not perform I/O operations in `BaseDecayMap`.
*   Do not perform logging in `BaseDecayMap`.
*   The `fum_rt.core.cortex.void_walkers.base_decay_map` module must perform event-driven folding only.
*   The `fum_rt.core.cortex.void_walkers.base_decay_map` module must maintain a bounded working set via `keep_max`.
*   The `fum_rt.core.cortex.void_walkers.base_decay_map` module must implement sample-based pruning for the working set.
*   The `fum_rt.core.cortex.void_walkers.base_decay_map` module must ensure snapshot operations are cheap and bounded by `head_k`/`keep_max`.
*   The `fum_rt.core.cortex.void_walkers.base_decay_map` module must ensure `fold` operations are O(#events) time per tick.
*   The `frontier_scout.py` module must serve as a shim for naming conventions.
*   Use the `void_frontier_scout.py` module for the actual class implementation of `FrontierScout`.
*   `HeatMap` must fold events only.
*   `HeatMap` must never scan global structures.
*   The single source of truth for the slow memory field must reside outside the `maps/` directory.
*   `MemoryMap` must act as a thin view/adapter when a `field` is provided.
*   If no `field` is provided, `MemoryMap` must operate as a bounded reducer proxy.
*   When operating as a reducer proxy, `MemoryMap` must only retain a small working set (no full-N vector).
*   `MemoryMap` must not perform global scans.
*   `MemoryMap` must maintain a bounded working set only when in reducer-proxy mode.
*   The `runner` module must be stateless.
*   The `runner` module must be a per-tick scout executor.
*   The `runner` module must not use schedulers.
*   `run_scouts_once` must not perform writes.
*   `run_scouts_once` must not use timers.
*   `run_scouts_once` must not manage cadence.
*   `run_scouts_once` must not use background threads.
*   `run_scouts_once` must be a pure function.
*   `run_scouts_once` must be called exactly once per tick.
*   The `scouts` module must act as a thin aggregator re-exporting scout classes and maps.
*   The `scouts` module must preserve legacy import paths.
*   The `scouts` module must enforce void-faithful, read-only traversal.
*   Scouts must not perform global scans.
*   Scouts must only use local neighbor reads.
*   Scouts must operate within bounded TTL/budgets.
*   Scouts must not mutate the connectome (read-only).
*   Scouts must not perform scans.
*   Scouts must not use schedulers.
*   `TrailMap` must not perform scans.
*   `TrailMap` must use a bounded working set via `BaseDecayMap.keep_max`.
*   `TrailMap.fold` must only use provided events.
*   `TrailMap.fold` must not perform adjacency or weight scans.
*   Updates in `TrailMap.fold` must be strictly local to nodes appearing in events.
*   `ColdScout` must not scan global structures.
*   `ColdScout` must use local neighbor reads.
*   `ColdScout` must operate within bounded TTL/budgets.
*   `CycleHunterScout` must be purely local.
*   `CycleHunterScout` must only read neighbor lists.
*   `CycleHunterScout` must not perform global scans or dense conversions.
*   `CycleHunterScout` must not use schedulers.
*   `CycleHunterScout` must execute once per tick under the runner.
*   `CycleHunterScout` must operate within bounded budgets for visits, edges, and TTL.
*   `CycleHunterScout` must not perform writes.
*   `CycleHunterScout` must emit events only.
*   `ExcitationScout` must not perform global scans.
*   `ExcitationScout` must not scan global structures.
*   `ExcitationScout` must use local neighbor reads.
*   `ExcitationScout` must operate within bounded TTL/budgets.
*   `FrontierScout` must use purely local heuristics.
*   `FrontierScout` must not perform scans.
*   `FrontierScout` must not use schedulers.
*   TTL and budgets must enforce bounds for `FrontierScout`.
*   `FrontierScout` must not perform writes.
*   `FrontierScout` must emit events only.
*   The `HeatScout.step` method must contain an inline copy of `BaseScout.step` logic to facilitate map-aware neighbor choice.
*   `HeatScout` must not perform writes.
*   `HeatScout` must not perform scans.
*   `InhibitionScout` must not perform global scans.
*   `InhibitionScout` must not scan global structures.
*   `InhibitionScout` must use local neighbor reads.
*   `InhibitionScout` must operate within bounded TTL/budgets.
*   `MemoryRayScout` must not perform writes.
*   `MemoryRayScout` must not perform scans.
*   `MemoryRayScout` must not perform global scans or dense conversions.
*   `MemoryRayScout` must only operate on neighbors.
*   `MemoryRayScout` must not use schedulers.
*   `MemoryRayScout` must be TTL/budget bounded.
*   `MemoryRayScout` must emit compact events only.
*   `VoidRayScout` must not perform global scans or dense conversions.
*   `VoidRayScout` must operate only on local neighbor lists.
*   `VoidRayScout` must operate only on small map snapshots.
*   `VoidRayScout` must not use schedulers.
*   `VoidRayScout` must be TTL/budget bounded.
*   `VoidRayScout` must emit compact events only.
*   `SentinelScout` must perform local reads only (neighbors of the current node).
*   `SentinelScout` must not perform scans.
*   `SentinelScout` must not perform writes.

### API Contract
*   The `BaseScout.step` method must return a `list[BaseEvent]`.
*   The `connectome` object passed to `BaseScout.step` must expose `N` (node count).
*   The `connectome` object passed to `BaseScout.step` must expose either `neighbors`, `get_neighbors` methods, or an `adj` mapping for neighbor access.
*   Read-only scouts must emit events by returning them, not by writing to the bus.
*   Subclasses of `BaseScout` may override `_priority_set`.
*   `BaseScout._priority_set` must return a bounded set of node indices.
*   Subclasses of `BaseDecayMap` must implement the `fold(events, tick)` method.
*   Subclasses of `BaseDecayMap` must call `add(node, tick, inc)` within their `fold` implementation.
*   The `BaseDecayMap.fold` method must raise `NotImplementedError` in the base class.
*   The `BaseDecayMap.snapshot` method must return `head` as the top-k [node, score] pairs, bounded by `head_k`.
*   The `BaseDecayMap.snapshot` method must return "p95", "p99", "max", and "count" summaries.
*   `ColdMap` must expose a `touch(node: int, tick: int)` method.
*   `ColdMap` must expose a `snapshot(tick: int, head_n: int = 16)` method returning a dictionary with specified fields.
*   `ColdMap.snapshot` must return a dictionary with "cold_head", "cold_p95", "cold_p99", and "cold_max" keys.
*   `ExcitationMap.snapshot` must return a dictionary with "exc_head", "exc_p95", "exc_p99", "exc_max", and "exc_count" keys.
*   `HeatMap.snapshot` must return a dictionary with "heat_head", "heat_p95", "heat_p99", "heat_max", and "heat_count" keys.
*   `InhibitionMap.snapshot` must return a dictionary with "inh_head", "inh_p95", "inh_p99", "inh_max", and "inh_count" keys.
*   The `MemoryMap.snapshot` method must return a dictionary with "memory_head", "memory_p95", "memory_p99", "memory_max", "memory_count", and "memory_dict" keys.
*   `run_scouts_once` must accept optional `seeds` and `map_heads`.
*   The facade (scouts.py) must expose `VoidColdScoutWalker` (aliasing `ColdScout`), `HeatScout`, `ExcitationScout`, `InhibitionScout`, `VoidRayScout`, `MemoryRayScout`, `FrontierScout`, `CycleHunterScout`, `SentinelScout`, `ColdMap`, `BaseScout`, `GDSPActuator`, and `RevGSP`.
*   `TrailMap.snapshot` must include "trail_head" and "trail_dict" keys.
*   `TrailMap.snapshot` must export a bounded snapshot including both the head list and the working-set dictionary.
*   The `trail_dict` in `TrailMap.snapshot` must be bounded by `keep_max`.

### Behavior
*   If the connectome has 0 or fewer nodes, the `BaseScout.step` method must return an empty event list.
*   `BaseScout._pick_neighbor` must prioritize neighbors from the `priority` set if available.
*   If no priority neighbors are available, `BaseScout._pick_neighbor` must choose a random neighbor (blue-noise hop).
*   The `BaseDecayMap._prune` method must be called if the number of tracked values exceeds `keep_max`.
*   `BaseDecayMap._prune` must drop the smallest entries from the working set.
*   The `ColdMap` coldness score must be calculated as `1 - 2^(-age / half_life_ticks)`.
*   The `ColdMap` coldness score must be monotonic in idle time.
*   The `ColdMap` coldness score must be bounded in [0,1).
*   The `ColdMap._prune` method must be called if the number of tracked `_last_seen` entries exceeds `keep_max`.
*   `ColdMap._prune` must reduce the tracked set to `keep_max` entries.
*   `ColdMap._prune` must preferentially drop the most recently seen nodes.
*   `ExcitationMap` must track excitatory-only activity.
*   `ExcitationMap` must filter `SpikeEvent` by `sign > 0`.
*   `ExcitationMap` must filter `DeltaWEvent` by `dw > 0`.
*   When processing `SpikeEvent`s, `ExcitationMap` must only consider events where `sign` is greater than 0.
*   When processing `DeltaWEvent`s, `ExcitationMap` must only consider events where `dw` is greater than 0.0.
*   `HeatMap` must track recency-weighted activity with a short half-life.
*   `HeatMap` must increment on `VTTouchEvent`s.
*   `HeatMap` must increment on any `SpikeEvent`.
*   `HeatMap` must increment on any `DeltaWEvent`.
*   When processing `VTTouchEvent`s, `HeatMap` must add `vt_touch_gain * w` to the node.
*   When processing `SpikeEvent`s, `HeatMap` must add `spike_gain * amp` to the node.
*   When processing `DeltaWEvent`s, `HeatMap` must add `dW_gain * |dw|` to the node.
*   `InhibitionMap` must track inhibitory-only activity.
*   `InhibitionMap` must filter `SpikeEvent` by `sign < 0`.
*   `InhibitionMap` must filter `DeltaWEvent` by `dw < 0`.
*   When processing `SpikeEvent`s, `InhibitionMap` must only consider events where `sign` is less than 0.
*   When processing `DeltaWEvent`s, `InhibitionMap` must only consider events where `dw` is less than 0.0.
*   If a `MemoryMap.field` is attached, the `fold()` method must be a no-op.
*   If a `MemoryMap.field` is attached, the view must delegate snapshot operations to the field's snapshot.
*   When capping the dictionary size from a field snapshot, `MemoryMap._snapshot_from_field` must do so deterministically by highest values.
*   If `MemoryMap.self.field` is not `None`, the `MemoryMap.fold` method must immediately return.
*   If the `MemoryMap` proxy-mode working set `_m` exceeds `keep_max`, `_prune` must be called.
*   `MemoryMap._prune` for proxy-mode must drop a sampled set of the smallest entries.
*   `MemoryMap.snapshot` must cap the `memory_dict` size to `self.dict_cap`.
*   `run_scouts_once` must run a bounded list of read-only scouts.
*   `run_scouts_once` must execute scouts exactly once per tick.
*   `run_scouts_once` must enforce a microsecond time budget across all scouts.
*   Drop-oldest behavior for events must be handled by the downstream bus implementation when `publish_many` is used.
*   `run_scouts_once` must rotate the starting scout by tick (round-robin) to ensure fairness and avoid starvation.
*   `run_scouts_once` must implement a global time guard, stopping scout execution if the `max_us` budget is exceeded.
*   `run_scouts_once` must implement a best-effort per-scout time guard.
*   If `bus` is provided and has `publish_many`, `bus.publish_many(evs)` must be invoked exactly once at the end.
*   If `bus` does not have `publish_many`, `bus.publish(e)` must be called for each event, with a bounded fallback mechanism.
*   `TrailMap` must be a short half-life trail/repulsion map.
*   `TrailMap` must be updated only by events.
*   `TrailMap` must be event-driven only.
*   `TrailMap` must fold `vt_touch` and `edge_on` events.
*   `TrailMap` is intended as a light repulsion field.
*   When processing `VTTouchEvent`s, `TrailMap` must add `vt_touch_gain * w` to the node.
*   When processing `EdgeOnEvent`s, `TrailMap` must add `edge_gain` to both `u` and `v` nodes if they are non-negative.
*   When processing `SpikeEvent`s, `TrailMap` must add `spike_gain * amp` to the node.
*   When processing `DeltaWEvent`s, `TrailMap` must add `dW_gain * |dw|` to the node.
*   `ColdScout` must prefer neighbors whose node IDs appear in the `ColdMap`'s "cold_head" snapshot.
*   `ColdScout._priority_set` must prioritize nodes from "cold_head" with a `cap` based on `budget_visits`.
*   `CycleHunterScout` must seek short cycles (3-6 hops).
*   `CycleHunterScout` must use a TTL-limited walk.
*   `CycleHunterScout` must use a tiny path window.
*   `CycleHunterScout` must maintain a small deque of the recent path.
*   `CycleHunterScout` must prefer stepping to a neighbor already in the recent path window.
*   If no preferred neighbor is found, `CycleHunterScout` must hop randomly (blue-noise) among neighbors.
*   `CycleHunterScout._priority_set` must return an empty set.
*   `ExcitationScout` must map excitatory corridors.
*   `ExcitationScout` must feed `ExcitationMap` strictly via events.
*   `ExcitationScout` must seed from `ExcitationMap.exc_head`.
*   During a walk, `ExcitationScout` must emit a `VTTouchEvent` per visit.
*   During a walk, `ExcitationScout` must synthesize `SpikeEvent(node, amp, sign=+1)` with bounded amplitude in [0,1].
*   `_head_lookup` must normalize scores by dividing by the maximum score over the truncated head.
*   `_head_lookup` must return an empty dictionary if the head is empty.
*   `ExcitationScout._priority_set` must prefer `ExcitationMap` head indices.
*   `SpikeEvent` amplitude emitted by `ExcitationScout` must be within [0,1].
*   If excitation is not found, default `SpikeEvent` amplitude from `ExcitationScout` to 0.5.
*   `FrontierScout` must skim component boundaries and likely bridge frontiers.
*   `FrontierScout._priority_set` must prioritize coldest tiles as starting seeds.
*   `FrontierScout._deg` must return the integer length of the neighbor list.
*   `FrontierScout._pick_neighbor_scored` must use a Softmax choice mechanism.
*   `HeatScout` must perform local-only neighbor selection.
*   `HeatScout` must use a softmax over map signals for neighbor selection.
*   `HeatScout` must support trail repulsion.
*   `HeatScout` must support optional memory steering.
*   `HeatScout.theta_mem` must control attraction or repulsion to memory.
*   `HeatScout.rho_trail` must repel recently traversed/hot nodes.
*   `HeatScout.gamma_heat` must bias toward heat fronts.
*   If map dictionaries are absent, `HeatScout` must fall back to priority head nodes.
*   If priority head nodes are also absent, `HeatScout` must fall back to a blue-noise hop.
*   The priority seed set must be used for initial pool bias via `HeatScout._priority_set()`.
*   `HeatScout._head_to_dict` must handle `head` being an already-formed dictionary.
*   If Softmax denominator Z is zero or negative, `_softmax_choice` must fallback to a uniform random choice among candidates.
*   `HeatScout._priority_set` must prefer `HeatMap` head indices for seeds.
*   `HeatScout._pick_neighbor_scored` must use `heat_dict` as a fallback for `trail_dict` if `trail_dict` is absent.
*   `InhibitionScout` must map inhibitory ridges.
*   `InhibitionScout` must feed `InhibitionMap` strictly via events.
*   `InhibitionScout` must seed from `InhibitionMap.inh_head`.
*   During a walk, `InhibitionScout` must emit a `VTTouchEvent` per visit.
*   During a walk, `InhibitionScout` must synthesize `SpikeEvent(node, amp, sign=-1)` with bounded amplitude in [0,1].
*   `InhibitionScout._priority_set` must prefer `InhibitionMap` head indices.
*   `SpikeEvent` amplitude emitted by `InhibitionScout` must be within [0,1].
*   If inhibition is not found, default `SpikeEvent` amplitude from `InhibitionScout` to 0.5.
*   `MemoryRayScout` must implement refractive-index steering using a slow memory field.
*   `MemoryRayScout` must fall back to `HeatMap` head/dict when memory is absent.
*   `MemoryRayScout._head_to_set` must accept multiple keys for head extraction.
*   `MemoryRayScout._dict_from_maps` must accept dict snapshots directly.
*   `MemoryRayScout._dict_from_maps` must minimally adapt if a head list is mistakenly passed.
*   `MemoryRayScout._priority_set` must prioritize "memory_head".
*   `MemoryRayScout._priority_set` must fall back to "heat_head" if "memory_head" is not available.
*   `MemoryRayScout._pick_neighbor_scored` must prioritize `memory_dict`.
*   `MemoryRayScout._pick_neighbor_scored` must fall back to `heat_dict` as a slow proxy if `memory_dict` is not available.
*   `VoidRayScout` must implement physics-aware routing.
*   `VoidRayScout` must prefer neighbors with favorable local change in a fast field φ.
*   If Softmax denominator Z is zero or negative, `_softmax_choice` in `VoidRayScout` must fallback to picking the first candidate.
*   `VoidRayScout._priority_set` must prefer `HeatMap` head for initial seeds.
*   If `connectome.phi` is `None`, `VoidRayScout._phi` must return 0.0.
*   `SentinelScout` must act as a blue-noise reseeder/de-trample walker.
*   `SentinelScout` must prevent path lock-in by sampling uniformly across space.
*   `SentinelScout` must announce coverage.
*   `SentinelScout` must use `budget["seeds"]` when provided.
*   If `budget["seeds"]` is not provided, `SentinelScout` must use uniform random nodes as seeds.
*   `SentinelScout` TTL must be kept minimal (default 1).
*   `SentinelScout` TTL must be minimal to avoid trampling.
*   `SentinelScout` TTL must be minimal to keep cost bounded.
*   The `SentinelScout.ttl` must be enforced as 1 to ensure single-step walks.
*   `SentinelScout._priority_set` must prefer low-visit or cold heads when available.

### Event Handling
*   Returned events from `BaseScout.step` must use only `VTTouchEvent` or `EdgeOnEvent`.
*   Subclasses of `BaseScout` may add `SpikeEvent`s.
*   Scouts in `runner.py` must emit only foldable events (`vt_touch`, `edge_on`, optional `spike`/`delta_w`).
*   Scouts (via `scouts.py` facade) must emit only foldable events: `vt_touch`, `edge_on`, and (optionally) `spike(+/-)`.
*   `ColdScout` must emit only `VTTouchEvent` and `EdgeOnEvent` events.
*   `CycleHunterScout` must emit `VTTouchEvent` and `EdgeOnEvent` events.
*   `FrontierScout` must emit `VTTouchEvent` and `EdgeOnEvent` only.
*   `HeatScout` must emit `VTTouchEvent` and `EdgeOnEvent` events.
*   `MemoryRayScout` must emit `VTTouchEvent` and `EdgeOnEvent` events.
*   `VoidRayScout` must emit `VTTouchEvent` and `EdgeOnEvent` events.
*   `SentinelScout` must emit `VTTouchEvent` for coverage.
*   `SentinelScout` must emit opportunistic `EdgeOnEvent` (one hop) when neighbors exist.

### Parameter Constraints
*   `BaseScout.__init__` `budget_visits` must be an integer, default 16.
*   `BaseScout.__init__` `budget_edges` must be an integer, default 8.
*   `BaseScout.__init__` `ttl` must be an integer, default 64.
*   `BaseScout.__init__` `seed` must be an integer, default 0.
*   `BaseScout.budget_visits` must be non-negative.
*   `BaseScout.budget_edges` must be non-negative.
*   `BaseScout.ttl` must be at least 1.
*   The effective `BaseScout.step` `budget_visits` must be between 0 and N (inclusive).
*   The effective `BaseScout.step` `budget_edges` must be non-negative.
*   The effective `BaseScout.step` `ttl` must be at least 1.
*   Seed nodes for `BaseScout.step` must be valid node indices (non-negative and less than N).
*   `BaseDecayMap.__init__` `head_k` must be an integer, default 256.
*   `BaseDecayMap.__init__` `half_life_ticks` must be an integer, default 200.
*   `BaseDecayMap.__init__` `keep_max` must be an integer or None, default None.
*   `BaseDecayMap.__init__` `seed` must be an integer, default 0.
*   `BaseDecayMap.head_k` must be at least 8.
*   `BaseDecayMap.half_life` must be at least 1.
*   `BaseDecayMap.keep_max` must be at least `head_k`.
*   Nodes added to `BaseDecayMap.add` must be non-negative.
*   `BaseDecayMap.snapshot` `head_n` parameter defaults to 16.
*   `BaseDecayMap.snapshot` `head_n` must be at least 1 and at most `self.head_k`.
*   Node IDs passed to `ColdMap.touch` must be non-negative integers.
*   `ColdMap.snapshot` `head_n` parameter must be at least 1 and at most `self.head_k`.
*   `ExcitationMap.__init__` `spike_gain` must be a float, default 1.0.
*   `ExcitationMap.__init__` `dW_gain` must be a float, default 0.5.
*   `HeatMap.__init__` `vt_touch_gain` must be a float, default 0.25.
*   `HeatMap.__init__` `spike_gain` must be a float, default 1.0.
*   `HeatMap.__init__` `dW_gain` must be a float, default 0.5.
*   `InhibitionMap.__init__` `spike_gain` must be a float, default 1.0.
*   `InhibitionMap.__init__` `dW_gain` must be a float, default 0.5.
*   `MemoryMap.__init__` `head_k` must be an integer, default 256.
*   `MemoryMap.__init__` `dict_cap` must be an integer, default 2048.
*   `MemoryMap.__init__` `keep_max` must be an integer or None, default None.
*   `MemoryMap.__init__` `seed` must be an integer, default 0.
*   `MemoryMap.__init__` `gamma` must be a float, default 0.05.
*   `MemoryMap.__init__` `delta` must be a float, default 0.01.
*   `MemoryMap.__init__` `kappa` must be a float, default 0.10.
*   `MemoryMap.__init__` `touch_gain` must be a float, default 1.0.
*   `MemoryMap.__init__` `spike_gain` must be a float, default 0.20.
*   `MemoryMap.__init__` `dW_gain` must be a float, default 0.10.
*   `MemoryMap.head_k` must be at least 8.
*   `MemoryMap.dict_cap` must be at least 8.
*   `MemoryMap.keep_max` must be at least `head_k`.
*   `MemoryMap.gamma` must be non-negative.
*   `MemoryMap.delta` must be between 0.0 and 1.0 (inclusive).
*   `MemoryMap.kappa` must be non-negative.
*   `MemoryMap.touch_gain` must be non-negative.
*   `MemoryMap.spike_gain` must be non-negative.
*   `MemoryMap.dW_gain` must be non-negative.
*   Nodes processed in `MemoryMap` proxy-mode `fold` must be non-negative.
*   `run_scouts_once` `max_us` parameter defaults to 2000.
*   `run_scouts_once` `max_us` must be non-negative.
*   `TrailMap.__init__` `half_life_ticks` defaults to 50.
*   `TrailMap.__init__` `vt_touch_gain` defaults to 0.15.
*   `TrailMap.__init__` `edge_gain` defaults to 0.05.
*   `TrailMap.__init__` `spike_gain` defaults to 0.05.
*   `TrailMap.__init__` `dW_gain` defaults to 0.02.
*   `_extract_head_nodes` (used by ColdScout) `cap` parameter defaults to 512.
*   Extracted node IDs from head lists by `_extract_head_nodes` must be non-negative.
*   `CycleHunterScout.__init__` `window` must be an integer, default 5.
*   `CycleHunterScout.window` must be at least 2.
*   Seed nodes for `CycleHunterScout.step` must be valid node indices (non-negative and less than N).
*   `_head_lookup` (used by ExcitationScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_head_lookup` must be non-negative.
*   `_extract_head_nodes` (used by ExcitationScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_extract_head_nodes` must be non-negative.
*   `_head_to_dict` (used by FrontierScout) `cap` parameter defaults to 1024.
*   Nodes extracted by `_head_to_dict` must be non-negative.
*   `FrontierScout.__init__` `w_cold` must be a float, default 1.0.
*   `FrontierScout.__init__` `w_heat` must be a float, default 0.5.
*   `FrontierScout.__init__` `w_shn` must be a float, default 0.25.
*   `FrontierScout.__init__` `w_deg` must be a float, default 0.5.
*   `FrontierScout.__init__` `tau` must be a float, default 1.0.
*   `FrontierScout.w_cold` must be non-negative.
*   `FrontierScout.w_heat` must be non-negative.
*   `FrontierScout.w_shn` must be non-negative.
*   `FrontierScout.w_deg` must be non-negative.
*   `FrontierScout.tau` must be at least 1e-6.
*   The `FrontierScout._priority_set` `cap` for "cold_head" must be at least 64 and `budget_visits * 8`.
*   `FrontierScout._shared_neighbors` `cap` parameter defaults to 128.
*   `_head_to_set` (used by HeatScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_head_to_set` must be non-negative.
*   `_head_to_dict` (used by HeatScout) `cap` parameter defaults to 2048.
*   Nodes extracted by `_head_to_dict` must be non-negative.
*   `HeatScout.__init__` `theta_mem` must be a float, default 0.0.
*   `HeatScout.__init__` `rho_trail` must be a float, default 0.0.
*   `HeatScout.__init__` `gamma_heat` must be a float, default 1.0.
*   `HeatScout.__init__` `tau` must be a float, default 1.0.
*   `HeatScout.rho_trail` must be non-negative.
*   `HeatScout.gamma_heat` must be non-negative.
*   `HeatScout.tau` must be at least 1e-6.
*   Seed nodes for `HeatScout.step` must be valid node indices (non-negative and less than N).
*   `_head_lookup` (used by InhibitionScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_head_lookup` must be non-negative.
*   `_extract_head_nodes` (used by InhibitionScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_extract_head_nodes` must be non-negative.
*   `_head_to_set` (used by MemoryRayScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_head_to_set` must be non-negative.
*   Nodes extracted by `_dict_from_maps` must be non-negative.
*   `MemoryRayScout.__init__` `theta_mem` must be a float, default 0.8.
*   `MemoryRayScout.__init__` `tau` must be a float, default 1.0.
*   `MemoryRayScout.tau` must be at least 1e-6.
*   Seed nodes for `MemoryRayScout.step` must be valid node indices (non-negative and less than N).
*   `_head_to_set` (used by VoidRayScout) `cap` parameter defaults to 512.
*   Nodes extracted by `_head_to_set` must be non-negative.
*   `VoidRayScout.__init__` `lambda_phi` must be a float, default 1.0.
*   `VoidRayScout.__init__` `theta_mem` must be a float, default 0.0.
*   `VoidRayScout.__init__` `tau` must be a float, default 1.0.
*   `VoidRayScout.tau` must be at least 1e-6.
*   The `VoidRayScout._priority_set` `cap` for "heat_head" must be at least 64 and `budget_visits * 8`.
*   Seed nodes for `VoidRayScout.step` must be valid node indices (non-negative and less than N).
*   `SentinelScout.__init__` `ttl` defaults to 1.
*   The effective `SentinelScout.step` `ttl` must be capped to 1.
*   The `SentinelScout._priority_set` `cap` for "visit_head" or "cold_head" must be at least 64 and `budget_visits * 8`.
*   Seed nodes for `SentinelScout.step` must be valid node indices (non-negative and less than N).

### Performance
*   `BaseDecayMap._prune` must avoid full O(N) sorts.
*   `ColdMap._prune` must use sampling to avoid O(N) passes.
*   `FrontierScout._shared_neighbors` must bound its cost by only checking up to `cap` neighbors of `v`.

### Syntax
*   `BaseScout` must define `__slots__` with "budget_visits", "budget_edges", "ttl", and "rng".
*   `BaseDecayMap` must define `__slots__` with "head_k", "half_life", "keep_max", "rng", "_val", and "_last_tick".
*   `ColdMap` must define `__slots__` with "head_k", "half_life", "keep_max", "rng", and "_last_seen".
*   `ExcitationMap` must define `__slots__` with "spike_gain" and "dW_gain".
*   `HeatMap` must define `__slots__` with "vt_touch_gain", "spike_gain", and "dW_gain".
*   `InhibitionMap` must define `__slots__` with "spike_gain" and "dW_gain".
*   `MemoryMap` must define `__slots__` with "field", "head_k", "dict_cap", "keep_max", "rng", "_m", "_last_tick", "gamma", "delta", "kappa", "touch_gain", "spike_gain", and "dW_gain".
*   `TrailMap` must define `__slots__` with "vt_touch_gain", "edge_gain", "spike_gain", and "dW_gain".
*   `ColdScout` must define `__slots__` as empty.
*   `CycleHunterScout` must define `__slots__` with "window".
*   `ExcitationScout` must define `__slots__` as empty.
*   `FrontierScout` must define `__slots__` with "w_cold", "w_heat", "w_shn", "w_deg", and "tau".
*   `HeatScout` must define `__slots__` with "theta_mem", "rho_trail", "gamma_heat", and "tau".
*   `InhibitionScout` must define `__slots__` as empty.
*   `MemoryRayScout` must define `__slots__` with "theta_mem" and "tau".
*   `VoidRayScout` must define `__slots__` with "lambda_phi", "theta_mem", and "tau".
*   `SentinelScout` must define `__slots__` as empty.

### Type Requirement
*   `ExcitationMap.spike_gain` must be convertible to float.
*   `ExcitationMap.dW_gain` must be convertible to float.
*   `HeatMap.vt_touch_gain` must be convertible to float.
*   `HeatMap.spike_gain` must be convertible to float.
*   `HeatMap.dW_gain` must be convertible to float.
*   `InhibitionMap.spike_gain` must be convertible to float.
*   `InhibitionMap.dW_gain` must be convertible to float.
*   `TrailMap.vt_touch_gain` must be convertible to float.
*   `TrailMap.edge_gain` must be convertible to float.
*   `TrailMap.spike_gain` must be convertible to float.
*   `TrailMap.dW_gain` must be convertible to float.

### Algorithm
*   The `FrontierScout._pick_neighbor_scored` (Softmax fallback) random choice must use a deterministic-ish hashing for reproducibility in its randomness.
*   `HeatScout` local selection must use `logit_j = (self.theta_mem * m_j) - (self.rho_trail * htrail_j) + (self.gamma_heat * h_j)` divided by `self.tau`.
*   `MemoryRayScout` local selection must follow `P(i→j) ∝ exp(Theta * m[j])` with temperature `tau` (Boltzmann choice).
*   If Softmax denominator Z is zero or negative, `_softmax_choice` (used by `HeatScout`, `MemoryRayScout`) must fallback to a uniform random choice among candidates.
*   `VoidRayScout` local scoring must follow `s_j = lambda_phi * (φ[j] - φ[i]) + theta_mem * m[j]`.
*   `VoidRayScout` local scoring must not perform scans.
*   `VoidRayScout` must use a temperatured choice via softmax over neighbors.
*   `VoidRayScout` must perform strictly local reads.
*   The `VoidRayScout._softmax_choice` (Softmax fallback) random choice must use a deterministic-ish hashing for reproducibility in its randomness.

### Purpose
*   `TrailMap` is intended as a light repulsion field.
*   `FrontierScout` must skim component boundaries and likely bridge frontiers.
*   `SentinelScout` must act as a blue-noise reseeder/de-trample walker.
*   `SentinelScout` must prevent path lock-in by sampling uniformly across space.
*   `SentinelScout` must announce coverage.

### Duty
*   `ExcitationScout` must map excitatory corridors.
*   `ExcitationScout` must feed `ExcitationMap` strictly via events.
*   `InhibitionScout` must map inhibitory ridges.
*   `InhibitionScout` must feed `InhibitionMap` strictly via events.

## Key Highlights

* All modules and scouts must operate strictly locally, avoiding global scans, dense conversions, and direct access to raw weight arrays or external graph libraries to ensure bounded and efficient computation.
* Maps and scouts are required to maintain bounded working sets and adhere to strict resource limits, including `head_k`, `keep_max`, TTLs, and per-tick time budgets, with explicit pruning strategies to manage memory.
* The system architecture is primarily event-driven, with scouts emitting small, foldable events and maps processing these events to update their state, fostering decoupled and reactive information flow.
* Scouts are strictly read-only entities, must not mutate the connectome, and are designed to communicate results solely by emitting events, thereby preserving the integrity of the underlying graph structure.
* The `runner` module, particularly `run_scouts_once`, functions as a stateless, per-tick executor that meticulously manages scout execution within strict microsecond time budgets, ensures fairness via round-robin rotation, and is responsible for publishing all collected events.
* Various specialized maps, including `ColdMap`, `HeatMap`, `ExcitationMap`, `InhibitionMap`, `MemoryMap`, and `TrailMap`, are designed to track and expose distinct localized activity patterns such as coldness, recency, excitation, inhibition, memory, and repulsion.
* Scouts like `CycleHunterScout`, `FrontierScout`, `HeatScout`, and `MemoryRayScout` utilize sophisticated, local-only heuristics and map-driven priority sets to intelligently guide their traversals and neighbor selection within the graph.

## Next Steps & Suggestions

* Evaluate the scalability and performance of `BaseDecayMap` pruning mechanisms and `keep_max` thresholds under varying graph sizes and event loads to ensure bounded resource consumption.
* Conduct empirical studies to quantify the effectiveness of specific scout heuristics (e.g., `FrontierScout` for boundaries, `CycleHunterScout` for short cycles, `MemoryRayScout` for memory steering) in achieving their stated exploration goals across diverse graph topologies.
* Develop a systematic framework for tuning the numerous behavioral parameters (e.g., gain values, `half_life_ticks`, `tau`, `theta_mem`) to optimize for desired system outcomes, such as exploration efficiency or map accuracy.
* Investigate the robustness and consistency of the entire event propagation pipeline, from scout emission and `run_scouts_once` execution (including time budgets and fairness) to event bus handling and map folding, particularly concerning the single source of truth for the 'slow memory field'.
]]></content>
    </file>
    <file>
      <path>cortex/Void-Walkers.md</path>
      <content><![CDATA[# Technical Summary Report

**Generated on:** October 4, 2025 at 2:10 AM CDT

---

### Identified Components

*   **`BaseScout` (Class):** An abstract base class for "void-faithful, read-only scouts." It provides common functionality for interacting with a `connectome` (graph structure) and managing exploration budgets. It defines the core `step` contract.
*   **`BaseDecayMap` (Class):** An abstract base class for bounded, exponentially decaying accumulators. It manages per-node scores (`_val`) and last update times (`_last_tick`), handling decay and pruning. It defines the `fold` and `snapshot` contracts.
*   **`ColdMap` (Class):** A concrete implementation of a "coldness tracker" that records the last seen `tick` for each node and calculates a coldness score based on idle time. It's used for telemetry and read-only purposes.
*   **`ExcitationMap` (Class):** A concrete `BaseDecayMap` subclass that accumulates positive `SpikeEvent`s and positive `DeltaWEvent`s to track excitatory activity.
*   **`HeatMap` (Class):** A concrete `BaseDecayMap` subclass that accumulates `VTTouchEvent`s, `SpikeEvent`s, and `DeltaWEvent`s (absolute `dw`) to track general recency-weighted activity.
*   **`InhibitionMap` (Class):** A concrete `BaseDecayMap` subclass that accumulates negative `SpikeEvent`s and negative `DeltaWEvent`s (absolute `dw`) to track inhibitory activity.
*   **`MemoryMap` (Class):** A flexible map that can act as a view/adapter over an external "MemoryField" or as a self-contained, bounded reducer (proxy mode) that folds various events (`VTTouchEvent`, `EdgeOnEvent`, `SpikeEvent`, `DeltaWEvent`) to update an internal memory-like value.
*   **`TrailMap` (Class):** A concrete `BaseDecayMap` subclass used as a short-half-life repulsion map, accumulating `VTTouchEvent`s, `EdgeOnEvent`s, `SpikeEvent`s, and `DeltaWEvent`s to track recent traversal footprints.
*   **`ColdScout` / `VoidColdScoutWalker` (Class):** A `BaseScout` subclass that prioritizes exploring nodes identified as "cold" by a `ColdMap` snapshot.
*   **`CycleHunterScout` (Class):** A `BaseScout` subclass that performs walks to seek short cycles, prioritizing neighbors already in its recent path history.
*   **`ExcitationScout` (Class):** A `BaseScout` subclass that prioritizes nodes with high excitatory scores and emits `SpikeEvent`s with positive `sign` and an amplitude based on local excitation.
*   **`FrontierScout` (Class):** A `BaseScout` subclass that explores graph frontiers and bridge-like structures, using a scoring heuristic based on coldness, heat, shared neighbors, and degree differences.
*   **`HeatScout` (Class):** A `BaseScout` subclass that routes based on a softmax choice over neighbors, considering memory values, trail repulsion, and heat scores.
*   **`InhibitionScout` (Class):** A `BaseScout` subclass that prioritizes nodes with high inhibitory scores and emits `SpikeEvent`s with negative `sign` and an amplitude based on local inhibition.
*   **`MemoryRayScout` (Class):** A `BaseScout` subclass that steers its walk using a "refractive-index" like mechanism, prioritizing neighbors with higher memory values, with fallback to heat if memory is unavailable.
*   **`VoidRayScout` (Class):** A `BaseScout` subclass that routes based on local gradients of an external `phi` field (if provided by `connectome`) combined with memory values.
*   **`SentinelScout` (Class):** A `BaseScout` subclass designed for blue-noise reseeding and de-trampling, performing minimal-TTL (typically 1 hop) walks to ensure broad coverage, potentially biased by "visit" or "cold" maps.
*   **`run_scouts_once` (Function):** A utility function responsible for executing a given sequence of scout objects within a time budget for a single tick.
*   **`fum_rt.core.proprioception.events` (Module/Events):** Contains `BaseEvent`, `VTTouchEvent` (node visit), `EdgeOnEvent` (edge traversal), `SpikeEvent` (node activity with sign/amplitude), and `DeltaWEvent` (weight change event). These are the primary data units emitted by scouts and processed by maps.
*   **`connectome` (Implicit Interface):** An object passed to scouts, exposing methods/attributes like `N` (number of nodes), `neighbors(u)` or `get_neighbors(u)` (list of neighbors for node `u`), `adj` (adjacency mapping), and potentially `phi` (scalar field).
*   **`bus` (Implicit Interface):** An optional object passed to `run_scouts_once` that, if present, is used to publish collected events (e.g., via `publish_many`).
*   **`scouts.py` (Facade Module):** A module that re-exports various scout and map classes, providing a centralized import path and preserving legacy compatibility.

### Observed Interactions & Data Flow

1.  **System Orchestration (`run_scouts_once`):**
    *   The `run_scouts_once` function is called per tick with a `connectome`, a list of `scouts`, a `maps` dictionary, a `budget`, an optional `bus`, and a `max_us` time limit.
    *   It determines the starting scout for fairness (round-robin by tick).
    *   It iterates through the `scouts`, calling each `scout.step()` method. It enforces a global time budget (`max_us`) and a soft per-scout budget (`per_us`).
    *   All `BaseEvent`s returned by `scout.step()` calls are aggregated into a single list.
    *   If a `bus` is provided and events were generated, `bus.publish_many(events)` is called (or `bus.publish(e)` for each event as a fallback).

2.  **Scout Execution (`BaseScout.step` and subclasses):**
    *   `scout.step()` receives a `connectome`, `bus` (ignored by scouts as they are read-only), `maps` (for contextual routing), and a `budget`.
    *   Scouts (via `BaseScout` helpers) query the `connectome` for graph size (`_get_N`) and neighbors of a given node (`_neighbors`). This is strictly local and read-only.
    *   Scouts consult the `maps` dictionary (e.g., `maps["heat_head"]`, `maps["cold_head"]`, `maps["memory_dict"]`) to derive priority sets (`_priority_set`) or scores (`_pick_neighbor_scored`) for guiding their walks. Helper functions (`_extract_head_nodes`, `_head_lookup`, `_head_to_dict`, `_head_to_set`, `_dict_from_maps`) facilitate parsing these map snapshots.
    *   During their bounded walks (controlled by `budget_visits`, `budget_edges`, `ttl`), scouts generate `BaseEvent`s:
        *   `VTTouchEvent`: emitted for each node visited.
        *   `EdgeOnEvent`: emitted for each edge traversed.
        *   `SpikeEvent` (e.g., by `ExcitationScout`, `InhibitionScout`): for specific activity.
    *   These events are returned as a `List[BaseEvent]` from `scout.step()`.

3.  **Map Updates (`BaseDecayMap.fold` and subclasses):**
    *   Maps (e.g., `HeatMap`, `ExcitationMap`, `InhibitionMap`, `TrailMap`) expose a `fold(events, tick)` method.
    *   An external orchestrator (not shown in this segment, but implied by the `fold` contract) would pass the collected events from `run_scouts_once` to the relevant maps.
    *   The `fold` method iterates through the `events`, filters them by `kind` and other attributes (e.g., `sign`, `dw` value), and calls `self.add(node, tick, increment)` to update its internal score for specific nodes.
    *   `ColdMap` uses a `touch(node, tick)` method instead of `fold` for its specific update logic.
    *   `MemoryMap` in proxy mode also implements `fold` to update its internal `_m` dictionary based on events. If it has an external `field`, `fold` becomes a no-op.

4.  **Map State and Snapshots (`BaseDecayMap.snapshot`):**
    *   All maps maintain internal state (`_val`, `_last_tick` for `BaseDecayMap` and its subclasses; `_last_seen` for `ColdMap`).
    *   Map state is subject to exponential decay (`_decay_to`) based on `half_life_ticks` and `tick` progression.
    *   Maps enforce bounded working sets (`keep_max`) by pruning (`_prune`) the smallest or oldest entries to limit memory usage.
    *   Maps provide `snapshot()` methods that return a `dict` containing summarized data:
        *   `head`: top-k nodes by score/coldness.
        *   `p95`, `p99`, `max`, `count`: percentile and max summaries.
        *   `_dict` (for `TrailMap`, `MemoryMap`): a bounded dictionary of node scores.
    *   These snapshot dictionaries are then passed to scouts via the `maps` parameter in `run_scouts_once`, completing a feedback loop.

### Inferred Design Rationale

1.  **"Void-faithful, Read-Only" Principle:** This is a fundamental constraint explicitly stated and enforced throughout the documentation. Scouts are strictly read-only, never modifying the `connectome` or directly altering global state. This promotes:
    *   **Loose Coupling:** Scouts are independent and don't create side effects, making them easier to test, reason about, and potentially run in parallel.
    *   **Determinism & Reproducibility:** By separating observation (scouts) from action/state change (maps, external systems via bus), the system's behavior can be more predictable.
    *   **Scalability:** Avoiding global scans and direct writes simplifies concurrent execution and distributed processing.

2.  **Event-Driven Architecture:** The system heavily relies on `BaseEvent`s (e.g., `VTTouchEvent`, `EdgeOnEvent`, `SpikeEvent`). Scouts emit events, and maps "fold" events. This pattern:
    *   **Decouples Components:** Producers (scouts) and consumers (maps, bus) of events don't need direct knowledge of each other.
    *   **Flexibility:** New event types or new event consumers can be added without modifying existing scouts.
    *   **Asynchronous Processing:** Events can be queued and processed asynchronously, which is useful for real-time systems.

3.  **Bounded Computation & Memory Footprint:**
    *   **Scouts:** Use `budget_visits`, `budget_edges`, `ttl` (Time-To-Live) to limit walk depth and resource consumption per tick. They perform only local neighbor reads.
    *   **Maps:** Implement `head_k`, `keep_max`, `half_life` for bounded storage and exponential decay. They avoid global scans by only processing events and pruning old/small entries.
    *   **Rationale:** This design is crucial for real-time systems or very large graphs, preventing performance degradation and excessive memory usage. It ensures the system remains responsive and predictable under high load.

4.  **Specialization and Modular Extension:**
    *   `BaseScout` and `BaseDecayMap` provide common interfaces and shared logic, allowing for easy creation of new, specialized scout strategies and map types. Each scout/map focuses on a single responsibility (e.g., `HeatScout` follows heat, `ColdScout` seeks cold).
    *   **Rationale:** Promotes code reuse, maintainability, and extensibility. New heuristics or state-tracking mechanisms can be integrated without modifying core components.

5.  **Abstraction of `Connectome` and `Maps` Data:**
    *   `BaseScout` abstracts how it queries graph neighbors (`_neighbors`) supporting multiple `connectome` interfaces (methods, dicts).
    *   Scouts interact with map data through a generic `maps` dictionary containing "snapshots" (head lists, summary stats, bounded dicts) rather than direct access to map objects. Helper functions manage extracting relevant data from these snapshots.
    *   **Rationale:** Decouples scouts from specific `connectome` and `map` implementations, allowing for flexible underlying data structures and preventing tight dependencies.

6.  **Real-time Performance and Fairness:**
    *   `run_scouts_once` includes a microsecond time budget (`max_us`) and round-robin scout scheduling.
    *   **Rationale:** Essential for a real-time system (`fum_rt`) to ensure predictable execution times and prevent any single scout from monopolizing resources or starving others.

7.  **Robustness (Extensive Error Handling):**
    *   Numerous `try-except` blocks around type conversions, dictionary accesses, and attribute lookups.
    *   **Rationale:** Makes the system resilient to malformed input data, incomplete `maps` dictionaries, or unexpected `connectome` object behavior, leading to graceful degradation rather than crashes.

### Operational Snippets

1.  **Initializing a scout:**
    ```python
    from fum_rt.core.cortex.void_walkers.void_heat_scout import HeatScout
    scout = HeatScout(budget_visits=32, half_life_ticks=100, seed=42)
    ```

2.  **Initializing a map:**
    ```python
    from fum_rt.core.cortex.maps.heatmap import HeatMap
    heatmap = HeatMap(half_life_ticks=200, vt_touch_gain=0.25)
    ```

3.  **Executing scouts for a tick:**
    ```python
    from fum_rt.core.cortex.void_walkers.runner import run_scouts_once
    from fum_rt.core.proprioception.events import BaseEvent
    # Assume 'my_connectome', 'my_scouts_list', 'current_maps_snapshot', 'my_bus_instance' are defined
    # and 'current_tick' is an integer
    
    budget = {"visits": 100, "edges": 50, "ttl": 10, "tick": current_tick, "seeds": [1, 5, 10]}
    
    emitted_events: List[BaseEvent] = run_scouts_once(
        connectome=my_connectome,
        scouts=my_scouts_list,
        maps=current_maps_snapshot,
        budget=budget,
        bus=my_bus_instance,
        max_us=5000 # 5 milliseconds total budget
    )
    # The bus would have published the events, or they are returned if no bus.
    ```

4.  **Map folding events:**
    ```python
    # Assume 'heatmap' is an instance of HeatMap and 'emitted_events' is a list of events from scouts
    current_tick = 123
    heatmap.fold(emitted_events, current_tick)
    ```

5.  **Generating a map snapshot:**
    ```python
    # After folding events, get a snapshot
    heatmap_snapshot = heatmap.snapshot()
    # Example output (simplified):
    # {
    #     "heat_head": [[12, 0.85], [34, 0.72]],
    #     "heat_p95": 0.6,
    #     "heat_p99": 0.9,
    #     "heat_max": 1.0,
    #     "heat_count": 500
    # }
    ```
    This `heatmap_snapshot` (and similar snapshots from other maps) would then be aggregated into the `maps` dictionary passed to `run_scouts_once` in a subsequent tick.

## Key Highlights

* The system is built on a "void-faithful, read-only" principle, with `BaseScout` abstracting graph exploration and `BaseDecayMap` managing dynamic, decaying graph state in a feedback loop.
* Scouts are strictly read-only, never modifying the `connectome` or global state directly, which ensures loose coupling, determinism, and scalability.
* An event-driven architecture, using `BaseEvent`s like `VTTouchEvent` and `SpikeEvent`, decouples scouts (event producers) from maps and other consumers for flexibility and asynchronous processing.
* Both scouts and maps incorporate bounded computation and memory management, utilizing walk budgets, exponential decay, and pruning to ensure real-time performance and prevent resource exhaustion on large graphs.
* The architecture promotes modularity and extensibility through abstract base classes, allowing for the easy creation and integration of specialized scout strategies and map types.
* The `run_scouts_once` function centrally orchestrates scout execution, enforcing per-tick time budgets and using round-robin scheduling to ensure fairness and predictable real-time operation.
* Scouts derive their exploration strategies by consulting dynamic `maps` snapshots, which are in turn updated by events emitted during scout walks, forming a continuous feedback mechanism.
]]></content>
    </file>
    <file>
      <path>cortex/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>cortex/maps/README.md</path>
      <content/>
    </file>
    <file>
      <path>cortex/maps/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from .coldmap import ColdMap
from .heatmap import HeatMap
from .excitationmap import ExcitationMap
from .inhibitionmap import InhibitionMap

__all__ = ["ColdMap", "HeatMap", "ExcitationMap", "InhibitionMap"]]]></content>
    </file>
    <file>
      <path>cortex/maps/base_decay_map.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.base_decay_map
Purpose: Shared bounded, exponential-decay event-driven map base for Heat/Exc/Inh reducers.

Void-faithful constraints:
- Event-driven folding only (no global scans over W or neighbors).
- Bounded working set via keep_max with sample-based pruning.
- O(#events) time per tick; snapshot is cheap and bounded by head_k/keep_max.
"""

from typing import Dict, Iterable, List
import math
import random


class BaseDecayMap:
    """
    Bounded, per-node exponentially decaying accumulator.
    Score_t(node) = Score_{t-Δ} * 2^(-Δ/half_life_ticks) + sum(increments at t)

    Snapshot:
      - head (top-16 [node, score] pairs by default; bounded by head_k)
      - p95, p99, max, count summaries

    Notes:
    - Subclasses must implement fold(events, tick) and call add(node, tick, inc).
    - No I/O/logging; pure core.
    """

    __slots__ = ("head_k", "half_life", "keep_max", "rng", "_val", "_last_tick")

    def __init__(self, head_k: int = 256, half_life_ticks: int = 200, keep_max: int | None = None, seed: int = 0) -> None:
        self.head_k = int(max(8, head_k))
        self.half_life = int(max(1, half_life_ticks))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._val: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

    # ------------- core updates -------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            factor = 2.0 ** (-(dt / float(self.half_life)))
            try:
                self._val[node] *= factor
            except Exception:
                self._val[node] = float(self._val.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def add(self, node: int, tick: int, inc: float) -> None:
        try:
            n = int(node)
            t = int(tick)
            dv = float(inc)
        except Exception:
            return
        if n < 0:
            return
        if n in self._val:
            self._decay_to(n, t)
            self._val[n] += dv
        else:
            self._val[n] = max(0.0, dv)
            self._last_tick[n] = t
        if len(self._val) > self.keep_max:
            self._prune()

    def _prune(self) -> None:
        # Drop a sampled set of the smallest entries (cheap; avoids full O(N) sort)
        size = len(self._val)
        target = size - self.keep_max
        if target <= 0:
            return
        keys = list(self._val.keys())
        sample_size = min(len(keys), max(256, target * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        sample.sort(key=lambda k: self._val.get(k, 0.0))  # ascending by score
        for k in sample[:target]:
            self._val.pop(k, None)
            self._last_tick.pop(k, None)

    # ------------- folding & snapshots -------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Subclasses override and call add(node, tick, inc) appropriately.
        """
        raise NotImplementedError

    def snapshot(self, head_n: int = 16) -> dict:
        if not self._val:
            return {"head": [], "p95": 0.0, "p99": 0.0, "max": 0.0, "count": 0}
        # head top-k by score
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._val.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._val.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]
        # quick percentiles over working set
        vals = sorted(float(v) for v in self._val.values())
        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])
        return {
            "head": [[int(k), float(v)] for k, v in head],
            "p95": q(0.95),
            "p99": q(0.99),
            "max": float(vals[-1]),
            "count": int(len(vals)),
        }


__all__ = ["BaseDecayMap"]]]></content>
    </file>
    <file>
      <path>cortex/maps/coldmap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.coldmap
Purpose: Persistent, bounded coldness tracker keyed by node id (telemetry-only, read-only).
Design: Pure core module; no IO/logging; compatible with existing CoreEngine usage.
"""

from typing import List
import random
import math


class ColdMap:
    """
    Persistent, bounded coldness tracker keyed by node id.

    Coldness score (monotonic in idle time, bounded in [0,1)):
        age = max(0, t - last_seen[node])
        score = 1 - 2^(-age / half_life_ticks)

    Snapshot fields:
      - cold_head: top-16 [node_id, score] pairs (most cold first)
      - cold_p95, cold_p99, cold_max: distribution summaries across tracked nodes

    Notes:
    - API-compatible with existing CoreEngine usage:
        * touch(node: int, tick: int) to record activity
        * snapshot(tick: int, head_n: int = 16) -> dict with fields listed above
    - Constructor accepts (head_k, half_life_ticks, keep_max, seed) to match current wiring.
    """
    __slots__ = ("head_k", "half_life", "keep_max", "rng", "_last_seen")

    def __init__(self, head_k: int = 256, half_life_ticks: int = 200, keep_max: int | None = None, seed: int = 0) -> None:
        self.head_k = int(max(8, head_k))
        self.half_life = int(max(1, half_life_ticks))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._last_seen: dict[int, int] = {}

    # ------------- updates -------------

    def touch(self, node: int, tick: int) -> None:
        """
        Record a touch for node at tick. Node ids must be non-negative ints.
        """
        try:
            n = int(node)
            t = int(tick)
        except Exception:
            return
        if n < 0:
            return
        self._last_seen[n] = t
        if len(self._last_seen) > self.keep_max:
            self._prune(t)

    def _prune(self, tick: int) -> None:
        """
        Reduce tracked set to keep_max entries, preferentially dropping the most recently seen nodes.
        Uses sampling to avoid O(N) passes.
        """
        try:
            size = len(self._last_seen)
            if size <= self.keep_max:
                return
            target = size - self.keep_max
            keys = list(self._last_seen.keys())
            sample_size = min(len(keys), max(256, target * 4))
            sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
            # Sort sample by recency (most recent first) and drop up to target from this set.
            sample.sort(key=lambda k: self._last_seen.get(k, -10**12), reverse=True)
            to_remove = min(target, len(sample))
            for k in sample[:to_remove]:
                self._last_seen.pop(k, None)
        except Exception:
            # Conservative fallback: random removals until within bound
            while len(self._last_seen) > self.keep_max:
                try:
                    k = self.rng.choice(tuple(self._last_seen.keys()))
                    self._last_seen.pop(k, None)
                except Exception:
                    break

    # ------------- scoring -------------

    def _score(self, age: int) -> float:
        a = max(0, int(age))
        # score in [0, 1): 1 - 2^(-age / half_life)
        try:
            return float(1.0 - math.pow(0.5, float(a) / float(self.half_life)))
        except Exception:
            return 0.0

    # ------------- snapshot -------------

    def snapshot(self, tick: int, head_n: int = 16) -> dict:
        """
        Compute a coldness snapshot at tick.

        Returns:
          {
            "cold_head": list[[node_id, score], ...]           # top head_n by score
            "cold_p95": float,
            "cold_p99": float,
            "cold_max": float,
          }
        """
        try:
            t = int(tick)
        except Exception:
            t = 0

        if not self._last_seen:
            return {"cold_head": [], "cold_p95": 0.0, "cold_p99": 0.0, "cold_max": 0.0}

        # Compute scores for all tracked nodes (bounded by keep_max)
        pairs: List[tuple[int, float]] = []
        for node, ts in self._last_seen.items():
            try:
                age = t - int(ts)
            except Exception:
                age = 0
            s = self._score(age)
            pairs.append((int(node), float(s)))

        # Top head_n by score
        head_n = max(1, min(int(head_n), self.head_k))
        try:
            import heapq as _heapq
            head = _heapq.nlargest(head_n, pairs, key=lambda kv: kv[1])
        except Exception:
            head = sorted(pairs, key=lambda kv: kv[1], reverse=True)[:head_n]

        # Percentiles over full tracked set (bounded)
        vals = [s for _, s in pairs]
        vals.sort()

        def _pct(p: float) -> float:
            if not vals:
                return 0.0
            i = int(max(0, min(len(vals) - 1, round((len(vals) - 1) * p))))
            return float(vals[i])

        p95 = _pct(0.95)
        p99 = _pct(0.99)
        vmax = float(vals[-1]) if vals else 0.0

        head_out: List[List[float]] = [[int(n), float(s)] for n, s in head]
        return {
            "cold_head": head_out,
            "cold_p95": float(p95),
            "cold_p99": float(p99),
            "cold_max": float(vmax),
        }


__all__ = ["ColdMap"]
]]></content>
    </file>
    <file>
      <path>cortex/maps/excitationmap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.excitationmap
Purpose: Excitatory-only activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import SpikeEvent, DeltaWEvent


class ExcitationMap(BaseDecayMap):
    """
    Excitatory-only activity map.
    Filters by sign>0 (spikes) and dw>0 (ΔW).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * dw for DeltaWEvent (dw > 0 only)
    """
    __slots__ = ("spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "spike" and isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) > 0:
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                dw = float(getattr(e, "dw", 0.0))
                if dw > 0.0:
                    self.add(int(e.node), int(tick), self.dW_gain * dw)

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "exc_head": s["head"],
            "exc_p95": s["p95"],
            "exc_p99": s["p99"],
            "exc_max": s["max"],
            "exc_count": s["count"],
        }


__all__ = ["ExcitationMap"]
]]></content>
    </file>
    <file>
      <path>cortex/maps/heatmap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.heatmap
Purpose: Recency-weighted activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import VTTouchEvent, SpikeEvent, DeltaWEvent


class HeatMap(BaseDecayMap):
    """
    Recency-weighted activity map (short half-life).
    Increments on vt_touch (small) and any spike/ΔW (scaled).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - vt_touch_gain: increment per vt_touch (e.g., 0.25)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * |dw| for DeltaWEvent (e.g., 0.5)

    Void-faithful: folds events only; never scans global structures.
    """
    __slots__ = ("vt_touch_gain", "spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        vt_touch_gain: float = 0.25,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.vt_touch_gain = float(vt_touch_gain)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                self.add(int(e.token), int(tick), self.vt_touch_gain * float(getattr(e, "w", 1.0)))
            elif k == "spike" and isinstance(e, SpikeEvent):
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                self.add(int(e.node), int(tick), self.dW_gain * abs(float(e.dw)))

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "heat_head": s["head"],
            "heat_p95": s["p95"],
            "heat_p99": s["p99"],
            "heat_max": s["max"],
            "heat_count": s["count"],
        }


__all__ = ["HeatMap"]
]]></content>
    </file>
    <file>
      <path>cortex/maps/inhibitionmap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.inhibitionmap
Purpose: Inhibitory-only activity map (short half-life), event-driven only (no scans).
"""

from typing import Iterable
from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import SpikeEvent, DeltaWEvent


class InhibitionMap(BaseDecayMap):
    """
    Inhibitory-only activity map.
    Filters by sign<0 (spikes) and dw<0 (ΔW).

    Parameters:
      - half_life_ticks: decay half-life in ticks (e.g., 200)
      - spike_gain: multiplier * amp for SpikeEvent (e.g., 1.0)
      - dW_gain: multiplier * |dw| for DeltaWEvent (dw < 0 only; absolute value applied)
    """
    __slots__ = ("spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 200,
        keep_max: int | None = None,
        seed: int = 0,
        spike_gain: float = 1.0,
        dW_gain: float = 0.5,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        for e in events:
            k = getattr(e, "kind", None)
            if k == "spike" and isinstance(e, SpikeEvent) and int(getattr(e, "sign", 0)) < 0:
                self.add(int(e.node), int(tick), self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                dw = float(getattr(e, "dw", 0.0))
                if dw < 0.0:
                    self.add(int(e.node), int(tick), self.dW_gain * abs(dw))

    def snapshot(self) -> dict:
        s = super().snapshot()
        return {
            "inh_head": s["head"],
            "inh_p95": s["p95"],
            "inh_p99": s["p99"],
            "inh_max": s["max"],
            "inh_count": s["count"],
        }


__all__ = ["InhibitionMap"]
]]></content>
    </file>
    <file>
      <path>cortex/maps/memorymap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.memorymap
Purpose: Memory map view for scouts/UI with bounded head/dict (void-faithful, no scans).

Design
- Single source of truth for slow memory field m[i] must live outside maps/ (e.g., core/memory/field.py).
- This class acts as a thin VIEW/ADAPTER over that field when provided (preferred).
- If no field is provided, it can optionally run as a bounded reducer proxy (Pattern B) that folds events
  but only retains a small working set (no full-N vector).

Contracts
- snapshot() returns:
    {
      "memory_head": list[[node, value], ...],   # top-k bounded
      "memory_p95": float,
      "memory_p99": float,
      "memory_max": float,
      "memory_count": int,
      "memory_dict": {node: value}               # bounded dictionary
    }

Guardrails
- No global scans; bounded working set only when operating in reducer-proxy mode.
- When a field is attached, fold() is a no-op; view delegates to the field snapshot.
"""

from typing import Any, Dict, Iterable, List
import math
import random

from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class MemoryMap:
    """
    Thin view over MemoryField (preferred), with bounded fallback reducer (proxy) if field absent.

    Parameters:
      - field: optional memory field owner (preferred). When set, this map only adapts snapshots.
      - head_k: top-k head size for "memory_head"
      - dict_cap: maximum items to include in "memory_dict"
      - keep_max: maximum retained working-set size when operating in proxy mode (defaults to 16×head_k)
      - gamma/delta/kappa/touch_gain/spike_gain/dW_gain: only used in proxy mode
    """

    __slots__ = (
        "field",
        "head_k",
        "dict_cap",
        "keep_max",
        "rng",
        "_m",           # proxy-mode working set (absent when field provided)
        "_last_tick",   # proxy-mode last-tick tracker
        "gamma",
        "delta",
        "kappa",
        "touch_gain",
        "spike_gain",
        "dW_gain",
    )

    def __init__(
        self,
        field: Any | None = None,
        *,
        head_k: int = 256,
        dict_cap: int = 2048,
        keep_max: int | None = None,
        seed: int = 0,
        # proxy-mode dynamics
        gamma: float = 0.05,
        delta: float = 0.01,
        kappa: float = 0.10,
        touch_gain: float = 1.0,
        spike_gain: float = 0.20,
        dW_gain: float = 0.10,
    ) -> None:
        self.field = field
        self.head_k = int(max(8, head_k))
        self.dict_cap = int(max(8, dict_cap))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))

        # proxy-mode state (only used when field is None)
        self._m: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

        # proxy-mode parameters
        self.gamma = float(max(0.0, gamma))
        self.delta = float(max(0.0, min(1.0, delta)))
        self.kappa = float(max(0.0, kappa))
        self.touch_gain = float(max(0.0, touch_gain))
        self.spike_gain = float(max(0.0, spike_gain))
        self.dW_gain = float(max(0.0, dW_gain))

    # ---------------- adapter path (preferred) ----------------

    def _snapshot_from_field(self) -> Dict[str, object]:
        """Delegate snapshot to the owning field and adapt keys/caps."""
        fld = self.field
        if fld is None:
            return {}
        try:
            snap = fld.snapshot(head_n=self.head_k)  # expects keys memory_head/memory_dict/etc.
        except Exception:
            return {}
        if not isinstance(snap, dict):
            return {}

        head = snap.get("memory_head", []) or []
        dct = snap.get("memory_dict", {}) or {}

        # Cap dictionary size deterministically by highest values
        if isinstance(dct, dict) and len(dct) > self.dict_cap:
            try:
                import heapq as _heapq
                items = list(dct.items())
                top = _heapq.nlargest(int(self.dict_cap), items, key=lambda kv: kv[1])
                dct = {int(k): float(v) for k, v in top}
            except Exception:
                # Fallback: arbitrary trim
                keys = list(dct.keys())[: self.dict_cap]
                dct = {int(k): float(dct[k]) for k in keys if k in dct}

        p95 = snap.get("memory_p95", 0.0)
        p99 = snap.get("memory_p99", 0.0)
        vmax = snap.get("memory_max", 0.0)
        cnt = snap.get("memory_count", len(dct) if isinstance(dct, dict) else 0)

        return {
            "memory_head": head,
            "memory_p95": float(p95),
            "memory_p99": float(p99),
            "memory_max": float(vmax),
            "memory_count": int(cnt),
            "memory_dict": dct,
        }

    # ---------------- proxy-mode helpers (no field) ----------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            try:
                base = max(0.0, 1.0 - self.delta)
                factor = base ** dt
            except Exception:
                factor = math.exp(-self.delta * float(dt))
            self._m[node] = float(self._m.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def _ensure_and_decay(self, node: int, tick: int) -> None:
        n = int(node)
        if n not in self._m:
            self._m[n] = 0.0
            self._last_tick[n] = int(tick)
        else:
            self._decay_to(n, int(tick))

    def _prune(self) -> None:
        size = len(self._m)
        target_drop = size - self.keep_max
        if target_drop <= 0:
            return
        keys = list(self._m.keys())
        sample_size = min(len(keys), max(256, target_drop * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        sample.sort(key=lambda k: self._m.get(k, 0.0))
        for k in sample[:target_drop]:
            self._m.pop(k, None)
            self._last_tick.pop(k, None)

    # ---------------- API ----------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events.
        - If a field is attached, this is a no-op (owner already folds).
        - If no field, run bounded proxy updates (no scans).
        """
        if self.field is not None:
            return  # delegate model dynamics elsewhere

        t = int(tick)
        γ = self.gamma
        κ = self.kappa
        tg = self.touch_gain
        sg = self.spike_gain
        wg = self.dW_gain

        for e in events:
            k = getattr(e, "kind", None)

            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                try:
                    i = int(e.token)
                except Exception:
                    continue
                if i < 0:
                    continue
                self._ensure_and_decay(i, t)
                r_i = float(getattr(e, "w", 1.0))
                self._m[i] = float(self._m.get(i, 0.0)) + float(γ * tg * r_i)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                try:
                    u = int(getattr(e, "u", -1))
                    v = int(getattr(e, "v", -1))
                except Exception:
                    continue
                if u < 0 or v < 0:
                    continue
                self._ensure_and_decay(u, t)
                self._ensure_and_decay(v, t)
                mu = float(self._m.get(u, 0.0))
                mv = float(self._m.get(v, 0.0))
                d = float(κ * (mv - mu))
                self._m[u] = mu + d
                self._m[v] = mv - d
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "spike" and isinstance(e, SpikeEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                amp = float(getattr(e, "amp", 1.0))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * sg * amp)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                dw = abs(float(getattr(e, "dw", 0.0)))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * wg * dw)
                if len(self._m) > self.keep_max:
                    self._prune()

    def snapshot(self, head_n: int = 16) -> Dict[str, object]:
        """
        Return bounded snapshot dictionary per contract.
        - If field is present: adapt its snapshot and cap dict to dict_cap.
        - Else: produce from proxy working set (bounded by keep_max).
        """
        if self.field is not None:
            out = self._snapshot_from_field()
            if out:
                return out
            # fallthrough on error to proxy-mode snapshot (empty)

        if not self._m:
            return {
                "memory_head": [],
                "memory_p95": 0.0,
                "memory_p99": 0.0,
                "memory_max": 0.0,
                "memory_count": 0,
                "memory_dict": {},
            }

        # Head top-k
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._m.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._m.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]

        vals = sorted(float(v) for v in self._m.values())

        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])

        # Cap dictionary size
        if len(self._m) > self.dict_cap:
            try:
                import heapq as _heapq
                items = list(self._m.items())
                top = _heapq.nlargest(int(self.dict_cap), items, key=lambda kv: kv[1])
                out_dict: Dict[int, float] = {int(k): float(v) for k, v in top}
            except Exception:
                keys = list(self._m.keys())[: self.dict_cap]
                out_dict = {int(k): float(self._m[k]) for k in keys if k in self._m}
        else:
            out_dict = {int(k): float(v) for k, v in self._m.items()}

        return {
            "memory_head": [[int(k), float(v)] for k, v in head],
            "memory_p95": q(0.95),
            "memory_p99": q(0.99),
            "memory_max": float(vals[-1]),
            "memory_count": int(len(vals)),
            "memory_dict": out_dict,
        }


__all__ = ["MemoryMap"]]]></content>
    </file>
    <file>
      <path>cortex/maps/trailmap.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.cortex.maps.trailmap
Purpose: Short half-life trail/repulsion map updated only by events (void-faithful, no scans).

Design:
- Event-driven only; folds vt_touch and edge_on into a fast-decaying accumulator.
- Intended as a light repulsion field to discourage immediate retracing (fan-out).
- Bounded working set via BaseDecayMap.keep_max (no global scans).

Snapshot keys:
- trail_head: top-k [[node, score], ...]
- trail_dict: bounded dict {node: score} over current working set (len ≤ keep_max)
"""

from typing import Iterable, Dict

from .base_decay_map import BaseDecayMap
from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class TrailMap(BaseDecayMap):
    """
    Short half-life trail/repulsion map.

    Parameters:
      - half_life_ticks: decay half-life in ticks (defaults short, e.g., 50)
      - vt_touch_gain: increment for a node touch (small, e.g., 0.15)
      - edge_gain: increment applied to both endpoints of an edge_on (very small, e.g., 0.05)
      - spike_gain / dW_gain: optional small contributions to treat bursts as footprints
    """

    __slots__ = ("vt_touch_gain", "edge_gain", "spike_gain", "dW_gain")

    def __init__(
        self,
        head_k: int = 256,
        half_life_ticks: int = 50,
        keep_max: int | None = None,
        seed: int = 0,
        vt_touch_gain: float = 0.15,
        edge_gain: float = 0.05,
        spike_gain: float = 0.05,
        dW_gain: float = 0.02,
    ):
        super().__init__(head_k, half_life_ticks, keep_max, seed)
        self.vt_touch_gain = float(vt_touch_gain)
        self.edge_gain = float(edge_gain)
        self.spike_gain = float(spike_gain)
        self.dW_gain = float(dW_gain)

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events into the trail accumulator.

        Void-faithful:
        - Only uses provided events; no adjacency/weight scans.
        - Updates are strictly local to the nodes appearing in events.
        """
        t = int(tick)
        for e in events:
            k = getattr(e, "kind", None)
            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                self.add(int(e.token), t, self.vt_touch_gain * float(getattr(e, "w", 1.0)))
            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                # Apply a small footprint on both endpoints
                u = int(getattr(e, "u", -1))
                v = int(getattr(e, "v", -1))
                if u >= 0:
                    self.add(u, t, self.edge_gain)
                if v >= 0:
                    self.add(v, t, self.edge_gain)
            elif k == "spike" and isinstance(e, SpikeEvent):
                self.add(int(e.node), t, self.spike_gain * float(getattr(e, "amp", 1.0)))
            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                self.add(int(e.node), t, self.dW_gain * abs(float(e.dw)))

    def snapshot(self, head_n: int = 16) -> dict:
        """
        Export a bounded snapshot including both head list and the working-set dictionary.
        """
        s = super().snapshot(head_n=head_n)
        # Working-set dict is bounded by keep_max by construction
        d: Dict[int, float] = {int(k): float(v) for k, v in getattr(self, "_val", {}).items()}
        return {
            "trail_head": s["head"],
            "trail_p95": s["p95"],
            "trail_p99": s["p99"],
            "trail_max": s["max"],
            "trail_count": s["count"],
            "trail_dict": d,
        }


__all__ = ["TrailMap"]]]></content>
    </file>
    <file>
      <path>cortex/scouts.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.scouts (facade)

This module is now a thin aggregator that re-exports modular scout classes and maps.
It preserves legacy import paths while enforcing void-faithful, read-only traversal.

Key points:
- No global scans; scouts only use local neighbor reads and bounded TTL/budgets.
- This facade exposes:
    * VoidColdScoutWalker (ColdScout)
    * HeatScout, ExcitationScout, InhibitionScout
    * ColdMap (from maps.coldmap)
    * BaseScout (interface) via void_walkers.base
    * GDSPActuator / RevGSP re-exported from core.neuroplasticity (for legacy imports)

Contract compliance:
- Scouts emit only foldable events: vt_touch, edge_on, and (optionally) spike(+/-)
- They do not mutate the connectome (read-only), no scans, no schedulers.
"""

# Prefer modular implementations
from .void_walkers.void_cold_scout import ColdScout as VoidColdScoutWalker
from .void_walkers.void_heat_scout import HeatScout
from .void_walkers.void_ray_scout import VoidRayScout
from .void_walkers.void_memory_ray_scout import MemoryRayScout
from .void_walkers.void_frontier_scout import FrontierScout
from .void_walkers.void_cycle_scout import CycleHunterScout
from .void_walkers.void_sentinel_scout import SentinelScout
try:
    from .void_walkers.void_excitation_scout import ExcitationScout
except Exception:  # pragma: no cover - optional during staged migration
    class ExcitationScout:  # type: ignore
        pass
try:
    from .void_walkers.void_inhibition_scout import InhibitionScout
except Exception:  # pragma: no cover - optional during staged migration
    class InhibitionScout:  # type: ignore
        pass

# Maps
try:
    from .maps.coldmap import ColdMap
except Exception:  # pragma: no cover
    ColdMap = None  # type: ignore

# Base interface (allow both "scouts.base" and "scouts: BaseScout" import styles)
try:
    from .void_walkers.base import BaseScout  # type: ignore
except Exception:  # pragma: no cover
    BaseScout = None  # type: ignore

# Neuroplasticity re-exports for legacy imports
try:
    from ..neuroplasticity.gdsp import GDSPActuator
except Exception:  # pragma: no cover
    GDSPActuator = None  # type: ignore
try:
    from ..neuroplasticity.revgsp import RevGSP
except Exception:  # pragma: no cover
    RevGSP = None  # type: ignore

__all__ = [
    "VoidColdScoutWalker",
    "HeatScout",
    "ExcitationScout",
    "InhibitionScout",
    "VoidRayScout",
    "MemoryRayScout",
    "FrontierScout",
    "CycleHunterScout",
    "SentinelScout",
    "ColdMap",
    "BaseScout",
    "GDSPActuator",
    "RevGSP",
]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/README.md</path>
      <content/>
    </file>
    <file>
      <path>cortex/void_walkers/base.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.base

Void-faithful, read-only scout base class.
- No global scans or dense conversions; no direct access to raw weight arrays or external graph libraries.
- Operates only on local neighbor reads provided by the active graph.
- Emits only small, foldable events for reducers and telemetry.

Contract:
- step(connectome, bus, maps, budget) - returns a list[BaseEvent]
  * connectome: object exposing N and neighbors/get_neighbors or adj mapping
  * bus: opaque (optional) announce bus; NOT used for writes here (read-only scouts emit events to return)
  * maps: optional dict-like snapshots; subclasses may consult e.g. {"heat_head":[[n,score],...]}
  * budget: optional dict with keys:
      - "visits": int (node touches to attempt)
      - "edges": int (edge probes to attempt)
      - "ttl":   int (max walk depth per seed)
      - "tick":  int (current tick for event timestamps)
      - "seeds": Sequence[int] (preferred start nodes; bounded; falls back to map heads or uniform)

Returned events use only core event types:
- VTTouchEvent(kind="vt_touch", t, token=node, w=1.0)
- EdgeOnEvent(kind="edge_on", t, u, v)
- Subclasses may add SpikeEvent with sign bias (still event-only).

This module defines the common, safe scaffolding. Heuristics live in subclasses.
"""

from typing import Any, Iterable, List, Optional, Sequence, Set, Dict
import random

from fum_rt.core.proprioception.events import (
    BaseEvent,
    VTTouchEvent,
    EdgeOnEvent,
    SpikeEvent,  # subclasses may use; base does not emit spikes
)


class BaseScout:
    __slots__ = ("budget_visits", "budget_edges", "ttl", "rng")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
    ) -> None:
        self.budget_visits = int(max(0, budget_visits))
        self.budget_edges = int(max(0, budget_edges))
        self.ttl = int(max(1, ttl))
        self.rng = random.Random(int(seed))

    # ---------------------- connectome helpers (read-only) ----------------------

    @staticmethod
    def _get_N(C: Any) -> int:
        try:
            N = int(getattr(C, "N", 0))
            if N > 0:
                return N
        except Exception:
            pass
        try:
            W = getattr(C, "W", None)
            shp = getattr(W, "shape", None)
            if shp and isinstance(shp, (tuple, list)) and len(shp) >= 1:
                n = int(shp[0])
                return n if n > 0 else 0
        except Exception:
            pass
        return 0

    @staticmethod
    def _neighbors(C: Any, u: int) -> List[int]:
        # Prefer explicit methods
        try:
            for meth in ("neighbors", "get_neighbors"):
                fn = getattr(C, meth, None)
                if callable(fn):
                    xs = fn(int(u))
                    if xs:
                        try:
                            return [int(x) for x in list(xs)]
                        except Exception:
                            return []
        except Exception:
            pass
        # Fallback: adjacency mapping
        try:
            adj = getattr(C, "adj", None)
            if isinstance(adj, dict):
                vals = adj.get(int(u), [])
                if isinstance(vals, dict):
                    return [int(x) for x in vals.keys()]
                return [int(x) for x in list(vals)]
        except Exception:
            pass
        return []

    # --------------------------- heuristic hooks --------------------------------

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        """
        Subclasses may override to bias routing locally using heads from reducers.
        Returns a bounded set of node indices to prefer when available.
        """
        return set()

    # ------------------------------ main API ------------------------------------

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused (read-only)
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        """
        Bounded, TTL-limited local exploration that returns foldable events.
        Default strategy:
        - Touch up to 'visits' seeds (uniform from [0..N) if no priority).
        - For each, walk up to TTL steps, emitting vt_touch on the current node
          and best-effort edge_on to a locally chosen neighbor (biased by priority set).
        - Edge probes total bounded by 'edges'.
        """
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        # Seed pool: prefer explicit seeds, else priority, else uniform
        seeds = None
        if isinstance(budget, dict):
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None
        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()
        pool: Sequence[int]
        if seeds:
            try:
                pool = tuple(int(s) for s in seeds if 0 <= int(s) < N)
                if not pool:
                    pool = tuple(priority) if priority else tuple(range(N))
            except Exception:
                pool = tuple(priority) if priority else tuple(range(N))
        else:
            pool = tuple(priority) if priority else tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            # TTL-limited micro-walk starting at u
            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node (coverage)
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor(neigh, priority)
                        if v is not None and v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            # random hop when no preference applies
                            try:
                                cur = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                    else:
                        break
                else:
                    break

                depth += 1

        return events

    # -------------------------- local routing policy ----------------------------

    def _pick_neighbor(self, neigh: Sequence[int], priority: Set[int]) -> Optional[int]:
        """
        Choose a neighbor biased toward 'priority' set when available, else blue-noise hop.
        """
        try:
            # Filter by priority first
            pref = [int(x) for x in neigh if int(x) in priority]
            if pref:
                return int(self.rng.choice(pref))
            # Blue-noise hop (random choice)
            return int(self.rng.choice(tuple(neigh)))
        except Exception:
            return None]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/frontier_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Shim module for naming convention.
Use void-prefixed class from [void_frontier_scout.py](fum_rt/core/cortex/void_walkers/void_frontier_scout.py).
"""

from .void_frontier_scout import FrontierScout

__all__ = ["FrontierScout"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/runner.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.runner

Stateless, per-tick scout executor (void-faithful, no schedulers).
- Runs a bounded list of read-only scouts exactly once per tick.
- Enforces a micro time budget (microseconds) across all scouts.
- Accepts optional seeds (e.g., recent UTE indices) and map heads (heat/exc/inh/cold).
- Emits only foldable events (vt_touch, edge_on, optional spike/delta_w); no writes.

Usage (in runtime loop per tick):
    from fum_rt.core.cortex.void_walkers.runner import run_scouts_once as _run_scouts_once
    evs = _run_scouts_once(connectome, scouts, maps, budget, bus, max_us)

Notes:
- No timers, no cadence, no background threads. This is a pure function called once per tick.
- Drop-oldest behavior is delegated to the downstream bus implementation when publish_many is used.
"""

from typing import Any, Dict, Iterable, List, Optional, Sequence
from time import perf_counter_ns
import os as _os

from fum_rt.core.proprioception.events import BaseEvent


def _truthy(x: Any) -> bool:
    try:
        if isinstance(x, (int, float, bool)):
            return bool(x)
        s = str(x).strip().lower()
        return s in ("1", "true", "yes", "on", "y", "t")
    except Exception:
        return False


def run_scouts_once(
    connectome: Any,
    scouts: Sequence[Any],
    maps: Optional[Dict[str, Any]] = None,
    budget: Optional[Dict[str, int]] = None,
    bus: Any = None,
    max_us: int = 2000,
) -> List[BaseEvent]:
    """
    Execute a bounded batch of scouts exactly once for this tick.

    Parameters:
      - connectome: object exposing read-only neighbor access (N, neighbors/get_neighbors or adj mapping)
      - scouts: sequence of instantiated scout objects with .step(connectome, bus, maps, budget) -> list[BaseEvent]
      - maps: optional dict of map heads: {"heat_head": [[node,score],...], "exc_head": [...], "inh_head": [...], "cold_head": [...]}
      - budget: {"visits": int, "edges": int, "ttl": int, "tick": int, "seeds": list[int]} (any subset)
      - bus: optional announce bus; when present, publish_many(evs) is invoked once at end
      - max_us: total per-tick microsecond budget across all scouts

    Returns:
      - list of BaseEvent emitted by all scouts within budget
    """
    evs: List[BaseEvent] = []
    if not scouts:
        return evs

    # Ensure safe numeric bounds
    try:
        max_us = int(max(0, int(max_us)))
    except Exception:
        max_us = 0  # 0 → gather but still permit at least the first scout call if desired

    t0 = perf_counter_ns()

    # Fairness: rotate starting scout by tick (round-robin) to avoid starvation
    start_idx = 0
    try:
        if isinstance(budget, dict):
            start_idx = int(budget.get("tick", 0))
    except Exception:
        start_idx = 0

    ordered: List[Any] = list(scouts or [])
    n_sc = len(ordered)
    if n_sc > 0 and start_idx:
        try:
            k = start_idx % n_sc
            ordered = ordered[k:] + ordered[:k]
        except Exception:
            # fallback: keep original order
            ordered = list(scouts or [])

    # Optional per-scout micro-slice (still one-shot runner; no schedulers)
    per_us = 0
    try:
        per_us = int(_os.getenv("SCOUTS_PER_SCOUT_US", "0"))
    except Exception:
        per_us = 0
    if per_us <= 0 and max_us > 0 and n_sc > 0:
        per_us = int(max_us // max(1, n_sc))

    for sc in ordered:
        # Global time guard (drop rest on over-budget)
        if max_us > 0:
            elapsed_us = (perf_counter_ns() - t0) // 1000
            if elapsed_us >= max_us:
                break

        sc_t0 = perf_counter_ns()
        try:
            out = sc.step(connectome=connectome, bus=None, maps=maps, budget=budget) or []
        except Exception:
            out = []
        if out:
            evs.extend(out)

        # Per-scout guard (best-effort; cannot preempt inside step)
        if per_us > 0:
            sc_elapsed_us = (perf_counter_ns() - sc_t0) // 1000
            if sc_elapsed_us > per_us:
                # soft-guard only: we don't penalize the scout, but this informs future tuning
                pass

    # Publish once (drop-oldest semantics live in bus implementation)
    if evs and bus is not None:
        try:
            if hasattr(bus, "publish_many"):
                bus.publish_many(evs)
            else:
                # bounded fallback
                for e in evs:
                    try:
                        bus.publish(e)  # type: ignore[attr-defined]
                    except Exception:
                        break
        except Exception:
            pass

    return evs


__all__ = ["run_scouts_once"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_cold_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_cold_scout

ColdScout (read-only, void-faithful):
- Prefers neighbors whose node ids appear in ColdMap snapshot head ("cold_head").
- Emits only vt_touch and edge_on events.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Compatibility:
- Provides alias VoidColdScoutWalker for existing imports in legacy code paths.
"""

from typing import Any, Dict, List, Optional, Sequence, Set
from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded head nodes from map snapshot structure: [[node, score], ...]
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class ColdScout(BaseScout):
    """
    Coldness-driven scout: routes toward nodes with higher coldness (less recently seen).
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        return _extract_head_nodes(maps, "cold_head", cap=max(64, self.budget_visits * 8))


# Back-compat alias used by runtime/engine wiring in existing code
VoidColdScoutWalker = ColdScout

__all__ = ["ColdScout", "VoidColdScoutWalker"]
]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_cycle_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_cycle_scout

CycleHunterScout (read-only, void-faithful):
- Seeks short cycles (3-6 hops) using a TTL-limited walk with a tiny path window.
- Purely local: only neighbor lists are read; no global scans or dense conversions.
- Emits vt_touch and edge_on events; reducers can infer cycle hits from returned edge traces.

Heuristic:
- Maintain a small deque of the recent path (window ~ 5).
- Prefer stepping to a neighbor that is already in the recent window (closes a short cycle).
- Otherwise, hop randomly (blue-noise) among neighbors.

Guardrails:
- No schedulers; executes once per tick under the runner.
- Bounded budgets: visits, edges, ttl.
- No writes; events only.
"""

from typing import Any, Dict, Optional, Sequence, Set, List, Deque
from collections import deque
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


class CycleHunterScout(BaseScout):
    """
    Short-cycle finder with tiny path memory.
    """

    __slots__ = ("window",)

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        window: int = 5,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.window = int(max(2, window))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Neutral: let runner seeds drive locality; no external heads required.
        return set()

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        # Seed pool: prefer runner-provided seeds; else uniform
        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(range(N))
            except Exception:
                pool = tuple(range(N))
        else:
            pool = tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            path: Deque[int] = deque(maxlen=self.window)

            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                path.append(int(cur))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if not neigh:
                        break

                    # Prefer neighbors that are in the recent path window (cycle closure)
                    try:
                        path_set = set(path)
                    except Exception:
                        path_set = set(int(x) for x in path) if path else set()

                    pref = [int(v) for v in neigh if int(v) in path_set and int(v) != int(cur)]
                    if pref:
                        v = int(self.rng.choice(pref))
                    else:
                        # Blue-noise hop
                        try:
                            v = int(self.rng.choice(tuple(neigh)))
                        except Exception:
                            break

                    if v != cur:
                        events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                        edges_emitted += 1
                        cur = int(v)
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["CycleHunterScout"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_excitation_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_excitation_scout

ExcitationScout (read-only, void-faithful):
- Duty: Map excitatory corridors, feeding ExcitationMap strictly via events.
- Strategy: Seed from ExcitationMap.exc_head; during walk, emit VTTouchEvent per visit and
            synthesize SpikeEvent(node, amp≈local_exc, sign=+1) with bounded amplitude in [0,1].
            Occasional EdgeOnEvent samples are produced by BaseScout's bounded walk; no global scans.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Physics alignment (docs in /derivation):
- finite_tube_mode_analysis.md, discrete_to_continuum.md: fast φ-fronts (c^2 = 2 J a^2) guide recent activity.
- memory_steering.md: scouts do not write; they only observe and announce, keeping the φ sector void-faithful.
"""

from typing import Any, Dict, List, Optional, Set

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, SpikeEvent


def _head_lookup(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Dict[int, float]:
    """
    Build a bounded lookup {node: norm_score in [0,1]} from a head list [[node, score], ...].
    Normalization: divide by max(score) over the truncated head; empty -> {}.
    """
    if not isinstance(maps, dict):
        return {}
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        pairs: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                pairs.append((n, s))
        if not pairs:
            return {}
        vmax = max(s for _, s in pairs) or 1.0
        return {n: max(0.0, min(1.0, s / vmax)) for (n, s) in pairs}
    except Exception:
        return {}


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded set of node ids from a head list [[node, score], ...].
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class ExcitationScout(BaseScout):
    """
    Excitation-driven scout: routes toward nodes with higher ExcitationMap scores.
    Adds SpikeEvent(sign=+1) upon each vt_touch visit with amplitude ~ local excitation.
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer ExcitationMap head indices
        return _extract_head_nodes(maps, "exc_head", cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Use BaseScout bounded walk to generate vt_touch and edge_on
        base_events = super().step(connectome, bus=bus, maps=maps, budget=budget)

        # Build local excitation amplitude lookup from snapshot head (bounded, read-only)
        exc_lookup = _head_lookup(maps, "exc_head", cap=max(64, self.budget_visits * 8))

        out: List[BaseEvent] = []
        for e in base_events:
            out.append(e)
            if getattr(e, "kind", None) == "vt_touch":
                token = getattr(e, "token", None)
                try:
                    node = int(token)
                except Exception:
                    node = None
                if node is not None and node >= 0:
                    # Amplitude in [0,1]; default to 0.5 when not found
                    amp = float(exc_lookup.get(node, 0.5))
                    out.append(SpikeEvent(kind="spike", t=getattr(e, "t", None), node=node, amp=amp, sign=+1))
        return out


__all__ = ["ExcitationScout"]
]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_frontier_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_frontier_scout

FrontierScout (read-only, void-faithful):
- Skims component boundaries and likely bridge frontiers to refresh cohesion/cycle estimators.
- Purely local heuristics; no scans. Emits vt_touch and edge_on only.

Local neighbor score for hop u→j (bounded, read-only):
    s_j = + w_cold * cold[j]
          - w_heat * heat[j]
          - w_shn  * shared_neighbors(u, j)
          + w_deg  * I[deg(j) != deg(u)]

Inputs (optional):
- maps["cold_head"] / maps["heat_head"] to derive small dicts (bounded).
- Only local neighbor lists are read; no global adjacency or dense-array access.

Guardrails:
- No schedulers; TTL/budgets enforce bounds.
- No writes; events only.
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_dict(maps: Optional[Dict[str, Any]], key: str, cap: int = 1024) -> Dict[int, float]:
    out: Dict[int, float] = {}
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        vmax = 0.0
        tmp: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                tmp.append((n, s))
                vmax = max(vmax, s)
        if vmax <= 0.0:
            for n, s in tmp:
                out[n] = 1.0
        else:
            for n, s in tmp:
                out[n] = max(0.0, min(1.0, s / vmax))
    except Exception:
        return out
    return out


class FrontierScout(BaseScout):
    """
    Boundary/cohesion probe: prefer edges that look like weak cuts or cross-degree boundaries.
    """

    __slots__ = ("w_cold", "w_heat", "w_shn", "w_deg", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        w_cold: float = 1.0,
        w_heat: float = 0.5,
        w_shn: float = 0.25,
        w_deg: float = 0.5,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.w_cold = float(max(0.0, w_cold))
        self.w_heat = float(max(0.0, w_heat))
        self.w_shn = float(max(0.0, w_shn))
        self.w_deg = float(max(0.0, w_deg))
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer coldest tiles as starting seeds
        out: Set[int] = set()
        if not isinstance(maps, dict):
            return out
        try:
            head = maps.get("cold_head", []) or []
            for pair in head[: max(64, self.budget_visits * 8)]:
                try:
                    n = int(pair[0])
                    if n >= 0:
                        out.add(n)
                except Exception:
                    continue
        except Exception:
            return out
        return out

    @staticmethod
    def _shared_neighbors(connectome: Any, u: int, v: int, cap: int = 128) -> int:
        try:
            nu = set(int(x) for x in (connectome.neighbors(u) or []))  # type: ignore[attr-defined]
        except Exception:
            try:
                nu = set(int(x) for x in (connectome.get_neighbors(u) or []))  # type: ignore[attr-defined]
            except Exception:
                nu = set()
        try:
            nv_list = (connectome.neighbors(v) or [])  # type: ignore[attr-defined]
        except Exception:
            try:
                nv_list = (connectome.get_neighbors(v) or [])  # type: ignore[attr-defined]
            except Exception:
                nv_list = []
        # Bound cost: only check up to 'cap' neighbors of v
        cnt = 0
        for x in list(nv_list)[: max(0, int(cap))]:
            try:
                if int(x) in nu:
                    cnt += 1
            except Exception:
                continue
        return int(cnt)

    @staticmethod
    def _deg(connectome: Any, u: int) -> int:
        try:
            xs = connectome.neighbors(u)  # type: ignore[attr-defined]
        except Exception:
            try:
                xs = connectome.get_neighbors(u)  # type: ignore[attr-defined]
            except Exception:
                xs = []
        try:
            return int(len(xs or []))
        except Exception:
            return 0

    def _pick_neighbor_scored(
        self,
        cur: int,
        neigh: Sequence[int],
        connectome: Any,
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        cold = _head_to_dict(maps, "cold_head", cap=1024)
        heat = _head_to_dict(maps, "heat_head", cap=1024)

        du = self._deg(connectome, int(cur))
        logits: List[tuple[int, float]] = []
        inv_tau = 1.0 / self.tau
        for v in neigh:
            j = int(v)
            shn = float(self._shared_neighbors(connectome, int(cur), j, cap=64))
            dj = self._deg(connectome, j)
            s = (
                (self.w_cold * float(cold.get(j, 0.0)))
                - (self.w_heat * float(heat.get(j, 0.0)))
                - (self.w_shn * shn)
                + (self.w_deg * (1.0 if dj != du else 0.0))
            )
            logits.append((j, s * inv_tau))

        # Softmax
        try:
            m = max(l for _, l in logits)
            ws = [math.exp(l - m) for _, l in logits]
            Z = sum(ws)
            if Z <= 0.0:
                return int(logits[0][0])
            r = (hash((cur, du, len(neigh))) & 0xFFFF) / 65535.0 * Z
            acc = 0.0
            for (i, _), w in zip(logits, ws):
                acc += w
                if r <= acc:
                    return i
            return int(logits[-1][0])
        except Exception:
            try:
                return int(logits[0][0])
            except Exception:
                return None

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(int(cur), neigh, connectome, maps)
                        if v is None or v == cur:
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["FrontierScout"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_heat_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_heat_scout

HeatScout (read-only, void-faithful):
- Local-only neighbor selection using a softmax over map signals.
- Supports trail repulsion (short-term) and optional memory steering (long-term).
- Emits vt_touch and edge_on events; no writes; no scans.

Logit per neighbor j:
    logit_j = theta_mem * m_j - rho_trail * htrail_j + gamma_heat * h_j

Where:
- m_j: slow memory value (maps.get("memory_dict", {})[j]) if provided; else 0.
- htrail_j: short-term trail/heat value (maps["trail_dict"] if present, else maps["heat_dict"]; fallback 0).
- h_j: HeatMap score (maps["heat_dict"] if present; fallback 0).
- theta_mem (can be ±) sets attraction (>) or repulsion (<) to memory.
- rho_trail >= 0 repels recently traversed/hot nodes.
- gamma_heat >= 0 biases toward heat fronts when desired (default 1.0).
- tau > 0 is temperature (lower tau = sharper decisions).

Notes:
- If maps dicts are absent, falls back to priority head nodes (if any), then blue-noise hop.
- Priority seed set still used for initial pool bias via _priority_set().
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


def _head_to_dict(maps: Optional[Dict[str, Any]], key: str, cap: int = 2048) -> Dict[int, float]:
    d: Dict[int, float] = {}
    if not isinstance(maps, dict):
        return d
    try:
        head = maps.get(key, []) or []
        if isinstance(head, dict):
            # already a dict
            for k, v in list(head.items())[: cap]:
                try:
                    d[int(k)] = float(v)
                except Exception:
                    continue
            return d
        for pair in head[: cap]:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                d[n] = s
    except Exception:
        return d
    return d


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            return random.choice([i for i, _ in pairs])
        r = random.random() * Z
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(random.choice([i for i, _ in pairs]))
        except Exception:
            return None


class HeatScout(BaseScout):
    """
    Activity-driven scout with optional memory steering and trail repulsion.
    Defaults preserve legacy behavior (follow heat; no memory, no repulsion).
    """

    __slots__ = ("theta_mem", "rho_trail", "gamma_heat", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        theta_mem: float = 0.0,
        rho_trail: float = 0.0,
        gamma_heat: float = 1.0,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.theta_mem = float(theta_mem)
        self.rho_trail = float(max(0.0, rho_trail))
        self.gamma_heat = float(max(0.0, gamma_heat))
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer HeatMap head indices for seeds
        return _head_to_set(maps, "heat_head", cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        neigh: Sequence[int],
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        # Pull small dicts from maps (bounded heads or dict snapshots)
        md = maps.get("memory_dict", {}) if isinstance(maps, dict) else {}
        hd = maps.get("heat_dict", {}) if isinstance(maps, dict) else {}
        td = maps.get("trail_dict", {}) if isinstance(maps, dict) else {}
        # Allow fallback to heat as trail if explicit trail absent
        use_td = td if td else hd

        logits: List[tuple[int, float]] = []
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            try:
                htrail_j = float(use_td.get(j, 0.0))
            except Exception:
                htrail_j = 0.0
            try:
                h_j = float(hd.get(j, 0.0))
            except Exception:
                h_j = 0.0
            s = (self.theta_mem * m_j) - (self.rho_trail * htrail_j) + (self.gamma_heat * h_j)
            logits.append((j, s / self.tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Inline copy of BaseScout.step to insert map-aware neighbor choice.
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(neigh, maps)
                        if v is None or v == cur:
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break
                depth += 1

        return events


__all__ = ["HeatScout"]
]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_inhibition_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_inhibition_scout

InhibitionScout (read-only, void-faithful):
- Duty: Map inhibitory ridges, feeding InhibitionMap strictly via events.
- Strategy: Seed from InhibitionMap.inh_head; during walk, emit VTTouchEvent per visit and
            synthesize SpikeEvent(node, amp≈local_inh, sign=-1) with bounded amplitude in [0,1].
            Occasional EdgeOnEvent samples are produced by BaseScout's bounded walk; no global scans.
- No scans of global structures; uses local neighbor reads and bounded TTL/budgets.

Physics alignment (docs in /derivation):
- finite_tube_mode_analysis.md, discrete_to_continuum.md: fast φ-fronts inform recent activity.
- memory_steering.md: scouts only observe and announce (no writes), keeping φ sector void-faithful.
"""

from typing import Any, Dict, List, Optional, Set

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, SpikeEvent


def _head_lookup(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Dict[int, float]:
    """
    Build a bounded lookup {node: norm_score in [0,1]} from a head list [[node, score], ...].
    Normalization: divide by max(score) over the truncated head; empty -> {}.
    """
    if not isinstance(maps, dict):
        return {}
    try:
        head = maps.get(key, []) or []
        head = head[: cap]
        pairs: List[tuple[int, float]] = []
        for pair in head:
            try:
                n = int(pair[0])
                s = float(pair[1]) if len(pair) > 1 else 1.0
            except Exception:
                continue
            if n >= 0:
                pairs.append((n, s))
        if not pairs:
            return {}
        vmax = max(s for _, s in pairs) or 1.0
        return {n: max(0.0, min(1.0, s / vmax)) for (n, s) in pairs}
    except Exception:
        return {}


def _extract_head_nodes(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    """
    Extract bounded set of node ids from a head list [[node, score], ...].
    """
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


class InhibitionScout(BaseScout):
    """
    Inhibition-driven scout: routes toward nodes with higher InhibitionMap scores.
    Adds SpikeEvent(sign=-1) upon each vt_touch visit with amplitude ~ local inhibition.
    """

    __slots__ = ()

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer InhibitionMap head indices
        return _extract_head_nodes(maps, "inh_head", cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        # Use BaseScout bounded walk to generate vt_touch and edge_on
        base_events = super().step(connectome, bus=bus, maps=maps, budget=budget)

        # Build local inhibition amplitude lookup from snapshot head (bounded, read-only)
        inh_lookup = _head_lookup(maps, "inh_head", cap=max(64, self.budget_visits * 8))

        out: List[BaseEvent] = []
        for e in base_events:
            out.append(e)
            if getattr(e, "kind", None) == "vt_touch":
                token = getattr(e, "token", None)
                try:
                    node = int(token)
                except Exception:
                    node = None
                if node is not None and node >= 0:
                    # Amplitude in [0,1]; default to 0.5 when not found
                    amp = float(inh_lookup.get(node, 0.5))
                    out.append(SpikeEvent(kind="spike", t=getattr(e, "t", None), node=node, amp=amp, sign=-1))
        return out


__all__ = ["InhibitionScout"]
]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_memory_ray_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_memory_ray_scout

MemoryRayScout (read-only, void-faithful):
- Implements refractive-index steering using a slow memory field m.
- Local selection: P(i→j) ∝ exp(Theta * m[j]) with temperature tau (Boltzmann choice).
- Falls back to HeatMap head/dict when memory is absent to keep behavior useful OOTB.
- Emits vt_touch and edge_on events; no writes; no scans.

Signals (read-only):
- maps["memory_dict"] (preferred): bounded dict {node: value}
- maps["memory_head"] (optional): head list [[node, score], ...] for seeds
- Fallbacks:
  * maps["heat_dict"] / maps["heat_head"] used when memory is not available

Guardrails:
- No global scans or dense conversions; neighbors only.
- No schedulers; TTL/budget bounded; emits compact events only.

Fork law (two-branch junction):
- P(A) = sigmoid(Theta * (m_A - m_B)) for tau = 1, aligning with Derivation/memory_steering.md
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math
import random

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], keys: Sequence[str], cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    for key in keys:
        try:
            head = maps.get(key, []) or []
            for pair in head[: cap]:
                try:
                    n = int(pair[0])
                except Exception:
                    continue
                if n >= 0:
                    out.add(n)
        except Exception:
            continue
    return out


def _dict_from_maps(maps: Optional[Dict[str, Any]], keys: Sequence[str]) -> Dict[int, float]:
    if not isinstance(maps, dict):
        return {}
    for key in keys:
        try:
            d = maps.get(key, {}) or {}
            # Accept dict snapshots directly; if head list was mistakenly passed, adapt minimally
            if isinstance(d, dict):
                return {int(k): float(v) for k, v in d.items()}  # type: ignore[arg-type]
            if isinstance(d, list):
                out: Dict[int, float] = {}
                for pair in d:
                    try:
                        n = int(pair[0])
                        s = float(pair[1]) if len(pair) > 1 else 1.0
                    except Exception:
                        continue
                    if n >= 0:
                        out[n] = s
                if out:
                    return out
        except Exception:
            continue
    return {}


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            return random.choice([i for i, _ in pairs])
        r = random.random() * Z
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(random.choice([i for i, _ in pairs]))
        except Exception:
            return None


class MemoryRayScout(BaseScout):
    """
    Memory-driven scout: routes toward neighbors with higher memory values m[j].
    """

    __slots__ = ("theta_mem", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        theta_mem: float = 0.8,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.theta_mem = float(theta_mem)
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer memory head; fallback to heat head for useful boot behavior
        return _head_to_set(maps, keys=("memory_head", "heat_head"), cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        neigh: Sequence[int],
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None
        # Prefer memory; fallback to heat as slow proxy
        md = _dict_from_maps(maps, keys=("memory_dict", "heat_dict"))
        logits: List[tuple[int, float]] = []
        th = float(self.theta_mem)
        inv_tau = 1.0 / float(self.tau)
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            s = th * m_j
            logits.append((j, s * inv_tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(neigh, maps)
                        if v is None or v == cur:
                            # fallback to blue-noise hop
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["MemoryRayScout"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_ray_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_ray_scout

VoidRayScout (read-only, void-faithful):
- Physics-aware routing that prefers neighbors with favorable local change in a fast field φ.
- Local scoring (no scans): for hop i→j, s_j = lambda_phi * (φ[j] - φ[i]) + theta_mem * m[j]
- Temperatured choice via softmax over neighbors; strictly local reads.
- Emits vt_touch and edge_on events; optional spike can be added by subclasses if needed.

Signals (read-only):
- connectome.phi: per-node scalar field (Sequence/ndarray-like) when present; otherwise treated as zeros.
- maps["memory_dict"]: optional slow memory field (bounded dict snapshot), default empty.

Guardrails:
- No global scans or dense conversions.
- Operates only on local neighbor lists and small map snapshots.
- No schedulers; TTL/budget bounded; emits compact events only.

References:
- Refractive-index steering law: P(i→j) ∝ exp(Θ · m[j]) with logistic 2-way fork (see Derivation/memory_steering.md).
"""

from typing import Any, Dict, Optional, Set, Sequence, List
import math

from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], key: str, cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    try:
        head = maps.get(key, []) or []
        for pair in head[: cap]:
            try:
                n = int(pair[0])
            except Exception:
                continue
            if n >= 0:
                out.add(n)
    except Exception:
        return out
    return out


def _softmax_choice(pairs: Sequence[tuple[int, float]]) -> Optional[int]:
    if not pairs:
        return None
    try:
        m = max(l for _, l in pairs)
        ws = [math.exp(l - m) for _, l in pairs]
        Z = sum(ws)
        if Z <= 0.0:
            # fallback to uniform pick among candidates
            return pairs[0][0]
        r = (hash((len(pairs), m, Z)) & 0xFFFF) / 65535.0 * Z  # deterministic-ish fallback
        acc = 0.0
        for (i, _), w in zip(pairs, ws):
            acc += w
            if r <= acc:
                return i
        return pairs[-1][0]
    except Exception:
        try:
            return int(pairs[0][0])
        except Exception:
            return None


class VoidRayScout(BaseScout):
    """
    Physics-aware scout: routes along favorable local φ gradients with optional memory steering.
    """

    __slots__ = ("lambda_phi", "theta_mem", "tau")

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 64,
        seed: int = 0,
        *,
        lambda_phi: float = 1.0,
        theta_mem: float = 0.0,
        tau: float = 1.0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)
        self.lambda_phi = float(lambda_phi)
        self.theta_mem = float(theta_mem)
        self.tau = float(max(1e-6, tau))

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer HeatMap head when available for initial seeds (bounded, read-only)
        return _head_to_set(maps, "heat_head", cap=max(64, self.budget_visits * 8))

    def _pick_neighbor_scored(
        self,
        cur: int,
        neigh: Sequence[int],
        connectome: Any,
        maps: Optional[Dict[str, Any]],
    ) -> Optional[int]:
        if not neigh:
            return None

        # φ values: read only local entries (no scans)
        phi = getattr(connectome, "phi", None)

        def _phi(idx: int) -> float:
            try:
                if phi is None:
                    return 0.0
                return float(phi[idx])
            except Exception:
                return 0.0

        phi_i = _phi(int(cur))
        md = maps.get("memory_dict", {}) if isinstance(maps, dict) else {}

        logits: List[tuple[int, float]] = []
        for v in neigh:
            j = int(v)
            try:
                m_j = float(md.get(j, 0.0))
            except Exception:
                m_j = 0.0
            s = (self.lambda_phi * (_phi(j) - phi_i)) + (self.theta_mem * m_j)
            logits.append((j, s / self.tau))
        return _softmax_choice(logits)

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                ttl = int(budget.get("ttl", ttl))
            except Exception:
                pass
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = max(1, ttl)

        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Touch current node
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Probe one neighbor edge if budget remains
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        v = self._pick_neighbor_scored(cur, neigh, connectome, maps)
                        if v is None or v == cur:
                            # fallback to blue-noise hop
                            try:
                                v = int(self.rng.choice(tuple(neigh)))
                            except Exception:
                                break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["VoidRayScout"]]]></content>
    </file>
    <file>
      <path>cortex/void_walkers/void_sentinel_scout.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.cortex.void_walkers.void_sentinel_scout

SentinelScout (read-only, void-faithful):
- Blue-noise reseeder / de-trample walker.
- Purpose: prevent path lock-in by sampling uniformly across space and announcing coverage.
- Emits vt_touch for coverage and opportunistic edge_on (one hop) when neighbors exist.

Behavior:
- Seeds = budget["seeds"] when provided (e.g., recent UTE indices) else uniform random nodes.
- TTL kept minimal (default 1) to avoid trampling and keep cost bounded.
- Local reads only (neighbors of the current node); no scans; no writes.

Optional inputs (maps):
- "visit_head" or "cold_head" can bias seeds slightly when present; still bounded heads.
"""

from typing import Any, Dict, Optional, Sequence, Set, List
from fum_rt.core.cortex.void_walkers.base import BaseScout
from fum_rt.core.proprioception.events import BaseEvent, VTTouchEvent, EdgeOnEvent


def _head_to_set(maps: Optional[Dict[str, Any]], keys: Sequence[str], cap: int = 512) -> Set[int]:
    out: Set[int] = set()
    if not isinstance(maps, dict):
        return out
    for key in keys:
        try:
            head = maps.get(key, []) or []
            for pair in head[: cap]:
                try:
                    n = int(pair[0])
                except Exception:
                    continue
                if n >= 0:
                    out.add(n)
        except Exception:
            continue
    return out


class SentinelScout(BaseScout):
    """
    Blue-noise reseeding walker with minimal TTL to refresh coverage.
    """

    __slots__ = ()

    def __init__(
        self,
        budget_visits: int = 16,
        budget_edges: int = 8,
        ttl: int = 1,   # one hop per seed by default
        seed: int = 0,
    ) -> None:
        super().__init__(budget_visits=budget_visits, budget_edges=budget_edges, ttl=ttl, seed=seed)

    def _priority_set(self, maps: Optional[Dict[str, Any]]) -> Set[int]:
        # Prefer low-visit or cold heads when available; bounded and read-only
        return _head_to_set(maps, keys=("visit_head", "cold_head"), cap=max(64, self.budget_visits * 8))

    def step(
        self,
        connectome: Any,
        bus: Any = None,  # unused
        maps: Optional[Dict[str, Any]] = None,
        budget: Optional[Dict[str, int]] = None,
    ) -> List[BaseEvent]:
        events: List[BaseEvent] = []
        N = self._get_N(connectome)
        if N <= 0:
            return events

        b_vis = self.budget_visits
        b_edg = self.budget_edges
        ttl = self.ttl
        tick = 0
        seeds = None
        if isinstance(budget, dict):
            try:
                b_vis = int(budget.get("visits", b_vis))
            except Exception:
                pass
            try:
                b_edg = int(budget.get("edges", b_edg))
            except Exception:
                pass
            try:
                # Sentinel is intentionally shallow; cap TTL to 1 even if provided larger
                ttl = max(1, min(1, int(budget.get("ttl", ttl))))
            except Exception:
                ttl = 1
            try:
                tick = int(budget.get("tick", 0))
            except Exception:
                tick = 0
            try:
                seeds = list(budget.get("seeds", []))
            except Exception:
                seeds = None

        b_vis = max(0, min(b_vis, N))
        b_edg = max(0, b_edg)
        ttl = 1  # enforce single-step walks to reduce trampling

        # Seeds: prefer explicit seeds; else priority; else uniform domain
        priority: Set[int] = set()
        try:
            priority = self._priority_set(maps)
        except Exception:
            priority = set()

        if seeds:
            try:
                pool: Sequence[int] = tuple(int(s) for s in seeds if 0 <= int(s) < N) or tuple(priority) or tuple(range(N))
            except Exception:
                pool = tuple(priority) or tuple(range(N))
        else:
            pool = tuple(priority) or tuple(range(N))

        edges_emitted = 0
        visits_done = 0

        while visits_done < b_vis and pool:
            try:
                u = int(self.rng.choice(pool))
            except Exception:
                break

            cur = u
            depth = 0
            while depth < ttl:
                # Announce coverage
                events.append(VTTouchEvent(kind="vt_touch", t=tick, token=int(cur), w=1.0))
                visits_done += 1
                if visits_done >= b_vis:
                    break

                # Opportunistic single hop
                if edges_emitted < b_edg:
                    neigh = self._neighbors(connectome, cur)
                    if neigh:
                        try:
                            v = int(self.rng.choice(tuple(neigh)))
                        except Exception:
                            break
                        if v != cur:
                            events.append(EdgeOnEvent(kind="edge_on", t=tick, u=int(cur), v=int(v)))
                            edges_emitted += 1
                            cur = int(v)
                        else:
                            break
                    else:
                        break
                else:
                    break

                depth += 1

        return events


__all__ = ["SentinelScout"]]]></content>
    </file>
    <file>
      <path>cosmology/README.md</path>
      <content/>
    </file>
    <file>
      <path>cosmology/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Cosmology router core primitives."""

from .events import (
    BudgetExceededError,
    BudgetTick,
    HorizonActivityEvent,
    RouterSplitEvent,
)
from .router import (
    DenseAccessError,
    GrainScatteringShim,
    RetardedKernelSH,
    RouterEnergyPartition,
    RouterRuntimeTelemetry,
    VacuumAccumulator,
    check_router_budget_invariant,
)

__all__ = [
    "BudgetExceededError",
    "BudgetTick",
    "HorizonActivityEvent",
    "RouterSplitEvent",
    "VacuumAccumulator",
    "GrainScatteringShim",
    "RouterEnergyPartition",
    "RouterRuntimeTelemetry",
    "RetardedKernelSH",
    "DenseAccessError",
    "check_router_budget_invariant",
]
]]></content>
    </file>
    <file>
      <path>cosmology/events.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event schema and guards for the cosmology router feature."""

from __future__ import annotations

from dataclasses import dataclass, field
import math
from typing import Iterable, Tuple

from fum_rt.core.proprioception.events import BaseEvent


class BudgetExceededError(RuntimeError):
    """Raised when a budget guard detects exhaustion for the current tick."""


def _ensure_finite(value: float, name: str) -> float:
    try:
        fv = float(value)
    except Exception as exc:  # pragma: no cover - defensive conversion
        raise ValueError(f"{name} must be convertible to float") from exc
    if not math.isfinite(fv):
        raise ValueError(f"{name} must be finite")
    return fv


@dataclass(frozen=True)
class HorizonActivityEvent(BaseEvent):
    """Local horizon activity routed through the cosmology event bus."""

    kind: str = "horizon_activity"
    t: int = 0
    x: Tuple[float, ...] = field(default_factory=tuple)
    dotA: float = 0.0
    horizon_id: str = ""
    dt_ret: float = 0.0

    def __post_init__(self) -> None:
        coords: Tuple[float, ...]
        if isinstance(self.x, Iterable) and not isinstance(self.x, tuple):
            coords = tuple(float(c) for c in self.x)  # type: ignore[arg-type]
            object.__setattr__(self, "x", coords)
        else:
            coords = self.x
        if not coords:
            raise ValueError("x must contain at least one coordinate")
        for c in coords:
            _ensure_finite(c, "x coordinate")
        if len(coords) > 4:
            raise ValueError("x must be local (≤4 coordinates)")
        dt_ret = _ensure_finite(self.dt_ret, "dt_ret")
        if dt_ret < 0.0:
            raise ValueError("dt_ret must be non-negative")
        if dt_ret == 0.0:
            raise ValueError("dt_ret must encode a strictly retarded window")
        dotA = _ensure_finite(self.dotA, "dotA")
        if self.horizon_id == "":
            raise ValueError("horizon_id must be a non-empty identifier")
        if self.t < 0:
            raise ValueError("t must be non-negative")
        if dotA == 0.0:
            raise ValueError("dotA must carry observable production rate")


@dataclass(frozen=True)
class RouterSplitEvent(BaseEvent):
    """Budget split instruction for the cosmology router channels."""

    kind: str = "router_split"
    energy_budget: float = 0.0
    f_vac: float = 0.0
    f_grain: float = 0.0
    f_gw: float = 0.0

    def __post_init__(self) -> None:
        budget = _ensure_finite(self.energy_budget, "energy_budget")
        if budget < 0.0:
            raise ValueError("energy_budget must be non-negative")
        fractions = (
            _ensure_finite(self.f_vac, "f_vac"),
            _ensure_finite(self.f_grain, "f_grain"),
            _ensure_finite(self.f_gw, "f_gw"),
        )
        for name, value in zip(("f_vac", "f_grain", "f_gw"), fractions):
            if value < 0.0 or value > 1.0:
                raise ValueError(f"{name} must lie in [0, 1]")
        if abs(sum(fractions) - 1.0) > 1e-9:
            raise ValueError("router fractions must sum to 1")

    @property
    def fractions(self) -> Tuple[float, float, float]:
        """Convenience accessor for downstream consumers."""

        return (self.f_vac, self.f_grain, self.f_gw)


@dataclass(frozen=True)
class BudgetTick(BaseEvent):
    """Tick-scoped budget guard used to bound router processing."""

    kind: str = "budget_tick"
    tick: int = 0
    max_ops: int = 0
    max_emits: int = 0
    ttl: int = 1

    def __post_init__(self) -> None:
        if self.tick < 0:
            raise ValueError("tick must be non-negative")
        for name in ("max_ops", "max_emits", "ttl"):
            value = getattr(self, name)
            if not isinstance(value, int):
                raise ValueError(f"{name} must be an integer")
            if value < 0:
                raise ValueError(f"{name} must be non-negative")
        if self.ttl == 0:
            raise ValueError("ttl must be at least 1 tick")

    def guard(self, ops_used: int, emits_used: int, elapsed_ticks: int) -> None:
        """Raise :class:`BudgetExceededError` when any budget is exhausted."""

        if ops_used > self.max_ops:
            raise BudgetExceededError("operation budget exhausted")
        if emits_used > self.max_emits:
            raise BudgetExceededError("emission budget exhausted")
        if elapsed_ticks >= self.ttl:
            raise BudgetExceededError("tick TTL exhausted")

]]></content>
    </file>
    <file>
      <path>cosmology/router.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Retarded sourcing kernel and router channel utilities for the cosmology module."""

from __future__ import annotations

import json
from dataclasses import dataclass
import math
from typing import Dict, Iterable, List, Sequence, Set, Tuple

from .events import BudgetExceededError, BudgetTick, HorizonActivityEvent, RouterSplitEvent


def _normalize_position(coords: Sequence[float]) -> Tuple[float, ...]:
    """Normalize a coordinate sequence into a finite tuple."""

    try:
        tupled = tuple(float(c) for c in coords)
    except Exception as exc:  # pragma: no cover - defensive conversion
        raise ValueError("coordinates must be convertible to float") from exc
    if not tupled:
        raise ValueError("coordinates must not be empty")
    if len(tupled) > 4:
        raise ValueError("coordinates must remain local (≤4 entries)")
    for value in tupled:
        if not math.isfinite(value):
            raise ValueError("coordinates must be finite")
    return tupled


def _percentile(values: Sequence[float], p: float) -> float:
    if not values:
        return 0.0
    if p <= 0.0:
        return float(min(values))
    if p >= 1.0:
        return float(max(values))
    ordered = sorted(float(v) for v in values)
    idx = int(math.floor(p * (len(ordered) - 1)))
    idx = max(0, min(len(ordered) - 1, idx))
    return float(ordered[idx])


@dataclass
class RetardedKernelSH:
    """Local, causal retarded kernel for horizon sourcing."""

    epsilon: float
    local_radius: float
    max_events: int = 64

    def __post_init__(self) -> None:
        self.epsilon = float(self.epsilon)
        if not math.isfinite(self.epsilon):
            raise ValueError("epsilon must be finite")
        self.local_radius = float(self.local_radius)
        if not math.isfinite(self.local_radius) or self.local_radius <= 0.0:
            raise ValueError("local_radius must be positive and finite")
        if not isinstance(self.max_events, int):
            raise ValueError("max_events must be an integer")
        if self.max_events <= 0:
            raise ValueError("max_events must be positive")

    def compute(
        self,
        t: float,
        position: Sequence[float],
        events: Iterable[HorizonActivityEvent],
        *,
        budget: BudgetTick | None = None,
    ) -> float:
        """Evaluate S_H(t, x) over a bounded, local horizon activity tape."""

        t_eval = float(t)
        if not math.isfinite(t_eval):
            raise ValueError("evaluation time must be finite")
        x = _normalize_position(position)

        total = 0.0
        count = 0
        ops_used = 0
        emits_used = 0
        for event in events:
            if budget is not None:
                budget.guard(ops_used, emits_used, 0)
            if not isinstance(event, HorizonActivityEvent):
                raise TypeError("events must be HorizonActivityEvent instances")
            count += 1
            if count > self.max_events:
                raise BudgetExceededError("retarded kernel event budget exhausted")
            ops_used += 1
            dt = t_eval - float(event.t)
            if dt <= 0.0:
                continue
            dt_ret = float(event.dt_ret)
            if not math.isfinite(dt_ret) or dt_ret <= 0.0:
                continue
            if dt > dt_ret:
                continue
            event_pos = _normalize_position(event.x)
            if len(event_pos) != len(x):
                raise ValueError("event and evaluation positions must share dimensionality")
            dist = self._distance(x, event_pos)
            if dist > self.local_radius:
                continue
            weight = self._weight(dt, dist, dt_ret)
            if weight <= 0.0:
                continue
            dotA = float(event.dotA)
            if not math.isfinite(dotA):
                continue
            total += dotA * weight
        if budget is not None:
            budget.guard(ops_used, emits_used, 0)
        emits_used += 1
        if budget is not None:
            budget.guard(ops_used, emits_used, 0)
        return self.epsilon * total

    def _weight(self, dt: float, distance: float, dt_ret: float) -> float:
        radius = self.local_radius
        if radius <= 0.0:
            return 0.0
        spatial = max(0.0, 1.0 - distance / radius)
        if spatial <= 0.0:
            return 0.0
        temporal = math.exp(-dt / dt_ret)
        return spatial * temporal

    @staticmethod
    def _distance(a: Tuple[float, ...], b: Tuple[float, ...]) -> float:
        return math.sqrt(sum((ai - bi) * (ai - bi) for ai, bi in zip(a, b)))


@dataclass
class VacuumAccumulator:
    """Retarded-kernel accumulator for the vacuum (dark energy) channel."""

    kernel: RetardedKernelSH
    rho_lambda: float
    eta: float

    def __post_init__(self) -> None:
        self.rho_lambda = float(self.rho_lambda)
        self.eta = float(self.eta)
        if not math.isfinite(self.rho_lambda):
            raise ValueError("rho_lambda must be finite")
        if not math.isfinite(self.eta):
            raise ValueError("eta must be finite")

    def evaluate(
        self,
        t: float,
        position: Sequence[float],
        events: Iterable[HorizonActivityEvent],
        *,
        budget: BudgetTick | None = None,
    ) -> float:
        """Return ρ_vac(t) = ρ_Λ + η·S_H(t, x)."""

        base = self.rho_lambda
        if self.eta == 0.0:
            if budget is not None:
                budget.guard(0, 1, 0)
            return base
        contribution = self.kernel.compute(
            t=t,
            position=position,
            events=events,
            budget=budget,
        )
        return base + self.eta * contribution


@dataclass
class GrainScatteringShim:
    """Finite-size soliton grain scattering shim with monotone cross-section."""

    r_star: float
    mass_scale: float
    v_scale: float
    alpha: float = 2.0
    sigma_floor: float = 1e-4
    sigma_ceiling: float = 50.0

    def __post_init__(self) -> None:
        self.r_star = float(self.r_star)
        self.mass_scale = float(self.mass_scale)
        self.v_scale = float(self.v_scale)
        self.alpha = float(self.alpha)
        self.sigma_floor = float(self.sigma_floor)
        self.sigma_ceiling = float(self.sigma_ceiling)
        for name in ("r_star", "mass_scale", "v_scale", "alpha", "sigma_floor", "sigma_ceiling"):
            value = getattr(self, name)
            if not math.isfinite(value):
                raise ValueError(f"{name} must be finite")
        if self.r_star <= 0.0:
            raise ValueError("r_star must be positive")
        if self.mass_scale <= 0.0:
            raise ValueError("mass_scale must be positive")
        if self.v_scale <= 0.0:
            raise ValueError("v_scale must be positive")
        if self.alpha <= 0.0:
            raise ValueError("alpha must be positive")
        if self.sigma_floor <= 0.0:
            raise ValueError("sigma_floor must be positive")
        if self.sigma_ceiling <= self.sigma_floor:
            raise ValueError("sigma_ceiling must exceed sigma_floor")

    @property
    def sigma0(self) -> float:
        """Geometric cross-section per unit mass at v → 0."""

        geom = math.pi * self.r_star * self.r_star
        return min(self.sigma_ceiling, max(self.sigma_floor, geom / self.mass_scale))

    def cross_section_per_mass(self, velocity: float) -> float:
        """Return σ_T/m(v) with monotone fall-off controlled by ``alpha``."""

        v = abs(float(velocity))
        if not math.isfinite(v):
            raise ValueError("velocity must be finite")
        ratio = (v / self.v_scale) ** self.alpha
        value = self.sigma0 / (1.0 + ratio)
        return min(self.sigma_ceiling, max(self.sigma_floor, value))

    def curve(self, velocities: Sequence[float]) -> List[Dict[str, float]]:
        """Return a monotone-decreasing σ_T/m curve over the supplied velocity grid."""

        sanitized = sorted(abs(float(v)) for v in velocities)
        if not sanitized:
            raise ValueError("velocities must contain at least one entry")
        values: List[Dict[str, float]] = []
        last_sigma = None
        for v in sanitized:
            sigma_val = self.cross_section_per_mass(v)
            if last_sigma is not None and sigma_val > last_sigma + 1e-12:
                sigma_val = last_sigma
            values.append({"v": v, "sigmaT_over_m": sigma_val})
            last_sigma = sigma_val
        return values

    def curve_to_json(self, velocities: Sequence[float]) -> str:
        """Serialize the scattering curve as JSON for downstream tooling."""

        curve = self.curve(velocities)
        return json.dumps(curve, sort_keys=True)


@dataclass(frozen=True)
class RouterEnergyPartition:
    """Energy bookkeeping across the vacuum, grain, and GW channels."""

    energy_budget: float
    vacuum: float
    grain: float
    gw: float
    tolerance: float = 1e-9

    def __post_init__(self) -> None:
        for name in ("energy_budget", "vacuum", "grain", "gw", "tolerance"):
            value = float(getattr(self, name))
            object.__setattr__(self, name, value)
            if not math.isfinite(value):
                raise ValueError(f"{name} must be finite")
        if self.energy_budget < 0.0:
            raise ValueError("energy_budget must be non-negative")
        for name in ("vacuum", "grain", "gw"):
            if getattr(self, name) < 0.0:
                raise ValueError(f"{name} allocation must be non-negative")
        if self.tolerance <= 0.0:
            raise ValueError("tolerance must be positive")
        self.check_conservation()

    def check_conservation(self) -> None:
        """Assert that the allocations sum to the router energy budget."""

        total = self.vacuum + self.grain + self.gw
        scale = max(1.0, self.energy_budget)
        if abs(total - self.energy_budget) > self.tolerance * scale:
            raise ValueError(
                "router channel allocations must conserve the energy budget"
            )

    def as_dict(self) -> Dict[str, float]:
        return {
            "energy_budget": self.energy_budget,
            "vacuum": self.vacuum,
            "grain": self.grain,
            "gw": self.gw,
        }

    @classmethod
    def from_split(
        cls,
        split: RouterSplitEvent,
        *,
        tolerance: float = 1e-9,
    ) -> "RouterEnergyPartition":
        energy = float(split.energy_budget)
        vacuum = energy * float(split.f_vac)
        grain = energy * float(split.f_grain)
        gw = energy * float(split.f_gw)
        return cls(
            energy_budget=energy,
            vacuum=vacuum,
            grain=grain,
            gw=gw,
            tolerance=tolerance,
        )


def check_router_budget_invariant(
    q_values: Sequence[float],
    *,
    epsilon: float,
    tol_abs: float = 1e-8,
    tol_p95: float = 1e-8,
    eps_gate: float = 1e-6,
) -> Dict[str, float | int | bool]:
    """Drift guard for the router accounting scalar Q_router."""

    try:
        seq = [float(v) for v in q_values]
    except Exception:
        return {
            "count": 0,
            "q0": 0.0,
            "drift_mean": 0.0,
            "drift_p95": 0.0,
            "drift_max": 0.0,
            "gate_active": False,
            "pass_abs": False,
            "pass_p95": False,
            "pass": False,
        }
    if not seq:
        return {
            "count": 0,
            "q0": 0.0,
            "drift_mean": 0.0,
            "drift_p95": 0.0,
            "drift_max": 0.0,
            "gate_active": False,
            "pass_abs": False,
            "pass_p95": False,
            "pass": False,
        }
    if any(not math.isfinite(v) for v in seq):
        return {
            "count": 0,
            "q0": 0.0,
            "drift_mean": 0.0,
            "drift_p95": 0.0,
            "drift_max": math.inf,
            "gate_active": False,
            "pass_abs": False,
            "pass_p95": False,
            "pass": False,
        }

    q0 = seq[0]
    drifts = [abs(v - q0) for v in seq]
    count = len(drifts)
    drift_mean = sum(drifts) / float(count)
    drift_p95 = _percentile(drifts, 0.95)
    drift_max = max(drifts)

    eps_val = abs(float(epsilon))
    gate_active = eps_val <= float(eps_gate)
    pass_abs = drift_max <= float(tol_abs)
    pass_p95 = drift_p95 <= float(tol_p95)
    passed = (not gate_active) or (pass_abs and pass_p95)

    return {
        "count": count,
        "q0": q0,
        "drift_mean": drift_mean,
        "drift_p95": drift_p95,
        "drift_max": drift_max,
        "gate_active": gate_active,
        "pass_abs": pass_abs,
        "pass_p95": pass_p95,
        "pass": passed,
    }


class DenseAccessError(RuntimeError):
    """Raised when a dense adjacency accessor is invoked inside the router."""


@dataclass
class RouterRuntimeTelemetry:
    """Per-tick router runtime telemetry with budget guards."""

    budget: BudgetTick
    dense_accessors: Tuple[str, ...] = (
        "adjacency_dense",
        "adjacency_matrix",
        "neighbors_dense",
        "full_state_vector",
    )

    ops: int = 0
    emits: int = 0
    neighborhood_max_deg: int = 0

    def __post_init__(self) -> None:
        self._dense_accessors: Set[str] = {str(name) for name in self.dense_accessors}
        self._touched_nodes: Set[str] = set()
        self._touched_edges: Set[Tuple[str, str]] = set()

    def record_operation(
        self,
        *,
        nodes: Iterable[str] | None = None,
        edges: Iterable[Tuple[str, str]] | None = None,
        neighborhood_degree: int | None = None,
    ) -> None:
        """Record an operation and associated locality footprint."""

        sanitized_nodes: Set[str] = set()
        if nodes is not None:
            for node in nodes:
                sanitized_nodes.add(str(node))
        sanitized_edges: Set[Tuple[str, str]] = set()
        if edges is not None:
            for edge in edges:
                if len(edge) != 2:
                    raise ValueError("edges must be 2-tuples")
                left, right = (str(edge[0]), str(edge[1]))
                sanitized_edges.add((left, right))
        degree_value = None
        if neighborhood_degree is not None:
            degree_value = int(neighborhood_degree)
            if degree_value < 0:
                raise ValueError("neighborhood degree must be non-negative")

        candidate_ops = self.ops + 1
        self.budget.guard(candidate_ops, self.emits, 0)
        self.ops = candidate_ops
        self._touched_nodes.update(sanitized_nodes)
        self._touched_edges.update(sanitized_edges)
        if degree_value is not None:
            self.neighborhood_max_deg = max(self.neighborhood_max_deg, degree_value)

    def record_emit(self) -> None:
        """Record an emitted event within the tick budget."""

        candidate_emits = self.emits + 1
        self.budget.guard(self.ops, candidate_emits, 0)
        self.emits = candidate_emits

    def flag_dense_accessor(self, accessor_name: str) -> None:
        """Raise when a known dense accessor is invoked within the router."""

        name = str(accessor_name)
        if name in self._dense_accessors:
            raise DenseAccessError(f"dense accessor '{name}' is prohibited in router scope")

    def snapshot(self) -> Dict[str, object]:
        """Return a deterministic snapshot for logging the tick telemetry."""

        return {
            "tick": self.budget.tick,
            "ops": self.ops,
            "emits": self.emits,
            "neighborhood_max_deg": self.neighborhood_max_deg,
            "touched_nodes": sorted(self._touched_nodes),
            "touched_edges": sorted(self._touched_edges),
        }

    def gates(self) -> Dict[str, bool]:
        """Return pass/fail flags for budget and locality CI gates."""

        budget_ok = (self.ops <= self.budget.max_ops) and (
            self.emits <= self.budget.max_emits
        )
        locality_ok = (
            len(self._touched_nodes) <= self.budget.max_ops
            and len(self._touched_edges) <= self.budget.max_ops
        )
        return {
            "budget_within_limits": budget_ok,
            "locality_respected": locality_ok,
        }

__all__ = [
    "RetardedKernelSH",
    "VacuumAccumulator",
    "GrainScatteringShim",
    "RouterEnergyPartition",
    "RouterRuntimeTelemetry",
    "DenseAccessError",
    "check_router_budget_invariant",
]

]]></content>
    </file>
    <file>
      <path>diagnostics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Physics-informed diagnostics for the FUM runtime.

This module provides:
- Mass-gap estimation from two-point correlations on the runtime graph
- Pulse-speed (group velocity) estimation from time-resolved activity

References:
- [Derivation/discrete_to_continuum.md](Derivation/discrete_to_continuum.md:125-193)
- [Derivation/kinetic_term_derivation.md](Derivation/kinetic_term_derivation.md:117-134)
- [Derivation/finite_tube_mode_analysis.md](Derivation/finite_tube_mode_analysis.md:1)
- [Derivation/fum_voxtrium_mapping.md](Derivation/fum_voxtrium_mapping.md:44-121)

Conventions:
- We treat graph shortest-path distance (in hops) as the discrete spatial metric r
  when geometric embedding is unavailable. This is a standard surrogate on networks.
- The continuum prediction for the static two-point correlator is
  C(r) ~ exp(-r / xi) with mass gap m_eff = 1 / xi (dimensionless units).
- The wave speed c enters the EOM via c^2 = 2 J a^2 (per-site convention), see
  [Derivation/kinetic_term_derivation.md](Derivation/kinetic_term_derivation.md:117-134).
  We estimate an effective group velocity v_g from an expanding activity front.

Author: Justin K. Lietz
Date: 2025-08-09
"""

from __future__ import annotations

import math
from typing import Dict, List, Optional, Tuple

import numpy as np


def _ensure_adjacency(connectome) -> np.ndarray:
    """
    Extract a dense binary adjacency matrix A (int8) from the runtime connectome.

    Expected connectome interface (from fum_rt.core.*):
      - connectome.A : np.ndarray (N x N), int8, 0/1
      - connectome.N : int, number of nodes

    Returns:
      A : np.ndarray (N x N) int8 in {0,1}
    """
    if not hasattr(connectome, "A"):
        raise AttributeError("connectome must expose .A (adjacency)")

    A = connectome.A
    if not isinstance(A, np.ndarray):
        A = np.asarray(A)
    # Ensure binary
    A = (A != 0).astype(np.int8)
    return A


def _shortest_path_distances(A: np.ndarray, seeds: np.ndarray, max_d: int) -> Dict[int, List[Tuple[int, int, int]]]:
    """
    Compute shortest-path distances from a subset of seed nodes using BFS.

    Args:
      A: binary adjacency (N x N)
      seeds: array of seed node indices
      max_d: maximum distance to consider

    Returns:
      distances_by_d: mapping d -> list of (src, dst, d) pairs observed with shortest-path distance d,
                      with 1 <= d <= max_d
    """
    N = A.shape[0]
    neighbors = [np.where(A[i] != 0)[0] for i in range(N)]
    distances_by_d: Dict[int, List[Tuple[int, int, int]]] = {d: [] for d in range(1, max_d + 1)}

    for s in seeds:
        dist = np.full(N, -1, dtype=np.int32)
        dist[s] = 0
        q = [s]
        head = 0
        while head < len(q):
            u = q[head]
            head += 1
            du = dist[u]
            if du >= max_d:
                continue
            for v in neighbors[u]:
                if dist[v] == -1:
                    dist[v] = du + 1
                    q.append(v)
                    if 1 <= dist[v] <= max_d:
                        distances_by_d[dist[v]].append((s, v, dist[v]))
    return distances_by_d


def estimate_mass_gap_from_phi(
    connectome,
    phi: Optional[np.ndarray] = None,
    sample_fraction: float = 0.1,
    max_d: int = 10,
    min_counts_per_d: int = 50,
) -> Dict[str, float]:
    """
    Estimate the correlation length xi (in graph hops) and mass gap m_eff = 1/xi
    from a static snapshot of a scalar field on the graph.

    Inputs:
      connectome: runtime connectome with fields .A (binary adjacency) and .N
      phi: optional np.ndarray (N,) scalar field per node. If None, use a weight-derived proxy:
           phi_i := sum_j |E_ij| if connectome.E available, else degree (sum of A_i*).
      sample_fraction: fraction of nodes to use as BFS seeds (subsamples for speed)
      max_d: maximum graph distance to consider
      min_counts_per_d: require at least this many pairs per distance bin for inclusion

    Outputs (in a dict):
      {
        "xi": correlation length (hops),
        "m_eff": 1/xi,
        "r_values": number of bins used,
        "fit_slope": slope of log C(d) vs d,
        "fit_intercept": intercept,
      }

    Notes:
      - Two-point connected correlator defined as C(d) = mean_{pairs at dist d}[ (phi_i - mu)(phi_j - mu) ],
        normalized by var to reduce scale dependence. We then fit log C(d) ~ -d/xi + const.
      - If no sufficient bins, returns NaNs.
    """
    A = _ensure_adjacency(connectome)
    N = A.shape[0]

    # Field proxy if none provided
    if phi is None:
        if hasattr(connectome, "E") and isinstance(connectome.E, np.ndarray):
            # node scalar = L1 sum of incident weights (absolute)
            phi = np.sum(np.abs(connectome.E), axis=1).astype(np.float64)
        else:
            # fallback: degree
            phi = np.sum(A, axis=1).astype(np.float64)

    phi = np.asarray(phi, dtype=np.float64)
    if phi.shape[0] != N:
        raise ValueError("phi length must equal connectome.N")

    mu = float(np.mean(phi))
    var = float(np.var(phi))
    if var <= 1e-18:
        return {"xi": float("nan"), "m_eff": float("nan"), "r_values": 0, "fit_slope": float("nan"), "fit_intercept": float("nan")}

    # Subsample seeds
    rng = np.random.default_rng(12345)
    seeds = np.arange(N)
    rng.shuffle(seeds)
    keep = max(1, int(sample_fraction * N))
    seeds = seeds[:keep]

    distances_by_d = _shortest_path_distances(A, seeds, max_d=max_d)

    # Compute correlator per distance
    C_vals = []
    d_vals = []
    for d in range(1, max_d + 1):
        pairs = distances_by_d[d]
        if len(pairs) < min_counts_per_d:
            continue
        # average over pairs: connected correlator normalized by var
        num = 0.0
        for (i, j, _) in pairs:
            num += (phi[i] - mu) * (phi[j] - mu)
        C_d = (num / len(pairs)) / var
        if C_d > 1e-12:
            C_vals.append(max(C_d, 1e-12))
            d_vals.append(d)

    if len(d_vals) < 2:
        return {"xi": float("nan"), "m_eff": float("nan"), "r_values": 0, "fit_slope": float("nan"), "fit_intercept": float("nan")}

    # Fit log C(d) ~ - d/xi + const
    y = np.log(np.asarray(C_vals))
    x = np.asarray(d_vals, dtype=np.float64)
    # Least squares fit
    A_fit = np.vstack([x, np.ones_like(x)]).T
    slope, intercept = np.linalg.lstsq(A_fit, y, rcond=None)[0]  # y = slope*x + intercept
    # slope should be negative: slope = -1/xi
    if slope >= -1e-12:
        xi = float("inf")
    else:
        xi = -1.0 / slope
    m_eff = 1.0 / xi if xi != float("inf") else 0.0

    return {
        "xi": float(xi),
        "m_eff": float(m_eff),
        "r_values": int(len(d_vals)),
        "fit_slope": float(slope),
        "fit_intercept": float(intercept),
    }


class PulseSpeedEstimator:
    """
    Online estimator for a pulse (activity front) group velocity on a graph.

    Usage:
      pse = PulseSpeedEstimator(connectome)
      pse.begin(center_node=some_index, tick=t0)
      for each tick t:
          pse.observe(tick=t, active_mask=spikes or thresholded field)
      result = pse.finalize()

    The estimator computes the mean geodesic radius of active nodes relative to the chosen center,
    then fits a linear model radius(t) ~ v_g * (t - t0) + const to recover v_g.
    """

    def __init__(self, connectome, max_radius: Optional[int] = None):
        self.A = _ensure_adjacency(connectome)
        self.N = self.A.shape[0]
        self.max_radius = max_radius if max_radius is not None else max(10, self.N // 10)
        self._dist_cache_center: Optional[int] = None
        self._dist_from_center: Optional[np.ndarray] = None
        self._t0: Optional[float] = None
        self._ts: List[float] = []
        self._radii: List[float] = []

    def _bfs_from_center(self, c: int) -> np.ndarray:
        dist = np.full(self.N, -1, dtype=np.int32)
        dist[c] = 0
        q = [c]
        head = 0
        rows = self.A
        neighbors = [np.where(rows[i] != 0)[0] for i in range(self.N)]
        while head < len(q):
            u = q[head]
            head += 1
            du = dist[u]
            if du >= self.max_radius:
                continue
            for v in neighbors[u]:
                if dist[v] == -1:
                    dist[v] = du + 1
                    q.append(v)
        return dist

    def begin(self, center_node: int, tick: float):
        self._dist_cache_center = int(center_node)
        self._dist_from_center = self._bfs_from_center(self._dist_cache_center)
        self._t0 = float(tick)
        self._ts.clear()
        self._radii.clear()

    def observe(self, tick: float, active_mask: np.ndarray):
        """
        Record the mean radius of currently active nodes.

        Args:
          tick: current time (integer tick or float time)
          active_mask: boolean array shape (N,) marking active nodes at this tick
                       (e.g., spikes, or |delta phi| > threshold)
        """
        if self._dist_from_center is None or self._t0 is None:
            raise RuntimeError("PulseSpeedEstimator.begin(...) must be called before observe(...)")

        active_mask = np.asarray(active_mask, dtype=bool)
        if active_mask.shape[0] != self.N:
            raise ValueError("active_mask length must equal number of nodes")

        idx = np.where(active_mask)[0]
        if idx.size == 0:
            return  # skip empty frames
        d = self._dist_from_center[idx]
        d = d[d >= 0]  # ignore unreachable or -1
        if d.size == 0:
            return
        mean_r = float(np.mean(d))
        self._ts.append(float(tick) - self._t0)
        self._radii.append(mean_r)

    def finalize(self) -> Dict[str, float]:
        """
        Fit radius(t) ~ v_g * (t - t0) + const. Return v_g and fit diagnostics.
        """
        if len(self._ts) < 2:
            return {"v_g": float("nan"), "frame_count": int(len(self._ts)), "slope": float("nan"), "intercept": float("nan")}
        x = np.asarray(self._ts, dtype=np.float64)
        y = np.asarray(self._radii, dtype=np.float64)
        A_fit = np.vstack([x, np.ones_like(x)]).T
        slope, intercept = np.linalg.lstsq(A_fit, y, rcond=None)[0]
        return {"v_g": float(max(0.0, slope)), "frame_count": int(len(self._ts)), "slope": float(slope), "intercept": float(intercept)}]]></content>
    </file>
    <file>
      <path>engine/README.md</path>
      <content/>
    </file>
    <file>
      <path>engine/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.


Core Engine package initializer.

Exports CoreEngine from the in-package implementation module to avoid any
cross-file redirects. Implementation resides under this package.
"""

from .core_engine import CoreEngine

__all__ = ["CoreEngine"]]]></content>
    </file>
    <file>
      <path>engine/core_engine.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

from __future__ import annotations

"""
Core seam: temporary adapter that forwards to existing Nexus internals without changing behavior.

Phase B goal:
- Define a stable Core API now to avoid rework later.
- Do NOT move logic yet; keep Nexus as source of truth.
- These methods either delegate to existing functions or act as explicit stubs.

Separation policy:
- This module must not import from fum_rt.io.* or fum_rt.runtime.* to keep core isolated.
- Only depend on fum_rt.core.* and the Nexus-like object passed at construction.
"""

from typing import Any, Dict, Optional, Tuple

from fum_rt.core.metrics import compute_metrics
from fum_rt.core.memory import (
    load_engram as _load_engram_state,
    save_checkpoint as _save_checkpoint,
)
from fum_rt.core.proprioception.events import EventDrivenMetrics as _EvtMetrics
from fum_rt.core.cortex.scouts import VoidColdScoutWalker as _VoidScout, ColdMap as _ColdMap
from fum_rt.core.cortex.maps.heatmap import HeatMap as _HeatMap
from fum_rt.core.cortex.maps.excitationmap import ExcitationMap as _ExcMap
from fum_rt.core.cortex.maps.inhibitionmap import InhibitionMap as _InhMap
from fum_rt.core.cortex.maps.trailmap import TrailMap as _TrailMap
from fum_rt.core.cortex.maps.memorymap import MemoryMap as _MemMap
from fum_rt.core.signals import (
    compute_active_edge_density as _sig_density,
    compute_td_signal as _sig_td,
    compute_firing_var as _sig_fvar,
)

# Local helpers (telemetry-only; remain inside core boundary)
from .maps_frame import stage_maps_frame
from .evt_snapshot import build_evt_snapshot


class CoreEngine:
    """
    Temporary adapter (seam) to the current runtime.

    - step(): folds event-driven reducers (no IO/logging) and stages maps/frame for telemetry.
    - snapshot(): exposes a minimal, safe snapshot using current metrics.
    - engram_load(): pass-through to the legacy loader.
    - engram_save(): pass-through to the legacy saver (saves into run_dir; path argument is advisory).
    """

    def __init__(self, nexus_like: Any) -> None:
        """
        nexus_like: an instance exposing the attributes currently used by the runtime:
          - connectome, adc, run_dir, checkpoint_format (optional), logger (optional), _phase (optional)
        """
        self._nx = nexus_like
        # Public alias for tests and adapters that expect a public handle
        try:
            self.nx = self._nx  # test convenience: allows eng.nx access
        except Exception:
            pass
        # Event-driven stack (lazy-initialized)
        self._evt_metrics: Optional[_EvtMetrics] = None
        self._void_scout: Optional[_VoidScout] = None
        self._cold_map: Optional[_ColdMap] = None
        self._heat_map: Optional[_HeatMap] = None
        self._exc_map: Optional[_ExcMap] = None
        self._inh_map: Optional[_InhMap] = None
        self._memory_map: Optional[_MemMap] = None
        self._trail_map: Optional[_TrailMap] = None
        self._last_evt_snapshot: Dict[str, Any] = {}

    # ---- Event-driven fold and telemetry staging ----
    def step(self, dt_ms: int, ext_events: list) -> None:
        """
        Fold provided core events and cold-scout events into event-driven reducers.
        Pure core; no IO/logging. Read-only against connectome.
        """
        # lazy init local reducers and VOID scout
        try:
            self._ensure_evt_init()
        except Exception:
            pass

        if getattr(self, "_evt_metrics", None) is None:
            return

        # latest tick observed this step (from ext events or scout)
        latest_tick = None
        collected_events: list = []

        # 1) fold external events (already core BaseEvent subclasses from runtime adapter)
        try:
            for ev in (ext_events or []):
                try:
                    # accept any object exposing 'kind' attribute (duck-typed BaseEvent)
                    if hasattr(ev, "kind"):
                        self._evt_metrics.update(ev)
                        collected_events.append(ev)
                        # update cold-map on node touches/endpoints when possible
                        if getattr(self, "_cold_map", None) is not None:
                            try:
                                kind = getattr(ev, "kind", "")
                                t_ev = getattr(ev, "t", None)
                                if t_ev is not None:
                                    if kind == "vt_touch":
                                        token = getattr(ev, "token", None)
                                        if isinstance(token, int) and token >= 0:
                                            self._cold_map.touch(int(token), int(t_ev))
                                    elif kind == "edge_on":
                                        u = getattr(ev, "u", None)
                                        v = getattr(ev, "v", None)
                                        if isinstance(u, int) and u >= 0:
                                            self._cold_map.touch(int(u), int(t_ev))
                                        if isinstance(v, int) and v >= 0:
                                            self._cold_map.touch(int(v), int(t_ev))
                            except Exception:
                                pass
                        # track latest tick seen
                        try:
                            tv = getattr(ev, "t", None)
                            if tv is not None:
                                if latest_tick is None or int(tv) > int(latest_tick):
                                    latest_tick = int(tv)
                        except Exception:
                            pass
                except Exception:
                    continue
        except Exception:
            pass

        # 2) fold VOID cold-scout reads (read-only traversal)
        try:
            if getattr(self, "_void_scout", None) is not None:
                # Prefer explicit tick from external events; fallback to predicted next tick.
                tick_hint = None
                try:
                    if ext_events:
                        # Pick the last event with a valid 't' (most recent)
                        for _e in reversed(ext_events):
                            tv = getattr(_e, "t", None)
                            if tv is not None:
                                tick_hint = int(tv)
                                break
                except Exception:
                    tick_hint = None
                if tick_hint is None:
                    try:
                        # Use next tick relative to last emitted step (updated later in runtime loop)
                        tick_hint = int(getattr(self._nx, "_emit_step", -1)) + 1
                    except Exception:
                        tick_hint = 0
                C = getattr(getattr(self, "_nx", None), "connectome", None)
                for _ev in self._void_scout.step(C, int(tick_hint)) or []:
                    try:
                        self._evt_metrics.update(_ev)
                        collected_events.append(_ev)
                        # update cold-map for scout-generated events
                        if getattr(self, "_cold_map", None) is not None:
                            try:
                                kind = getattr(_ev, "kind", "")
                                if kind == "vt_touch":
                                    token = getattr(_ev, "token", None)
                                    if isinstance(token, int) and token >= 0:
                                        self._cold_map.touch(int(token), int(tick_hint))
                                elif kind == "edge_on":
                                    u = getattr(_ev, "u", None)
                                    v = getattr(_ev, "v", None)
                                    if isinstance(u, int) and u >= 0:
                                        self._cold_map.touch(int(u), int(tick_hint))
                                    if isinstance(v, int) and v >= 0:
                                        self._cold_map.touch(int(v), int(tick_hint))
                            except Exception:
                                pass
                    except Exception:
                        continue
                # update latest tick from scout pass
                try:
                    if latest_tick is None or int(tick_hint) > int(latest_tick):
                        latest_tick = int(tick_hint)
                except Exception:
                    pass
        except Exception:
            pass

        # 2.5) fold heat/excitation/inhibition (+memory/trail) maps with collected events (telemetry-only)
        try:
            try:
                fold_tick = int(latest_tick) if latest_tick is not None else int(getattr(self._nx, "_emit_step", -1)) + 1
            except Exception:
                fold_tick = 0
            if getattr(self, "_heat_map", None) is not None:
                try:
                    self._heat_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_exc_map", None) is not None:
                try:
                    self._exc_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_inh_map", None) is not None:
                try:
                    self._inh_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_memory_map", None) is not None:
                try:
                    self._memory_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
            if getattr(self, "_trail_map", None) is not None:
                try:
                    self._trail_map.fold(collected_events, int(fold_tick))
                except Exception:
                    pass
        except Exception:
            pass

        # 2.75) stage maps/frame payload for UI bus (header JSON + Float32 LE payload)
        try:
            stage_maps_frame(
                nx=self._nx,
                heat_map=self._heat_map,
                exc_map=self._exc_map,
                inh_map=self._inh_map,
                fold_tick=int(
                    latest_tick
                    if latest_tick is not None
                    else (int(getattr(self._nx, "_emit_step", -1)) + 1)
                ),
            )
        except Exception:
            pass

        # 3) refresh cached evt snapshot
        try:
            self._last_evt_snapshot = build_evt_snapshot(
                evt_metrics=self._evt_metrics,
                cold_map=self._cold_map,
                heat_map=self._heat_map,
                exc_map=self._exc_map,
                inh_map=self._inh_map,
                memory_map=getattr(self, "_memory_map", None),
                trail_map=getattr(self, "_trail_map", None),
                latest_tick=(
                    int(latest_tick)
                    if latest_tick is not None
                    else (int(getattr(self._nx, "_emit_step", -1)) + 1)
                ),
                nx=self._nx,
            )
        except Exception:
            self._last_evt_snapshot = {}

    def _ensure_evt_init(self) -> None:
        """
        Initialize event-driven reducers (EventDrivenMetrics) and VOID scout lazily
        using configuration exposed by the nexus-like object when available.
        """
        # reducers
        if getattr(self, "_evt_metrics", None) is None:
            try:
                det = getattr(self._nx, "b1_detector", None)
                z_spike = float(getattr(det, "z_spike", 1.0)) if det is not None else 1.0
                hysteresis = float(getattr(det, "hysteresis", 1.0)) if det is not None else 1.0
                half_life = int(getattr(self._nx, "b1_half_life_ticks", 50))
                seed = int(getattr(self._nx, "seed", 0))
                self._evt_metrics = _EvtMetrics(
                    z_half_life_ticks=max(1, half_life),
                    z_spike=z_spike,
                    hysteresis=hysteresis,
                    seed=seed,
                )
            except Exception:
                self._evt_metrics = None
        # VOID scout
        if getattr(self, "_void_scout", None) is None:
            try:
                sv = int(getattr(self._nx, "scout_visits", 16))
            except Exception:
                sv = 16
            try:
                se = int(getattr(self._nx, "scout_edges", 8))
            except Exception:
                se = 8
            try:
                seed = int(getattr(self._nx, "seed", 0))
            except Exception:
                seed = 0
            try:
                self._void_scout = _VoidScout(
                    budget_visits=max(0, sv), budget_edges=max(0, se), seed=seed
                )
            except Exception:
                self._void_scout = None
        # Cold-map reducer
        if getattr(self, "_cold_map", None) is None:
            try:
                ck = int(getattr(self._nx, "cold_head_k", 256))
            except Exception:
                ck = 256
            try:
                hl = int(getattr(self._nx, "cold_half_life_ticks", 200))
            except Exception:
                hl = 200
            try:
                seed = int(getattr(self._nx, "seed", 0))
            except Exception:
                seed = 0
            try:
                self._cold_map = _ColdMap(
                    head_k=max(8, ck), half_life_ticks=max(1, hl), keep_max=None, seed=seed
                )
            except Exception:
                self._cold_map = None
        # Heat/Excitation/Inhibition reducers (mirror cold-map settings; telemetry-only)
        try:
            hk = int(getattr(self._nx, "cold_head_k", 256))
        except Exception:
            hk = 256
        try:
            hl2 = int(getattr(self._nx, "cold_half_life_ticks", 200))
        except Exception:
            hl2 = 200
        try:
            seed = int(getattr(self._nx, "seed", 0))
        except Exception:
            seed = 0
        if getattr(self, "_heat_map", None) is None:
            try:
                self._heat_map = _HeatMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 1
                )
            except Exception:
                self._heat_map = None
        if getattr(self, "_exc_map", None) is None:
            try:
                self._exc_map = _ExcMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 2
                )
            except Exception:
                self._exc_map = None
        if getattr(self, "_inh_map", None) is None:
            try:
                self._inh_map = _InhMap(
                    head_k=max(8, hk), half_life_ticks=max(1, hl2), keep_max=None, seed=seed + 3
                )
            except Exception:
                self._inh_map = None
        # Memory/Trail reducers (event-driven steering fields; telemetry-only exposure)
        if getattr(self, "_memory_map", None) is None:
            try:
                self._memory_map = _MemMap(
                    head_k=max(8, hk), keep_max=None, seed=seed + 4
                )
                # expose a read-only pointer for local getters without scans
                try:
                    C = getattr(self._nx, "connectome", None)
                    if C is not None:
                        setattr(C, "_memory_map", self._memory_map)
                except Exception:
                    pass
            except Exception:
                self._memory_map = None
        if getattr(self, "_trail_map", None) is None:
            try:
                self._trail_map = _TrailMap(
                    head_k=max(8, hk),
                    half_life_ticks=max(1, int(max(1, hl2 // 4))),
                    keep_max=None,
                    seed=seed + 5,
                )
            except Exception:
                self._trail_map = None

    # --- Connectome interface (single entrypoint for runtime) ---
    def stimulate_indices(self, indices, amp: float = 0.05) -> None:
        try:
            self._nx.connectome.stimulate_indices(list(indices), amp=float(amp))
        except Exception:
            pass

    def step_connectome(
        self,
        t: float,
        domain_modulation: float = 1.0,
        sie_gate: float = 0.0,
        use_time_dynamics: bool = True,
    ) -> None:
        try:
            self._nx.connectome.step(
                t,
                domain_modulation=float(domain_modulation),
                sie_drive=float(sie_gate),
                use_time_dynamics=bool(use_time_dynamics),
            )
        except Exception:
            pass

    def compute_metrics(self) -> Dict[str, Any]:
        try:
            return compute_metrics(self._nx.connectome)
        except Exception:
            return {}

    def snapshot_graph(self):
        try:
            return self._nx.connectome.snapshot_graph()
        except Exception:
            return None

    # --- Numeric helpers (wrap core.signals) ---
    def compute_active_edge_density(self) -> Tuple[int, float]:
        try:
            N = int(getattr(self._nx, "N", 0))
        except Exception:
            N = 0
        try:
            return _sig_density(getattr(self._nx, "connectome", None), N)
        except Exception:
            return 0, 0.0

    def compute_td_signal(
        self, prev_E: int | None, E: int, vt_prev: float | None = None, vt_last: float | None = None
    ) -> float:
        try:
            return float(_sig_td(prev_E, E, vt_prev, vt_last))
        except Exception:
            return 0.0

    def compute_firing_var(self):
        try:
            return _sig_fvar(getattr(self._nx, "connectome", None))
        except Exception:
            return None

    def get_homeostasis_counters(self) -> Tuple[int, int]:
        try:
            pruned = int(getattr(self._nx.connectome, "_last_pruned_count", 0))
            bridged = int(getattr(self._nx.connectome, "_last_bridged_count", 0))
            return pruned, bridged
        except Exception:
            return 0, 0

    def get_findings(self) -> Dict[str, Any]:
        try:
            f = getattr(self._nx.connectome, "findings", None)
            return dict(f) if isinstance(f, dict) else {}
        except Exception:
            return {}

    def get_last_sie2_valence(self) -> float:
        try:
            return float(getattr(self._nx.connectome, "_last_sie2_valence", 0.0))
        except Exception:
            return 0.0

    def snapshot(self) -> Dict[str, Any]:
        """
        Build a minimal state snapshot via current compute_metrics without mutating the model.
        Adds common context fields used by Why providers when available.
        Also merges cached event-driven metrics under an 'evt_' prefix to preserve canonical fields.
        """
        nx = self._nx
        m = compute_metrics(nx.connectome)
        # Attach minimal, non-intrusive context
        try:
            m["t"] = int(getattr(nx, "_emit_step", 0))
        except Exception:
            pass
        try:
            m["phase"] = int(getattr(nx, "_phase", {}).get("phase", 0))
        except Exception:
            pass
        # Merge event-driven snapshot without overriding canonical keys
        try:
            evs = getattr(self, "_last_evt_snapshot", None)
            if isinstance(evs, dict):
                for k, v in evs.items():
                    try:
                        # preserve existing canonical b1_* if present
                        if str(k).startswith("b1_") and k in m:
                            continue
                        m[f"evt_{k}"] = v
                    except Exception:
                        continue
        except Exception:
            pass
        return m

    def engram_load(self, path: str) -> None:
        """
        Pass-through to the existing engram loader with ADC included when available.
        Mirrors the call used in Nexus, preserving logs/events and behavior.
        """
        nx = self._nx
        _load_engram_state(str(path), nx.connectome, adc=getattr(nx, "adc", None))
        # Optional: let the caller log; we keep core side-effect free except the actual load.

    def engram_save(
        self, path: Optional[str] = None, step: Optional[int] = None, fmt: Optional[str] = None
    ) -> str:
        """
        Pass-through to the existing checkpoint saver. Saves into nx.run_dir using the legacy naming scheme.
        Arguments:
          - path: advisory only (ignored by the legacy saver, which chooses its own path under run_dir)
          - step: when None, the caller should provide an explicit step; if missing, a safe default is used (0)
          - fmt: optional override for format (e.g., 'h5' or 'npz'); defaults to nx.checkpoint_format or 'h5'

        Returns:
          The filesystem path returned by the legacy saver.
        """
        nx = self._nx
        use_step = int(step if step is not None else getattr(nx, "_emit_step", 0))
        use_fmt = str(
            fmt if fmt is not None else getattr(nx, "checkpoint_format", "h5") or "h5"
        )
        return _save_checkpoint(
            nx.run_dir, use_step, nx.connectome, fmt=use_fmt, adc=getattr(nx, "adc", None)
        )


__all__ = ["CoreEngine"]]]></content>
    </file>
    <file>
      <path>engine/evt_snapshot.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event-driven snapshot builder (core-local).

- Aggregates lightweight telemetry from:
  * EventDrivenMetrics.snapshot()
  * ColdMap.snapshot(t)
  * Heat/Excitation/Inhibition reducer snapshots
- Pure function; no IO, no scans, no side-effects outside returning a dict.
"""

from __future__ import annotations

from typing import Any, Dict, Optional


def _safe_merge(dst: Dict[str, Any], src: Optional[Dict[str, Any]]) -> None:
    if not isinstance(src, dict):
        return
    for k, v in src.items():
        try:
            dst[k] = v
        except Exception:
            continue


def build_evt_snapshot(
    *,
    evt_metrics: Optional[Any],
    cold_map: Optional[Any],
    heat_map: Optional[Any],
    exc_map: Optional[Any],
    inh_map: Optional[Any],
    memory_map: Optional[Any] = None,
    trail_map: Optional[Any] = None,
    latest_tick: int = 0,
    nx: Any = None,
) -> Dict[str, Any]:
    """
    Construct a consolidated event-driven snapshot without mutating model state.

    Parameters:
      evt_metrics: EventDrivenMetrics instance (or None)
      cold_map: ColdMap reducer (or None)
      heat_map/exc_map/inh_map: reducers exposing snapshot() -> dict
      latest_tick: integer tick associated with this fold/snapshot
      nx: nexus-like handle (unused; reserved for future keys)

    Returns:
      dict of event-driven fields (raw keys) to be prefixed by the caller when merging
      into the canonical telemetry map (e.g., "evt_*" in CoreEngine.snapshot()).
    """
    out: Dict[str, Any] = {}

    # 1) Base event-driven metrics
    try:
        if evt_metrics is not None:
            evs = evt_metrics.snapshot()
            if isinstance(evs, dict):
                _safe_merge(out, evs)
    except Exception:
        pass

    # 2) Cold map snapshot at the current tick (bounded head only; no scans)
    try:
        if cold_map is not None:
            cs = cold_map.snapshot(int(latest_tick))
            if isinstance(cs, dict):
                _safe_merge(out, cs)
    except Exception:
        pass

    # 3) Heat/Exc/Inh reducer snapshots (bounded heads; telemetry-only)
    try:
        if heat_map is not None:
            hs = heat_map.snapshot()
            if isinstance(hs, dict):
                _safe_merge(out, hs)
    except Exception:
        pass

    try:
        if exc_map is not None:
            es = exc_map.snapshot()
            if isinstance(es, dict):
                _safe_merge(out, es)
    except Exception:
        pass

    try:
        if inh_map is not None:
            ins = inh_map.snapshot()
            if isinstance(ins, dict):
                _safe_merge(out, ins)
    except Exception:
        pass

    # 4) Optional steering fields (views): memory/trail (bounded heads/dicts; no scans)
    try:
        if memory_map is not None:
            ms = memory_map.snapshot()
            if isinstance(ms, dict):
                _safe_merge(out, ms)
    except Exception:
        pass

    try:
        if trail_map is not None:
            ts = trail_map.snapshot()
            if isinstance(ts, dict):
                _safe_merge(out, ts)
    except Exception:
        pass

    return out


__all__ = ["build_evt_snapshot"]]]></content>
    </file>
    <file>
      <path>engine/maps_frame.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Telemetry maps/frame builder (core-local, no IO).

- Builds Float32 LE arrays for heat/excitation/inhibition from bounded reducer working sets.
- Computes header with contract:
  {topic:'maps/frame', tick, n, shape, channels:['heat','exc','inh'], dtype:'f32', endianness:'LE', stats}
- Stages result onto nx._maps_frame_ready for runtime telemetry emitters to publish.
- Strictly avoids any W/CSR/adjacency scans; operates only on small reducer dictionaries.
"""

from __future__ import annotations

from typing import Any, Dict, Optional

import numpy as _np


def _max_from(d: Dict[int, float]) -> float:
    try:
        if not d:
            return 0.0
        # Mirror payload dtype (float32 LE): cast values to float32 before max to ensure
        # header['stats']['max'] ≥ observed max from the serialized payload.
        return float(max((_np.float32(v) for v in d.values())))
    except Exception:
        return 0.0


def _fill_array_from_map(arr: _np.ndarray, d: Dict[int, float]) -> None:
    try:
        n = int(arr.shape[0])
    except Exception:
        n = len(arr)
    try:
        for k, v in (d or {}).items():
            try:
                ik = int(k)
                if 0 <= ik < n:
                    arr[ik] = float(v)
            except Exception:
                continue
    except Exception:
        pass


def stage_maps_frame(
    nx: Any,
    heat_map: Optional[Any],
    exc_map: Optional[Any],
    inh_map: Optional[Any],
    fold_tick: int,
) -> None:
    """
    Construct and stage the maps/frame payload on nx._maps_frame_ready.

    Parameters:
      nx: nexus-like object, must provide integer attribute N (<= few 10^6) for shape.
      heat_map/exc_map/inh_map: reducers exposing a _val: Dict[int,float] working set (bounded).
      fold_tick: integer tick associated to this fold (monotonic).
    """
    try:
        N = int(getattr(nx, "N", 0))
    except Exception:
        N = 0
    if N <= 0:
        return

    # Allocate arrays (Float32 LE by frombuffer/tobytes contract downstream)
    heat_arr = _np.zeros(N, dtype=_np.float32)
    exc_arr = _np.zeros(N, dtype=_np.float32)
    inh_arr = _np.zeros(N, dtype=_np.float32)

    # Fill from bounded dictionaries (no global scans)
    try:
        _fill_array_from_map(heat_arr, getattr(heat_map, "_val", {}))
    except Exception:
        pass
    try:
        _fill_array_from_map(exc_arr, getattr(exc_map, "_val", {}))
    except Exception:
        pass
    try:
        _fill_array_from_map(inh_arr, getattr(inh_map, "_val", {}))
    except Exception:
        pass

    # Sanitize non-finite
    for arr in (heat_arr, exc_arr, inh_arr):
        try:
            _np.nan_to_num(arr, copy=False, nan=0.0, posinf=0.0, neginf=0.0)
        except Exception:
            pass

    # Square-ish shape heuristic
    try:
        side = int(max(1, int(_np.ceil(_np.sqrt(N)))))
    except Exception:
        side = int(max(1, int((N or 1) ** 0.5)))
    shape = [side, side]

    # Stats from bounded dictionaries (min fixed to 0.0 by construction)
    stats = {
        "heat": {"min": 0.0, "max": _max_from(getattr(heat_map, "_val", {}))},
        "exc": {"min": 0.0, "max": _max_from(getattr(exc_map, "_val", {}))},
        "inh": {"min": 0.0, "max": _max_from(getattr(inh_map, "_val", {}))},
    }

    header = {
        "topic": "maps/frame",
        "tick": int(fold_tick),
        "n": int(N),
        "shape": shape,
        "channels": ["heat", "exc", "inh"],
        "dtype": "f32",
        "endianness": "LE",
        "stats": stats,
    }

    payload = heat_arr.tobytes() + exc_arr.tobytes() + inh_arr.tobytes()

    try:
        setattr(nx, "_maps_frame_ready", (header, payload))
    except Exception:
        pass]]></content>
    </file>
    <file>
      <path>fum_growth_arbiter.py</path>
      <content><![CDATA[# fum_growth_arbiter.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Purpose
- Unified growth/cull arbiter driven by the void-equation philosophy.
- Extends your original GrowthArbiter with explicit culling and debt accounting.
- Keeps growth pressure as "void debt" when stable and releases it into organic growth.
- When unstable, trends toward culling and reduces debt accordingly.

Blueprint references
- Rule 1: Parallel Local & Global Systems (arbiter is part of the Global System).
- Rule 3: SIE total_reward is the global pressure input (valence/drive).
- Rule 4/4.1: Structural homeostasis and repair triggers.
Time complexity: O(1) per tick (deque pushes and simple checks).
"""

from collections import deque
from typing import Dict
import numpy as np

# Try to align defaults with your universal constants; fall back to safe values.
try:
    from Void_Equations import get_universal_constants  # noqa: F401
    _UC = get_universal_constants()
    _ALPHA_DEF = float(_UC.get("ALPHA", 0.25))
    _BETA_DEF = float(_UC.get("BETA", 0.10))
except Exception:
    _ALPHA_DEF = 0.25
    _BETA_DEF = 0.10


class GrowthArbiter:
    """
    Monitors rolling metrics to decide when and how much to grow or cull.

    Parameters
    - stability_window: ticks considered to test for a "flat" plateau.
    - trend_threshold: max delta across the window to be considered flat.
    - debt_growth_factor: scale accumulated void debt into new neurons when stable.
    - alpha_growth: growth rate scaling (maps to ALPHA).
    - beta_cull: cull rate scaling (maps to BETA).

    Returns from accumulate_and_decide()
    - dict(grow=int, cull=int, void_debt=float, stable=bool)

    Notes
    - Debt accumulates only while stable; culling reduces debt (cannot drop below 0).
    - Actual target selection for cull/growth (which neurons/edges) is done by the connectome,
      guided by void pulses and S_ij; this arbiter only decides magnitudes and timing.
    """

    def __init__(self,
                 stability_window: int = 10,
                 trend_threshold: float = 0.001,
                 debt_growth_factor: float = 0.10,
                 alpha_growth: float = _ALPHA_DEF,
                 beta_cull: float = _BETA_DEF):
        self.stability_window = int(stability_window)
        self.trend_threshold = float(trend_threshold)
        self.debt_growth_factor = float(debt_growth_factor)
        self.alpha_growth = float(alpha_growth)
        self.beta_cull = float(beta_cull)

        self.weight_history = deque(maxlen=self.stability_window)
        self.synapse_history = deque(maxlen=self.stability_window)
        self.complexity_history = deque(maxlen=self.stability_window)
        self.cohesion_history = deque(maxlen=self.stability_window)

        self.is_stable: bool = False
        self.void_debt_accumulator: float = 0.0  # grows when stable; reduced on cull

    def clear_history(self):
        self.weight_history.clear()
        self.synapse_history.clear()
        self.complexity_history.clear()
        self.cohesion_history.clear()

    def update_metrics(self, metrics: Dict):
        """
        Update historical metrics and recompute stability flag.

        Expected keys (robust to naming differences used elsewhere):
        - avg_weight: float
        - active_synapses: int
        - total_b1_persistence or complexity_cycles: float
        - cohesion_components or cluster_count: int
        """
        self.weight_history.append(float(metrics.get("avg_weight", 0.0)))
        self.synapse_history.append(int(metrics.get("active_synapses", 0)))

        complexity = metrics.get("total_b1_persistence", metrics.get("complexity_cycles", 0.0))
        self.complexity_history.append(float(complexity))

        cohesion = metrics.get("cohesion_components", metrics.get("cluster_count", 1))
        self.cohesion_history.append(int(cohesion))

        if len(self.weight_history) < self.stability_window:
            self.is_stable = False
            return

        is_cohesive = all(c == 1 for c in self.cohesion_history)
        is_weight_flat = abs(self.weight_history[0] - self.weight_history[-1]) < self.trend_threshold
        is_synapse_flat = abs(self.synapse_history[0] - self.synapse_history[-1]) < 3
        is_complexity_flat = abs(self.complexity_history[0] - self.complexity_history[-1]) < self.trend_threshold

        self.is_stable = bool(is_cohesive and is_weight_flat and is_synapse_flat and is_complexity_flat)

    def accumulate_and_decide(self, valence_signal: float) -> Dict:
        """
        Unified decision surface for growth/cull driven by a global pressure (valence).

        Logic
        - If stable: accumulate void debt with |valence|. When debt > 1.0, grow:
            grow = ceil(debt * debt_growth_factor * alpha_growth)
            reset debt, mark unstable, clear history (system re-equilibrates).
        - If unstable: propose cull proportional to beta_cull and |valence|:
            cull = floor(|valence| * beta_cull)
            reduce debt by beta_cull * cull (clamped at 0).
        """
        grow = 0
        cull = 0
        pressure = abs(float(valence_signal))

        if self.is_stable:
            self.void_debt_accumulator += pressure
            if self.void_debt_accumulator > 1.0:
                grow = int(np.ceil(self.void_debt_accumulator * self.debt_growth_factor * self.alpha_growth))
                grow = max(0, grow)
                self.void_debt_accumulator = 0.0
                self.is_stable = False  # perturbation from growth expected
                self.clear_history()
        else:
            cull = int(np.floor(pressure * self.beta_cull))
            cull = max(0, cull)
            if cull > 0:
                self.void_debt_accumulator = max(0.0, self.void_debt_accumulator - (self.beta_cull * cull))

        return {
            "grow": grow,
            "cull": cull,
            "void_debt": float(self.void_debt_accumulator),
            "stable": bool(self.is_stable),
        }]]></content>
    </file>
    <file>
      <path>fum_sie.py</path>
      <content><![CDATA[# fum_sie.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
from scipy.sparse import csc_matrix

# --- FUM Modules (optional external helpers) ---
# Keep copyright and your original API intact; add robust fallbacks so fum_rt
# does not depend on external paths being on PYTHONPATH.
try:
    from fum_validated_math import (
        calculate_modulation_factor as _ext_calculate_modulation_factor,
        calculate_stabilized_reward as _ext_calculate_stabilized_reward,
    )
    _HAVE_VALIDATED_MATH = True
except Exception:
    _HAVE_VALIDATED_MATH = False
    _ext_calculate_modulation_factor = None
    _ext_calculate_stabilized_reward = None


def _sigmoid(x: float) -> float:
    x = float(np.clip(x, -500.0, 500.0))
    return 1.0 / (1.0 + np.exp(-x))


def _calculate_modulation_factor(total_reward: float) -> float:
    """
    Blueprint Rule 3 helper: squash to [-1, 1].
    Falls back to internal implementation if the external helper is not available.
    """
    if _HAVE_VALIDATED_MATH and _ext_calculate_modulation_factor is not None:
        try:
            return float(_ext_calculate_modulation_factor(total_reward))
        except Exception:
            pass
    return 2.0 * _sigmoid(total_reward) - 1.0


def _calculate_stabilized_reward(td_error, novelty, habituation, self_benefit, external_reward):
    """
    Blueprint Rule 3 helper: stabilized reward blend (weights + damping).
    Mirrors early_FUM_tests/FUM_Demo/fum_validated_math.py semantics when available.
    """
    if _HAVE_VALIDATED_MATH and _ext_calculate_stabilized_reward is not None:
        try:
            return float(_ext_calculate_stabilized_reward(td_error, novelty, habituation, self_benefit, external_reward))
        except Exception:
            pass
    # Internal fallback (mirrors the reference file)
    W_TD, W_NOVELTY, W_HABITUATION, W_SELF_BENEFIT, W_EXTERNAL = 0.5, 0.2, 0.1, 0.2, 0.8
    td_norm = float(np.clip(td_error, -1.0, 1.0))
    alpha_damping = 1.0 - np.tanh(abs(novelty - self_benefit))
    damped_novelty_term = alpha_damping * (W_NOVELTY * novelty - W_HABITUATION * habituation)
    damped_self_benefit_term = alpha_damping * (W_SELF_BENEFIT * self_benefit)
    if external_reward is not None:
        w_r = W_EXTERNAL if external_reward > 0 else (1.0 - W_EXTERNAL)
        total_reward = w_r * external_reward
    else:
        total_reward = (W_TD * td_norm) + damped_novelty_term + damped_self_benefit_term
    return float(total_reward)

class SelfImprovementEngine:
    """
    The FUM's Self-Improvement Engine (SIE).
    
    This module is the system's intrinsic motivation. It generates the
    internal, multi-objective valence signal that guides all learning and
    adaptation within the Substrate.
    """
    def __init__(self, num_neurons):
        self.num_neurons = num_neurons
        # --- Core Valence Components ---
        self.td_error = 0.0      # Represents unexpectedness or prediction error
        self.novelty = 0.0       # The drive to explore new informational states
        self.habituation = np.zeros(num_neurons) # Counter-force to Novelty
        self.self_benefit = 0.0  # The drive for efficiency and stability

        # --- Phase 2+ / buffers kept intact (preserve your original API/state) ---
        self.cret_buffer = np.zeros(num_neurons, dtype=np.float32)        # CRET
        self.td_value_function = np.zeros(num_neurons, dtype=np.float32)  # TD V

        # Internal bookkeeping (not externally required)
        self.last_reward_time = -1
        self.last_drive = None  # stores the latest computed drive packet (see get_drive)
        self._prev_density = None  # for intrinsic TD proxy (density delta)

    def update_and_calculate_valence(self, W: csc_matrix, external_signal: float, time_step: int) -> float:
        """
        Updates the Core's internal state and returns a unified valence signal in [0, 1].
        Backward-compatible with your original API, while also computing a Rule 3 drive packet.
        """
        drive = self.get_drive(W=W, external_signal=external_signal, time_step=time_step)
        # Preserve legacy behavior: return the [0,1] valence (VGSP-compatible magnitude)
        return float(drive["valence_01"])

    def update_from_runtime_metrics(self, density: float, external_signal: float, time_step: int) -> float:
        """
        Lightweight Rule 3 drive update that avoids requiring a CSC matrix.
        Preserves novelty decay and self_benefit semantics; returns valence in [0,1]
        for VGSP gating. Also updates self.last_drive for introspection.

        Args:
            density: float in [0,1] computed as W.nnz / possible_edges; self_benefit = 1 - density
            external_signal: task or environment feedback, can be None
            time_step: current tick

        Returns:
            float in [0,1]: valence magnitude for gating RE‑VGSP.
        """
        try:
            self.self_benefit = float(1.0 - float(density))
        except Exception:
            self.self_benefit = 0.0

        self.td_error = 0.0 if external_signal is None else float(external_signal)
        habituation_mean = float(self.habituation.mean() if self.habituation.size else 0.0)

        total_reward = _calculate_stabilized_reward(
            td_error=self.td_error,
            novelty=self.novelty,
            habituation=habituation_mean,
            self_benefit=self.self_benefit,
            external_reward=None if external_signal is None else float(external_signal),
        )
        modulation = _calculate_modulation_factor(total_reward)

        # Preserve your novelty trigger dynamics
        if modulation > 0.5 and (time_step - self.last_reward_time) > 150:
            self.novelty = 0.9
            self.last_reward_time = int(time_step)
        else:
            self.novelty *= 0.98

        valence_01 = max(0.0, abs((modulation + self.novelty) / 2.0))

        # Maintain a compact drive packet for downstream consumers
        self.last_drive = {
            "total_reward": float(np.clip(total_reward, -1.0, 1.0)),
            "modulation_factor": float(np.clip(modulation, -1.0, 1.0)),
            "valence_01": float(np.clip(valence_01, 0.0, 1.0)),
            "components": {
                "td_error": float(self.td_error),
                "novelty": float(self.novelty),
                "habituation_mean": float(habituation_mean),
                "self_benefit": float(self.self_benefit),
                "density": float(density),
            }
        }
        return float(self.last_drive["valence_01"])

    # --- Blueprint Rule 3 canonical helpers (kept additive to your API) ---

    def _compute_hsi_norm(self, firing_var: float = None, target_var: float = 0.15) -> float:
        """
        Rule 3 HSI component: higher when firing variance is close to target.
        Returns value in [-1, 1].
        """
        if firing_var is None:
            return 0.0
        target = max(1e-6, float(target_var))
        # Map proximity to target into [-1,1] where exact match -> +1, far -> negative
        prox = 1.0 - min(1.0, abs(float(firing_var) - target) / target)
        # Center around 0; maintain symmetry
        return float(2.0 * prox - 1.0)

    def get_drive(self, W: csc_matrix, external_signal: float, time_step: int,
                  firing_var: float = None, target_var: float = 0.15,
                  weights: dict | None = None,
                  density_override: float | None = None,
                  novelty_idf_scale: float = 1.0) -> dict:
        """
        Compute the canonical Rule 3 drive packet, preserving your novelty and sparsity logic.
        Returns:
            {
              'total_reward': [-1,1],
              'modulation_factor': [-1,1],
              'valence_01': [0,1],            # legacy-compatible magnitude
              'components': {
                   'td_error': ..., 'novelty': ..., 'habituation_mean': ...,
                   'self_benefit': ..., 'hsi_norm': ..., 'density': ...
              }
            }
        """
        # TD error and sparsity (self_benefit) as in your code
        # Allow None to mean "no external reward signal" (intrinsic-only blending)
        ext_val = 0.0 if (external_signal is None) else float(external_signal)

        # Density can be provided directly to avoid converting W; falls back to W if available
        if density_override is not None:
            density = float(min(1.0, max(0.0, density_override)))
        else:
            if W is not None:
                n = int(W.shape[0])
                num_possible_connections = n * max(0, (W.shape[1] - 1))
                density = (W.nnz / num_possible_connections) if num_possible_connections > 0 else 0.0
            else:
                density = 0.0

        # Intrinsic TD proxy from density change if external is absent/negligible
        prev = getattr(self, "_prev_density", None)
        ddens = 0.0 if prev is None else float(density - prev)
        try:
            self._prev_density = float(density)
        except Exception:
            pass
        intrinsic_td = float(np.clip(ddens * 10.0, -1.0, 1.0))
        td = float(ext_val) if abs(float(ext_val)) > 1e-9 else intrinsic_td
        self.td_error = float(td)

        # Self-benefit and a very-light EMA toward topology saturation as a habituation proxy
        self.self_benefit = float(1.0 - density)
        try:
            self.habituation = (0.995 * self.habituation) + (0.005 * float(density))
        except Exception:
            pass

        # Habituation (aggregate for now; you already maintain the vector)
        habituation_mean = float(self.habituation.mean() if self.habituation.size else 0.0)

        # HSI via variance target
        hsi_norm = self._compute_hsi_norm(firing_var=firing_var, target_var=target_var)

        # Stabilized total reward (signed), then squashed to modulation factor
        # Use intrinsic blend so TD/novelty/habituation/self_benefit all contribute
        total_reward = _calculate_stabilized_reward(
            td_error=self.td_error,
            novelty=self.novelty,
            habituation=habituation_mean,
            self_benefit=self.self_benefit,
            external_reward=None,
        )
        modulation_factor = _calculate_modulation_factor(total_reward)

        # Novelty dynamics: trigger on modulation or topology change spikes
        trigger = (modulation_factor > 0.5) or (abs(ddens) > 1e-3) or (abs(self.td_error) > 0.05)
        # IDF rarity scale ∈ [0.5, 2.0] modulates novelty toward rare, content-bearing tokens
        scale = float(max(0.5, min(2.0, 1.0 if novelty_idf_scale is None else novelty_idf_scale)))
        if trigger and (time_step - self.last_reward_time) > 50:
            # proportional to spike, capped and with partial retention; scaled by rarity
            self.novelty = float(min(0.95, max(self.novelty * 0.5, scale * (0.3 + 3.0 * abs(intrinsic_td)))))
            self.last_reward_time = int(time_step)
        else:
            self.novelty *= 0.995

        # Legacy [0,1] valence magnitude for VGSP gating as needed
        valence_01 = max(0.0, abs((modulation_factor + self.novelty) / 2.0))

        packet = {
            "total_reward": float(np.clip(total_reward, -1.0, 1.0)),
            "modulation_factor": float(np.clip(modulation_factor, -1.0, 1.0)),
            "valence_01": float(np.clip(valence_01, 0.0, 1.0)),
            "components": {
                "td_error": float(self.td_error),
                "novelty": float(self.novelty),
                "habituation_mean": float(habituation_mean),
                "self_benefit": float(self.self_benefit),
                "hsi_norm": float(hsi_norm),
                "density": float(density),
                "novelty_scale": float(scale),
            }
        }
        self.last_drive = packet
        return packet

    def resize_buffers(self, new_num_neurons: int) -> None:
        """
        Resizes the internal buffers to accommodate a new number of neurons after growth.
        """
        old_num_neurons = int(self.num_neurons)
        if int(new_num_neurons) <= old_num_neurons:
            return

        # Calculate the number of neurons added
        num_added = int(new_num_neurons) - old_num_neurons

        # Create zero arrays for the new neurons
        zeros_to_add = np.zeros(num_added, dtype=np.float32)

        # Add the new zero elements to the end of the existing buffers
        self.cret_buffer = np.concatenate([self.cret_buffer, zeros_to_add])
        self.td_value_function = np.concatenate([self.td_value_function, zeros_to_add])
        # Keep dtype stable for habituation buffer
        self.habituation = np.concatenate(
            [self.habituation, np.zeros(num_added, dtype=self.habituation.dtype if hasattr(self.habituation, "dtype") else np.float32)]
        )

        # Update the neuron count
        self.num_neurons = int(new_num_neurons)
        try:
            print(f"--- SIE buffers resized to accommodate {self.num_neurons} neurons. ---")
        except Exception:
            pass]]></content>
    </file>
    <file>
      <path>fum_structural_homeostasis.py</path>
      <content><![CDATA[# fum_structural_homeostasis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

# Rule reference (Blueprint)
# - Rule 4 / 4.1: EHTP + Global Directed Synaptic Plasticity (GDSP)
#   Stage‑1 cohesion repair and light, adaptive pruning. Subquadratic; act
#   only on loci/components provided by the runtime, not full graph scans.
#
# Time complexity:
# - O(N + M) to get component labels upstream (Connectome already computes count).
# - Bridging: O(B * k) where B is number of bridges (small), k is per‑node neighbor cap.
# - Pruning: O(M) over active edges via masked thresholding (vectorized).
#
# Formulae:
# - S_ij = ReLU(Δalpha_i) * ReLU(Δalpha_j) - λ * |Δomega_i - Δomega_j|
# - Prune if |E_ij| < prune_threshold (adaptive fraction of |E| mean)
#
# Parameters:
# - bundle_size: number of parallel edges to reinforce a bridge (encourages fusion)
# - lambda_omega: penalty weight in S_ij
# - prune_factor: fraction of mean(|E|) below which edges are removed


def _compute_affinity(a: np.ndarray, w: np.ndarray, lambda_omega: float) -> np.ndarray:
    """Compute void‑affinity matrix S_ij from Δalpha (a) and Δomega (w)."""
    a_relu = np.maximum(0.0, a.astype(np.float32))
    w = w.astype(np.float32)
    # S_ij = relu(a_i) * relu(a_j) - λ |Δω_i - Δω_j|
    S = a_relu[:, None] * a_relu[None, :] - lambda_omega * np.abs(w[:, None] - w[None, :])
    np.fill_diagonal(S, -np.inf)
    return S


def _select_bridge_pairs(labels: np.ndarray,
                         S: np.ndarray,
                         degrees: np.ndarray,
                         bundle_size: int = 3):
    """
    Choose bridge node pairs (u,v) across different components by maximizing S_ij.
    Tie‑break to prefer strong→weak: high degree u, low degree v.
    Returns a list of (u, v) pairs of length up to bundle_size per component pair.
    """
    pairs = []
    unique = np.unique(labels)
    if unique.size < 2:
        return pairs

    # Generate candidate component pairs (greedy: connect largest to others)
    # Rank components by size descending.
    comp_sizes = {c: int((labels == c).sum()) for c in unique}
    order = sorted(unique, key=lambda c: comp_sizes[c], reverse=True)
    root = order[0]
    others = order[1:]

    for tgt in others:
        idx_root = np.where(labels == root)[0]
        idx_tgt = np.where(labels == tgt)[0]
        if idx_root.size == 0 or idx_tgt.size == 0:
            continue

        # Submatrix of S over the boundary (root x tgt)
        S_block = S[np.ix_(idx_root, idx_tgt)]
        if np.all(~np.isfinite(S_block)):
            continue

        # For bundle, iteratively pick max, then suppress chosen rows/cols lightly
        local_pairs = []
        S_copy = S_block.copy()
        for _ in range(bundle_size):
            i_flat = np.nanargmax(S_copy)  # works since -inf stays < any finite
            r, c = divmod(i_flat, S_copy.shape[1])
            u = idx_root[r]
            v = idx_tgt[c]
            local_pairs.append((u, v))
            # soft suppression to diversify selection
            S_copy[r, :] = -np.inf
            S_copy[:, c] = -np.inf
            if not np.isfinite(S_copy).any():
                break

        # Apply strong→weak preference reordering (optional)
        local_pairs.sort(key=lambda p: (-degrees[p[0]], degrees[p[1]]))
        pairs.extend(local_pairs)

    return pairs


def perform_structural_homeostasis(connectome,
                                   labels: np.ndarray,
                                   d_alpha: np.ndarray,
                                   d_omega: np.ndarray,
                                   lambda_omega: float = 0.1,
                                   bundle_size: int = 3,
                                   prune_factor: float = 0.10):
    """
    Perform cohesion healing (bridging) and light pruning on the runtime connectome.

    Args:
        connectome: fum_rt.core.connectome.Connectome instance (current runtime).
        labels: np.ndarray of component labels per node for the ACTIVE subgraph.
        d_alpha: np.ndarray Δalpha (delta_re_vgsp) for current tick.
        d_omega: np.ndarray Δomega (delta_gdsp) for current tick.
        lambda_omega: float; S_ij penalty weight.
        bundle_size: int; number of parallel edges to reinforce each bridge.
        prune_factor: float; fraction of mean(|E|) used as adaptive pruning threshold.

    Effects:
        - Modifies connectome.A (adjacency) by adding symmetric bridge edges between
          components using S_ij max rule.
        - Updates connectome.E to follow nodes after topology change.
        - Prunes edges whose |E_ij| < prune_threshold (adaptive).
    """
    N = connectome.N
    if N <= 1:
        return

    # 1) Pruning (adaptive to current edge weights)
    E = connectome.E  # float32 N x N but sparse via threshold
    if E.size > 0:
        mean_w = float(np.mean(np.abs(E[E != 0]))) if np.any(E != 0) else 0.0
        prune_threshold = prune_factor * mean_w if mean_w > 0 else 0.0
        if prune_threshold > 0.0:
            mask_keep = np.abs(E) >= prune_threshold
            # keep symmetry shape
            connectome.A = np.where(mask_keep, connectome.A, 0).astype(np.int8)
            connectome.E = np.where(mask_keep, E, 0.0).astype(np.float32)
            # Expose pruning stats for diagnostics (undirected edges)
            try:
                pruned_count = int(np.count_nonzero((~mask_keep) & (E != 0)) // 2)
                setattr(connectome, "_last_pruned_count", int(pruned_count))
            except Exception:
                pass

    # 2) Bridging if multiple components
    unique = np.unique(labels) if labels is not None else np.array([0], dtype=int)
    if unique.size > 1:
        # Compute S_ij over current nodes
        S = _compute_affinity(d_alpha, d_omega, lambda_omega=lambda_omega)

        # degrees for strong→weak preference
        degrees = connectome.A.sum(axis=1).astype(np.int32)

        bridge_pairs = _select_bridge_pairs(labels, S, degrees, bundle_size=bundle_size)

        # Add symmetric edges for selected pairs
        for u, v in bridge_pairs:
            if u == v:
                continue
            connectome.A[u, v] = 1
            connectome.A[v, u] = 1

        # Edge weights follow nodes (reuse existing vectorized function)
        connectome.E = (np.outer(connectome.W, connectome.W) * connectome.A).astype(np.float32)
        # Expose bridging stats for diagnostics
        try:
            setattr(connectome, "_last_bridged_count", int(len(bridge_pairs)))
        except Exception:
            pass]]></content>
    </file>
    <file>
      <path>global_system.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.


Global System Components: Active Domain Cartography (ADC) and Self‑Improvement Engine (SIE)
Blueprint References:
- Rule 1: Core Architectural Principle (Parallel Local & Global Systems)
- Rule 3: The Self-Improvement Engine (SIE) and Its Components
- Rule 4.1: Pathology Detection Mechanisms (connectome_entropy input)
- Rule 7: Active Domain Cartography (ADC) with adaptive scheduling
Time Complexity:
- ADC (1D k-means over W): O(N * k * iters) with small k-range and few iterations (subquadratic)
- SIE (per tick updates): O(N + k_states) dominated by simple reductions (subquadratic)
Formulas: documented inline per method docstrings
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict
import numpy as np

# -------------------------------
# Active Domain Cartography (ADC)
# -------------------------------

@dataclass
class ADC:
    """
    Active Domain Cartography (Rule 7)
    - Maps neurons to territories (State S) using a bespoke, efficient process.
    - Scheduling: t_cartography = schedule_base * exp(-alpha * connectome_entropy)
    - Optimization: narrow k search [k_min, max_k]; 1D k-means on node field W
    - Complexity: O(N * k * iters) per trial k; k-range is small; iters small (default 5)
    Parameters:
        k_min: minimum number of territories to consider (>=2)
        max_k: maximum number of territories to consider
        alpha: decay constant for adaptive cadence
        schedule_base: base interval for cartography
        iters: k-means iterations (small constant)
        performance_threshold: cohesion score threshold for reactive adaptation
    """
    k_min: int = 2
    max_k: int = 16
    alpha: float = 0.30
    schedule_base: int = 100_000
    iters: int = 5
    performance_threshold: float = 1e-2
    next_t: int = 0
    last_k: int = 0

    def should_run(self, step: int, connectome_entropy: float) -> bool:
        """Blueprint Rule 7: scheduling. Returns True if step reached next_t."""
        if step >= self.next_t:
            # t_cartography = schedule_base * exp(-alpha * entropy)
            interval = int(max(1, round(self.schedule_base * np.exp(-self.alpha * float(connectome_entropy)))))
            self.next_t = step + interval
            return True
        return False

    @staticmethod
    def _kmeans_1d(x: np.ndarray, k: int, iters: int, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:
        """
        1D k-means over x (shape (N,)), returns (centroids, labels)
        Complexity: O(N * k * iters)
        """
        N = x.size
        # Init centroids from quantiles (stable)
        qs = np.linspace(0.0, 1.0, num=k+2, endpoint=True)[1:-1]
        c = np.quantile(x, qs) if k > 1 else np.array([float(np.median(x))], dtype=np.float32)
        c = np.asarray(c, dtype=np.float32)
        labels = np.zeros(N, dtype=np.int32)
        for _ in range(max(1, iters)):
            # Assign
            dist = np.abs(x[:, None] - c[None, :])  # (N,k)
            labels = np.argmin(dist, axis=1).astype(np.int32)
            # Update
            for j in range(k):
                sel = (labels == j)
                if np.any(sel):
                    c[j] = float(np.mean(x[sel]))
        return c, labels

    @staticmethod
    def _cohesion_score(x: np.ndarray, labels: np.ndarray, k: int) -> float:
        """
        Blueprint Rule 7 ('The How'): overall cohesion = mean over territories of inverse intra-variance.
        We use a numerically stable version: mean(1 / (var + eps)).
        """
        eps = 1e-8
        score = 0.0
        for j in range(k):
            sel = (labels == j)
            if not np.any(sel):
                continue
            v = float(np.var(x[sel]))
            score += 1.0 / (v + eps)
        return score / float(max(1, k))

    def run(self, W: np.ndarray, rng: np.random.Generator) -> Tuple[np.ndarray, int, float]:
        """
        Perform cartography over 1D feature W (node field).
        Returns: (territory_ids, k_opt, cohesion_score)
        - Trials k in [k_min, max_k]; pick best by cohesion score (higher is better).
        - Reactive adaptation (Rule 7): if best cohesion below threshold, try k+1 once.
        """
        N = W.size
        k_min = max(2, int(self.k_min))
        k_max = max(k_min, int(self.max_k))
        best = (-np.inf, None, None)  # (score, labels, k)

        for k in range(k_min, min(k_max, N) + 1):
            _, lbl = self._kmeans_1d(W, k, self.iters, rng)
            s = self._cohesion_score(W, lbl, k)
            if s > best[0]:
                best = (s, lbl, k)

        score, labels, k_opt = best
        # Reactive adaptation (bifurcation)
        if score < self.performance_threshold and (k_opt + 1) <= min(k_max, N):
            _, lbl = self._kmeans_1d(W, k_opt + 1, self.iters, rng)
            s2 = self._cohesion_score(W, lbl, k_opt + 1)
            if s2 > score:
                score, labels, k_opt = s2, lbl, k_opt + 1

        self.last_k = int(k_opt)
        return labels.astype(np.int32), self.last_k, float(score)


# -----------------------------------
# Self‑Improvement Engine (SIE) Rule 3
# -----------------------------------

@dataclass
class SIE: # TODO: This isnt a canonical SIE, examine this file and determine if this is the canonical: fum_rt/core/fum_sie.py
    """
    Self‑Improvement Engine (Rule 3)
    total_reward = w_td * TD_error_norm + w_nov * novelty_norm - w_hab * habituation_norm + w_hsi * hsi_norm
    State:
        V_states: value function per territory id (dense vector sized on demand)
        visit_counts: visitation counts per territory (for novelty/habituation)
    Complexity:
        Per tick: O(N) to aggregate territory stats (+ O(k_states) bookkeeping)
    """
    w_td: float = 0.35
    w_nov: float = 0.25
    w_hab: float = 0.15
    w_hsi: float = 0.25
    alpha: float = 0.10  # value function learning rate
    gamma: float = 0.95  # discount for TD
    target_var: float = 0.05  # target firing variance for HSI

    V_states: Dict[int, float] = field(default_factory=dict)
    visit_counts: Dict[int, int] = field(default_factory=dict)
    last_state: Optional[int] = None
    last_value: float = 0.0

    def _ensure_state(self, s: int):
        if s not in self.V_states:
            self.V_states[s] = 0.0
        if s not in self.visit_counts:
            self.visit_counts[s] = 0

    @staticmethod
    def _normalize(z: float) -> float:
        # Map arbitrary scalar to [-1, 1] with tanh
        return float(np.tanh(z))

    def _hsi_norm(self, W: np.ndarray) -> float:
        """
        Homeostatic Stability Index (proxy): 1 - |var(W) - target| / (target + eps), clipped to [-1,1]
        Cheap O(N) measure aligned with Rule 3 inputs.
        """
        eps = 1e-8
        v = float(np.var(W))
        diff = abs(v - self.target_var) / (self.target_var + eps)
        return float(np.clip(1.0 - diff, -1.0, 1.0))

    def compute(self, territories: Optional[np.ndarray], W: np.ndarray, external_R: Optional[float] = None) -> Dict[str, float]:
        """
        Compute SIE components and total_reward.
        Inputs:
            territories: array of length N with territory id per neuron (from ADC). If None, uses a single implicit state 0.
            W: node field (for HSI proxy)
            external_R: optional external reward R_t
        Returns dict with components and total_reward.
        """
        if territories is None or territories.size == 0:
            # Single territory fallback
            S_t = 0
            terr_ids = np.array([0], dtype=np.int32)
        else:
            # Choose current territory by majority (cheap proxy)
            # In future wire from UTE stream tagging
            vals, counts = np.unique(territories, return_counts=True)
            S_t = int(vals[np.argmax(counts)])
            terr_ids = vals.astype(np.int32)

        # Ensure state containers
        self._ensure_state(S_t)

        # Novelty/Habituation from visitation counts
        n_vis = self.visit_counts.get(S_t, 0)
        novelty_norm = self._normalize(1.0 / np.sqrt(max(1, n_vis)))
        habituation_norm = self._normalize(n_vis / (n_vis + 10.0))  # increases with repeated visits

        # HSI from W variance
        hsi_norm = self._hsi_norm(W)

        # TD error for current state (no external reward by default)
        R_t = 0.0 if external_R is None else float(external_R)
        V_s = self.V_states.get(S_t, 0.0)
        V_next = V_s  # single-state proxy unless territories change next tick

        # If previous state differs, approximate bootstrapping using its value
        if self.last_state is not None and self.last_state != S_t:
            V_next = self.V_states.get(self.last_state, 0.0)

        td_error = R_t + self.gamma * V_next - V_s
        # Normalize TD error to [-1,1] via tanh
        TD_error_norm = self._normalize(td_error)

        # Update value function
        self.V_states[S_t] = V_s + self.alpha * td_error

        # Update visit counts
        self.visit_counts[S_t] = n_vis + 1

        # Total reward per Rule 3
        total_reward = (
            self.w_td * TD_error_norm
            + self.w_nov * novelty_norm
            - self.w_hab * habituation_norm
            + self.w_hsi * hsi_norm
        )

        # Track last
        self.last_state = S_t
        self.last_value = self.V_states[S_t]

        return {
            "S_t": float(S_t),
            "TD_error_norm": float(TD_error_norm),
            "novelty_norm": float(novelty_norm),
            "habituation_norm": float(habituation_norm),
            "hsi_norm": float(hsi_norm),
            "total_reward": float(total_reward),
        }]]></content>
    </file>
    <file>
      <path>guards/README.md</path>
      <content/>
    </file>
    <file>
      <path>guards/invariants.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.guards.invariants

Physics ↔ code guard helpers (CI/runtime-safe, no IO, no scans).

Purpose
- Provide minimal, deterministic checks that connect implementation to physics notes.
- Intended for CI tests and optional runtime warnings (callers decide policy).

Included
- check_site_constant_of_motion: sample-based drift check of a simple constant-of-motion proxy Q_FUM.
- compute_memory_groups: expose dimensionless memory steering groups from MemoryField.

Notes
- Q_FUM here uses a conservative, implementation-agnostic proxy over the on-site state vector W:
    Q_i = 0.5 * W_i^2 + α * W_i + β
  The exact analytical form depends on the chosen on-site law; callers can tune (alpha, beta) and tolerances.
- Sampling is caller-controlled and bounded; no global scans required.

Void-faithful
- Pure numeric helpers; no external imports beyond typing/math/statistics.
- O(#samples) time; callers pass bounded samples (e.g., 256-2048 indices).
"""

from typing import Dict, Iterable, Optional, Sequence, Tuple
import math


def _percentile(xs: Sequence[float], p: float) -> float:
    if not xs:
        return 0.0
    if p <= 0.0:
        return float(min(xs))
    if p >= 1.0:
        return float(max(xs))
    xs_sorted = sorted(float(v) for v in xs)
    i = int(math.floor(p * (len(xs_sorted) - 1)))
    return float(xs_sorted[max(0, min(len(xs_sorted) - 1, i))])


def check_site_constant_of_motion(
    W_prev: Sequence[float],
    W_curr: Sequence[float],
    *,
    alpha: float = 0.0,
    beta: float = 0.0,
    dt: float = 1.0,
    samples: Optional[Iterable[int]] = None,
    tol_abs: float = 1e-6,
    tol_p99: float = 1e-5,
) -> Dict[str, float | int | bool]:
    """
    Sample-based constant-of-motion proxy drift check.

    Definitions (per-site):
      Q_prev = 0.5 * W_prev^2 + alpha * W_prev + beta
      Q_curr = 0.5 * W_curr^2 + alpha * W_curr + beta
      dQ = (Q_curr - Q_prev)

    Returns dict with:
      {
        "count": int,          # number of sampled sites evaluated
        "dQ_mean": float,
        "dQ_p95": float,
        "dQ_p99": float,
        "dQ_max": float,
        "pass_abs": bool,      # max |dQ| <= tol_abs
        "pass_p99": bool,      # p99 |dQ| <= tol_p99
      }

    Notes
    - This is a conservative drift check. Tighten tolerances as your on-site ODE is finalized.
    - dt is accepted for API symmetry; current proxy is discrete in time and uses raw differences.
    """
    try:
        n_prev = len(W_prev)
        n_curr = len(W_curr)
    except Exception:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}
    if n_prev <= 0 or n_prev != n_curr:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    idxs: Iterable[int]
    if samples is None:
        # bounded default: first min(1024, N)
        k = min(1024, n_prev)
        idxs = range(k)
    else:
        idxs = samples

    dqs_abs: list[float] = []
    s = 0.0
    c = 0
    for i in idxs:
        try:
            ii = int(i)
            if ii < 0 or ii >= n_prev:
                continue
            wp = float(W_prev[ii])
            wc = float(W_curr[ii])
            q_prev = 0.5 * wp * wp + alpha * wp + beta
            q_curr = 0.5 * wc * wc + alpha * wc + beta
            dq = q_curr - q_prev
            dqs_abs.append(abs(float(dq)))
            s += float(dq)
            c += 1
        except Exception:
            continue

    if c <= 0:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    dQ_mean = float(s) / float(c)
    dQ_p95 = _percentile(dqs_abs, 0.95)
    dQ_p99 = _percentile(dqs_abs, 0.99)
    dQ_max = float(max(dqs_abs)) if dqs_abs else 0.0

    return {
        "count": int(c),
        "dQ_mean": float(dQ_mean),
        "dQ_p95": float(dQ_p95),
        "dQ_p99": float(dQ_p99),
        "dQ_max": float(dQ_max),
        "pass_abs": bool(dQ_max <= float(tol_abs)),
        "pass_p99": bool(dQ_p99 <= float(tol_p99)),
    }


def compute_memory_groups(field: object) -> Dict[str, float]:
    """
    Read dimensionless memory steering groups from MemoryField (if present).

    Expected MemoryField properties (read-only):
      - Theta   (steering coefficient placeholder; walkers use it when applicable)
      - D_a     (write gain, γ in dimensionless units)
      - Lambda  (decay, δ)
      - Gamma   (one-edge smoothing, κ)

    Returns dict: {"mem_Theta":..., "mem_Da":..., "mem_Lambda":..., "mem_Gamma":...}
    Missing properties default to 0.0.
    """
    def _get(obj: object, name: str) -> float:
        try:
            return float(getattr(obj, name, 0.0))
        except Exception:
            return 0.0

    return {
        "mem_Theta": _get(field, "Theta"),
        "mem_Da": _get(field, "D_a") if hasattr(field, "D_a") else _get(field, "Da"),
        "mem_Lambda": _get(field, "Lambda"),
        "mem_Gamma": _get(field, "Gamma"),
    }


# --- Physics invariants additions (CI helpers; pure numeric, no scans) ---

def qfum_logistic_value(w: float, t: float, *, alpha: float, beta: float, eps: float = 1e-12) -> float:
    """
    Closed-form on-site invariant for the logistic law:
        dW/dt = (α - β) W - α W^2  ≡  k W (1 - W/K)
    with k = α - β, K = (α - β)/α (assuming α > 0).

    Time-invariant quantity:
        Q = t - (1/α) ln( W / (K - W) ) - (β/α) t

    Justification:
      Using W/(K - W) = e^{k t}/A, we obtain
        Q = t - (k/α) t + (1/α) ln A - (β/α) t = (1/α) ln A (constant).
    This helper computes Q(w, t) robustly with safe clamping at boundaries.
    If α ≤ 0 or K ≤ 0, returns 0.0 (fail-soft) since the invariant is undefined.
    """
    try:
        a = float(alpha)
        b = float(beta)
        tt = float(t)
        if not (a > eps):
            return 0.0
        K = (a - b) / a
        if not (K > eps):
            return 0.0
        # clamp w to (eps, K - eps) to avoid singularities at 0 and K
        wv = float(w)
        if wv <= eps:
            wv = eps
        Km = float(K) - eps
        if wv >= Km:
            wv = Km
        base = tt - (1.0 / a) * math.log(wv / (float(K) - wv))
        return base - (b / a) * tt
    except Exception:
        return 0.0


def check_qfum_logistic(
    W_prev: Sequence[float],
    W_curr: Sequence[float],
    *,
    t_prev: float,
    t_curr: float,
    alpha: float,
    beta: float,
    samples: Optional[Iterable[int]] = None,
    tol_abs: float = 1e-6,
    tol_p99: float = 1e-5,
) -> Dict[str, float | int | bool]:
    """
    Sample-based drift check for the analytic Q_FUM constant of motion under the logistic on-site law.

    Definitions:
      Q_i(t, W) = t - (1/α) ln( W / (K - W) ), with K = (α - β)/α, α>0.
      ΔQ_i = Q_i(t_curr, W_curr[i]) - Q_i(t_prev, W_prev[i])

    Returns dict analogous to check_site_constant_of_motion with p95/p99/max of |ΔQ|.
    """
    try:
        n_prev = len(W_prev)
        n_curr = len(W_curr)
    except Exception:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}
    if n_prev <= 0 or n_prev != n_curr:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    idxs: Iterable[int]
    if samples is None:
        k = min(1024, n_prev)
        idxs = range(k)
    else:
        idxs = samples

    dqs_abs: list[float] = []
    s = 0.0
    c = 0
    for i in idxs:
        try:
            ii = int(i)
            if ii < 0 or ii >= n_prev:
                continue
            q0 = qfum_logistic_value(float(W_prev[ii]), float(t_prev), alpha=alpha, beta=beta)
            q1 = qfum_logistic_value(float(W_curr[ii]), float(t_curr), alpha=alpha, beta=beta)
            dq = float(q1 - q0)
            dqs_abs.append(abs(dq))
            s += dq
            c += 1
        except Exception:
            continue

    if c <= 0:
        return {"count": 0, "dQ_mean": 0.0, "dQ_p95": 0.0, "dQ_p99": 0.0, "dQ_max": 0.0, "pass_abs": False, "pass_p99": False}

    dQ_mean = float(s) / float(c)
    dQ_p95 = _percentile(dqs_abs, 0.95)
    dQ_p99 = _percentile(dqs_abs, 0.99)
    dQ_max = float(max(dqs_abs)) if dqs_abs else 0.0

    return {
        "count": int(c),
        "dQ_mean": float(dQ_mean),
        "dQ_p95": float(dQ_p95),
        "dQ_p99": float(dQ_p99),
        "dQ_max": float(dQ_max),
        "pass_abs": bool(dQ_max <= float(tol_abs)),
        "pass_p99": bool(dQ_p99 <= float(tol_p99)),
    }


def kinetic_c2_from_kappa(kappa: float, a: float) -> float:
    """
    Kinetic normalization: c^2 = κ a^2, where κ is the spatial coupling in L_K.
    """
    try:
        return float(kappa) * float(a) * float(a)
    except Exception:
        return 0.0


def kinetic_c2_from_J(J: float, a: float) -> float:
    """
    Site-coupling convention: c^2 = 2 J a^2. Useful equivalence check against κ=2J.
    """
    try:
        return 2.0 * float(J) * float(a) * float(a)
    except Exception:
        return 0.0


__all__ = [
    "check_site_constant_of_motion",
    "compute_memory_groups",
    "qfum_logistic_value",
    "check_qfum_logistic",
    "kinetic_c2_from_kappa",
    "kinetic_c2_from_J",
]]]></content>
    </file>
    <file>
      <path>memory/README.md</path>
      <content/>
    </file>
    <file>
      <path>memory/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
fum_rt.core.memory package

Exports:
- MemoryField: event-driven memory field owner (from .field)
- load_engram, save_checkpoint: engram IO (from .engram_io)

This resolves the prior module/package name conflict by making
fum_rt.core.memory a proper package namespace with explicit re-exports.
"""

from .field import MemoryField
from .engram_io import load_engram, save_checkpoint

__all__ = ["MemoryField", "load_engram", "save_checkpoint"]]]></content>
    </file>
    <file>
      <path>memory/engram_io.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os
import json
import numpy as np
from typing import List, Tuple

# Optional HDF5 backend (preferred)
try:
    import h5py  # type: ignore
    HAVE_H5 = True
except Exception:
    HAVE_H5 = False

# Import ADC dataclasses for (de)serialization
try:
    from .adc import Territory as ADCTerritory, Boundary as ADCBoundary, _EWMA as _ADC_EWMA  # type: ignore
except Exception:
    ADCTerritory = None  # type: ignore
    ADCBoundary = None  # type: ignore
    _ADC_EWMA = None  # type: ignore


def _adj_to_csr(adj: List[np.ndarray], N: int) -> Tuple[np.ndarray, np.ndarray]:
    """Convert neighbor-lists (sparse adjacency) to CSR arrays: row_ptr, col_idx."""
    row_ptr = np.zeros(N + 1, dtype=np.int64)
    total = 0
    for i in range(N):
        deg = int(adj[i].size)
        row_ptr[i] = total
        total += deg
    row_ptr[N] = total
    col_idx = np.zeros(total, dtype=np.int32)
    pos = 0
    for i in range(N):
        nbrs = adj[i]
        if nbrs.size:
            k = nbrs.size
            col_idx[pos : pos + k] = nbrs.astype(np.int32, copy=False)
            pos += k
    return row_ptr, col_idx


def _csr_to_adj(row_ptr: np.ndarray, col_idx: np.ndarray, N: int) -> List[np.ndarray]:
    """Convert CSR arrays to neighbor-lists (sparse adjacency)."""
    adj = []
    for i in range(N):
        start = int(row_ptr[i])
        end = int(row_ptr[i + 1])
        if end > start:
            adj.append(col_idx[start:end].astype(np.int32, copy=False))
        else:
            adj.append(np.zeros(0, dtype=np.int32))
    return adj


# -----------------------
# ADC (de)serialization
# -----------------------
def _ewma_to_dict(w) -> dict:
    try:
        return {
            "alpha": float(getattr(w, "alpha", 0.15)),
            "mean": float(getattr(w, "mean", 0.0)),
            "var": float(getattr(w, "var", 0.0)),
            "init": bool(getattr(w, "init", False)),
        }
    except Exception:
        return {"alpha": 0.15, "mean": 0.0, "var": 0.0, "init": False}


def _ewma_from_dict(d):
    try:
        a = float(d.get("alpha", 0.15))
        m = float(d.get("mean", 0.0))
        v = float(d.get("var", 0.0))
        ini = bool(d.get("init", False))
        if _ADC_EWMA is not None:
            return _ADC_EWMA(alpha=a, mean=m, var=v, init=ini)  # type: ignore
    except Exception:
        pass
    if _ADC_EWMA is not None:
        return _ADC_EWMA(alpha=0.15)  # type: ignore
    return None


def _adc_to_dict(adc) -> dict:
    """Serialize ADC internals into a JSON-friendly dict."""
    try:
        terr = []
        for key, t in getattr(adc, "_territories", {}).items():
            try:
                dom, cov = key
            except Exception:
                dom, cov = "", 0
            terr.append({
                "key": [str(dom), int(cov)],
                "id": int(getattr(t, "id", 0)),
                "mass": float(getattr(t, "mass", 0.0)),
                "conf": float(getattr(t, "conf", 0.0)),
                "ttl": int(getattr(t, "ttl", 0)),
                "w_stats": _ewma_to_dict(getattr(t, "w_stats", None)),
                "s_stats": _ewma_to_dict(getattr(t, "s_stats", None)),
            })
        bounds = []
        for key, b in getattr(adc, "_boundaries", {}).items():
            try:
                a, c = key
            except Exception:
                a, c = 0, 0
            bounds.append({
                "a": int(a),
                "b": int(c),
                "ttl": int(getattr(b, "ttl", 0)),
                "cut_stats": _ewma_to_dict(getattr(b, "cut_stats", None)),
                "churn": _ewma_to_dict(getattr(b, "churn", None)),
            })
        fcnt = []
        for key, cnt in getattr(adc, "_frontier_counter", {}).items():
            try:
                dom, cov = key
            except Exception:
                dom, cov = "", 0
            fcnt.append({
                "key": [str(dom), int(cov)],
                "count": int(cnt),
            })
        return {
            "id_seq": int(getattr(adc, "_id_seq", 1)),
            "territories": terr,
            "boundaries": bounds,
            "frontier_counter": fcnt,
        }
    except Exception:
        return {}


def _adc_load_from_dict(adc, state: dict) -> None:
    """Populate ADC internals from a previously serialized dict."""
    if adc is None or not isinstance(state, dict):
        return
    try:
        terr_d = {}
        max_id = 0
        for t in state.get("territories", []):
            try:
                dom, cov = t.get("key", ["", 0])
                tid = int(t.get("id", 0))
                max_id = max(max_id, tid)
                wj = t.get("w_stats", {})
                sj = t.get("s_stats", {})
                w_stats = _ewma_from_dict(wj)
                s_stats = _ewma_from_dict(sj)
                if ADCTerritory is not None:
                    terr = ADCTerritory(
                        key=(str(dom), int(cov)),
                        id=tid,
                        mass=float(t.get("mass", 0.0)),
                        conf=float(t.get("conf", 0.0)),
                        ttl=int(t.get("ttl", 0)),
                        w_stats=w_stats if w_stats is not None else (_ADC_EWMA(alpha=0.15) if _ADC_EWMA else None),  # type: ignore
                        s_stats=s_stats if s_stats is not None else (_ADC_EWMA(alpha=0.15) if _ADC_EWMA else None),  # type: ignore
                    )
                    terr_d[(str(dom), int(cov))] = terr
            except Exception:
                continue
        bnd_d = {}
        for b in state.get("boundaries", []):
            try:
                a = int(b.get("a", 0))
                c = int(b.get("b", 0))
                cut = _ewma_from_dict(b.get("cut_stats", {}))
                chrn = _ewma_from_dict(b.get("churn", {}))
                if ADCBoundary is not None:
                    bnd = ADCBoundary(
                        a=min(a, c),
                        b=max(a, c),
                        cut_stats=cut if cut is not None else (_ADC_EWMA(alpha=0.2) if _ADC_EWMA else None),  # type: ignore
                        churn=chrn if chrn is not None else (_ADC_EWMA(alpha=0.2) if _ADC_EWMA else None),  # type: ignore
                        ttl=int(b.get("ttl", 0)),
                    )
                    bnd_d[(min(a, c), max(a, c))] = bnd
            except Exception:
                continue
        fcnt = {}
        for fc in state.get("frontier_counter", []):
            try:
                dom, cov = fc.get("key", ["", 0])
                fcnt[(str(dom), int(cov))] = int(fc.get("count", 0))
            except Exception:
                continue
        setattr(adc, "_territories", terr_d)
        setattr(adc, "_boundaries", bnd_d)
        setattr(adc, "_frontier_counter", fcnt)
        try:
            id_seq = int(state.get("id_seq", max_id + 1))
        except Exception:
            id_seq = max_id + 1
        setattr(adc, "_id_seq", max(1, id_seq))
    except Exception:
        return


def save_checkpoint(run_dir: str, step: int, connectome, fmt: str = "h5", adc=None) -> str:
    """
    Save runtime state (engram) for dense or sparse backends.

    Args:
        run_dir: run directory
        step: tick index
        connectome: Connectome or SparseConnectome
        fmt: "h5" (preferred) or "npz" (compat)
        adc: Optional ADC instance to persist alongside the connectome
    """
    os.makedirs(run_dir, exist_ok=True)
    backend = "sparse" if hasattr(connectome, "adj") else "dense"

    if fmt.lower() == "h5":
        if not HAVE_H5:
            # Fallback transparently to npz if h5py isn't available
            fmt = "npz"
        else:
            path = os.path.join(run_dir, f"state_{step}.h5")
            _save_h5(path, connectome, backend, adc)
            return path

    # default/fallback npz
    path = os.path.join(run_dir, f"state_{step}.npz")
    _save_npz(path, connectome, backend, adc)
    return path


def _save_h5(path: str, connectome, backend: str, adc=None):
    with h5py.File(path, "w") as f:
        # Metadata as attributes
        f.attrs["backend"] = backend
        f.attrs["N"] = int(connectome.N)
        f.attrs["k"] = int(getattr(connectome, "k", 0))
        f.attrs["threshold"] = float(getattr(connectome, "threshold", 0.0))
        f.attrs["lambda_omega"] = float(getattr(connectome, "lambda_omega", 0.0))
        f.attrs["dtype"] = "float32"

        if backend == "dense":
            g = f.create_group("dense")
            g.create_dataset("W", data=connectome.W.astype(np.float32, copy=False), compression="gzip")
            g.create_dataset("A", data=connectome.A.astype(np.int8, copy=False), compression="gzip")
            g.create_dataset("E", data=connectome.E.astype(np.float32, copy=False), compression="gzip")
        else:
            # Sparse: store neighbor lists as CSR
            row_ptr, col_idx = _adj_to_csr(connectome.adj, int(connectome.N))
            g = f.create_group("sparse")
            g.create_dataset("W", data=connectome.W.astype(np.float32, copy=False), compression="gzip")
            g.create_dataset("row_ptr", data=row_ptr, compression="gzip")
            g.create_dataset("col_idx", data=col_idx, compression="gzip")

        # Optional: persist ADC in a single JSON dataset for portability
        if adc is not None:
            try:
                state_json = json.dumps(_adc_to_dict(adc))
                f.create_dataset("adc_json", data=state_json, dtype=h5py.string_dtype(encoding="utf-8"))
            except Exception:
                pass


def _save_npz(path: str, connectome, backend: str, adc=None):
    adc_json = None
    if adc is not None:
        try:
            adc_json = json.dumps(_adc_to_dict(adc))
        except Exception:
            adc_json = None

    if backend == "dense":
        if adc_json is None:
            np.savez_compressed(
                path,
                backend="dense",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                A=connectome.A.astype(np.int8, copy=False),
                E=connectome.E.astype(np.float32, copy=False),
            )
        else:
            np.savez_compressed(
                path,
                backend="dense",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                A=connectome.A.astype(np.int8, copy=False),
                E=connectome.E.astype(np.float32, copy=False),
                adc_json=adc_json,
            )
    else:
        row_ptr, col_idx = _adj_to_csr(connectome.adj, int(connectome.N))
        if adc_json is None:
            np.savez_compressed(
                path,
                backend="sparse",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                row_ptr=row_ptr,
                col_idx=col_idx,
            )
        else:
            np.savez_compressed(
                path,
                backend="sparse",
                N=int(connectome.N),
                k=int(getattr(connectome, "k", 0)),
                threshold=float(getattr(connectome, "threshold", 0.0)),
                lambda_omega=float(getattr(connectome, "lambda_omega", 0.0)),
                W=connectome.W.astype(np.float32, copy=False),
                row_ptr=row_ptr,
                col_idx=col_idx,
                adc_json=adc_json,
            )


def load_engram(path: str, connectome, adc=None) -> None:
    """
    Load an engram from .h5 or .npz and populate the provided connectome instance.
    If ADC state is present and 'adc' is provided, populate it as well.

    - Dense: sets W, A, E, threshold
    - Sparse: sets W, adj (neighbor lists), threshold
    - ADC (optional): territories, boundaries, counters
    """
    p = str(path)
    if p.lower().endswith(".h5"):
        if not HAVE_H5:
            raise RuntimeError("h5py not installed but .h5 requested")
        _load_h5(p, connectome, adc)
        return
    # npz fallback
    _load_npz(p, connectome, adc)


def _apply_common_attrs(meta: dict, connectome):
    # Resize N if needed (safe for our numpy arrays here)
    N = int(meta.get("N", connectome.N))
    connectome.N = N
    # threshold, lambda_omega if present
    if "threshold" in meta:
        connectome.threshold = float(meta["threshold"])
    if "lambda_omega" in meta:
        connectome.lambda_omega = float(meta["lambda_omega"])


def _load_h5(path: str, connectome, adc=None):
    with h5py.File(path, "r") as f:
        backend = f.attrs.get("backend", "dense")
        meta = {
            "N": int(f.attrs.get("N", connectome.N)),
            "threshold": float(f.attrs.get("threshold", getattr(connectome, "threshold", 0.0))),
            "lambda_omega": float(f.attrs.get("lambda_omega", getattr(connectome, "lambda_omega", 0.0))),
        }
        _apply_common_attrs(meta, connectome)

        if backend == "dense":
            g = f["dense"]
            connectome.W = g["W"][...].astype(np.float32, copy=False)
            connectome.A = g["A"][...].astype(np.int8, copy=False)
            connectome.E = g["E"][...].astype(np.float32, copy=False)
        else:
            g = f["sparse"]
            connectome.W = g["W"][...].astype(np.float32, copy=False)
            row_ptr = g["row_ptr"][...]
            col_idx = g["col_idx"][...]
            connectome.adj = _csr_to_adj(row_ptr, col_idx, int(connectome.N))

        # Load ADC if present
        if adc is not None:
            try:
                ds = f.get("adc_json", None)
                if ds is not None:
                    raw = ds[()]
                    if isinstance(raw, bytes):
                        raw = raw.decode("utf-8", errors="ignore")
                    state = json.loads(raw)
                    _adc_load_from_dict(adc, state)
            except Exception:
                pass


def _load_npz(path: str, connectome, adc=None):
    data = np.load(path, allow_pickle=False)
    backend = str(data.get("backend", "dense"))
    meta = {
        "N": int(data.get("N", connectome.N)),
        "threshold": float(data.get("threshold", getattr(connectome, "threshold", 0.0))),
        "lambda_omega": float(data.get("lambda_omega", getattr(connectome, "lambda_omega", 0.0))),
    }
    _apply_common_attrs(meta, connectome)
    if backend == "dense":
        connectome.W = data["W"].astype(np.float32, copy=False)
        connectome.A = data["A"].astype(np.int8, copy=False)
        connectome.E = data["E"].astype(np.float32, copy=False)
    else:
        connectome.W = data["W"].astype(np.float32, copy=False)
        row_ptr = data["row_ptr"]
        col_idx = data["col_idx"]
        connectome.adj = _csr_to_adj(row_ptr, col_idx, int(connectome.N))

    # Load ADC if present
    if adc is not None:
        try:
            if hasattr(data, "files") and "adc_json" in data.files:
                raw = data["adc_json"]
                # raw could be 0-d array of str/bytes
                if isinstance(raw, np.ndarray):
                    if raw.dtype.kind in ("U", "S") and raw.shape == ():
                        raw_val = raw.item()
                    else:
                        raw_val = raw.tolist()
                        if isinstance(raw_val, list) and raw_val:
                            raw_val = raw_val[0]
                else:
                    raw_val = raw
                if isinstance(raw_val, bytes):
                    raw_val = raw_val.decode("utf-8", errors="ignore")
                if isinstance(raw_val, (str,)):
                    state = json.loads(raw_val)
                    _adc_load_from_dict(adc, state)
        except Exception:
            pass
]]></content>
    </file>
    <file>
      <path>memory/field.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.memory.field
Purpose: Event-driven memory field with write-decay-spread dynamics (void-faithful).

Design constraints
- Sparse-first: no dense global passes; updates are event-local only.
- No schedulers: called from the per-tick CoreEngine fold path.
- No scans in core/ or maps/: bounded working set; pruning uses sampling.
- Maps/frame v1/v2 unchanged (telemetry-only); this reducer is for local steering.
- Guards pass; control-impact is minimal since this only folds small event batches.

Dynamics (per tick, event-driven)
- On node touch (vt_touch at node i):
    m[i] ← m[i] * exp(-δ·Δt) + γ · r_i · Δt
  where r_i is a small stimulus inferred from event weight (default 1.0).

- On edge_on(i, j) smoothing (one-edge local spread):
    δm = κ · (m[j] - m[i]) · Δt
    m[i] += δm
    m[j] -= δm

- Optional burst footprints:
    SpikeEvent(node=j, amp) → m[j] += γ_s · amp · Δt
    DeltaWEvent(node=j, dw) → m[j] += γ_w · |dw| · Δt

Snapshot
- memory_head: top-k [[node, value], ...] by current m value (k=head_k, default 256)
- memory_p95/p99/max/count: summaries of working set
- memory_dict: bounded dictionary {node: m} (size ≤ keep_max)

Tuning (dimensionless)
- gamma (γ): write gain
- delta (δ): exponential decay rate per tick
- kappa (κ): one-edge smoothing coupling
- touch_gain, spike_gain, dW_gain: per-event scalings feeding the write term
"""

from typing import Dict, Iterable, List, Tuple
import math
import random

from fum_rt.core.proprioception.events import VTTouchEvent, EdgeOnEvent, SpikeEvent, DeltaWEvent


class MemoryField:
    """
    Event-driven, bounded memory field.

    Parameters:
      - head_k: top-k head size for memory_head
      - keep_max: max retained working-set size (defaults to 16×head_k)
      - seed: RNG seed for pruning sampling
      - gamma: write gain (γ)
      - delta: decay rate (δ) per tick (0..1)
      - kappa: one-edge smoothing coupling (κ)
      - touch_gain/spike_gain/dW_gain: event-to-write scaling
    """

    __slots__ = (
        "head_k",
        "keep_max",
        "rng",
        "_m",
        "_last_tick",
        "gamma",
        "delta",
        "kappa",
        "touch_gain",
        "spike_gain",
        "dW_gain",
    )

    def __init__(
        self,
        head_k: int = 256,
        keep_max: int | None = None,
        seed: int = 0,
        *,
        gamma: float = 0.05,
        delta: float = 0.01,
        kappa: float = 0.10,
        touch_gain: float = 1.0,
        spike_gain: float = 0.20,
        dW_gain: float = 0.10,
    ) -> None:
        self.head_k = int(max(8, head_k))
        km = int(keep_max) if keep_max is not None else self.head_k * 16
        self.keep_max = int(max(self.head_k, km))
        self.rng = random.Random(int(seed))
        self._m: Dict[int, float] = {}
        self._last_tick: Dict[int, int] = {}

        # dynamics
        self.gamma = float(max(0.0, gamma))
        self.delta = float(max(0.0, min(1.0, delta)))
        self.kappa = float(max(0.0, kappa))
        self.touch_gain = float(max(0.0, touch_gain))
        self.spike_gain = float(max(0.0, spike_gain))
        self.dW_gain = float(max(0.0, dW_gain))

    # ---------------- internal helpers ----------------

    def _decay_to(self, node: int, tick: int) -> None:
        lt = self._last_tick.get(node)
        if lt is None:
            self._last_tick[node] = tick
            return
        dt = max(0, int(tick) - int(lt))
        if dt > 0:
            # Exponential decay: m *= exp(-δ·Δt). Use (1-δ)^Δt for stability when δ small.
            try:
                base = max(0.0, 1.0 - self.delta)
                factor = base ** dt
            except Exception:
                factor = math.exp(-self.delta * float(dt))
            self._m[node] = float(self._m.get(node, 0.0)) * float(factor)
            self._last_tick[node] = tick

    def _ensure_and_decay(self, node: int, tick: int) -> None:
        n = int(node)
        if n not in self._m:
            self._m[n] = 0.0
            self._last_tick[n] = int(tick)
        else:
            self._decay_to(n, int(tick))

    def _prune(self) -> None:
        size = len(self._m)
        target_drop = size - self.keep_max
        if target_drop <= 0:
            return
        keys = list(self._m.keys())
        sample_size = min(len(keys), max(256, target_drop * 4))
        sample = self.rng.sample(keys, sample_size) if sample_size > 0 else keys
        # Drop smallest m in the sample
        sample.sort(key=lambda k: self._m.get(k, 0.0))
        for k in sample[:target_drop]:
            self._m.pop(k, None)
            self._last_tick.pop(k, None)

    # ---------------- public API ----------------

    def fold(self, events: Iterable[object], tick: int) -> None:
        """
        Fold a batch of events at integer tick.
        """
        t = int(tick)
        γ = self.gamma
        δ = self.delta
        κ = self.kappa
        tg = self.touch_gain
        sg = self.spike_gain
        wg = self.dW_gain

        for e in events:
            k = getattr(e, "kind", None)

            if k == "vt_touch" and isinstance(e, VTTouchEvent):
                try:
                    i = int(e.token)
                except Exception:
                    continue
                if i < 0:
                    continue
                # decay-then-write at node i
                self._ensure_and_decay(i, t)
                r_i = float(getattr(e, "w", 1.0))
                self._m[i] = float(self._m.get(i, 0.0)) + float(γ * tg * r_i)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "edge_on" and isinstance(e, EdgeOnEvent):
                try:
                    u = int(getattr(e, "u", -1))
                    v = int(getattr(e, "v", -1))
                except Exception:
                    continue
                if u < 0 or v < 0:
                    continue
                # local smoothing on the edge (u, v)
                self._ensure_and_decay(u, t)
                self._ensure_and_decay(v, t)
                mu = float(self._m.get(u, 0.0))
                mv = float(self._m.get(v, 0.0))
                d = float(κ * (mv - mu))
                self._m[u] = mu + d
                self._m[v] = mv - d
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "spike" and isinstance(e, SpikeEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                amp = float(getattr(e, "amp", 1.0))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * sg * amp)
                if len(self._m) > self.keep_max:
                    self._prune()

            elif k == "delta_w" and isinstance(e, DeltaWEvent):
                try:
                    j = int(getattr(e, "node", -1))
                except Exception:
                    continue
                if j < 0:
                    continue
                self._ensure_and_decay(j, t)
                dw = abs(float(getattr(e, "dw", 0.0)))
                self._m[j] = float(self._m.get(j, 0.0)) + float(γ * wg * dw)
                if len(self._m) > self.keep_max:
                    self._prune()

    def snapshot(self, head_n: int = 16) -> Dict[str, object]:
        """
        Return bounded snapshot of the field.
        """
        if not self._m:
            return {
                "memory_head": [],
                "memory_p95": 0.0,
                "memory_p99": 0.0,
                "memory_max": 0.0,
                "memory_count": 0,
                "memory_dict": {},
            }

        # head top-k
        try:
            import heapq as _heapq
            head = _heapq.nlargest(int(min(self.head_k, max(1, head_n))), self._m.items(), key=lambda kv: kv[1])
        except Exception:
            head = sorted(self._m.items(), key=lambda kv: kv[1], reverse=True)[: int(min(self.head_k, max(1, head_n)))]

        vals = sorted(float(v) for v in self._m.values())

        def q(p: float) -> float:
            if not vals:
                return 0.0
            i = min(len(vals) - 1, max(0, int(math.floor(p * (len(vals) - 1)))))
            return float(vals[i])

        out_dict: Dict[int, float] = {int(k): float(v) for k, v in self._m.items()}

        return {
            "memory_head": [[int(k), float(v)] for k, v in head],
            "memory_p95": q(0.95),
            "memory_p99": q(0.99),
            "memory_max": float(vals[-1]),
            "memory_count": int(len(vals)),
            "memory_dict": out_dict,
        }

    # ---------------- taps / adapters ----------------

    def get_m(self, i: int) -> float:
        """
        O(1) local read for node i.
        """
        try:
            return float(self._m.get(int(i), 0.0))
        except Exception:
            return 0.0

    def get_many(self, idxs) -> Dict[int, float]:
        """
        Return {i: m[i]} for a small iterable of indices.
        """
        out: Dict[int, float] = {}
        try:
            for j in idxs or []:
                try:
                    out[int(j)] = float(self._m.get(int(j), 0.0))
                except Exception:
                    continue
        except Exception:
            pass
        return out

    def update_from_events(self, events, dt_ms: int | float | None = None) -> None:
        """
        Alias for fold() to support adapter-style interfaces. dt_ms is ignored; tick should be provided in events or by the caller.
        """
        # Use last seen tick for touched nodes when missing; safe fallback 0.
        # Here we just pass 0; runtime passes proper tick to fold() already.
        self.fold(events, tick=0)

    def snapshot_head(self, head_k: int | None = None) -> List[List[float]]:
        """
        Convenience: return just the head list (top-k) [[node, value], ...]
        """
        k = int(self.head_k if head_k is None else max(1, head_k))
        snap = self.snapshot(head_n=k)
        head = snap.get("memory_head", []) if isinstance(snap, dict) else []
        return head or []

    def snapshot_dict(self, cap: int = 2048) -> Dict[int, float]:
        """
        Convenience: return a bounded dict {node:value}.
        """
        snap = self.snapshot(head_n=self.head_k)
        dct = snap.get("memory_dict", {}) if isinstance(snap, dict) else {}
        if not isinstance(dct, dict):
            return {}
        if len(dct) <= int(cap):
            return {int(k): float(v) for k, v in dct.items()}
        try:
            import heapq as _heapq
            items = list(dct.items())
            top = _heapq.nlargest(int(cap), items, key=lambda kv: kv[1])
            return {int(k): float(v) for k, v in top}
        except Exception:
            keys = list(dct.keys())[: int(cap)]
            return {int(k): float(dct[k]) for k in keys if k in dct}

    # ---------------- dimensionless knobs (read-only) ----------------

    @property
    def D_a(self) -> float:
        """Write gain (γ)"""
        return float(self.gamma)

    @property
    def Lambda(self) -> float:
        """Decay rate (δ)"""
        return float(self.delta)

    @property
    def Gamma(self) -> float:
        """One-edge smoothing (κ)"""
        return float(self.kappa)

    @property
    def Theta(self) -> float:
        """
        Steering coefficient placeholder (field does not steer directly).
        Provided for dimensional consistency; steering lives in walkers.
        """
        return 0.0


__all__ = ["MemoryField"]]]></content>
    </file>
    <file>
      <path>metrics.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

def compute_metrics(connectome):
   """
   Rule Ref: Blueprint Rule 4.1 (Pathology Detection Mechanisms)
   - Adds connectome_entropy to support Active Domain Cartography (Rule 7) scheduling.
   - Prefers connectome.connectome_entropy() when available (sparse-mode), falling back to local function.
   """
   # TODO GET THESE FOR FREE FROM THE VOID WALKERS
   # Prefer a connectome-native entropy calculator for sparse-mode
   try:
       h = float(connectome.connectome_entropy())
   except Exception:
       h = float(connectome_entropy(connectome))
   return {
       "avg_weight": float(connectome.W.mean()),
       "active_synapses": int(connectome.active_edge_count()),
       "cohesion_components": int(connectome.connected_components()),
       "complexity_cycles": int(connectome.cyclomatic_complexity()),
       "connectome_entropy": h,
   }

def connectome_entropy(connectome) -> float:
   """
   Rule Ref: Blueprint Rule 4.1 - Global Pathological Structure (Connectome Entropy)
   Formula: H = -Σ p_i log(p_i), where p is degree distribution of the active subgraph.
   Returns 0.0 when no active edges are present.
   """
   # Active, undirected mask
   mask = (connectome.E > connectome.threshold) & (connectome.A == 1)
   # Degree per node (count upper+lower symmetrically from full mask)
   deg = mask.sum(axis=1).astype(np.float64)
   total = deg.sum()
   if total <= 0:
       return 0.0
   p = deg / total
   # Numerical stability
   p = np.clip(p, 1e-12, 1.0)
   return float(-(p * np.log(p)).sum())


# --- Streaming z-score detector for first-difference of a scalar series (tick-based) ---
# This keeps state across ticks to detect "spikes" in a topology metric such as
# cyclomatic complexity (a B1 proxy). It is void-faithful: no tokens, only graph-native signals.
import math as _math

class StreamingZEMA:
    """
    EMA-based z-score detector on first differences of a scalar time series.

    Parameters (tick-based):
    - half_life_ticks: EMA half-life in ticks (controls smoothing window)
    - z_spike: z-threshold to enter spiking
    - hysteresis: subtract from z_spike to exit spiking (prevents chatter)
    - min_interval_ticks: minimum ticks between spike fires (cooldown)
    """
    def __init__(self, half_life_ticks: int = 50, z_spike: float = 3.0, hysteresis: float = 1.0, min_interval_ticks: int = 10):
        self.alpha = 1.0 - _math.exp(_math.log(0.5) / float(max(1, int(half_life_ticks))))
        self.z_spike = float(z_spike)
        self.hysteresis = float(max(0.0, hysteresis))
        self.min_interval = int(max(1, int(min_interval_ticks)))

        self.mu = 0.0        # EMA mean of delta
        self.var = 1e-8      # EMA variance of delta
        self.prev = None     # previous value for delta computation
        self._spiking = False
        self.last_fire_tick = -10**12

    def update(self, value: float, tick: int):
        v = float(value)
        if self.prev is None:
            self.prev = v
            return {
                "value": v, "delta": 0.0, "mu": self.mu,
                "sigma": self.var ** 0.5, "z": 0.0, "spike": False
            }

        d = v - self.prev
        self.prev = v

        a = self.alpha
        # EMA on first-difference
        self.mu = (1.0 - a) * self.mu + a * d
        diff = d - self.mu
        self.var = (1.0 - a) * self.var + a * (diff * diff)
        sigma = (self.var if self.var > 1e-24 else 1e-24) ** 0.5
        z = diff / sigma

        # Hysteresis + cooldown
        fire = False
        high = self.z_spike
        low = max(0.0, self.z_spike - self.hysteresis)
        if not self._spiking and z >= high and (int(tick) - int(self.last_fire_tick)) >= self.min_interval:
            self._spiking = True
            self.last_fire_tick = int(tick)
            fire = True
        elif self._spiking and z <= low:
            self._spiking = False

        return {
            "value": v,
            "delta": float(d),
            "mu": float(self.mu),
            "sigma": float(sigma),
            "z": float(z),
            "spike": bool(fire),
        }
]]></content>
    </file>
    <file>
      <path>neuroplasticity/README.md</path>
      <content/>
    </file>
    <file>
      <path>neuroplasticity/__init__.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
]]></content>
    </file>
    <file>
      <path>neuroplasticity/gdsp.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.neuroplasticity.gdsp
Purpose: Organism-native GDSP structural actuator with budgeted, territory-scoped, sparse-masked operations.

Design constraints
- One class per file; pure core; no IO/logging; NumPy + SciPy only.
- Budgeted algorithms:
  - Repairs: component-bridging with node/pair caps (no global mask sweeps).
  - Growth: reinforcement within territory by eligibility percentile; exploratory via similarity+eligibility prefilter.
  - Pruning: timer-based over weak, non-persistent synapses with CSR-safe operations.
"""

from typing import Any
import numpy as np


class GDSPActuator:
    """
    Goal-Directed Structural Plasticity (GDSP) actuator.

    - Homeostatic repairs (fragmentation healing; locus pruning)
    - Performance-driven growth (reinforcement, exploratory)
    - Maintenance pruning (weak, non-persistent synapses over time)

    Budget controls:
      bridge_budget_nodes: sample cap per component for gap bridging
      bridge_budget_pairs: max candidate (u,v) eligibility checks per tick
    """

    class _AdaptiveThresholds:
        def __init__(self) -> None:
            self.reward_threshold = 0.8
            self.td_error_threshold = 0.5
            self.novelty_threshold = 0.7
            self.sustained_window_size = 10

            self.structural_activity_counter = 0
            self.timesteps_since_growth = 0

            self.min_reward_threshold = 0.3
            self.max_reward_threshold = 0.9
            self.min_td_threshold = 0.1
            self.max_td_threshold = 0.8
            self.min_novelty_threshold = 0.2
            self.max_novelty_threshold = 0.9

            self.reward_history: list[float] = []
            self.td_error_history: list[float] = []
            self.novelty_history: list[float] = []

        def update_and_adapt(self, sie_report: dict, b1_persistence: float) -> None:
            self.reward_history.append(float(sie_report.get("total_reward", 0.0)))
            self.td_error_history.append(float(sie_report.get("td_error", 0.0)))
            self.novelty_history.append(float(sie_report.get("novelty", 0.0)))

            # truncate
            if len(self.reward_history) > 100:
                self.reward_history = self.reward_history[-100:]
                self.td_error_history = self.td_error_history[-100:]
                self.novelty_history = self.novelty_history[-100:]

            self.timesteps_since_growth += 1

            # encourage growth when stagnant
            if self.timesteps_since_growth > 500 and float(b1_persistence) <= 0.001:
                self.reward_threshold = max(self.min_reward_threshold, self.reward_threshold * 0.95)
                self.td_error_threshold = max(self.min_td_threshold, self.td_error_threshold * 0.95)
                self.novelty_threshold = max(self.min_novelty_threshold, self.novelty_threshold * 0.95)

            # dampen when activity is high
            elif self.structural_activity_counter > 20:
                self.reward_threshold = min(self.max_reward_threshold, self.reward_threshold * 1.05)
                self.td_error_threshold = min(self.max_td_threshold, self.td_error_threshold * 1.05)
                self.novelty_threshold = min(self.max_novelty_threshold, self.novelty_threshold * 1.05)
                self.structural_activity_counter = 0

            # statistical adaptation
            if len(self.reward_history) >= 50:
                r75 = float(np.percentile(self.reward_history, 75))
                td90 = float(np.percentile(self.td_error_history, 90))
                n75 = float(np.percentile(self.novelty_history, 75))

                target_reward = max(self.min_reward_threshold, min(self.max_reward_threshold, r75))
                target_td = max(self.min_td_threshold, min(self.max_td_threshold, td90))
                target_nov = max(self.min_novelty_threshold, min(self.max_novelty_threshold, n75))

                self.reward_threshold = 0.95 * self.reward_threshold + 0.05 * target_reward
                self.td_error_threshold = 0.95 * self.td_error_threshold + 0.05 * target_td
                self.novelty_threshold = 0.95 * self.novelty_threshold + 0.05 * target_nov

        def record_structural_activity(self) -> None:
            self.structural_activity_counter += 1
            self.timesteps_since_growth = 0

    def __init__(self, bridge_budget_nodes: int = 128, bridge_budget_pairs: int = 2048, rng_seed: int = 0) -> None:
        self._thr = GDSPActuator._AdaptiveThresholds()
        # Per-territory histories (keyed by frozenset(indices))
        from collections import deque
        self._reward_hist: dict[frozenset, Any] = {}
        self._td_hist: dict[frozenset, Any] = {}
        self._deque = deque  # constructor for deques

        # Budgets for homeostatic repairs
        self._bridge_nodes = int(max(1, int(bridge_budget_nodes)))
        self._bridge_pairs = int(max(1, int(bridge_budget_pairs)))
        self._rng = np.random.default_rng(int(rng_seed))

    # ---------------- Homeostatic repairs ----------------

    def _grow_connection_across_gap(self, substrate: Any) -> Any:
        """
        Bridge a topological gap by adding a single best edge evaluated under strict budgets.
        - Compute connected components once (O(N+E)).
        - Sample up to _bridge_nodes from each of the two largest components.
        - Evaluate up to _bridge_pairs candidate pairs by reading eligibility_traces[u,v].
        """
        try:
            from scipy.sparse.csgraph import connected_components
        except Exception:
            return substrate

        try:
            W = substrate.synaptic_weights
            E = substrate.eligibility_traces
        except Exception:
            return substrate

        n_components, labels = connected_components(csgraph=W, directed=False, connection="weak")
        if n_components <= 1:
            return substrate

        component_ids, counts = np.unique(labels, return_counts=True)
        if len(counts) < 2:
            return substrate
        idx = np.argsort(counts)[-2:]
        comp1_id, comp2_id = component_ids[idx[0]], component_ids[idx[1]]
        comp1_nodes = np.where(labels == comp1_id)[0]
        comp2_nodes = np.where(labels == comp2_id)[0]

        # Sample bounded node sets
        k1 = min(len(comp1_nodes), self._bridge_nodes)
        k2 = min(len(comp2_nodes), self._bridge_nodes)
        if k1 == 0 or k2 == 0:
            return substrate

        try:
            s1_idx = self._rng.choice(len(comp1_nodes), size=k1, replace=False)
            s2_idx = self._rng.choice(len(comp2_nodes), size=k2, replace=False)
            S1 = comp1_nodes[s1_idx]
            S2 = comp2_nodes[s2_idx]
        except Exception:
            S1 = comp1_nodes[:k1]
            S2 = comp2_nodes[:k2]

        # Generate candidate pairs within cap
        pairs: list[tuple[int, int]] = []
        for u in S1:
            for v in S2:
                if len(pairs) >= self._bridge_pairs:
                    break
                pairs.append((int(u), int(v)))
            if len(pairs) >= self._bridge_pairs:
                break

        best_val = None
        best_pair: tuple[int, int] | None = None
        for (u, v) in pairs:
            try:
                if W[u, v] != 0:
                    continue
                val = float(E[u, v])
                if best_val is None or val > best_val:
                    best_val = val
                    best_pair = (u, v)
            except Exception:
                continue

        if best_pair is None:
            return substrate

        uu, vv = best_pair
        try:
            W_lil = W.tolil()
            P_lil = substrate.persistent_synapses.tolil()
            W_lil[uu, vv] = 0.01
            P_lil[uu, vv] = True
            substrate.synaptic_weights = W_lil.tocsr()
            substrate.persistent_synapses = P_lil.tocsr()
        except Exception:
            pass
        return substrate

    @staticmethod
    def _prune_connections_in_locus(substrate: Any, locus_indices: np.ndarray) -> Any:
        if locus_indices is None or len(locus_indices) == 0:
            return substrate
        try:
            locus_mask = np.ix_(locus_indices, locus_indices)
            locus_weights_csr = substrate.synaptic_weights[locus_mask]
            if locus_weights_csr.nnz == 0:
                return substrate
            min_idx = int(np.argmin(np.abs(locus_weights_csr.data)))
            rows, cols = locus_weights_csr.nonzero()
            global_row = int(locus_indices[rows[min_idx]])
            global_col = int(locus_indices[cols[min_idx]])

            W = substrate.synaptic_weights.tolil()
            W[global_row, global_col] = 0
            substrate.synaptic_weights = W.tocsr()
        except Exception:
            pass
        return substrate

    def trigger_homeostatic_repairs(self, substrate: Any, probe_analysis: dict) -> Any:
        comp_cnt = int(probe_analysis.get("component_count", 1))
        # Attempt a single budgeted bridge per tick to bound cost
        if comp_cnt > 1:
            before = int(getattr(substrate.synaptic_weights, "nnz", 0))
            substrate = self._grow_connection_across_gap(substrate)
            after = int(getattr(substrate.synaptic_weights, "nnz", 0))
            # subsequent ticks will try again if still fragmented

        if float(probe_analysis.get("b1_persistence", 0.0)) > 0.9:
            locus = probe_analysis.get("locus_indices")
            if locus is not None:
                substrate = self._prune_connections_in_locus(substrate, locus)
        return substrate

    # ---------------- Performance-based growth ----------------

    def trigger_performance_growth(self, substrate: Any, sie_report: dict, territory_indices: np.ndarray, b1_persistence: float = 0.0) -> Any:
        self._thr.update_and_adapt(sie_report, b1_persistence)

        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        tid = frozenset(int(i) for i in territory_indices)

        if tid not in self._reward_hist:
            self._reward_hist[tid] = self._deque(maxlen=self._thr.sustained_window_size)
        if tid not in self._td_hist:
            self._td_hist[tid] = self._deque(maxlen=self._thr.sustained_window_size)

        self._reward_hist[tid].append(float(sie_report.get("total_reward", 0.0)))
        self._td_hist[tid].append(float(sie_report.get("td_error", 0.0)))
        novelty = float(sie_report.get("novelty", 0.0))

        # Reinforcement growth: strengthen existing connections with high eligibility
        if (len(self._reward_hist[tid]) == self._thr.sustained_window_size and
            all(r > self._thr.reward_threshold for r in self._reward_hist[tid])):
            substrate = self._execute_reinforcement_growth(substrate, territory_indices)
            self._thr.record_structural_activity()
            self._reward_hist[tid].clear()

        # Exploratory growth: persistent high error + novelty
        if (len(self._td_hist[tid]) == self._thr.sustained_window_size and
            all(e > self._thr.td_error_threshold for e in self._td_hist[tid]) and
            novelty > self._thr.novelty_threshold):
            substrate = self._execute_exploratory_growth(substrate, territory_indices)
            self._thr.record_structural_activity()
            self._td_hist[tid].clear()

        return substrate

    @staticmethod
    def _execute_reinforcement_growth(substrate: Any, territory_indices: np.ndarray) -> Any:
        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        try:
            W_lil = substrate.synaptic_weights.tolil()
            E_lil = substrate.eligibility_traces.tolil()

            mask = np.ix_(territory_indices, territory_indices)
            E_sub = E_lil[mask].tocsr()
            if E_sub.nnz > 0:
                thr = float(np.percentile(E_sub.data, 75))
                for r in territory_indices:
                    for c in territory_indices:
                        try:
                            if W_lil[r, c] != 0 and float(E_lil[r, c]) > thr:
                                W_lil[r, c] = float(W_lil[r, c]) * 1.1
                        except Exception:
                            continue
            substrate.synaptic_weights = W_lil.tocsr()
        except Exception:
            pass
        return substrate

    @staticmethod
    def _execute_exploratory_growth(substrate: Any, territory_indices: np.ndarray) -> Any:
        """
        Exploratory growth (budgeted, territory-scoped, sparse-masked):
          - Prefilter external candidates by firing-rate similarity (cheap)
          - Blend with sparse eligibility hint from territory boundary
          - Pick a tiny top-M set and create bidirectional edges (capped)
        """
        if territory_indices is None or len(territory_indices) == 0:
            return substrate
        try:
            num_neurons = int(getattr(substrate.firing_rates, "shape", [0])[0]) if hasattr(substrate, "firing_rates") else 0
            if num_neurons <= len(territory_indices):
                return substrate

            all_neurons = np.arange(num_neurons, dtype=int)
            external = np.setdiff1d(all_neurons, territory_indices)
            if len(external) == 0:
                return substrate

            W_lil = substrate.synaptic_weights.tolil()
            P_lil = substrate.persistent_synapses.tolil()

            # 1) similarity prefilter
            terr_avg = float(np.mean(substrate.firing_rates[territory_indices])) if hasattr(substrate, "firing_rates") else 0.0
            ext_rates = substrate.firing_rates[external] if hasattr(substrate, "firing_rates") else np.zeros_like(external, dtype=float)
            diff = np.abs(ext_rates - terr_avg)

            prefilter_k = min(64, len(external))
            try:
                pf_idx = np.argpartition(diff, prefilter_k - 1)[:prefilter_k]
            except Exception:
                pf_idx = np.argsort(diff)[:prefilter_k]
            prefilter = external[pf_idx]
            diff_pf = diff[pf_idx]

            # 2) eligibility hint from territory boundary
            try:
                E_sub = substrate.eligibility_traces[territory_indices][:, prefilter]
                elig_hint = np.asarray(E_sub.max(axis=0)).ravel()
            except Exception:
                elig_hint = np.zeros_like(prefilter, dtype=float)

            # blend
            sim = 1.0 / (1.0 + diff_pf)
            try:
                emax = float(np.max(np.abs(elig_hint))) if elig_hint.size else 0.0
            except Exception:
                emax = 0.0
            elig_norm = (elig_hint / (emax + 1e-8)) if emax > 0.0 else np.zeros_like(elig_hint, dtype=float)
            score = 0.7 * sim + 0.3 * elig_norm

            # 3) top-M tiny set
            M = min(8, prefilter_k)
            try:
                chosen_idx = np.argpartition(score, -M)[-M:]
            except Exception:
                chosen_idx = np.argsort(score)[-M:]
            compat = prefilter[chosen_idx]

            # 4) add bidirectional edges under caps
            created = 0
            max_new = min(10, len(territory_indices) * max(1, len(compat)) // 4)
            for u in territory_indices[: min(3, len(territory_indices))]:
                for v in compat[: min(2, len(compat))]:
                    if created >= max_new:
                        break
                    try:
                        if W_lil[u, v] == 0:
                            W_lil[u, v] = 0.01
                            P_lil[u, v] = True
                            created += 1
                        if W_lil[v, u] == 0 and created < max_new:
                            W_lil[v, u] = 0.01
                            P_lil[v, u] = True
                            created += 1
                    except Exception:
                        continue

            substrate.synaptic_weights = W_lil.tocsr()
            substrate.persistent_synapses = P_lil.tocsr()
        except Exception:
            pass
        return substrate

    # ---------------- Maintenance pruning ----------------

    @staticmethod
    def trigger_maintenance_pruning(substrate: Any, T_prune: int, pruning_threshold: float = 0.01) -> Any:
        """
        Increment timers for weak, non-persistent synapses and prune when exceeding T_prune.
        """
        try:
            from scipy.sparse import csr_matrix
            W = substrate.synaptic_weights
            timers = substrate.synapse_pruning_timers.copy()
            P = substrate.persistent_synapses

            weak_mask = np.abs(W.data) < float(pruning_threshold)
            strong_mask = ~weak_mask

            persistent_bool = P.astype(bool)
            weak_mat = csr_matrix((weak_mask, W.nonzero()), shape=W.shape)
            eligible = weak_mat - weak_mat.multiply(persistent_bool)
            timers += eligible

            strong_mat = csr_matrix((strong_mask, W.nonzero()), shape=W.shape)
            timers = timers.multiply(strong_mat.astype(bool) == False)

            prune_mask = timers > int(T_prune)
            pruned = prune_mask.nnz
            if pruned > 0:
                W_lil = W.tolil()
                t_lil = timers.tolil()
                rows, cols = prune_mask.nonzero()
                if rows.size:
                    for r, c in zip(rows, cols):
                        try:
                            W_lil[r, c] = 0
                            t_lil[r, c] = 0
                        except Exception:
                            continue
                substrate.synaptic_weights = W_lil.tocsr()
                substrate.synapse_pruning_timers = t_lil.tocsr()
                substrate.synaptic_weights.eliminate_zeros()
            else:
                substrate.synapse_pruning_timers = timers
        except Exception:
            pass
        return substrate

    # ---------------- Orchestration ----------------

    def run(
        self,
        substrate: Any,
        introspection_report: dict | None = None,
        sie_report: dict | None = None,
        territory_indices: np.ndarray | None = None,
        T_prune: int = 100,
        pruning_threshold: float = 0.01,
    ) -> Any:
        b1_persistence = float(introspection_report.get("b1_persistence", 0.0)) if introspection_report else 0.0
        if introspection_report is not None and bool(introspection_report.get("repair_triggered", False)):
            substrate = self.trigger_homeostatic_repairs(substrate, introspection_report)
            self._thr.record_structural_activity()
            return substrate  # highest priority this tick

        if sie_report is not None and territory_indices is not None and len(territory_indices) > 0:
            substrate = self.trigger_performance_growth(substrate, sie_report, territory_indices, b1_persistence)

        substrate = self.trigger_maintenance_pruning(substrate, int(T_prune), float(pruning_threshold))
        return substrate

    @staticmethod
    def status_report(substrate: Any) -> dict:
        try:
            from scipy.sparse.csgraph import connected_components
            n_components, _ = connected_components(substrate.synaptic_weights, directed=False)
        except Exception:
            n_components = 1
        total_syn = int(getattr(substrate.synaptic_weights, "nnz", 0))
        total_neu = int(getattr(getattr(substrate, "firing_rates", None), "shape", [0])[0]) if hasattr(substrate, "firing_rates") else 0
        avg_deg = float(total_syn / total_neu) if total_neu > 0 else 0.0
        pers = int(getattr(substrate.persistent_synapses, "nnz", 0)) if hasattr(substrate, "persistent_synapses") else 0
        ratio = float(pers / total_syn) if total_syn > 0 else 0.0
        data = getattr(substrate.synaptic_weights, "data", np.array([], dtype=float))
        weight_stats = {
            "mean": float(np.mean(data)) if data.size > 0 else 0.0,
            "std": float(np.std(data)) if data.size > 0 else 0.0,
            "min": float(np.min(data)) if data.size > 0 else 0.0,
            "max": float(np.max(data)) if data.size > 0 else 0.0,
        }
        return {
            "total_neurons": int(total_neu),
            "total_synapses": int(total_syn),
            "persistent_synapses": int(pers),
            "persistent_ratio": float(ratio),
            "average_degree": float(avg_deg),
            "connected_components": int(n_components),
            "connectivity_health": "healthy" if n_components == 1 else "fragmented",
            "gdsp_operational": True,
        }


def run_gdsp_synaptic_actuator(
    substrate: Any,
    introspection_report: dict | None = None,
    sie_report: dict | None = None,
    territory_indices: Any | None = None,
    T_prune: int = 100,
    pruning_threshold: float = 0.01,
) -> Any:
    """
    Legacy-compatible wrapper (emergent-only trigger, no schedulers).
    Mirrors older runtime adapters by exposing a function entrypoint.

    Complexity: O(#bounded-ops) per tick (budgeted repairs/growth + pruning).
    """
    try:
        inst = getattr(run_gdsp_synaptic_actuator, "_inst", None)
        if inst is None:
            inst = GDSPActuator()
            setattr(run_gdsp_synaptic_actuator, "_inst", inst)
        return inst.run(
            substrate=substrate,
            introspection_report=introspection_report or {},
            sie_report=sie_report or {},
            territory_indices=territory_indices,
            T_prune=int(T_prune),
            pruning_threshold=float(pruning_threshold),
        )
    except Exception:
        return substrate


def get_gdsp_status_report(substrate: Any) -> dict:
    """
    Legacy-compatible status function.

    Returns a compact operational snapshot (component count, degree, weight stats).
    """
    try:
        return GDSPActuator.status_report(substrate)
    except Exception:
        return {"gdsp_operational": False}
__all__ = ["GDSPActuator"]
]]></content>
    </file>
    <file>
      <path>neuroplasticity/revgsp.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Module: fum_rt.core.neuroplasticity.revgsp
Purpose: Resonance‑Enhanced Valence‑Gated Synaptic Plasticity (REV‑GSP), organism‑native.

Design constraints
- Pure core; no IO/logging; NumPy + SciPy only.
- Budgeted pair sampling (no global candidate sweep).
- CSR‑safe updates for eligibility traces and weights.
"""

from typing import Any, Dict, List, Tuple
import math
import numpy as np


class RevGSP:
    """
    REV‑GSP learner (class form, organism‑native).
    - No IO/logging. Pure numeric state updates on a Substrate‑like object.
    - Accepts any 'substrate' exposing:
        synaptic_weights (CSR), eligibility_traces (CSR), neuron_polarities (ndarray)
    """

    def __init__(
        self,
        reward_sigmoid_scale: float = 1.5,
        pi_params: dict | None = None,
        rng_seed: int | None = None,
        max_pairs: int = 2048,
        sample_spikes_cap: int | None = None,
    ) -> None:
        """
        Parameters:
          - reward_sigmoid_scale: gain for eta_effective sigmoid
          - pi_params: base params for PI kernel (a±, tau±)
          - rng_seed: deterministic sampling for budgets
          - max_pairs: hard cap on pre/post spike candidate evaluations per tick
          - sample_spikes_cap: optional cap on filtered spike list (down-sample before pairing)
        """
        self.reward_sigmoid_scale = float(reward_sigmoid_scale)
        self.pi_params = pi_params or {
            "a_plus_base": 0.1,
            "a_minus_base": 0.1,
            "tau_plus_base": 20.0,
            "tau_minus_base": 20.0,
        }
        self.rng = np.random.default_rng(rng_seed)
        self.max_pairs = int(max(1, int(max_pairs)))
        self.sample_spikes_cap = None if sample_spikes_cap is None else int(max(1, int(sample_spikes_cap)))

    # --- helpers ---
    def _clamped_normal(self, mu: float, sigma: float, lo: float, hi: float) -> float:
        try:
            val = float(self.rng.normal(mu, sigma))
        except Exception:
            val = float(mu)
        if val < lo:
            val = lo
        if val > hi:
            val = hi
        return float(val)

    def _base_pi(self, delta_t: float) -> float:
        # STDP‑like impulse with constrained bio diversity per call
        a_plus = self._clamped_normal(self.pi_params["a_plus_base"], 0.01, 0.03, 0.07)
        a_minus = self._clamped_normal(self.pi_params["a_minus_base"], 0.01, 0.04, 0.08)
        tau_plus = self._clamped_normal(self.pi_params["tau_plus_base"], 2.0, 18.0, 22.0)
        tau_minus = self._clamped_normal(self.pi_params["tau_minus_base"], 2.0, 18.0, 22.0)
        if delta_t > 0:
            return float(a_plus * math.exp(-delta_t / tau_plus))
        return float(-a_minus * math.exp(delta_t / tau_minus))

    def _eta_effective(self, base_lr: float, total_reward: float) -> float:
        """
        Canonical eta_effective(total_reward):
          eta_mag = base_lr * (1 + (2*sigmoid(k*R) - 1))
          eta_eff = eta_mag * sign(R)
        This strictly gates learning by the sign of the global modulatory factor (SIE).
        """
        k = self.reward_sigmoid_scale
        x = k * float(total_reward)
        mod = 2.0 / (1.0 + math.exp(-x)) - 1.0  # 2*sigmoid - 1 in [-1,1]
        eta_mag = float(base_lr) * (1.0 + mod)
        # Explicit sign gate (sign(0)=0): no weight drift when reward ~ 0
        if total_reward > 0.0:
            return float(eta_mag)
        if total_reward < 0.0:
            return float(-eta_mag)
        return 0.0

    @staticmethod
    def _gamma_from_plv(plv: float, base_decay: float = 0.95, sensitivity: float = 0.1) -> float:
        """
        PLV‑gated eligibility trace decay:
            gamma = base_decay + sensitivity*(PLV - 0.5)
        Clamp to [0, 1] for stability under noisy PLV estimates.
        """
        g = float(base_decay + sensitivity * (float(plv) - 0.5))
        if g < 0.0:
            g = 0.0
        if g > 1.0:
            g = 1.0
        return g

    @staticmethod
    def _temporal_filter(spike_times: List[Tuple[int, int]], window_size: int = 5) -> List[Tuple[int, float]]:
        if len(spike_times) < window_size:
            return spike_times
        out: List[Tuple[int, float]] = []
        for i in range(len(spike_times) - window_size + 1):
            window = spike_times[i : i + window_size]
            avg_time = sum(t for _, t in window) / float(window_size)
            neuron_idx = window[-1][0]
            out.append((neuron_idx, avg_time))
        return out

    @staticmethod
    def _adaptive_window(base_ms: int, max_latency: float) -> int:
        return int(base_ms + float(max_latency))

    @staticmethod
    def _latency_scale(pi_value: float, latency_error: float, max_latency: float) -> float:
        if float(max_latency) > 0.0:
            return float(pi_value) * (1.0 - float(latency_error) / float(max_latency))
        return float(pi_value)

    # --- main API ---
    def adapt(
        self,
        substrate: Any,
        spike_train: List[Tuple[int, int]],
        spike_phases: Dict[Tuple[int, int], float],
        learning_rate: float,
        lambda_decay: float,
        total_reward: float,
        plv: float,
        network_latency_estimate: Dict[str, float],
        time_window_ms: int = 20,
    ) -> tuple[Any, dict]:
        """
        Update substrate in‑place using REV‑GSP rule; returns (substrate, metrics).
        Budgeted: samples pairs from recent spikes only; respects max_pairs and sample_spikes_cap.
        """
        try:
            from scipy.sparse import lil_matrix  # local import to avoid hard dependency at import-time
        except Exception:
            # Cannot operate without scipy
            return substrate, {"eta_effective": 0.0, "gamma": 0.0}

        filtered = self._temporal_filter(spike_train)
        win = self._adaptive_window(int(time_window_ms), float(network_latency_estimate.get("max", 0.0)))

        # Optional down‑sample of filtered spikes to respect complexity cap
        if self.sample_spikes_cap is not None and len(filtered) > self.sample_spikes_cap:
            try:
                idx = self.rng.choice(len(filtered), size=self.sample_spikes_cap, replace=False)
                filtered = [filtered[int(i)] for i in idx]
            except Exception:
                filtered = filtered[: self.sample_spikes_cap]

        W = getattr(substrate, "synaptic_weights", None)
        E = getattr(substrate, "eligibility_traces", None)
        P = getattr(substrate, "neuron_polarities", None)
        if W is None or E is None or P is None:
            return substrate, {"eta_effective": 0.0, "gamma": 0.0}

        # Build PI sparsely
        try:
            shape = W.shape
        except Exception:
            shape = (0, 0)
        PI = lil_matrix(shape, dtype=np.float32)

        # Budgeted pair evaluation to ensure sub‑quadratic behavior
        pairs_evaluated = 0
        break_outer = False
        for pre_neuron, pre_time in filtered:
            if break_outer:
                break
            for post_neuron, post_time in filtered:
                if pre_neuron == post_neuron:
                    continue
                try:
                    # Quick existence check (CSR O(1) average)
                    if W[pre_neuron, post_neuron] == 0:
                        continue
                    delta_t = float(post_time) - float(pre_time)
                    if 0.0 < abs(delta_t) < float(win):
                        base_pi = self._base_pi(delta_t)
                        phase_pre = float(spike_phases.get((pre_neuron, int(pre_time)), 0.0))
                        phase_post = float(spike_phases.get((post_neuron, int(post_time)), 0.0))
                        phase_mod = (1.0 + math.cos(phase_pre - phase_post)) * 0.5
                        pi_val = base_pi * phase_mod
                        pi_val = self._latency_scale(pi_val, float(network_latency_estimate.get("error", 0.0)), float(network_latency_estimate.get("max", 0.0)))
                        PI[pre_neuron, post_neuron] += float(pi_val)
                        pairs_evaluated += 1
                        if pairs_evaluated >= self.max_pairs:
                            break_outer = True
                            break
                except Exception:
                    continue

        PI_csr = PI.tocsr()

        # Eligibility update: E = gamma*E + PI
        gamma = self._gamma_from_plv(float(plv))
        try:
            E *= float(gamma)
        except Exception:
            # fallback reconstruct
            E = E.multiply(float(gamma))
        E += PI_csr

        # Row‑scale by neuron polarity (CSR‑friendly)
        try:
            indptr = E.indptr
            data = E.data
            for i in range(E.shape[0]):
                p = float(P[i])
                if p == 1.0:
                    continue
                start = indptr[i]
                end = indptr[i + 1]
                if end > start:
                    data[start:end] *= p
        except Exception:
            pass

        # Three‑factor update
        eta = self._eta_effective(float(learning_rate), float(total_reward))
        try:
            trace_update = E * float(eta)
            decay_update = W * float(lambda_decay)
            dW = trace_update - decay_update
            W += dW
            # clip
            try:
                W.data = np.clip(W.data, -1.0, 1.0)
            except Exception:
                pass
        except Exception:
            pass

        return substrate, {"eta_effective": float(eta), "gamma": float(gamma)}

    # Compatibility wrapper matching the task board signature
    def adapt_connectome(
        self,
        substrate: Any,
        spike_train: List[Tuple[int, int]],
        spike_phases: Dict[Tuple[int, int], float],
        total_reward: float,
        network_latency: Dict[str, float],
        *,
        max_pairs: int | None = None,
        spike_sampling_cap: int | None = None,
        pi_params: dict | None = None,
        lambda_decay: float = 1e-3,
        base_lr: float = 1e-2,
        plv: float | None = None,
        time_window_ms: int = 20,
    ) -> tuple[Any, dict]:
        """
        Thin compatibility wrapper:
        - Allows per‑call override of budgets and PI parameters.
        - Computes eta from total_reward via internal nonlinearity.
        - Uses network_latency['max'|'error'] to adapt the time window and latency scaling.
        - plv defaults to network_latency.get('plv', 0.5) if not provided.
        """
        # Stash old config and override temporarily
        old_mp = self.max_pairs
        old_cap = self.sample_spikes_cap
        old_pi = dict(self.pi_params) if isinstance(self.pi_params, dict) else self.pi_params

        try:
            if max_pairs is not None:
                self.max_pairs = int(max(1, int(max_pairs)))
            if spike_sampling_cap is not None:
                self.sample_spikes_cap = int(max(1, int(spike_sampling_cap)))
            if pi_params is not None:
                self.pi_params = dict(pi_params)

            use_plv = float(plv if plv is not None else float(network_latency.get("plv", 0.5)))
            return self.adapt(
                substrate=substrate,
                spike_train=spike_train,
                spike_phases=spike_phases,
                learning_rate=float(base_lr),
                lambda_decay=float(lambda_decay),
                total_reward=float(total_reward),
                plv=float(use_plv),
                network_latency_estimate=network_latency,
                time_window_ms=int(time_window_ms),
            )
        finally:
            # Restore config
            self.max_pairs = old_mp
            self.sample_spikes_cap = old_cap
            self.pi_params = old_pi


__all__ = ["RevGSP"]]]></content>
    </file>
    <file>
      <path>primitives/README.md</path>
      <content/>
    </file>
    <file>
      <path>primitives/dsu.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Disjoint Set Union (Union-Find) primitive with O(1) component counting
and optional masked counting for rare telemetry.

Void-faithful: event-folded unions only; no global scans in hot path.
Copyright © 2025 Justin K. Lietz, Neuroca, Inc.
"""
from __future__ import annotations

from typing import Optional
import numpy as np


class DSU:
    """
    Path-compressed, union-by-rank disjoint set with size tracking and
    O(1) component counting.

    Hot-path methods:
      - find(x) -> int
      - union(a, b) -> bool            # True if merged (reduced components)
      - same_set(a, b) -> bool
      - count_sets() -> int            # O(1)

    Rare/telemetry method:
      - count_sets(mask: Optional[np.ndarray]) -> int
        When mask is provided, counts sets only across masked indices
        via a local scan of the mask (not for per-tick use).
    """
    __slots__ = ("parent", "rank", "size", "components")

    def __init__(self, n: int):
        n = int(n)
        self.parent = np.arange(n, dtype=np.int32)
        self.rank = np.zeros(n, dtype=np.int8)
        self.size = np.ones(n, dtype=np.int32)
        self.components = n

    def find(self, x: int) -> int:
        p = self.parent
        x = int(x)
        while p[x] != x:
            p[x] = p[p[x]]
            x = p[x]
        return int(x)

    def union(self, a: int, b: int) -> bool:
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return False
        # attach lower-rank to higher-rank
        if self.rank[ra] < self.rank[rb]:
            ra, rb = rb, ra
        self.parent[rb] = ra
        self.size[ra] += self.size[rb]
        if self.rank[ra] == self.rank[rb]:
            self.rank[ra] = self.rank[ra] + 1
        self.components -= 1
        return True

    def grow_to(self, n: int) -> None:
        """
        Grow DSU to cover indices [0, n), preserving existing sets.
        O(n - old_n) initialization; does not scan existing structure.
        """
        n = int(n)
        cur = int(self.parent.size)
        if n <= cur:
            return
        add = n - cur
        self.parent = np.concatenate([self.parent, np.arange(cur, n, dtype=np.int32)])
        self.rank = np.concatenate([self.rank, np.zeros(add, dtype=np.int8)])
        self.size = np.concatenate([self.size, np.ones(add, dtype=np.int32)])
        self.components += add

    def same_set(self, a: int, b: int) -> bool:
        return self.find(a) == self.find(b)
 
    def count_sets(self, mask: Optional[np.ndarray] = None) -> int:
        if mask is None:
            return int(self.components)
        m = np.asarray(mask)
        if m.dtype != np.bool_:
            m = m.astype(bool, copy=False)
        if m.shape[0] != self.parent.size:
            raise ValueError("mask length must equal DSU size")
        idx = np.nonzero(m)[0]
        if idx.size == 0:
            return 0
        roots = set(self.find(int(i)) for i in idx)
        return len(roots)
 
 
 
__all__ = ["DSU"]]]></content>
    </file>
    <file>
      <path>proprioception/README.md</path>
      <content/>
    </file>
    <file>
      <path>proprioception/events.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Event schema and incremental reducers for event-driven metrics (Phase C scaffolding).

Design:
- Pure core module. No imports from fum_rt.io.* or fum_rt.runtime.*.
- Does not perform any I/O or logging. Export numbers only.
- Safe-by-default: Can be instantiated but not wired unless feature flags enable event-driven path.
- O(1) per-event update; no scans over W in the hot path.

Provided:
- Event types: DeltaEvent, VTTouchEvent, EdgeOnEvent, EdgeOffEvent, MotifEnterEvent, MotifExitEvent, ADCEvent
- Incremental reducers:
    - StreamingMeanVar: Welford online (mean/var/std)
    - EWMA: exponential moving average
    - CountMinSketchHead: CMS plus exact head for entropy/coverage approximation
    - UnionFindCohesion: incremental cohesion via union set on edge_on; marks edge_off as dirty
- EventDrivenMetrics: folds events and exposes snapshot() dict of numeric metrics

Integration plan:
- Connectome/walkers publish events on the announce bus (outside core).
- Runtime/orchestrator drains bus and forwards events to EventDrivenMetrics.update().
- Telemetry snapshot reads EventDrivenMetrics.snapshot() and packages 'why'.
- Old scan-based metrics remain the default until flags enable event-driven path.

Caveats:
- Edge_off is marked dirty; a low-cadence auditor is expected to reconcile connectivity.
- VT coverage/entropy are approximate via CMS+head; auditor can validate.
"""

from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple
import math
import random

# Allowed core import
from fum_rt.core.metrics import StreamingZEMA  # existing Z detector in core


# ----------------------------- Event Types -----------------------------------

@dataclass(frozen=True)
class BaseEvent:
    kind: str
    t: Optional[int] = None


@dataclass(frozen=True)
class DeltaEvent(BaseEvent):
    """
    Local structural/learning delta.
    Fields:
      - b1: contribution to B1-like topology signal (float)
      - novelty: novelty component in [0, +inf)
      - hab: habituation component in [0, +inf)
      - td: temporal-difference-like component (float)
      - hsi: homeostatic stability/instability component (float)
    """
    b1: float = 0.0
    novelty: float = 0.0
    hab: float = 0.0
    td: float = 0.0
    hsi: float = 0.0


@dataclass(frozen=True)
class VTTouchEvent(BaseEvent):
    """
    Vocabulary/feature touch (used for VT coverage/entropy approximations).
    Fields:
      - token: hashable token id or string
      - w: optional weight (float, default 1.0)
    """
    token: Any = ""
    w: float = 1.0


@dataclass(frozen=True)
class EdgeOnEvent(BaseEvent):
    u: int = 0
    v: int = 0


@dataclass(frozen=True)
class EdgeOffEvent(BaseEvent):
    u: int = 0
    v: int = 0


# Polarity-aware activity/spike event (void-faithful, event-driven only)
@dataclass(frozen=True)
class SpikeEvent(BaseEvent):
    node: int = 0       # neuron id
    amp: float = 1.0    # activity magnitude (or |ΔW| proxy)
    sign: int = +1      # +1 excitatory, -1 inhibitory, 0 unknown


# Optional signed weight delta event for local learning updates
@dataclass(frozen=True)
class DeltaWEvent(BaseEvent):
    node: int = 0
    dw: float = 0.0


@dataclass(frozen=True)
class MotifEnterEvent(BaseEvent):
    motif_id: int = 0


@dataclass(frozen=True)
class MotifExitEvent(BaseEvent):
    motif_id: int = 0


@dataclass(frozen=True)
class ADCEvent(BaseEvent):
    """
    ADC estimator readout event (fold metrics in place of reading raw structures).
    Suggested fields (all optional, numeric):
      - adc_territories
      - adc_boundaries
      - adc_cycle_hits
    """
    adc_territories: Optional[int] = None
    adc_boundaries: Optional[int] = None
    adc_cycle_hits: Optional[float] = None


# Optional hint for biasing exploration/actuation (not folded by metrics)
@dataclass(frozen=True)
class BiasHintEvent(BaseEvent):
    """
    Hint to bias exploration or actuation to a region/tile for a short TTL.
    - region: free-form label (e.g., "unknown", "tile:3,4")
    - nodes: bounded set of indices to hint (tuple for immutability)
    - ttl:   time-to-live in ticks (downstream consumer-managed)
    Note: EventDrivenMetrics ignores this; it travels on the bus for optional consumers.
    """
    region: str = "unknown"
    nodes: Tuple[int, ...] = tuple()
    ttl: int = 2


# -------------------------- Incremental Primitives ---------------------------

class StreamingMeanVar:
    """
    Welford's algorithm for streaming mean/variance/std.
    """
    __slots__ = ("n", "mean", "M2")

    def __init__(self) -> None:
        self.n = 0
        self.mean = 0.0
        self.M2 = 0.0

    def update(self, x: float) -> None:
        try:
            x = float(x)
        except Exception:
            return
        self.n += 1
        delta = x - self.mean
        self.mean += delta / self.n
        delta2 = x - self.mean
        self.M2 += delta * delta2

    def variance(self) -> float:
        if self.n < 2:
            return 0.0
        return self.M2 / (self.n - 1)

    def std(self, eps: float = 1e-9) -> float:
        v = self.variance()
        if v <= 0.0:
            return eps
        return math.sqrt(v) + eps


class EWMA:
    """
    Exponential Weighted Moving Average.
    """
    __slots__ = ("alpha", "y")

    def __init__(self, alpha: float = 0.05, init: float = 0.0) -> None:
        self.alpha = float(max(0.0, min(1.0, alpha)))
        self.y = float(init)

    def update(self, x: float) -> float:
        x = float(x)
        self.y = self.alpha * x + (1.0 - self.alpha) * self.y
        return self.y

    def value(self) -> float:
        return float(self.y)


class CountMinSketchHead:
    """
    Approximate frequency model for VT coverage/entropy.
    - CMS for tail, plus an exact head map for top-k tokens.
    - Entropy and coverage computed from head counts; tail mass estimated via CMS minimum row sum.

    Note: This is a lightweight approximation; an auditor can reconcile periodically.
    """
    def __init__(self, width: int = 256, depth: int = 3, head_k: int = 256, seed: int = 0) -> None:
        self.w = max(8, int(width))
        self.d = max(1, int(depth))
        self.head_k = max(8, int(head_k))
        rng = random.Random(int(seed))
        self._a = [rng.randrange(1, 2**61 - 1) for _ in range(self.d)]
        self._b = [rng.randrange(0, 2**61 - 1) for _ in range(self.d)]
        self._P = (2**61 - 1)
        self._M = [[0.0 for _ in range(self.w)] for _ in range(self.d)]
        self._head: Dict[Any, float] = {}
        self._total = 0.0

    def _h(self, i: int, key_hash: int) -> int:
        return ((self._a[i] * key_hash + self._b[i]) % self._P) % self.w

    def _hash_key(self, key: Any) -> int:
        # Stable hash across process; for best stability use explicit str
        return hash(str(key))

    def update(self, key: Any, w: float = 1.0) -> None:
        try:
            w = float(w)
        except Exception:
            return
        if w <= 0.0:
            return
        self._total += w
        kh = self._hash_key(key)
        for i in range(self.d):
            j = self._h(i, kh)
            self._M[i][j] += w
        # Update head exact counts; keep only top-K
        cur = self._head.get(key, 0.0) + w
        self._head[key] = cur
        if len(self._head) > self.head_k * 2:
            # prune lower half
            items = sorted(self._head.items(), key=lambda kv: kv[1], reverse=True)[: self.head_k]
            self._head = dict(items)

    def estimate(self, key: Any) -> float:
        if key in self._head:
            return float(self._head[key])
        kh = self._hash_key(key)
        est = min(self._M[i][self._h(i, kh)] for i in range(self.d))
        return float(est)

    def coverage(self) -> float:
        """
        Approximate coverage as fraction of head mass over total.
        """
        if self._total <= 0.0:
            return 0.0
        head_mass = sum(self._head.values())
        return float(max(0.0, min(1.0, head_mass / self._total)))

    def entropy(self, eps: float = 1e-12) -> float:
        """
        Shannon entropy over head distribution (tail ignored), in nats.
        """
        head_mass = sum(self._head.values())
        if head_mass <= 0.0:
            return 0.0
        H = 0.0
        for _, c in self._head.items():
            p = float(c / head_mass)
            if p > 0.0:
                H -= p * math.log(p + eps)
        return float(H)

    def snapshot(self) -> Dict[str, float]:
        return {
            "vt_coverage": self.coverage(),
            "vt_entropy": self.entropy(),
        }


class UnionFindCohesion:
    """
    Incremental cohesion via union-find on edge_on events.
    Edge_off events mark dirty; auditor should reconcile at low cadence.

    Exposes:
      - union(u,v) on edge_on
      - mark_dirty(u,v) on edge_off
      - components() approximate count (dirty edges may increase this transiently)
    """
    def __init__(self, n_hint: int = 0) -> None:
        self.parent: Dict[int, int] = {}
        self.size: Dict[int, int] = {}
        self._dirty = 0  # count of edge_off marks since last audit

    def _find(self, x: int) -> int:
        # path compression
        if self.parent.get(x, x) != x:
            self.parent[x] = self._find(self.parent[x])
        return self.parent.get(x, x)

    def _ensure(self, x: int) -> None:
        if x not in self.parent:
            self.parent[x] = x
            self.size[x] = 1

    def union(self, u: int, v: int) -> None:
        self._ensure(u)
        self._ensure(v)
        ru = self._find(u)
        rv = self._find(v)
        if ru == rv:
            return
        su = self.size.get(ru, 1)
        sv = self.size.get(rv, 1)
        if su < sv:
            ru, rv = rv, ru
            su, sv = sv, su
        self.parent[rv] = ru
        self.size[ru] = su + sv

    def mark_dirty(self, _u: int, _v: int) -> None:
        self._dirty += 1

    def components(self) -> int:
        roots = sum(1 for k, p in self.parent.items() if k == p)
        # naive dirty inflation (auditor should reconcile)
        return int(roots + 0)


# ---------------------------- Event-Driven Metrics ---------------------------

class EventDrivenMetrics:
    """
    O(1) per-event folding of key telemetry metrics.

    Maintained:
      - b1_value, b1_z (via StreamingZEMA)
      - vt_coverage, vt_entropy (via CountMinSketchHead)
      - cohesion_components (via UnionFindCohesion)
      - adc_territories, adc_boundaries (fold ADCEvent)
      - complexity_cycles proxy from ADC (adc_cycle_hits)

    Snapshot includes fields expected by telemetry packagers. Missing fields default to 0.0 or 0.
    """
    def __init__(
        self,
        z_half_life_ticks: int = 50,
        z_spike: float = 1.0,
        hysteresis: float = 1.0,
        vt_width: int = 256,
        vt_depth: int = 3,
        vt_head_k: int = 256,
        seed: int = 0,
    ) -> None:
        self.b1_detector = StreamingZEMA(
            half_life_ticks=int(max(1, z_half_life_ticks)),
            z_spike=float(z_spike),
            hysteresis=float(hysteresis),
            min_interval_ticks=1,
        )
        self._b1_value = 0.0
        self._b1_last: Dict[str, float] = {}
        self._vt = CountMinSketchHead(width=vt_width, depth=vt_depth, head_k=vt_head_k, seed=seed)
        self._cohesion = UnionFindCohesion()
        self._adc_territories = 0
        self._adc_boundaries = 0
        self._adc_cycle_hits = 0.0
        self._tick = 0

    def update(self, ev: BaseEvent) -> None:
        self._tick = int(getattr(ev, "t", self._tick))
        k = getattr(ev, "kind", None)
        if not k:
            return
        if k == "delta":
            dev: DeltaEvent = ev  # type: ignore[assignment]
            # b1_value as additive proxy; alternative mappings can be calibrated
            try:
                self._b1_value += float(dev.b1)
                z = self.b1_detector.update(self._b1_value, tick=self._tick)
                self._b1_last = {
                    "b1_value": float(z.get("value", 0.0)),
                    "b1_delta": float(z.get("delta", 0.0)),
                    "b1_z": float(z.get("z", 0.0)),
                    "b1_spike": bool(z.get("spike", False)),
                }
            except Exception:
                pass
            # SIE components can be folded externally; this class does not compute valence
        elif k == "vt_touch":
            tev: VTTouchEvent = ev  # type: ignore[assignment]
            try:
                self._vt.update(tev.token, w=float(getattr(tev, "w", 1.0)))
            except Exception:
                pass
        elif k == "edge_on":
            e: EdgeOnEvent = ev  # type: ignore[assignment]
            try:
                self._cohesion.union(int(e.u), int(e.v))
            except Exception:
                pass
        elif k == "edge_off":
            e: EdgeOffEvent = ev  # type: ignore[assignment]
            try:
                self._cohesion.mark_dirty(int(e.u), int(e.v))
            except Exception:
                pass
        elif k == "adc":
            a: ADCEvent = ev  # type: ignore[assignment]
            try:
                if a.adc_territories is not None:
                    self._adc_territories = int(a.adc_territories)
                if a.adc_boundaries is not None:
                    self._adc_boundaries = int(a.adc_boundaries)
                if a.adc_cycle_hits is not None:
                    self._adc_cycle_hits = float(a.adc_cycle_hits)
            except Exception:
                pass
        elif k in ("motif_enter", "motif_exit"):
            # Motif events can be used to refine b1_value or cohesion; placeholder noop.
            pass
        else:
            # Unknown event kinds are ignored (forward-compat)
            pass

    def snapshot(self) -> Dict[str, Any]:
        vt = self._vt.snapshot()
        snap = {
            # B1
            "b1_value": float(self._b1_last.get("b1_value", 0.0)),
            "b1_delta": float(self._b1_last.get("b1_delta", 0.0)),
            "b1_z": float(self._b1_last.get("b1_z", 0.0)),
            "b1_spike": bool(self._b1_last.get("b1_spike", False)),
            # VT
            "vt_coverage": float(vt.get("vt_coverage", 0.0)),
            "vt_entropy": float(vt.get("vt_entropy", 0.0)),
            # Cohesion
            "cohesion_components": int(self._cohesion.components()),
            # ADC readouts
            "adc_territories": int(self._adc_territories),
            "adc_boundaries": int(self._adc_boundaries),
            # Cycle proxy feed-through (can be added to complexity_cycles by caller)
            "adc_cycle_hits": float(self._adc_cycle_hits),
        }
        return snap


__all__ = [
    # events
    "BaseEvent",
    "DeltaEvent",
    "VTTouchEvent",
    "EdgeOnEvent",
    "EdgeOffEvent",
    "SpikeEvent",
    "DeltaWEvent",
    "MotifEnterEvent",
    "MotifExitEvent",
    "ADCEvent",
    "BiasHintEvent",
    # reducers
    "StreamingMeanVar",
    "EWMA",
    "CountMinSketchHead",
    "UnionFindCohesion",
    "EventDrivenMetrics",
]]]></content>
    </file>
    <file>
      <path>proprioception/territory.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Void-faithful cohesion territories (incremental, event-folded; no scans).

- Maintains a union-find structure over observed edge_on-like events
  to approximate connected components ("territories").
- Keeps a bounded per-component head (reservoir) of member indices to serve
  territory_indices to actuators (e.g., GDSP) without graph scans.
- O(1) amortized per observation; no reads of W/CSR/adjacency.

API
- fold(observations): consumes Observation-like objects (kind/nodes fields)
  and unions endpoints found in cycle_hit (first two nodes) or edge_on(u,v).
- components_count(): number of current components (approximate).
- sample_indices(component_id, k): returns up to k indices from the requested
  component (component_id can be any node in the component).
- sample_any(k): returns up to k indices across largest components (by UF size).
"""

from __future__ import annotations

from typing import Any, Dict, Iterable, List, Optional, Set


class TerritoryUF:
    def __init__(self, head_k: int = 512) -> None:
        self.parent: Dict[int, int] = {}
        self.size: Dict[int, int] = {}
        # bounded head members per root (kept small; no scans)
        self._head: Dict[int, List[int]] = {}
        self._head_k = max(8, int(head_k))
        self._dirty = 0  # for future auditors

    # ------------- UF core -------------

    def _find(self, x: int) -> int:
        p = self.parent.get(x, x)
        if p != x:
            self.parent[x] = self._find(p)
        return self.parent.get(x, x)

    def _ensure(self, x: int) -> int:
        if x not in self.parent:
            self.parent[x] = x
            self.size[x] = 1
            # seed head with the node itself
            self._head[x] = [x]
        return x

    def _merge_heads(self, r_to: int, r_from: int) -> None:
        """
        Merge bounded heads; keep uniqueness and cap to head_k.
        """
        h_to = self._head.get(r_to, [])
        h_from = self._head.get(r_from, [])
        if not h_from:
            self._head[r_to] = list(dict.fromkeys(h_to))[: self._head_k]
            return
        # Favor r_to contents then supplement with r_from
        merged: List[int] = []
        seen: Set[int] = set()
        for src in (h_to, h_from):
            for n in src:
                if n not in seen:
                    merged.append(n)
                    seen.add(n)
                    if len(merged) >= self._head_k:
                        break
            if len(merged) >= self._head_k:
                break
        self._head[r_to] = merged

    def _add_member(self, r: int, x: int) -> None:
        """
        Add a member to root r's head (bounded); no-ops on duplicates.
        """
        head = self._head.setdefault(r, [])
        if x in head:
            return
        if len(head) < self._head_k:
            head.append(x)
        # else: drop (bounded by design)

    def union(self, u: int, v: int) -> None:
        ru = self._find(self._ensure(u))
        rv = self._find(self._ensure(v))
        if ru == rv:
            return
        su = self.size.get(ru, 1)
        sv = self.size.get(rv, 1)
        # union by size
        if su < sv:
            ru, rv = rv, ru
            su, sv = sv, su
        self.parent[rv] = ru
        self.size[ru] = su + sv
        # merge bounded heads
        self._merge_heads(ru, rv)
        # cleanup from root moved
        try:
            if rv in self._head:
                del self._head[rv]
        except Exception:
            pass

    def mark_dirty(self, _u: int, _v: int) -> None:
        self._dirty += 1

    # ------------- public -------------

    def fold(self, observations: Iterable[Any]) -> None:
        """
        Fold Observation-like objects:
          - cycle_hit: if nodes has ≥ 2, union(nodes[0], nodes[1])
          - edge_on: union(u, v) if fields present
          - edge_off: mark_dirty (no reconciliation here)
        """
        if not observations:
            return
        for obs in observations:
            try:
                k = getattr(obs, "kind", None)
                if not k:
                    continue
                if k == "cycle_hit":
                    nodes = list(getattr(obs, "nodes", []) or [])
                    if len(nodes) >= 2:
                        u, v = int(nodes[0]), int(nodes[1])
                        self.union(u, v)
                        # seed members (bounded) for fast sampling
                        self._add_member(self._find(u), u)
                        self._add_member(self._find(v), v)
                elif k == "edge_on":
                    u = int(getattr(obs, "u", 0))
                    v = int(getattr(obs, "v", 0))
                    self.union(u, v)
                    self._add_member(self._find(u), u)
                    self._add_member(self._find(v), v)
                elif k == "edge_off":
                    u = int(getattr(obs, "u", 0))
                    v = int(getattr(obs, "v", 0))
                    self.mark_dirty(u, v)
            except Exception:
                continue

    def components_count(self) -> int:
        return sum(1 for n, p in self.parent.items() if n == p)

    def sample_indices(self, component_id: int, k: int) -> List[int]:
        """
        Sample up to k indices from the component containing 'component_id'.
        """
        if k <= 0:
            return []
        if component_id not in self.parent:
            return []
        r = self._find(int(component_id))
        head = self._head.get(r, [])
        return head[: int(k)]

    def sample_any(self, k: int) -> List[int]:
        """
        Sample up to k indices across largest components (bounded heads).
        """
        if k <= 0:
            return []
        # sort roots by UF size desc (bounded to number of heads)
        roots = list(self._head.keys())
        roots.sort(key=lambda r: self.size.get(r, len(self._head.get(r, []))), reverse=True)
        out: List[int] = []
        for r in roots:
            if len(out) >= k:
                break
            head = self._head.get(r, [])
            for n in head:
                out.append(n)
                if len(out) >= k:
                    break
        return out]]></content>
    </file>
    <file>
      <path>sie_v2.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
# sie_v2.py
# Void-faithful per-tick intrinsic drive computed directly from W and dW
# Produces a per-neuron reward vector and a smooth scalar valence in [0,1]
from __future__ import annotations
import math
import numpy as np
from dataclasses import dataclass

@dataclass
class SIECfg:
    td_w: float = 0.50
    nov_w: float = 0.20
    hab_w: float = 0.10
    hsi_w: float = 0.20
    half_life_ticks: int = 600     # EMA half-life for habituation stats
    target_var: float = 0.10       # desired variance of |dW|
    reward_clip: float = 1.0
    valence_beta: float = 0.30     # smoothing for scalar valence

class SIEState:
    def __init__(self, N: int, cfg: SIECfg):
        self.cfg = cfg
        self.mu = np.zeros(N, dtype=np.float32)   # EMA mean of |dW|
        self.var = np.zeros(N, dtype=np.float32)  # EMA var of |dW|
        self.prev_W = np.zeros(N, dtype=np.float32)
        self.valence = 0.0

def _ema_update(old: np.ndarray, new: np.ndarray, half_life_ticks: int) -> np.ndarray:
    # Convert half-life to per-tick EMA alpha
    a = 1.0 - math.exp(math.log(0.5) / float(max(1, int(half_life_ticks))))
    return (1.0 - a) * old + a * new

def _novelty_norm(spike_mag: np.ndarray) -> np.ndarray:
    m = float(np.max(spike_mag)) if spike_mag.size else 0.0
    if m <= 1e-12:
        return np.zeros_like(spike_mag, dtype=np.float32)
    return (spike_mag / m).astype(np.float32)

def _hsi_norm(mu: np.ndarray, var: np.ndarray, target_var: float) -> np.ndarray:
    # mean term high when mu ~ 0.5 after implicit normalization (here we use |dW| EMA; bias toward mid-range activity)
    mean_term = 1.0 - np.minimum(1.0, np.abs(mu - 0.5) * 2.0)
    # variance term high when var close to target_var
    tv = max(1e-8, float(target_var))
    var_term = 1.0 - np.minimum(1.0, np.abs(var - tv) / tv)
    return (0.5 * (mean_term + var_term)).astype(np.float32)

def sie_step(state: SIEState, W: np.ndarray, dW: np.ndarray):
    """
    Compute per-neuron reward and smooth scalar valence:
    - novelty from |dW|
    - habituation via EMA(mu,var) of |dW|
    - TD from W - γ·prev_W (γ≈0.99), normalized
    - HSI from closeness of (mu,var) to (0.5, target_var)
    Returns:
        (reward_vec: np.ndarray[float32], valence_01: float)
    """
    cfg = state.cfg
    spikes = np.abs(dW).astype(np.float32)

    # Update habituation statistics (EMA mean/var of |dW|)
    mu_new = _ema_update(state.mu, spikes, cfg.half_life_ticks)
    diff = spikes - mu_new
    var_new = _ema_update(state.var, diff * diff, cfg.half_life_ticks)
    state.mu = mu_new.astype(np.float32)
    state.var = var_new.astype(np.float32)

    nov = _novelty_norm(spikes)
    hab = state.mu

    # TD on field with light discount, normalized by max |td|
    td = (W.astype(np.float32) - 0.99 * state.prev_W.astype(np.float32))
    mtd = float(np.max(np.abs(td))) if td.size else 0.0
    if mtd > 1e-12:
        td = (td / mtd).astype(np.float32)
    else:
        td = np.zeros_like(td, dtype=np.float32)
    state.prev_W = W.astype(np.float32)

    # HSI stability indicator
    stab = _hsi_norm(state.mu, state.var, cfg.target_var)

    # Weighted reward, clipped
    r = (cfg.td_w * td) + (cfg.nov_w * nov) - (cfg.hab_w * hab) + (cfg.hsi_w * stab)
    r = np.clip(r, -cfg.reward_clip, cfg.reward_clip).astype(np.float32)

    # Scalar valence in [0,1], smoothed
    r_bar = float(np.mean(r)) if r.size else 0.0
    v_raw = 0.5 + 0.5 * (r_bar / (cfg.reward_clip + 1e-8))
    state.valence = (1.0 - cfg.valence_beta) * float(state.valence) + cfg.valence_beta * float(v_raw)
    # numerically clip
    state.valence = float(max(0.0, min(1.0, state.valence)))

    return r, state.valence]]></content>
    </file>
    <file>
      <path>signals.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
from __future__ import annotations

"""
Core signals seam (Phase B): stable function-level API for core numeric signals.

Intent:
- Define pure, numeric helpers that outside layers can depend on immediately.
- Initially forward to existing math/state (move-only). No logging/IO/emitters here.
- Safe defaults return 0.0/0 for unavailable signals to preserve behavior.

Rules:
- May import only fum_rt.core.* and numeric libs. Never import fum_rt.io.* or fum_rt.runtime.*.
- These helpers do not mutate external state; they read and derive scalars/dicts.

Migration path:
- Phase C will move incremental/event-driven implementations into core.{cortex,proprioception,neuroplasticity},
  and these wrappers will dispatch to the new implementations while preserving the same signatures.
"""

from typing import Any, Dict, Tuple
from fum_rt.core.metrics import compute_metrics


def _safe_getattr(obj: Any, name: str, default: float = 0.0) -> float:
    try:
        return float(getattr(obj, name))
    except Exception:
        return float(default)


def compute_b1_z(state: Any) -> float:
    """
    Derive b1_z scalar in a behavior-preserving way.

    Priority (non-mutating):
    1) Connectome intrinsic last b1_z if exposed by a detector cache (not guaranteed).
    2) Last computed runtime metrics if available on the 'state' (e.g., Nexus._emit_last_metrics).
    3) Recompute metrics via compute_metrics(connectome) and read 'b1_z' if exposed by runtime stack.
    4) Fallback to 0.0.

    Note: This is a seam; future implementations will obtain b1_z from event-driven reducers in core.
    """
    # 1) connectome-local cache (rare)
    try:
        cz = getattr(getattr(state, "connectome", None), "_last_b1_z", None)
        if cz is not None:
            return float(cz)
    except Exception:
        pass

    # 2) runtime snapshot cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "b1_z" in m:
            return float(m.get("b1_z", 0.0))
    except Exception:
        pass

    # 3) recompute metrics and read b1_z if runtime contributes it
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            return float(m2.get("b1_z", 0.0))
    except Exception:
        pass

    # 4) default
    return 0.0


def sie_valence(state: Any, dstate: Any = None) -> float:
    """
    Derive valence scalar in [0,1] using current prioritized sources:

    Priority:
    1) Connectome intrinsic SIE v2 snapshot (preferred): connectome._last_sie2_valence
    2) Runtime SieEngine legacy valence if exposed via last metrics or engine
    3) compute_metrics(connectome) field: 'sie_v2_valence_01' or 'sie_valence_01'
    4) Fallback 0.0

    This is read-only and does not alter SIE internals.
    """
    # 1) intrinsic v2
    try:
        v2 = getattr(getattr(state, "connectome", None), "_last_sie2_valence", None)
        if v2 is not None:
            return float(v2)
    except Exception:
        pass

    # 2) runtime last metrics cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict):
            if "sie_v2_valence_01" in m:
                return float(m.get("sie_v2_valence_01", 0.0))
            if "sie_valence_01" in m:
                return float(m.get("sie_valence_01", 0.0))
    except Exception:
        pass

    # 3) recompute metrics
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            if "sie_v2_valence_01" in m2:
                return float(m2.get("sie_v2_valence_01", 0.0))
            return float(m2.get("sie_valence_01", 0.0))
    except Exception:
        pass

    return 0.0


def compute_cohesion(state: Any) -> int:
    """
    Compute/derive cohesion_components (approximate number of connected components
    in active subgraph, as defined by the current runtime metrics layer).

    Priority:
    1) Use last metrics cache when present.
    2) Recompute via compute_metrics(connectome).
    3) Fallback 0.
    """
    # 1) cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "cohesion_components" in m:
            return int(m.get("cohesion_components", 0))
    except Exception:
        pass

    # 2) recompute
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            return int(m2.get("cohesion_components", 0))
    except Exception:
        pass

    return 0


def compute_vt_metrics(state: Any) -> Tuple[float, float]:
    """
    Derive (vt_coverage, vt_entropy).

    Priority:
    1) Last metrics cache if present on state
    2) compute_metrics(connectome)
    3) Fallback (0.0, 0.0)
    """
    # 1) cache
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict):
            if "vt_coverage" in m or "vt_entropy" in m:
                cov = float(m.get("vt_coverage", 0.0))
                ent = float(m.get("vt_entropy", 0.0))
                return (cov, ent)
    except Exception:
        pass

    # 2) recompute
    try:
        C = getattr(state, "connectome", None)
        if C is not None:
            m2 = compute_metrics(C)
            cov = float(m2.get("vt_coverage", 0.0))
            ent = float(m2.get("vt_entropy", 0.0))
            return (cov, ent)
    except Exception:
        pass

    # 3) default
    return (0.0, 0.0)


def snapshot_numbers(state: Any) -> Dict[str, float]:
    """
    Convenience aggregator that composes the core snapshot dictionary expected
    by the runtime telemetry seam. Non-intrusive and read-only.

    Returns:
      {
        "b1_z": float, "vt_coverage": float, "vt_entropy": float,
        "cohesion_components": int, "sie_valence_01": float, "sie_v2_valence_01": float
      }
    """
    cov, ent = compute_vt_metrics(state)
    # Gather as many values as are cheaply available
    out: Dict[str, float] = {
        "b1_z": float(compute_b1_z(state)),
        "vt_coverage": float(cov),
        "vt_entropy": float(ent),
        "cohesion_components": float(compute_cohesion(state)),
        "sie_valence_01": 0.0,
        "sie_v2_valence_01": 0.0,
    }
    # attempt to fill valence fields
    try:
        v2 = sie_valence(state)
        out["sie_v2_valence_01"] = float(v2)
    except Exception:
        pass
    try:
        m = getattr(state, "_emit_last_metrics", None)
        if isinstance(m, dict) and "sie_valence_01" in m:
            out["sie_valence_01"] = float(m.get("sie_valence_01", 0.0))
    except Exception:
        pass
    return out


def apply_b1_detector(state: Any, metrics: Dict[str, Any], step: int) -> Dict[str, Any]:
    """
    Behavior-preserving B1 detector update using state.b1_detector.
    Mutates metrics in place; returns metrics for convenience.

    This seam delegates to the existing StreamingZEMA instance configured in runtime (Nexus.b1_detector),
    avoiding any duplication of detector parameters and preserving gating behavior.
    """
    m = metrics if isinstance(metrics, dict) else {}
    try:
        b1_value = float(m.get("complexity_cycles", 0.0))
    except Exception:
        b1_value = 0.0
    try:
        det = getattr(state, "b1_detector", None)
        if det is not None:
            z = det.update(b1_value, tick=int(step))
            m["b1_value"] = float(z.get("value", 0.0))
            m["b1_delta"] = float(z.get("delta", 0.0))
            m["b1_z"] = float(z.get("z", 0.0))
            m["b1_spike"] = bool(z.get("spike", False))
    except Exception:
        # Leave metrics unchanged on failure
        pass
    return m


def compute_active_edge_density(connectome: Any, N: int) -> Tuple[int, float]:
    """
    Compute undirected active-edge density and return (E, density).

    Mirrors Nexus logic (behavior-preserving):
      E = max(0, active_edge_count)
      N = max(1, N)
      density = 2*E / (N*(N-1)) if denom > 0 else 0
    """
    try:
        E = max(0, int(connectome.active_edge_count()))
        Nn = max(1, int(N))
        denom = float(Nn * (Nn - 1))
        density = (2.0 * E / denom) if denom > 0.0 else 0.0
        return int(E), float(density)
    except Exception:
        return 0, 0.0


def compute_td_signal(prev_E: int | None, E: int, vt_prev: float | None = None, vt_last: float | None = None) -> float:
    """
    Compute TD-like signal combining structural change (delta_e) and traversal entropy change (vt_delta).

    Behavior-preserving mapping from Nexus:
      delta_e  = (E - prev_E) / max(1, E)                  # prev_E defaults to E on first use → 0
      vt_delta = 0.0 if missing else (vt_last - vt_prev)
      td_raw   = 4.0*delta_e + 1.5*vt_delta
      td       = clip(td_raw, -2.0,  2.0)
    """
    try:
        E_int = int(E)
        pE = E_int if prev_E is None else int(prev_E)
        delta_e = float(E_int - pE) / float(max(1, E_int))
    except Exception:
        delta_e = 0.0

    try:
        if vt_prev is None or vt_last is None:
            vt_delta = 0.0
        else:
            vt_delta = float(vt_last) - float(vt_prev)
    except Exception:
        vt_delta = 0.0

    td_raw = 4.0 * float(delta_e) + 1.5 * float(vt_delta)
    if td_raw > 2.0:
        return 2.0
    if td_raw < -2.0:
        return -2.0
    return float(td_raw)


def compute_firing_var(connectome: Any) -> float | None:
    """
    Compute variance of the field W; None on failure.
    """
    try:
        return float(connectome.W.var())
    except Exception:
        return None


__all__ = [
    "compute_b1_z",
    "sie_valence",
    "compute_cohesion",
    "compute_vt_metrics",
    "snapshot_numbers",
    "apply_b1_detector",
    "compute_active_edge_density",
    "compute_td_signal",
    "compute_firing_var",
]]]></content>
    </file>
    <file>
      <path>sparse_connectome.py</path>
      <content><![CDATA[# sparse_connectome.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.


Void-faithful sparse connectome for ultra-scale runs.

- Adjacency stored as neighbor lists (list[np.ndarray[int32]]) with symmetric edges
- No dense NxN matrices; all metrics computed by streaming over adjacency
- Traversal and measuring use void equations (Rule: use void equations for traversal/measuring)
- Stage-1 cohesion measured on topology-only adjacency (A_sparse)
- Active subgraph for cycle/entropy uses implicit edge weight W[i]*W[j] > threshold

API mirrors Connectome sufficiently for Nexus and metrics:
    - step(t, domain_modulation, sie_drive=1.0, use_time_dynamics=True)
    - active_edge_count()
    - connected_components()
    - cyclomatic_complexity()
    - snapshot_graph()  (safe for small N)
    - connectome_entropy()  (preferred by metrics if present)

Note: Stage‑1 healing/pruning here omits dense S_ij bridging to avoid NxN;
      bridging logic is already executed upstream in dense Connectome. For sparse,
      we rely on void‑guided rewiring each tick to fuse components (top‑k by S_ij).
"""

from __future__ import annotations
import numpy as np
import networkx as nx
from typing import List, Set
import os as _os
from .void_dynamics_adapter import universal_void_dynamics, delta_re_vgsp, delta_gdsp
from .announce import Observation


from .primitives.dsu import DSU as _DSU


class SparseConnectome:
    def __init__(self, N: int, k: int, seed: int = 0,
                 threshold: float = 0.15, lambda_omega: float = 0.1,
                 candidates: int = 64, structural_mode: str = "alias",
                 traversal_walkers: int = 256, traversal_hops: int = 3,
                 bundle_size: int = 3, prune_factor: float = 0.10):
        self.N = int(N)
        self.k = int(k)
        self.rng = np.random.default_rng(seed)
        self.threshold = float(threshold)
        self.lambda_omega = float(lambda_omega)
        self.candidates = int(max(1, candidates))
        self.structural_mode = structural_mode  # reserved; alias path used by default

        # Node state
        self.W = self.rng.uniform(0.0, 1.0, size=(self.N,)).astype(np.float32)

        # Sparse symmetric topology as neighbor lists (arrays of int32)
        self.adj: List[np.ndarray] = [np.zeros(0, dtype=np.int32) for _ in range(self.N)]

        # Traversal config
        self.traversal_walkers = int(max(1, traversal_walkers))
        self.traversal_hops = int(max(1, traversal_hops))

        # Findings each tick for Global System consumers (SIE/ADC)
        self.findings = {}
        # Homeostasis tuning (mirrors dense backend; currently stored for parity)
        self.bundle_size = int(max(1, bundle_size))
        self.prune_factor = float(max(0.0, prune_factor))
        # Local tick counter for announcement timestamps
        self._tick = 0
        # External stimulation buffer (deterministic symbol→group; decays each tick)
        self._stim = np.zeros(self.N, dtype=np.float32)
        self._stim_decay = 0.90

        # Sparse cohesion bridging budget and degree heterogeneity controls
        try:
            self.bridge_budget = int(_os.getenv("SPARSE_BRIDGE_BUDGET", "24"))
        except Exception:
            self.bridge_budget = 24
        try:
            self.min_k_frac = float(_os.getenv("MIN_K_FRAC", "0.5"))
        except Exception:
            self.min_k_frac = 0.5
        # Active-edge/fragment trackers (incremental; void-faithful)
        self._edges_active = 0
        self._vertices_active = 0
        self._last_edges_active = 0
        self._last_vertices_active = 0
        self._frag_dsu = _DSU(self.N)
        self._frag_components_lb = self.N
        self._frag_dirty_since = None

    # --- Alias sampler (Vose) to sample candidates ~ ReLU(Δalpha) in O(N) build + O(1) draw ---
    def _build_alias(self, p: np.ndarray):
        n = p.size
        if n == 0:
            return np.array([], dtype=np.float32), np.array([], dtype=np.int32)
        p = p.astype(np.float64, copy=False)
        s = float(p.sum())
        if s <= 0:
            p = np.full(n, 1.0 / n, dtype=np.float64)
        else:
            p = p / s
        prob = np.zeros(n, dtype=np.float64)
        alias = np.zeros(n, dtype=np.int32)
        scaled = p * n
        small = [i for i, v in enumerate(scaled) if v < 1.0]
        large = [i for i, v in enumerate(scaled) if v >= 1.0]
        while small and large:
            s_idx = small.pop()
            l_idx = large.pop()
            prob[s_idx] = scaled[s_idx]
            alias[s_idx] = l_idx
            scaled[l_idx] = scaled[l_idx] - (1.0 - prob[s_idx])
            if scaled[l_idx] < 1.0:
                small.append(l_idx)
            else:
                large.append(l_idx)
        for i in large:
            prob[i] = 1.0
        for i in small:
            prob[i] = 1.0
        return prob.astype(np.float32), alias

    def _alias_draw(self, prob: np.ndarray, alias: np.ndarray, s: int):
        n = prob.size
        if n == 0 or s <= 0:
            return np.array([], dtype=np.int64)
        k = self.rng.integers(0, n, size=s, endpoint=False)
        u = self.rng.random(s)
        choose_alias = (u >= prob[k])
        out = k.copy()
        out[choose_alias] = alias[k[choose_alias]]
        return out.astype(np.int64)

    def stimulate_indices(self, idxs, amp: float = 0.05):
        """
        Deterministic stimulus injection for sparse backend:
        - idxs: iterable of neuron indices to stimulate
        - amp: additive boost to the stimulus buffer (decays each tick)
        """
        try:
            if idxs is None:
                return
            arr = np.asarray(list(set(int(i) % self.N for i in idxs)), dtype=np.int64)
            if arr.size == 0:
                return
            self._stim[arr] = np.clip(self._stim[arr] + float(amp), 0.0, 1.0)
            # small immediate bump to W to seed associations
            self.W[arr] = np.clip(self.W[arr] + 0.01 * float(amp), 0.0, 1.0)
        except Exception:
            pass

    def _void_traverse(self, a: np.ndarray, om: np.ndarray):
        """
        Continuous void‑equation traversal on sparse graph (neighbor lists).
        Seeds ~ ReLU(Δalpha). Transition weight to neighbor j: max(0, a[i]*a[j] - λ*|ω_i-ω_j|).

        Also publishes compact Observation events to the ADC bus if present.
        """
        N = self.N
        walkers = self.traversal_walkers
        hops = self.traversal_hops

        prob, alias = self._build_alias(a)
        seeds = self._alias_draw(prob, alias, walkers)
        visit = np.zeros(N, dtype=np.int32)

        # Optional ADC bus and tick
        bus = getattr(self, "bus", None)
        tick = int(getattr(self, "_tick", 0))

        # Event accumulators
        sample_cap = 64
        sel_w_sum = 0.0
        sel_steps = 0
        sample_nodes = set()

        for s in seeds:
            cur = int(s)
            seen = {cur: 0}
            path = [cur]
            for step_idx in range(1, hops + 1):
                nbrs = self.adj[cur]
                if nbrs.size == 0:
                    break
                w = a[cur] * a[nbrs] - self.lambda_omega * np.abs(om[cur] - om[nbrs])
                w = np.clip(w, 0.0, None)
                if np.all(w <= 0):
                    break
                wp = w / (w.sum() + 1e-12)
                r = self.rng.random()
                cdf = np.cumsum(wp)
                idx = int(np.searchsorted(cdf, r, side="right"))
                nxt = int(nbrs[min(idx, nbrs.size - 1)])
                visit[nxt] += 1

                # accumulate simple local stats
                sel_w = float(w[min(idx, nbrs.size - 1)])
                sel_w_sum += max(0.0, sel_w)
                sel_steps += 1
                if len(sample_nodes) < sample_cap:
                    sample_nodes.add(nxt)

                # simple loop detection on this walk
                if nxt in seen:
                    if bus is not None:
                        try:
                            obs = Observation(
                                tick=tick,
                                kind="cycle_hit",
                                nodes=[cur, nxt],
                                w_mean=float(a.mean()),
                                w_var=float(a.var()),
                                s_mean=0.0,
                                loop_len=int(len(path) - seen[nxt] + 1),
                                loop_gain=float(sel_w),
                                coverage_id=0,
                                domain_hint=""
                            )
                            bus.publish(obs)
                        except Exception:
                            pass
                else:
                    seen[nxt] = step_idx
                    path.append(nxt)

                cur = nxt

        total_visits = int(visit.sum())
        unique = int(np.count_nonzero(visit))
        coverage = float(unique) / float(max(1, N))
        if total_visits > 0:
            p = visit.astype(np.float64) / float(total_visits)
            p = p[p > 0]
            vt_entropy = float(-(p * np.log(p)).sum())
        else:
            vt_entropy = 0.0

        self.findings = {
            "vt_visits": total_visits,
            "vt_unique": unique,
            "vt_coverage": coverage,
            "vt_entropy": vt_entropy,
            "vt_walkers": float(walkers),
            "vt_hops": float(hops),
            "a_mean": float(a.mean()),
            "omega_mean": float(om.mean()),
        }

        # publish a compact region_stat
        if bus is not None:
            try:
                s_mean = float(sel_w_sum / max(1, sel_steps))
                cov_id = int(min(9, max(0, int(coverage * 10.0))))
                obs = Observation(
                    tick=tick,
                    kind="region_stat",
                    nodes=list(sample_nodes),
                    w_mean=float(self.W.mean()),
                    w_var=float(self.W.var()),
                    s_mean=s_mean,
                    coverage_id=cov_id,
                    domain_hint=""
                )
                bus.publish(obs)
            except Exception:
                pass

    def step(self, t: float, domain_modulation: float, sie_drive: float = 1.0, use_time_dynamics: bool = True):
        """
        Sparse, void‑faithful tick:
        - Compute Δalpha/Δomega by void equations
        - Build per-node candidate list via alias sampler ~ ReLU(Δalpha)
        - Score candidates by S_ij = ReLU(Δα_i)·ReLU(Δα_j) - λ·|Δω_i - Δω_j|
        - Take symmetric top‑k neighbors (undirected)
        - Update node field with universal_void_dynamics gated by SIE valence
        - Run traversal to publish vt_* findings
        """
        # 1) Elemental deltas from void equations
        d_alpha = delta_re_vgsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        d_omega = delta_gdsp(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        a = np.maximum(0.0, d_alpha.astype(np.float32))
        om = d_omega.astype(np.float32)
        # External stimulation: add and decay deterministic symbol→group drive
        try:
            a = np.clip(a + self._stim, 0.0, None)
            self._stim *= getattr(self, "_stim_decay", 0.90)
        except Exception:
            pass

        # 2) Candidate sampler ~ a
        prob, alias = self._build_alias(a)

        # 3) Per-node top-k selection from candidates by void affinity S_ij
        N = self.N
        k_base = int(max(1, self.k))
        s = int(max(self.candidates, 2 * k_base))
        neigh_sets: List[Set[int]] = [set() for _ in range(N)]
        # Global activity scale for degree heterogeneity
        amax = float(np.max(a)) if a.size else 0.0
        _eps = 1e-12

        for i in range(N):
            js = self._alias_draw(prob, alias, s)
            if js.size == 0:
                continue
            js = js[js != i]
            if js.size == 0:
                continue
            js = np.unique(js)
            Si = a[i] * a[js] - self.lambda_omega * np.abs(om[i] - om[js])

            # Enforce positive void-affinity to avoid forced, non-supported edges (prevents ring-lattice collapse)
            pos_mask = Si > 0.0
            if not np.any(pos_mask):
                continue
            js_pos = js[pos_mask]
            Si_pos = Si[pos_mask]

            # Per-node target degree in [min_k_frac*k_base, k_base], proportional to activity a[i]
            if amax > _eps:
                frac = float(getattr(self, "min_k_frac", 0.5)) + (1.0 - float(getattr(self, "min_k_frac", 0.5))) * float(a[i]) / amax
            else:
                frac = 1.0
            k_i = int(max(1, round(frac * k_base)))

            take = min(k_i, Si_pos.size)
            if take <= 0:
                continue
            idx = np.argpartition(Si_pos, -take)[-take:]
            nbrs = js_pos[idx]
            for j in nbrs:
                neigh_sets[i].add(int(j))

        # Undirected symmetrization
        for i in range(N):
            for j in neigh_sets[i]:
                neigh_sets[j].add(i)

        # Sparse structural maintenance (adaptive pruning, lightweight)
        # Rationale:
        # - In sparse mode, adjacency is rebuilt each tick; to expose real pruning dynamics and
        #   avoid permanent over-connection, we drop edges whose implicit weight |W_i*W_j| is
        #   below prune_factor * mean(|W_i*W_j|) over current edges.
        # - This keeps complexity bounded and allows components to split when pathologies exist.
        try:
            # Collect undirected effective weights for current edges
            weights = []
            for i in range(N):
                wi = float(self.W[i])
                for j in neigh_sets[i]:
                    jj = int(j)
                    if jj <= i:
                        continue
                    weights.append(abs(wi * float(self.W[jj])))

            pruned_pairs = 0
            if weights:
                mean_w = float(np.mean(np.asarray(weights, dtype=np.float64)))
                prune_factor = float(getattr(self, "prune_factor", 0.10))
                prune_threshold = (prune_factor * mean_w) if mean_w > 0.0 else 0.0
                if prune_threshold > 0.0:
                    # Remove edges below adaptive threshold; maintain undirected symmetry
                    for i in range(N):
                        wi = float(self.W[i])
                        # collect first to avoid mutating set during iteration
                        to_remove = []
                        for j in neigh_sets[i]:
                            jj = int(j)
                            if jj <= i:
                                continue
                            wij = abs(wi * float(self.W[jj]))
                            if wij < prune_threshold:
                                to_remove.append(jj)
                        for jj in to_remove:
                            if jj in neigh_sets[i]:
                                neigh_sets[i].remove(jj)
                            if i in neigh_sets[jj]:
                                neigh_sets[jj].remove(i)
                                pruned_pairs += 1
            # Expose pruning stats for diagnostics (undirected pairs)
            try:
                setattr(self, "_last_pruned_count", int(pruned_pairs))
            except Exception:
                pass
            # No bridging in sparse maintenance (keep light); report zero bridged edges
            try:
                setattr(self, "_last_bridged_count", 0)
            except Exception:
                pass
        except Exception:
            # Fail-soft to preserve runtime continuity
            pass

        # --- Sparse cohesion bridging (event-driven, budgeted, no scans) ---
        # Goal: when multiple components exist, propose up to B symmetric bridges using the
        # same void-affinity sampler used for growth. This keeps dynamics lively (cycles/components)
        # without any NxN work. Budget defaults to 8 per tick; can be tuned via instance attribute.
        try:
            # Use frag tracker lower-bound components (active graph), avoid structural scans
            comp_count = int(getattr(self, "_frag_components_lb", 1))
            _dirty = getattr(self, "_frag_dirty_since", None)
            # Optionally early-audit with a small budget to refresh active components
            try:
                _budget = int(_os.getenv("FRAG_AUDIT_EDGES", "200000"))
            except Exception:
                _budget = 200000
            try:
                if _dirty is not None and _budget > 0:
                    # Best-effort refresh of active components (bounded)
                    # This calls an internal audit that streams over up to _budget active edges.
                    self._maybe_audit_frag(int(_budget))
                    comp_count = int(getattr(self, "_frag_components_lb", comp_count))
            except Exception:
                pass
            dsu = getattr(self, "_frag_dsu", _DSU(N))

            bridged_pairs = 0
            if comp_count > 1:
                B = int(getattr(self, "bridge_budget", 8))
                B = max(0, B)
                if B > 0:
                    # Use alias sampler (a) to pick candidate endpoints cheaply
                    attempts = 0
                    max_attempts = int(max(32, B * 64))
                    while bridged_pairs < B and attempts < max_attempts:
                        attempts += 1
                        ui = self._alias_draw(prob, alias, 1)
                        vi = self._alias_draw(prob, alias, 1)
                        if ui.size == 0 or vi.size == 0:
                            continue
                        u = int(ui[0]); v = int(vi[0])
                        if u == v:
                            continue
                        # Skip if already adjacent
                        if (v in neigh_sets[u]) or (u in neigh_sets[v]):
                            continue
                        # Bridge only across distinct components
                        if dsu.find(u) == dsu.find(v):
                            continue
                        # Score by void affinity; require positive support
                        s_uv = float(a[u] * a[v] - self.lambda_omega * abs(om[u] - om[v]))
                        if s_uv <= 0.0:
                            continue
                        # Add symmetric bridge and union components
                        neigh_sets[u].add(v)
                        neigh_sets[v].add(u)
                        dsu.union(u, v)
                        try:
                            self._frag_dsu = dsu
                            if int(getattr(self, "_frag_components_lb", 1)) > 1:
                                self._frag_components_lb = int(self._frag_components_lb) - 1
                            self._frag_dirty_since = None
                        except Exception:
                            pass
                        bridged_pairs += 1
            # Expose bridged count for diagnostics
            try:
                setattr(self, "_last_bridged_count", int(bridged_pairs))
            except Exception:
                pass
        except Exception:
            # Fail-soft for bridging; keep prior counters if present
            try:
                _ = int(getattr(self, "_last_bridged_count", 0))
            except Exception:
                pass

        # Freeze adjacency
        self.adj = [np.fromiter(sorted(s), dtype=np.int32) if s else np.zeros(0, dtype=np.int32) for s in neigh_sets]

        # 4) Node field update via universal void dynamics, gated by SIE valence in [0,1]
        dW = universal_void_dynamics(self.W, t, domain_modulation=domain_modulation, use_time_dynamics=use_time_dynamics)
        gate = float(max(0.0, min(1.0, sie_drive)))
        dW_eff = gate * dW
        self.W = np.clip(self.W + dW_eff, 0.0, 1.0).astype(np.float32)

        # 4.1) SIE v2 intrinsic reward/valence from W and dW (void-native)
        try:
            if not hasattr(self, "_sie2"):
                from .sie_v2 import SIECfg, SIEState
                self._sie2 = SIEState(self.N, SIECfg())
            from .sie_v2 import sie_step
            r_vec, v01 = sie_step(self._sie2, self.W, dW_eff)
            self._last_sie2_reward = float(np.mean(r_vec))
            self._last_sie2_valence = float(v01)
        except Exception:
            pass

        # 4.2) Active-edge counters and frag tracker (void-faithful, streaming)
        try:
            E_new = 0
            _seen = set()
            for (ii, jj) in self._active_edge_iter():
                E_new += 1
                _seen.add(int(ii)); _seen.add(int(jj))
            V_new = int(len(_seen))
            # mark dirty on edge-off (E decreased)
            try:
                if int(getattr(self, "_edges_active", 0)) > int(E_new):
                    if getattr(self, "_frag_dirty_since", None) is None:
                        self._frag_dirty_since = int(getattr(self, "_tick", 0))
            except Exception:
                pass
            self._last_edges_active = int(getattr(self, "_edges_active", 0))
            self._last_vertices_active = int(getattr(self, "_vertices_active", 0))
            self._edges_active = int(E_new)
            self._vertices_active = int(V_new)
            # optional budgeted audit
            try:
                _audit_every = int(_os.getenv("FRAG_AUDIT_EVERY", "50"))
            except Exception:
                _audit_every = 50
            try:
                _budget = int(_os.getenv("FRAG_AUDIT_EDGES", "200000"))
            except Exception:
                _budget = 200000
            if (getattr(self, "_frag_dirty_since", None) is not None or (int(getattr(self, "_tick", 0)) % max(1, _audit_every) == 0)) and _budget > 0:
                self._maybe_audit_frag(int(_budget))
            # cycles estimate
            try:
                comp_lb = int(getattr(self, "_frag_components_lb", 1))
            except Exception:
                comp_lb = 1
            cycles_est = int(max(0, int(E_new) - int(V_new) + int(comp_lb)))
            # augment findings
            try:
                self.findings.update({
                    "edges_active": int(E_new),
                    "vertices_active": int(V_new),
                    "components_lb": int(comp_lb),
                    "frag_dirty_age": int((int(getattr(self, "_tick", 0)) - int(getattr(self, "_frag_dirty_since", 0)))) if getattr(self, "_frag_dirty_since", None) is not None else 0,
                    "cycles_est": int(cycles_est),
                })
            except Exception:
                pass
        except Exception:
            pass

        # 5) Continuous traversal to emit vt_* findings
        try:
            self._void_traverse(a, om)
        except Exception:
            pass

        # increment local tick for announcement timestamps
        try:
            self._tick += 1
        except Exception:
            pass

    def _active_edge_iter(self):
        """Yield undirected edges (i, j) with i < j whose implicit weight is active."""
        th = self.threshold
        W = self.W
        for i in range(self.N):
            wi = float(W[i])
            nbrs = self.adj[i]
            if nbrs.size == 0:
                continue
            for j in nbrs:
                j = int(j)
                if j <= i:
                    continue
                if (wi * float(W[j])) > th:
                    yield (i, j)

    def _maybe_audit_frag(self, budget_edges: int) -> None:
        """
        Budgeted active-fragment audit (void-faithful):
        - Rebuild a DSU over ACTIVE edges only, streaming via _active_edge_iter().
        - Only processes up to 'budget_edges' edges; computes a lower-bound component count
          across the 'seen' active vertices.
        - Clears the dirty flag only when processing completes before budget exhaust.
        """
        try:
            N = int(self.N)
            dsu = _DSU(N)
            seen: set[int] = set()
            processed = 0
            b = int(max(0, int(budget_edges)))
            for (i, j) in self._active_edge_iter():
                ii = int(i); jj = int(j)
                dsu.union(ii, jj)
                seen.add(ii); seen.add(jj)
                processed += 1
                if b > 0 and processed >= b:
                    break
            if seen:
                roots = set(int(dsu.find(idx)) for idx in seen)
                comp_lb = int(len(roots))
            else:
                # No active vertices observed → treat as fully fragmented across N nodes
                comp_lb = N
            # Update trackers
            self._frag_dsu = dsu
            self._frag_components_lb = int(comp_lb)
            # Clear dirty flag only if we did not exhaust budget
            if not (b > 0 and processed >= b):
                self._frag_dirty_since = None
        except Exception:
            # Fail-soft: keep previous DSU/lower-bound
            pass

    def active_edge_count(self) -> int:
        return sum(1 for _ in self._active_edge_iter())

    def connected_components(self) -> int:
        """Active-subgraph components (Stage‑1 cohesion) over active edges only."""
        dsu = _DSU(self.N)
        e_active = 0
        act_nodes = set()
        for (i, j) in self._active_edge_iter():
            dsu.union(i, j)
            e_active += 1
            act_nodes.add(int(i))
            act_nodes.add(int(j))
        return (len(set(int(dsu.find(idx)) for idx in act_nodes)) if e_active > 0 else self.N)

    def cyclomatic_complexity(self) -> int:
        """
        Active-subgraph cyclomatic complexity: cycles = E_active - N + C_active
        where unions are formed only by active edges (W[i]*W[j] > threshold).
        """
        dsu = _DSU(self.N)
        e_active = 0
        act_nodes = set()
        for (i, j) in self._active_edge_iter():
            dsu.union(i, j)
            e_active += 1
            act_nodes.add(int(i))
            act_nodes.add(int(j))
        c_active = (len(set(int(dsu.find(idx)) for idx in act_nodes)) if e_active > 0 else self.N)
        cycles = e_active - self.N + c_active
        return int(max(0, cycles))

    def snapshot_graph(self):
        """
        Build a NetworkX graph of the active subgraph for visualization.
        Guarded for scale: returns empty graph if N is large.
        """
        if self.N > 5000:
            return nx.Graph()
        G = nx.Graph()
        G.add_nodes_from(range(self.N))
        for (i, j) in self._active_edge_iter():
            G.add_edge(int(i), int(j))
        return G

    def get_phi(self, i: int) -> float:
        """O(1) local read of fast field φ at node i; returns 0.0 when absent."""
        try:
            phi = getattr(self, "phi", None)
            if phi is None:
                return 0.0
            return float(phi[int(i)])
        except Exception:
            return 0.0

    def get_memory(self, i: int) -> float:
        """O(1) local read of slow memory m at node i via attached field/map when present."""
        # Prefer an attached MemoryField
        try:
            mf = getattr(self, "_memory_field", None)
            if mf is not None and hasattr(mf, "get_m"):
                return float(mf.get_m(int(i)))
        except Exception:
            pass
        # Fallback to attached MemoryMap adapter
        try:
            mm = getattr(self, "_memory_map", None)
            if mm is not None:
                fld = getattr(mm, "field", None)
                if fld is not None and hasattr(fld, "get_m"):
                    return float(fld.get_m(int(i)))
                dct = getattr(mm, "_m", None)
                if isinstance(dct, dict):
                    return float(dct.get(int(i), 0.0))
        except Exception:
            pass
        return 0.0

    def connectome_entropy(self) -> float:
        """
        Global pathological structure metric on the active subgraph.
        H = -Σ p_i log p_i where p_i proportional to degree(i) in active subgraph.
        """
        deg = np.zeros(self.N, dtype=np.int64)
        for i in range(self.N):
            wi = float(self.W[i])
            cnt = 0
            for j in self.adj[i]:
                j = int(j)
                if (wi * float(self.W[j])) > self.threshold:
                    cnt += 1
            deg[i] = cnt
        total = int(deg.sum())
        if total <= 0:
            return 0.0
        p = deg.astype(np.float64) / float(total)
        p = np.clip(p, 1e-12, 1.0)
        return float(-(p * np.log(p)).sum())]]></content>
    </file>
    <file>
      <path>substrate/README.md</path>
      <content/>
    </file>
    <file>
      <path>substrate/growth_arbiter.py</path>
      <content><![CDATA[# fum_rt/core/substrate/growth_arbiter.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module contains the GrowthArbiter, a class responsible for deciding
when and *how much* to grow the network. It implements the "super saturation" 
and "void debt" principles, where growth is triggered by stability and the 
amount of growth is determined by accumulated system pressure.
"""
import numpy as np
from collections import deque

class GrowthArbiter:
    """
    Monitors network metrics to decide when and how much to grow.
    """
    def __init__(self, stability_window=10, trend_threshold=0.001, debt_growth_factor=0.1):
        """
        Initializes the GrowthArbiter.

        Args:
            stability_window (int): Number of recent steps to check for stability.
            trend_threshold (float): Max change for a metric to be "flat."
            debt_growth_factor(float): Scales accumulated debt to number of neurons.
        """
        self.stability_window = stability_window
        self.trend_threshold = trend_threshold
        self.debt_growth_factor = debt_growth_factor

        self.weight_history = deque(maxlen=stability_window)
        self.synapse_history = deque(maxlen=stability_window)
        self.complexity_history = deque(maxlen=stability_window)
        self.cohesion_history = deque(maxlen=stability_window)
        
        self.is_stable = False
        self.void_debt_accumulator = 0.0

    def update_metrics(self, metrics):
        """
        Updates the historical metrics and checks for system stability.
        
        Args:
            metrics (dict): A dictionary containing the latest network metrics.
        """
        self.weight_history.append(metrics.get('avg_weight', 0))
        self.synapse_history.append(metrics.get('active_synapses', 0))
        self.complexity_history.append(metrics.get('total_b1_persistence', 0))
        self.cohesion_history.append(metrics.get('cluster_count', -1))

        if len(self.weight_history) < self.stability_window:
            self.is_stable = False
            return

        is_cohesive = all(count == 1 for count in self.cohesion_history)
        is_weight_flat = abs(self.weight_history[0] - self.weight_history[-1]) < self.trend_threshold
        is_synapse_flat = abs(self.synapse_history[0] - self.synapse_history[-1]) < 3
        is_complexity_flat = abs(self.complexity_history[0] - self.complexity_history[-1]) < self.trend_threshold

        if is_cohesive and is_weight_flat and is_synapse_flat and is_complexity_flat:
            if not self.is_stable:
                print("\n--- GROWTH ARBITER: System has achieved STABILITY ---")
                print("--- Now accumulating 'void debt' from residual valence. ---")
            self.is_stable = True
        else:
            if self.is_stable:
                print("\n--- GROWTH ARBITER: System has left stable state. Resetting debt.---")
                self.void_debt_accumulator = 0.0 # Reset debt if stability is lost
            self.is_stable = False

    def accumulate_and_check_growth(self, valence_signal):
        """
        If the system is stable, accumulates void debt. If the debt crosses
        a threshold, returns the number of neurons to grow.

        Args:
            valence_signal (float): The residual system pressure signal.

        Returns:
            int: The number of new neurons to add, or 0.
        """
        if not self.is_stable:
            return 0

        self.void_debt_accumulator += abs(valence_signal) # Accumulate pressure

        # Check if the accumulated debt triggers growth
        # We'll use a simple linear threshold for now.
        if self.void_debt_accumulator > 1.0: 
            num_new_neurons = int(np.ceil(self.void_debt_accumulator * self.debt_growth_factor))
            
            print("\n--- GROWTH ARBITER: VOID DEBT THRESHOLD REACHED ---")
            print(f"Accumulated Debt: {self.void_debt_accumulator:.3f}")
            print(f"Triggering organic growth of {num_new_neurons} new neuron(s).")
            print("---------------------------------------------------\n")

            self.void_debt_accumulator = 0.0 # Reset debt after triggering
            self.is_stable = False # System will become unstable after growth, reset
            self.clear_history() # Clear history to re-evaluate stability
            return num_new_neurons

        return 0

    def clear_history(self):
        """Resets all metric histories."""
        self.weight_history.clear()
        self.synapse_history.clear()
        self.complexity_history.clear()
        self.cohesion_history.clear()]]></content>
    </file>
    <file>
      <path>substrate/neurogenesis.py</path>
      <content><![CDATA[# fum_rt/core/substrate/neurogenesis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This module handles the growth of the FUM substrate, including the addition
of new computational units (neurons) and the formation of their initial
connections based on void dynamics.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix

from Void_Equations import universal_void_dynamics

class Neurogenesis:
    """
    Manages the growth of the Substrate's connectome.
    """
    def __init__(self, seed=42):
        """
        Initializes the growth manager.
        """
        self.rng = np.random.default_rng(seed=seed)

    def grow(self, substrate, num_new_neurons):
        """
        Grows the substrate by expanding the connectome and all associated state arrays.
        This method is designed to be called on a Substrate instance.

        Args:
            substrate: The Substrate instance to modify.
            num_new_neurons (int): The number of new neurons to add.
        """
        if num_new_neurons <= 0:
            return

        old_n = substrate.num_neurons
        new_n = old_n + num_new_neurons
        
        print(f"\n--- NEUROGENESIS: SUBSTRATE GROWTH ---")
        print(f"Adding {num_new_neurons} new neurons to the existing {old_n}.")

        # --- Create new neuron properties on CPU first ---
        new_is_excitatory = self.rng.choice([True, False], num_new_neurons, p=[0.8, 0.2])
        new_tau_m = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_thresh = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_new_neurons)
        new_v_rest = np.full(num_new_neurons, -65.0)
        new_refractory_period = np.full(num_new_neurons, 5.0)
        new_r_mem = np.full(num_new_neurons, 10.0)

        # --- Expand the connectome based on the backend ---
        if substrate.device_type == 'gpu':
            W_cpu = substrate.W.cpu().numpy()
        else:
            W_cpu = substrate.W.toarray() if isinstance(substrate.W, csc_matrix) else substrate.W

        new_W = np.zeros((new_n, new_n))
        new_W[:old_n, :old_n] = W_cpu

        # --- Connect new neurons using Void Dynamics ---
        # 1. Create a potential connection matrix for new neurons (outgoing)
        potential_connections_out = self.rng.random((num_new_neurons, old_n)) * 0.05 
        # 2. Evolve it with void dynamics
        delta_out = universal_void_dynamics(potential_connections_out, substrate.time_step)
        evolved_connections_out = potential_connections_out + delta_out
        # 3. Threshold to form actual connections
        new_connections_out = np.where(evolved_connections_out > 0.01, evolved_connections_out, 0)
        
        # 1. Create a potential connection matrix for new neurons (incoming)
        potential_connections_in = self.rng.random((old_n, num_new_neurons)) * 0.05
        # 2. Evolve it
        delta_in = universal_void_dynamics(potential_connections_in, substrate.time_step)
        evolved_connections_in = potential_connections_in + delta_in
        # 3. Threshold
        new_connections_in = np.where(evolved_connections_in > 0.01, evolved_connections_in, 0)

        # Add new connections to the main matrix
        new_W[old_n:new_n, :old_n] = new_connections_out
        new_W[:old_n, old_n:new_n] = new_connections_in
        
        # --- Handle backend-specific state expansions ---
        if substrate.device_type == 'gpu':
            substrate.W = torch.from_numpy(new_W).float().to(substrate.device)
            substrate.is_excitatory = torch.cat([substrate.is_excitatory, torch.from_numpy(new_is_excitatory).to(substrate.device)])
            substrate.tau_m = torch.cat([substrate.tau_m, torch.from_numpy(new_tau_m).float().to(substrate.device)])
            substrate.v_thresh = torch.cat([substrate.v_thresh, torch.from_numpy(new_v_thresh).float().to(substrate.device)])
            substrate.v_m = torch.cat([substrate.v_m, torch.from_numpy(new_v_rest).float().to(substrate.device)])
            substrate.refractory_time = torch.cat([substrate.refractory_time, torch.zeros(num_new_neurons, device=substrate.device)])
            substrate.refractory_period = torch.cat([substrate.refractory_period, torch.from_numpy(new_refractory_period).float().to(substrate.device)])
            substrate.r_mem = torch.cat([substrate.r_mem, torch.from_numpy(new_r_mem).float().to(substrate.device)])
            substrate.v_reset_tensor = torch.cat([substrate.v_reset_tensor, torch.from_numpy(np.full(num_new_neurons, -70.0)).float().to(substrate.device)])
            substrate.spikes = torch.cat([substrate.spikes, torch.zeros(num_new_neurons, dtype=torch.bool, device=substrate.device)])
        else: # CPU
            substrate.W = csc_matrix(new_W)
            substrate.is_excitatory = np.concatenate([substrate.is_excitatory, new_is_excitatory])
            substrate.tau_m = np.concatenate([substrate.tau_m, new_tau_m])
            substrate.v_thresh = np.concatenate([substrate.v_thresh, new_v_thresh])
            substrate.v_m = np.concatenate([substrate.v_m, new_v_rest])
            substrate.refractory_time = np.concatenate([substrate.refractory_time, np.zeros(num_new_neurons)])
            substrate.refractory_period = np.concatenate([substrate.refractory_period, new_refractory_period])
            substrate.r_mem = np.concatenate([substrate.r_mem, new_r_mem])
            substrate.v_reset = np.concatenate([substrate.v_reset, np.full(num_new_neurons, -70.0)])
            substrate.spikes = np.concatenate([substrate.spikes, np.zeros(num_new_neurons, dtype=bool)])
            substrate.neuron_polarities = np.concatenate([substrate.neuron_polarities, np.ones(num_new_neurons)])
            substrate.refractory_periods = np.concatenate([substrate.refractory_periods, np.zeros(num_new_neurons)])

        # Universal state expansions
        substrate.spike_times.extend([[] for _ in range(num_new_neurons)])
        substrate.num_neurons = new_n

        print(f"Growth complete. Total neurons: {substrate.num_neurons}")
        print("-------------------------------------\n")]]></content>
    </file>
    <file>
      <path>substrate/structural_homeostasis.py</path>
      <content><![CDATA[# fum_rt/core/substrate/structural_homeostasis.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
from scipy.sparse import csc_matrix, lil_matrix

def perform_structural_homeostasis(W: csc_matrix, ccc_metrics: dict) -> csc_matrix:
    """
    Performs Structural Homeostasis on the Emergent Connectome (UKG).
    
    This is a purpose-driven, self-regulating process that uses TDA metrics
    (Cohesion and Complexity) to maintain the network's topological health,
    ensuring the FUM remains in a stable and efficient state.

    Args:
        W (csc_matrix): The current sparse weight matrix representing the UKG.
        ccc_metrics (dict): A dictionary of metrics from the CCC_Module.

    Returns:
        csc_matrix: The modified, healthier weight matrix.
    """
    num_neurons = W.shape[0]
    
    # To avoid performance warnings, all structural modifications (pruning and
    # growth) are performed on a `lil_matrix`, which is efficient for
    # changing sparsity structure.
    W_lil = W.tolil()
    
    # --- 1. Pruning (Complexity Homeostasis) ---
    # The pruning threshold is now adaptive, based on the current mean weight.
    # This prevents the network from getting stuck and allows for dynamic rearrangement.
    # We prune any synapse that is less than 10% of the mean strength.
    if W.nnz > 0:
        mean_weight = np.mean(np.abs(W.data))
        pruning_threshold = 0.1 * mean_weight
    else:
        pruning_threshold = 0.01 # Fallback for empty graph

    rows_cols = W_lil.rows
    data_rows = W_lil.data
    for i in range(num_neurons):
        to_prune_indices = [
            idx for idx, weight in enumerate(data_rows[i])
            if abs(weight) < pruning_threshold
        ]
        for idx in sorted(to_prune_indices, reverse=True):
            del rows_cols[i][idx]
            del data_rows[i][idx]

    # --- 2. Growth (Cohesion Homeostasis) ---
    component_count = ccc_metrics.get('cohesion_cluster_count', 1)
    if isinstance(component_count, np.integer):
        component_count = component_count.item()

    if component_count > 1 and 'cluster_labels' in ccc_metrics:
        # A "pathological" state of low cohesion has been detected. The system
        # implements the documented strategy of "biasing plasticity towards
        # growing connections" to heal the fragmentation.
        labels = ccc_metrics['cluster_labels']
        unique_labels = np.unique(labels)
        
        # To make the healing effective, we create a "bundle" of new connections
        # to ensure the clustering algorithm recognizes the new bridge.
        BUNDLE_SIZE = 3
        
        # We'll build one bridge for each excess cluster to encourage fusion.
        num_bridges_to_build = component_count - 1

        for _ in range(num_bridges_to_build):
            # Choose two different territories to bridge
            cluster_a, cluster_b = np.random.choice(unique_labels, 2, replace=False)
            
            indices_a = np.where(labels == cluster_a)[0]
            indices_b = np.where(labels == cluster_b)[0]
            
            if len(indices_a) > 0 and len(indices_b) > 0:
                # Create a bundle of connections between the two territories
                for _ in range(BUNDLE_SIZE):
                    neuron_u = np.random.choice(indices_a)
                    neuron_v = np.random.choice(indices_b)
                    if neuron_u != neuron_v and W_lil[neuron_u, neuron_v] == 0:
                        W_lil[neuron_u, neuron_v] = np.random.uniform(0.05, 0.1)
    
    # Convert back to csc_matrix once at the end for efficient calculations
    W_csc = W_lil.tocsc()
    W_csc.prune()
    return W_csc]]></content>
    </file>
    <file>
      <path>substrate/substrate.py</path>
      <content><![CDATA[# fum_rt/core/substrate/substrate.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz. 
See LICENSE file for full terms.
"""
import numpy as np
import torch
from scipy.sparse import csc_matrix, find

# FUM Modules
from fum_initialization import create_knn_graph

class Substrate:
    """
    Represents the FUM's computational medium or "Substrate".
    
    VERSION 4 (MERGED): This version combines the stable V3 architecture with
    the GPU acceleration and Growth features from the V9 refactor.
    """
    def __init__(self, num_neurons: int, k: int, device: str = 'auto'):
        """
        Initializes the Substrate.

        Args:
            num_neurons (int): The number of Computational Units (CUs).
            k (int): The number of nearest neighbors for the initial k-NN graph.
            device (str): 'auto', 'gpu', or 'cpu'.
        """
        self.num_neurons = num_neurons
        self._setup_device(device)
        self.backend = np if self.device_type == 'cpu' else torch
        
        # --- Neuron Types (80% Excitatory, 20% Inhibitory) ---
        self.rng = np.random.default_rng(seed=42)
        is_excitatory_np = self.rng.choice([True, False], num_neurons, p=[0.8, 0.2])

        # --- CU parameters (vectorized, created on CPU first) ---
        tau_m_np = self.rng.normal(loc=20.0, scale=np.sqrt(2.0), size=num_neurons)
        self.v_rest = np.full(num_neurons, -65.0)
        v_reset_np = np.full(num_neurons, -70.0)
        v_thresh_np = self.rng.normal(loc=-55.0, scale=np.sqrt(2.0), size=num_neurons)
        refractory_period_np = np.full(num_neurons, 5.0)
        r_mem_np = np.full(num_neurons, 10.0)

        # --- Parameters for Intrinsic Plasticity (A.6) ---
        self.ip_target_rate_min = 0.1 # Hz
        self.ip_target_rate_max = 0.5 # Hz
        self.ip_v_thresh_adjustment = 0.1 # mV
        self.ip_tau_m_adjustment = 0.1 # ms
        self.ip_v_thresh_bounds = (-60.0, -50.0)
        self.ip_tau_m_bounds = (15.0, 25.0)
        
        # --- Synaptic Pathways: k-NN Initialization (on CPU first) ---
        W_np = create_knn_graph(num_neurons, k, is_excitatory_np).toarray()
        
        # --- Create state vars and move to GPU if requested ---
        if self.device_type == 'gpu':
            self.is_excitatory = torch.from_numpy(is_excitatory_np).to(self.device)
            self.tau_m = torch.from_numpy(tau_m_np).float().to(self.device)
            self.v_thresh = torch.from_numpy(v_thresh_np).float().to(self.device)
            self.v_m = torch.from_numpy(self.v_rest).float().to(self.device)
            self.refractory_time = torch.zeros(num_neurons, device=self.device)
            self.refractory_period = torch.from_numpy(refractory_period_np).float().to(self.device)
            self.r_mem = torch.from_numpy(r_mem_np).float().to(self.device)
            self.v_reset_tensor = torch.from_numpy(v_reset_np).float().to(self.device)
            self.spikes = torch.zeros(num_neurons, dtype=torch.bool, device=self.device)
            self.W = torch.from_numpy(W_np).float().to(self.device)
        else: # cpu
            self.is_excitatory = is_excitatory_np
            self.tau_m = tau_m_np
            self.v_reset = v_reset_np
            self.v_thresh = v_thresh_np
            self.refractory_period = refractory_period_np
            self.r_mem = r_mem_np
            self.v_m = np.full(num_neurons, self.v_rest)
            self.refractory_time = np.zeros(num_neurons)
            self.neuron_polarities = np.ones(num_neurons)
            self.refractory_periods = np.zeros(num_neurons)
            self.W = csc_matrix(W_np)
            self.spikes = np.zeros(num_neurons, dtype=bool)
        self.spike_times = [[] for _ in range(num_neurons)]
        self.time_step = 0

    def run_step(self, external_currents, dt=1.0):
        """
        Runs one full step of the Substrate's dynamics, dispatching to the correct backend.
        """
        if self.device_type == 'gpu':
            self._run_step_gpu(external_currents, dt)
        else:
            self._run_step_cpu(external_currents, dt)
        
        self.time_step += 1

    def _run_step_cpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the CPU.
        """
        # Correctly apply membrane resistance only to synaptic currents inside the dv calculation
        synaptic_currents = self.W.dot(self.spikes.astype(np.float32))
        
        not_in_refractory = self.refractory_time <= 0
        
        # The full, correct ELIF update equation from the documentation
        dv = (
            -(self.v_m[not_in_refractory] - self.v_rest[not_in_refractory])
            + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory]
            + external_currents[not_in_refractory]
        ) / self.tau_m[not_in_refractory]
        
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt

        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = np.where(spiking_mask)[0]
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def _run_step_gpu(self, external_currents, dt):
        """
        Runs one full, vectorized step of the Substrate's dynamics on the GPU.
        """
        external_currents_gpu = torch.from_numpy(external_currents).float().to(self.device)
        synaptic_currents = torch.mv(self.W, self.spikes.float())
        
        not_in_refractory = self.refractory_time <= 0
        v_rest_gpu = torch.from_numpy(self.v_rest).float().to(self.device)
        
        dv = (-(self.v_m[not_in_refractory] - v_rest_gpu[not_in_refractory]) + self.r_mem[not_in_refractory] * synaptic_currents[not_in_refractory] + external_currents_gpu[not_in_refractory]) / self.tau_m[not_in_refractory]
        self.v_m[not_in_refractory] += dv * dt
        
        self.refractory_time -= dt
        self.refractory_time.clamp_(min=0)
        
        spiking_mask = self.v_m >= self.v_thresh
        self.spikes = spiking_mask
        
        self.v_m[spiking_mask] = self.v_reset_tensor[spiking_mask]
        self.refractory_time[spiking_mask] = self.refractory_period[spiking_mask]
        
        spiking_indices = torch.where(spiking_mask)[0].cpu().numpy()
        current_time = self.time_step * dt
        for i in spiking_indices:
            self.spike_times[i].append(current_time)

    def apply_intrinsic_plasticity(self, window_ms=50, dt=1.0):
        """
        Applies intrinsic plasticity to neuron parameters based on their recent
        firing rate, as per documentation section A.6.
        """
        window_steps = int(window_ms / dt)
        analysis_start_time = max(0, (self.time_step - window_steps) * dt)
        window_duration_s = (self.time_step * dt - analysis_start_time) / 1000.0

        if window_duration_s == 0:
            return

        for i in range(self.num_neurons):
            spikes_in_window = [t for t in self.spike_times[i] if t >= analysis_start_time]
            rate_hz = len(spikes_in_window) / window_duration_s
            
            # Adjust v_thresh
            if rate_hz > self.ip_target_rate_max:
                self.v_thresh[i] += self.ip_v_thresh_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.v_thresh[i] -= self.ip_v_thresh_adjustment
                
            # Adjust tau_m
            if rate_hz > self.ip_target_rate_max:
                self.tau_m[i] -= self.ip_tau_m_adjustment
            elif rate_hz < self.ip_target_rate_min:
                self.tau_m[i] += self.ip_tau_m_adjustment

        # Clamp parameters to their bounds
        np.clip(self.v_thresh, self.ip_v_thresh_bounds[0], self.ip_v_thresh_bounds[1], out=self.v_thresh)
        np.clip(self.tau_m, self.ip_tau_m_bounds[0], self.ip_tau_m_bounds[1], out=self.tau_m)

    def apply_synaptic_scaling(self, target_sum=1.0):
        """
        Applies simple multiplicative scaling to incoming excitatory weights to
        keep the total input around a target value. Based on the reference
        validation script and documentation B.7.ii.
        """
        W_dense = self.W.toarray()
        
        # Calculate sum of incoming positive (excitatory) weights for each neuron
        incoming_exc_sums = np.sum(np.maximum(W_dense, 0), axis=0)
        
        # Avoid division by zero
        incoming_exc_sums[incoming_exc_sums < 1e-6] = 1.0
        
        # Calculate scaling factors needed to bring sum to target
        scale_factors = target_sum / incoming_exc_sums
        
        # Get a dense matrix of the excitatory weights only
        exc_W_dense = W_dense.copy()
        exc_W_dense[W_dense < 0] = 0
        
        # Apply scaling multiplicatively to the excitatory weights
        scaled_exc_W = exc_W_dense * scale_factors[np.newaxis, :]
        
        # Reconstruct the full weight matrix
        W_dense[W_dense > 0] = scaled_exc_W[W_dense > 0]
        
        self.W = csc_matrix(W_dense)
        self.W.prune()

    def _setup_device(self, device_preference):
        if device_preference == 'gpu':
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                print("Warning: GPU requested but not available. Falling back to CPU.")
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
        elif device_preference == 'cpu':
            self.device = torch.device("cpu")
            self.device_type = 'cpu'
            print("--- Substrate configured to run on CPU. ---")
        else: # auto
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                self.device_type = 'gpu'
                print("--- Substrate configured to run on GPU. ---")
            else:
                self.device = torch.device("cpu")
                self.device_type = 'cpu'
                print("--- Substrate configured to run on CPU. ---")]]></content>
    </file>
    <file>
      <path>tests/README.md</path>
      <content/>
    </file>
    <file>
      <path>tests/test_conservation_flux.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles.

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""
import os
import time
import json
from pathlib import Path

# Enable dense connectome for validation-only import (won't modify runtime files)
os.environ["FORCE_DENSE"] = "1"

import numpy as np

# Import the real Connectome implementation (dense/validation mode)
from fum_rt.core.connectome import Connectome


def Q_invariant(r: float, u: float, W: np.ndarray, t: float) -> np.ndarray:
    """Vectorized Q invariant from qfum_validate: ln|W| - ln|r - u W| - r t
    Numerically guarded to avoid division by zero.
    """
    eps = 1e-16
    W = np.asarray(W, dtype=np.float64)
    denom = (r - u * W)
    denom = np.where(np.abs(denom) < eps, np.copysign(eps, denom), denom)
    W_safe = np.where(np.abs(W) < eps, np.copysign(eps, W), W)
    return np.log(np.abs(W_safe)) - np.log(np.abs(denom)) - r * float(t)


def test_sum_Q_delta_records(tmp_path: Path) -> None:
    """Run one dense Connectome.step on a small network, compute Δ(sum_i Q_i), and log results.

    This test is non-invasive: it sets FORCE_DENSE to allow importing the validation-only
    Connectome and does not modify any existing project source files.
    """
    # Small network for fast validation
    N = 32
    k = 4
    seed = 42

    # Physical mapping used in Q validator (documented in repo)
    r = 0.15
    u = 0.25
    t = 0.0

    # Construct connectome in dense/validation mode
    conn = Connectome(N=N, k=k, seed=seed)

    # Snapshot initial node states
    W0 = conn.W.astype(np.float64).copy()
    Q0 = Q_invariant(r, u, W0, t)

    # Execute one update tick using the real runtime mapping
    conn.step(t=t, domain_modulation=1.0, sie_drive=1.0, use_time_dynamics=True)

    # Snapshot after-step node states
    W1 = conn.W.astype(np.float64).copy()
    Q1 = Q_invariant(r, u, W1, t)

    # Aggregate diagnostics
    delta_vec = (Q1 - Q0)
    delta_sum = float(np.sum(delta_vec))

    payload = {
        "timestamp": int(time.time()),
        "N": int(N),
        "k": int(k),
        "seed": int(seed),
        "r": float(r),
        "u": float(u),
        "t": float(t),
        "delta_sum_Q": delta_sum,
        "delta_max_abs": float(np.max(np.abs(delta_vec))),
        "W0_mean": float(np.mean(W0)),
        "W1_mean": float(np.mean(W1)),
        "W0_min": float(np.min(W0)),
        "W1_max": float(np.max(W1)),
    }

    # Write JSON log to derivation outputs (matches qfum_validate layout)
    # Determine repo root from this file's path
    repo_root = Path(__file__).resolve().parents[4]
    out_dir = repo_root / "derivation" / "code" / "outputs" / "logs" / "conservation_law"
    out_dir.mkdir(parents=True, exist_ok=True)
    fname = out_dir / f"flux_test_{int(time.time())}.json"
    with open(fname, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    # Pass if we successfully wrote the log and computed the residual (non-invasive observation)
    assert fname.exists()
]]></content>
    </file>
    <file>
      <path>text_utils.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import re, random, time
from collections import Counter

# Minimal stopword list; purely for compact summaries at the I/O boundary.
STOP = set(
    """
    the a an and or for with into of to from in on at by is are was were be been being
    it this that as if then than so thus such not no nor but over under up down out
    you your yours me my mine we our ours they their theirs he him his she her hers
    i am do does did done have has had will would can could should shall may might
    """.split()
)

def summarize_keywords(text: str, k: int = 4) -> str:
    """
    Extract a compact, lowercased keyword summary from recent text.
    Deterministic and lightweight; used only for composing human-readable
    context in UTD macros. Core remains void-native.
    """
    if not text:
        return ""
    words = [w.lower() for w in re.findall(r"[A-Za-z][A-Za-z0-9_+\-]*", text)]
    words = [w for w in words if w not in STOP and len(w) > 2]
    if not words:
        return ""
    top_words = [w for w, _ in Counter(words).most_common(k)]
    return ", ".join(top_words)

def tokenize_text(text: str):
    """Capture any sequence of non-whitespace characters."""
    return [w.lower() for w in re.findall(r"\S+", str(text))]

def update_ngrams(tokens, ng2, ng3):
    """Update streaming n-gram models (bigram/trigram)."""
    toks = [t for t in tokens if t]
    n = len(toks)
    for i in range(n - 1):
        a, b = toks[i], toks[i+1]
        d = ng2.setdefault(a, {})
        d[b] = d.get(b, 0) + 1
    for i in range(n - 2):
        key = (toks[i], toks[i+1])
        c = toks[i+2]
        d = ng3.setdefault(key, {})
        d[c] = d.get(c, 0) + 1

def generate_emergent_sentence(lexicon: dict, ng2: dict, ng3: dict, seed=None, seed_tokens: set = None):
    """Assemble a sentence from a lexicon and learned n-grams, optionally seeded from recent tokens."""
    if not lexicon:
        return ""
    
    # 1. Determine candidate pool for start word
    if seed_tokens:
        candidates = {k: v for k, v in lexicon.items() if k in seed_tokens}
        if candidates:
            items = list(candidates.items())
        else:
            items = list(lexicon.items())
    else:
        items = list(lexicon.items())
    if not items:
        return ""
    
    rnd = random.Random(seed if seed is not None else int(time.time() * 1000))
    
    # 2. Weighted start token draw from the candidate pool
    weights = [max(1, int(cnt)) for _, cnt in items]
    total = sum(weights)
    r = rnd.uniform(0, total)
    acc = 0.0
    start = items[0][0]
    for tok, cnt in items:
        acc += max(1, int(cnt))
        if acc >= r:
            start = tok
            break
    
    words = [start]
    # Markov walk using trigram then bigram
    while True:
        nxt = None
        if len(words) >= 2:
            key = (words[-2], words[-1])
            d = ng3.get(key)
            if d:
                total_c = sum(d.values())
                r = rnd.uniform(0, total_c)
                s = 0.0
                for tok, c in d.items():
                    s += c
                    if s >= r:
                        nxt = tok
                        break
        if nxt is None:
            d2 = ng2.get(words[-1], {})
            if d2:
                total_c = sum(d2.values())
                r = rnd.uniform(0, total_c)
                s = 0.0
                for tok, c in d2.items():
                    s += c
                    if s >= r:
                        nxt = tok
                        break
        if nxt is None:
            break
        words.append(nxt)
    
    sent = " ".join(words).strip()
    if sent:
        sent = sent[0].upper() + sent[1:]
        if not sent.endswith((".", "!", "?")):
            sent += "."
    return sent]]></content>
    </file>
    <file>
      <path>visualizer.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import os

class Visualizer:
    def __init__(self, run_dir: str):
        self.run_dir = run_dir

    def dashboard(self, history):
        os.makedirs(self.run_dir, exist_ok=True)
        try:
            import matplotlib.pyplot as plt
        except Exception:
            # Matplotlib not available; skip rendering without crashing the runtime
            return None
        fig, axs = plt.subplots(2, 2, figsize=(12, 9))
        t = [h['t'] for h in history]

        axs[0,0].plot(t, [h['active_synapses'] for h in history])
        axs[0,0].set_title('UKG Sparsity Over Time')
        axs[0,0].set_xlabel('Tick')
        axs[0,0].set_ylabel('Active Synapses')

        axs[0,1].plot(t, [h['avg_weight'] for h in history], marker='o', linestyle='-')
        axs[0,1].set_title('Average Synaptic Weight Over Time')
        axs[0,1].set_xlabel('Tick')
        axs[0,1].set_ylabel('Average Weight')

        axs[1,0].plot(t, [h['cohesion_components'] for h in history])
        axs[1,0].set_title('UKG Cohesion (Component Count)')
        axs[1,0].set_xlabel('Tick')
        axs[1,0].set_ylabel('Components')

        axs[1,1].plot(t, [h['complexity_cycles'] for h in history])
        axs[1,1].set_title('UKG Complexity (Cycles)')
        axs[1,1].set_xlabel('Tick')
        axs[1,1].set_ylabel('Cycles')

        fig.suptitle('FUM Performance Dashboard', fontsize=14)
        fig.tight_layout()
        # Overlay Control URL (Load Engram) if available
        try:
            import json as _json
            ctrl_url = None
            try:
                with open(os.path.join(self.run_dir, 'control.json'), 'r', encoding='utf-8') as _fh:
                    _ctrl = _json.load(_fh)
                    if isinstance(_ctrl, dict):
                        ctrl_url = _ctrl.get('url')
            except Exception:
                ctrl_url = None
            if ctrl_url:
                try:
                    import matplotlib.pyplot as _plt
                    _plt.subplots_adjust(bottom=0.16)
                except Exception:
                    pass
                fig.text(0.01, 0.01, f'Controls: {ctrl_url} - Load Engram', fontsize=9, color='#8b949e')
        except Exception:
            pass

        path = os.path.join(self.run_dir, 'dashboard.png')
        fig.savefig(path, dpi=150)
        plt.close(fig)
        return path

    def graph(self, G, fname='connectome.png'):
        os.makedirs(self.run_dir, exist_ok=True)
        try:
            import matplotlib.pyplot as plt
            import networkx as nx
        except Exception:
            # Missing viz deps; skip without crashing
            return None
        fig = plt.figure(figsize=(8,8))
        pos = nx.spring_layout(G, seed=42, dim=2)
        nx.draw_networkx_nodes(G, pos, node_size=10)
        nx.draw_networkx_edges(G, pos, alpha=0.3)
        fig.suptitle('Foundational UKG Structure', fontsize=14)
        path = os.path.join(self.run_dir, fname)
        fig.savefig(path, dpi=150)
        plt.close(fig)
        return path
]]></content>
    </file>
    <file>
      <path>void_b1.py</path>
      <content><![CDATA[# void_b1.py
"""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. Commercial use requires written permission from Justin K. Lietz.
See LICENSE file for full terms.

Void-faithful, streaming B1 surrogate and Euler-rank estimate.

Design goals
- No dense NxN; use adjacency lists or the connectome's active-edge iterator
- O(E_active) for sparse; sampling to keep per-tick cost bounded
- Outputs:
  - void_b1: [0,1] scalar capturing cyclic structure density with smoothing
  - euler_rank: E_active - V_active + C_active (graph-level Betti-1 proxy)
  - triangles_per_edge: local triangle frequency around sampled active edges
  - active_node_ratio: V_active / N
  - active_edges_est: estimated or exact count of active undirected edges

Notes
- For SparseConnectome we rely on connectome._active_edge_iter() to enumerate
  active edges without scanning non-active neighbors.
- For dense Connectome this module only executes for small N where mask ops
  are acceptable; large-N runs auto-sparse in Nexus.
"""

from __future__ import annotations
import math
from typing import Iterable, List, Tuple, Dict, Any, Optional

import numpy as np
from .primitives.dsu import DSU as _DSU


def _count_intersection_sorted(a: np.ndarray, b: np.ndarray) -> int:
    """
    Count |a ∩ b| given two ascending-sorted int arrays.
    """
    i = j = 0
    na, nb = a.size, b.size
    cnt = 0
    while i < na and j < nb:
        ai = int(a[i]); bj = int(b[j])
        if ai == bj:
            cnt += 1
            i += 1; j += 1
        elif ai < bj:
            i += 1
        else:
            j += 1
    return cnt


def _alpha_from_half_life(half_life_ticks: int) -> float:
    hl = max(1, int(half_life_ticks))
    return 1.0 - math.exp(math.log(0.5) / float(hl))


class _Reservoir:
    """
    Fixed-size reservoir sampler for a stream of unknown-length items.
    Items are small tuples (i,j).
    """
    def __init__(self, k: int, rng: np.random.Generator):
        self.k = int(max(1, k))
        self.rng = rng
        self.buf: List[Tuple[int, int]] = []
        self.seen = 0

    def push(self, item: Tuple[int, int]):
        self.seen += 1
        if len(self.buf) < self.k:
            self.buf.append(item)
            return
        # Replace with probability k/seen
        if self.rng.random() < (float(self.k) / float(self.seen)):
            idx = int(self.rng.integers(0, self.k))
            self.buf[idx] = item

    def items(self) -> List[Tuple[int, int]]:
        return self.buf

    def count(self) -> int:
        return self.seen


class VoidB1Meter:
    """
    Streaming surrogate for B1 with Euler-rank estimate.

    - Maintains EMA of b1_raw to produce void_b1 in [0,1]
    - Computes euler_rank = E_active - V_active + C_active (cycles count)
    - Estimates triangles_per_edge over a bounded reservoir of active edges

    Parameters
    - sample_edges: maximum number of active edges sampled per tick
    - half_life_ticks: EMA half-life for void_b1 smoothing
    """
    def __init__(self, sample_edges: int = 4096, half_life_ticks: int = 50):
        self.sample_edges = int(max(32, sample_edges))
        self.alpha = _alpha_from_half_life(half_life_ticks)
        self._ema_b1: Optional[float] = None

    # ---------------- Sparse path (preferred) ----------------

    def _update_sparse(
        self,
        adj: List[np.ndarray],
        W: np.ndarray,
        threshold: float,
        rng: np.random.Generator,
        active_edge_iter: Iterable[Tuple[int, int]],
        N: int,
    ) -> Dict[str, Any]:
        """
        O(E_active) with bounded sampling for triangles.
        """
        N = int(N)
        W = np.asarray(W, dtype=np.float32)
        th = float(threshold)

        # Reservoir over active edges; avoid global scans by building DSU over active vertices only
        res = _Reservoir(self.sample_edges, rng)
        E_active = 0

        # Active-vertex DSU keyed by local contiguous ids (no O(N) scans)
        dsu = _DSU(0)
        idmap: Dict[int, int] = {}
        local_n = 0

        for (i, j) in active_edge_iter:
            i = int(i); j = int(j)
            # Active edge guaranteed by iterator contract
            E_active += 1
            ii = idmap.get(i)
            if ii is None:
                ii = local_n
                idmap[i] = ii
                dsu.grow_to(local_n + 1)
                local_n += 1
            jj = idmap.get(j)
            if jj is None:
                jj = local_n
                idmap[j] = jj
                dsu.grow_to(local_n + 1)
                local_n += 1
            dsu.union(ii, jj)
            res.push((i, j))

        V_active = int(local_n)
        if E_active == 0:
            C_active = N  # no active edges: consider each node isolated
        else:
            C_active = int(getattr(dsu, "components", dsu.count_sets()))

        # Triangles-per-edge over the reservoir
        tri = 0
        m = len(res.items())
        if m > 0:
            for (i, j) in res.items():
                ai = adj[i]; aj = adj[j]
                # Intersect active neighbors only: W[i]*W[k] > th and W[j]*W[k] > th
                # Fast path: build filtered lists then intersect
                if ai.size == 0 or aj.size == 0:
                    continue
                # Filter by active threshold
                ai_act = ai[(W[i] * W[ai]) > th]
                if ai_act.size == 0:
                    continue
                aj_act = aj[(W[j] * W[aj]) > th]
                if aj_act.size == 0:
                    continue
                tri += _count_intersection_sorted(ai_act, aj_act)

        triangles_per_edge = (float(tri) / float(m)) if m > 0 else 0.0

        # Cycles (Euler-rank for graphs)
        cycles = max(0, int(E_active - V_active + C_active))

        # Node activity ratio
        active_node_ratio = (float(V_active) / float(max(1, N)))

        return {
            "E_active": int(E_active),
            "V_active": int(V_active),
            "C_active": int(C_active),
            "cycles": int(cycles),
            "triangles_per_edge": float(triangles_per_edge),
            "active_node_ratio": float(active_node_ratio),
            "reservoir_seen": int(res.count()),
            "reservoir_used": int(m),
        }

    # ---------------- Dense path (small-N only) ----------------

    def _update_dense(
        self,
        A: np.ndarray,
        E: np.ndarray,
        W: np.ndarray,
        threshold: float,
        rng: np.random.Generator,
    ) -> Dict[str, Any]:
        """
        Small-N fallback using masks; cost is acceptable only for validation runs.
        """
        N = int(A.shape[0])
        th = float(threshold)

        # Active mask, undirected, upper triangle to avoid double counting
        mask = (E > th) & (A == 1)
        # Degrees and active vertices
        deg = mask.sum(axis=1).astype(np.int64)
        V_active = int((deg > 0).sum())

        # Active edge list (upper triangle)
        iu, ju = np.where(np.triu(mask, k=1))
        edges = np.stack([iu, ju], axis=1).astype(np.int32, copy=False)
        E_active = int(edges.shape[0])

        # DSU for active components (restricted to active vertices)
        parent = np.arange(N, dtype=np.int32)
        rank = np.zeros(N, dtype=np.int8)

        def find(x: int) -> int:
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a: int, b: int):
            ra, rb = find(a), find(b)
            if ra == rb:
                return
            if rank[ra] < rank[rb]:
                parent[ra] = rb
            elif rank[rb] < rank[ra]:
                parent[rb] = ra
            else:
                parent[rb] = ra
                rank[ra] = rank[ra] + 1

        for i, j in edges:
            union(int(i), int(j))

        if E_active == 0:
            C_active = N
        else:
            act_idx = np.nonzero(deg > 0)[0]
            roots = set(int(find(int(idx))) for idx in act_idx)
            C_active = len(roots)

        # Reservoir over edges
        k = min(self.sample_edges, E_active)
        triangles_per_edge = 0.0
        if k > 0:
            sel = rng.choice(E_active, size=k, replace=False)
            sel_edges = edges[sel]
            # Build neighbor lists once (sorted)
            # For small N, extracting sorted neighbor arrays is fine
            nbrs = [np.where(A[i] > 0)[0].astype(np.int32) for i in range(N)]
            # Intersect with active condition using threshold
            tri = 0
            for (i, j) in sel_edges:
                ai = nbrs[int(i)]; aj = nbrs[int(j)]
                if ai.size == 0 or aj.size == 0:
                    continue
                ai_act = ai[(W[int(i)] * W[ai]) > th]
                if ai_act.size == 0:
                    continue
                aj_act = aj[(W[int(j)] * W[aj]) > th]
                if aj_act.size == 0:
                    continue
                tri += _count_intersection_sorted(ai_act, aj_act)
            triangles_per_edge = float(tri) / float(k)

        cycles = max(0, int(E_active - V_active + C_active))
        active_node_ratio = float(V_active) / float(max(1, N))

        return {
            "E_active": int(E_active),
            "V_active": int(V_active),
            "C_active": int(C_active),
            "cycles": int(cycles),
            "triangles_per_edge": float(triangles_per_edge),
            "active_node_ratio": float(active_node_ratio),
            "reservoir_seen": int(E_active),
            "reservoir_used": int(k),
        }

    # ---------------- Public API ----------------

    def update(self, connectome) -> Dict[str, Any]:
        """
        Compute a void-faithful topology packet and return:
        {
          'void_b1': [0,1],
          'euler_rank': int,
          'cycles': int,
          'triangles_per_edge': float,
          'active_node_ratio': float,
          'active_edges_est': int
        }

        The method chooses sparse vs dense automatically.
        """
        rng = getattr(connectome, "rng", np.random.default_rng(0))

        if hasattr(connectome, "_active_edge_iter"):
            # Sparse path
            adj = getattr(connectome, "adj", None)
            if adj is None:
                raise RuntimeError("Sparse path requires 'adj' on connectome")
            W = np.asarray(connectome.W, dtype=np.float32)
            th = float(getattr(connectome, "threshold", 0.0))
            N = int(getattr(connectome, "N", W.shape[0]))
            pkt = self._update_sparse(
                adj=adj,
                W=W,
                threshold=th,
                rng=rng,
                active_edge_iter=getattr(connectome, "_active_edge_iter")(),
                N=N,
            )
        else:
            # Dense path
            A = np.asarray(connectome.A, dtype=np.int8)
            E = np.asarray(connectome.E, dtype=np.float32)
            W = np.asarray(connectome.W, dtype=np.float32)
            th = float(getattr(connectome, "threshold", 0.0))
            pkt = self._update_dense(A=A, E=E, W=W, threshold=th, rng=rng)

        # Compose a normalized void_b1 score with EMA smoothing.
        # Mix cycles density and triangles per edge; both emphasize local cyclic structure.
        E_act = max(1, int(pkt["E_active"]))
        cycles_density = float(pkt["cycles"]) / float(E_act)  # [0, +]
        # Heuristic normalization: triangles_per_edge is usually small (0..few)
        tri_norm = min(1.0, float(pkt["triangles_per_edge"]) / 4.0)

        b1_raw = 0.6 * cycles_density + 0.4 * tri_norm
        b1_raw = max(0.0, min(1.0, b1_raw))  # clamp to [0,1]

        if self._ema_b1 is None:
            self._ema_b1 = b1_raw
        else:
            a = self.alpha
            self._ema_b1 = (1.0 - a) * float(self._ema_b1) + a * float(b1_raw)

        out = {
            "void_b1": float(self._ema_b1),
            "euler_rank": int(pkt["cycles"]),  # Graph Euler-rank equals cycles for 1D complexes
            "cycles": int(pkt["cycles"]),
            "triangles_per_edge": float(pkt["triangles_per_edge"]),
            "active_node_ratio": float(pkt["active_node_ratio"]),
            "active_edges_est": int(pkt["E_active"]),
            "active_vertices_est": int(pkt["V_active"]),
            "active_components_est": int(pkt["C_active"]),
            "reservoir_seen": int(pkt["reservoir_seen"]),
            "reservoir_used": int(pkt["reservoir_used"]),
        }
        return out


# Convenience singleton for quick integration
_GLOBAL_B1_METER: Optional[VoidB1Meter] = None


def update_void_b1(connectome, sample_edges: int = 4096, half_life_ticks: int = 50) -> Dict[str, Any]:
    """
    Module-level helper to update and return the topology packet.
    Lazily initializes a process-local meter.
    """
    global _GLOBAL_B1_METER
    if _GLOBAL_B1_METER is None:
        _GLOBAL_B1_METER = VoidB1Meter(sample_edges=sample_edges, half_life_ticks=half_life_ticks)
    return _GLOBAL_B1_METER.update(connectome)]]></content>
    </file>
    <file>
      <path>void_dynamics_adapter.py</path>
      <content><![CDATA["""
Copyright © 2025 Justin K. Lietz, Neuroca, Inc. All Rights Reserved.

This research is protected under a dual-license to foster open academic
research while ensuring commercial applications are aligned with the project's ethical principles. 

Commercial use of proprietary VDM code requires written permission from Justin K. Lietz.
See LICENSE file for full terms.
"""

import numpy as np

# Try to import user's libraries first
try:
    # Import only the elemental deltas from the user's equations.
    # We compose universal_void_dynamics locally to guarantee growth+decay are combined.
    from Void_Equations import delta_re_vgsp, delta_gdsp, get_universal_constants
    HAVE_EXTERNAL = True
except Exception:
    HAVE_EXTERNAL = False
    # Minimal fallback (keeps runtime alive if your file isn't on PYTHONPATH yet)
    ALPHA, BETA, F_REF, PHASE_SENS = 0.25, 0.1, 0.02, 0.5
    def get_universal_constants():
        return {'ALPHA': ALPHA, 'BETA': BETA, 'F_REF': F_REF, 'PHASE_SENS': PHASE_SENS}
    def delta_re_vgsp(W, t, alpha=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
        if alpha is None: alpha = ALPHA
        if f_ref is None: f_ref = F_REF
        if phase_sens is None: phase_sens = PHASE_SENS
        eff = alpha * domain_modulation
        noise = np.random.uniform(-0.02, 0.02, size=W.shape)
        base = eff * W * (1 - W) + noise
        if use_time_dynamics:
            phase = np.sin(2 * np.pi * f_ref * t)
            return base * (1 + phase_sens * phase)
        return base
    def delta_gdsp(W, t, beta=None, f_ref=None, phase_sens=None, use_time_dynamics=True, domain_modulation=1.0):
        if beta is None: beta = BETA
        if f_ref is None: f_ref = F_REF
        if phase_sens is None: phase_sens = PHASE_SENS
        eff = beta * domain_modulation
        base = -eff * W
        if use_time_dynamics:
            phase = np.sin(2 * np.pi * f_ref * t)
            return base * (1 + phase_sens * phase)
        return base

# Domain modulation
# Compose universal dynamics locally so growth+decay are always present (avoids saturation).
def universal_void_dynamics(W, t, domain_modulation=1.0, use_time_dynamics=True):
    re = delta_re_vgsp(W, t, use_time_dynamics=use_time_dynamics, domain_modulation=domain_modulation)
    gd = delta_gdsp(W, t, use_time_dynamics=use_time_dynamics, domain_modulation=domain_modulation)
    return re + gd

# Domain modulation
def get_domain_modulation(domain: str):
    # Try user's universal modulation
    try:
        from Void_Debt_Modulation import VoidDebtModulation
        mod = VoidDebtModulation().get_universal_domain_modulation(domain)
        return float(mod['domain_modulation'])
    except Exception:
        # Fallback to safe default
        targets = {
            'quantum': 0.15, 'standard_model': 0.22, 'dark_matter': 0.27,
            'biology_consciousness': 0.20, 'cosmogenesis': 0.84, 'higgs': 0.80
        }
        ALPHA = get_universal_constants()['ALPHA']
        BETA  = get_universal_constants()['BETA']
        void_debt_ratio = BETA/ALPHA if ALPHA != 0 else 0.4
        s = targets.get(domain, 0.25)
        return 1.0 + (s ** 2) / void_debt_ratio
]]></content>
    </file>
  </files>
</fum_code_report>
